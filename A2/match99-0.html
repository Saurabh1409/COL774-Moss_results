<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_B4G3R.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_B4G3R.py<p><PRE>


import numpy as np

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # Stores log P(C)
        self.word_counts = {}  # Stores word frequencies per class
        self.class_word_totals = {}  # Total words per class
        self.vocab = set()  # Vocabulary set
        self.smoothening = 1.0
        
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed.

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                Each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.smoothening = smoothening
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        
        # Compute prior probabilities P(C)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}
        
        # Initialize structures for word counts
        self.word_counts = {cls: {} for cls in class_counts}
        self.class_word_totals = {cls: 0 for cls in class_counts}
        
        # Compute word counts per class
        for _, row in df.iterrows():
            cls = row[class_col]
            tokens = row[text_col]
            self.class_word_totals[cls] += len(tokens)
            
            for token in tokens:
                if token not in self.word_counts[cls]:
                    self.word_counts[cls][token] = 0
                self.word_counts[cls][token] += 1
                self.vocab.add(token)
    
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                Each entry of text_col is a list of tokens.
        """
        predictions = []
        V = len(self.vocab)  # Vocabulary size
        
        for _, row in df.iterrows():
            tokens = row[text_col]
            class_scores = {}
            
            # Compute log-probabilities for each class
            for cls in self.class_priors:
                log_prob = self.class_priors[cls]
                total_words_in_class = self.class_word_totals[cls]
                
                for token in tokens:
                    word_freq = self.word_counts[cls].get(token, 0)
                    log_prob += np.log((word_freq + self.smoothening) / 
                                       (total_words_in_class + self.smoothening * V))
                    
                class_scores[cls] = log_prob
            
            # Assign class with maximum probability
            predictions.append(max(class_scores, key=class_scores.get))
        
        df[predicted_col] = predictions




#!/usr/bin/env python
# coding: utf-8

# In[6]:


import pandas as pd

# Load the training and test datasets
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")


# In[7]:


import numpy as np

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # Stores log P(C)
        self.word_counts = {}  # Stores word frequencies per class
        self.class_word_totals = {}  # Total words per class
        self.vocab = set()  # Vocabulary set
        self.smoothening = 1.0
        
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed.

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                Each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.smoothening = smoothening
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        
        # Compute prior probabilities P(C)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}
        
        # Initialize structures for word counts
        self.word_counts = {cls: {} for cls in class_counts}
        self.class_word_totals = {cls: 0 for cls in class_counts}
        
        # Compute word counts per class
        for _, row in df.iterrows():
            cls = row[class_col]
            tokens = row[text_col]
            self.class_word_totals[cls] += len(tokens)
            
            for token in tokens:
                if token not in self.word_counts[cls]:
                    self.word_counts[cls][token] = 0
                self.word_counts[cls][token] += 1
                self.vocab.add(token)
    
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                Each entry of text_col is a list of tokens.
        """
        predictions = []
        V = len(self.vocab)  # Vocabulary size
        
        for _, row in df.iterrows():
            tokens = row[text_col]
            class_scores = {}
            
            # Compute log-probabilities for each class
            for cls in self.class_priors:
                log_prob = self.class_priors[cls]
                total_words_in_class = self.class_word_totals[cls]
                
                for token in tokens:
                    word_freq = self.word_counts[cls].get(token, 0)
                    log_prob += np.log((word_freq + self.smoothening) / 
                                       (total_words_in_class + self.smoothening * V))
                    
                class_scores[cls] = log_prob
            
            # Assign class with maximum probability
            predictions.append(max(class_scores, key=class_scores.get))
        
        df[predicted_col] = predictions


# In[8]:


import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams

nltk.download("punkt")
nltk.download("stopwords")

def tokenize_text(text, flag=0):
    """
    Tokenizes input text based on the flag parameter.

    Args:
        text (str): The input text to be tokenized.
        flag (int): Determines the level of tokenization:
            0 - Simple tokenization (lowercasing and splitting by whitespace).
            1 - Tokenization with stopword removal, punctuation removal, and stemming.
            2 - Tokenization with stopword removal, stemming, and bigram feature engineering.

    Returns:
        List[str]: A list of tokenized words or bigrams.
    """
    text = text.lower()

    if flag == 0:
        # Simple tokenization (lowercase + split by space)
        return text.split()

    # Tokenize words using NLTK
    tokens = word_tokenize(text)

    # Remove punctuation
    tokens = [word for word in tokens if word.isalnum()]

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [word for word in tokens if word not in stop_words]

    # Apply stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    if flag == 1:
        return tokens  # Return unigrams with stemming

    elif flag == 2:
        # Generate bigrams
        bigram_tokens = list(ngrams(tokens, 2))
        bigram_tokens = [" ".join(bigram) for bigram in bigram_tokens]

        # Return both unigrams and bigrams
        return tokens + bigram_tokens

    return tokens  # Default case (shouldn't be reached)


# In[9]:


from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordclouds(df, class_col="Class Index", text_col="Tokenized Description", class_labels=None):
    """
    Generate and display word clouds for each class.

    Args:
        df (pd.DataFrame): Dataframe containing the tokenized text.
        class_col (str): Column representing class labels.
        text_col (str): Column containing tokenized text.
        class_labels (dict): Mapping from class indices to class names.
    """
    if class_labels is None:
        class_labels = {1: "World", 2: "Sports", 3: "Business", 4: "Science/Tech"}

    plt.figure(figsize=(6, 5))
    
    for idx, (class_id, class_name) in enumerate(class_labels.items(), 1):
        text_data = df[df[class_col] == class_id][text_col].explode()  # Flatten token lists
        word_freq = text_data.value_counts().to_dict()  # Count word occurrences
        
        wordcloud = WordCloud(width=600, height=400, background_color="white").generate_from_frequencies(word_freq)
        
        plt.subplot(2, 2, idx)
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title(class_name)
    
    plt.tight_layout()
    plt.show()


# In[13]:


nltk.download('punkt_tab')
train_df["Tokenized Description 1"] = train_df["Description"].apply(lambda x: tokenize_text(x, 0))
train_df["Tokenized Description 2"] = train_df["Description"].apply(lambda x: tokenize_text(x, 1))
test_df["Tokenized Description 1"] = test_df["Description"].apply(lambda x: tokenize_text(x, 0))
test_df["Tokenized Description 2"] = test_df["Description"].apply(lambda x: tokenize_text(x, 1))


# In[14]:


print("WordCloud for Unigram Tokenization on train data")
generate_wordclouds(train_df, text_col="Tokenized Description 1")

print("WordCloud for Unigram Tokenization on test data")
generate_wordclouds(test_df, text_col="Tokenized Description 1")

print("WordCloud for Unigram Tokenization + Stopword Removal + Stemming on train data")
generate_wordclouds(train_df, text_col="Tokenized Description 2")

print("WordCloud for Unigram Tokenization + Stopword Removal + Stemming on test data")
generate_wordclouds(test_df, text_col="Tokenized Description 2")


# In[20]:


from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def evaluate_model(train_df, test_df, flag, smoothening=1.0):
    """
    Trains the NaiveBayes model with a given flag setting and evaluates its performance on both train and test data.

    Args:
        train_df (pd.DataFrame): Training dataset with tokenized text.
        test_df (pd.DataFrame): Test dataset with tokenized text.
        flag (int): Tokenization flag (0: simple, 1: stopword removal, 2: bigrams).
        smoothening (float): Laplace smoothing parameter.

    Returns:
        dict: Dictionary containing train and test accuracy, precision, recall, F1-score.
    """

    # Tokenize training and test data
    train_df = train_df.copy()
    test_df = test_df.copy()
    train_df["Tokenized Description"] = train_df["Description"].apply(lambda x: tokenize_text(x, flag))
    test_df["Tokenized Description"] = test_df["Description"].apply(lambda x: tokenize_text(x, flag))

    # Train Na誰ve Bayes model
    nb = NaiveBayes()
    nb.fit(train_df, smoothening, text_col="Tokenized Description")

    # Make predictions on train and test data
    nb.predict(train_df, text_col="Tokenized Description")
    nb.predict(test_df, text_col="Tokenized Description")

    # Get true labels and predictions
    y_train_true, y_train_pred = train_df["Class Index"], train_df["Predicted"]
    y_test_true, y_test_pred = test_df["Class Index"], test_df["Predicted"]

    # Compute train metrics
    train_metrics = precision_recall_fscore_support(y_train_true, y_train_pred, average="weighted")
    train_support = len(y_train_true)

    # Compute test metrics
    test_metrics = precision_recall_fscore_support(y_test_true, y_test_pred, average="weighted")
    test_support = len(y_test_true)

    return {
        "Train Accuracy": accuracy_score(y_train_true, y_train_pred),
        "Train Precision": train_metrics[0],
        "Train Recall": train_metrics[1],
        "Train F1-score": train_metrics[2],
        "Train Support": train_support,
        "Test Accuracy": accuracy_score(y_test_true, y_test_pred),
        "Test Precision": test_metrics[0],
        "Test Recall": test_metrics[1],
        "Test F1-score": test_metrics[2],
        "Test Support": test_support
    }


# In[21]:


# Evaluate models with different feature engineering settings
results = {
    "Unigram ": evaluate_model(train_df, test_df, flag=0),
    "Unigram + Stopword Removal + Stemming ": evaluate_model(train_df, test_df, flag=1),
    "Unigram + Bigrams on top of above ": evaluate_model(train_df, test_df, flag=2),
}


# In[37]:


# Display results
import pandas as pd
results_df = pd.DataFrame(results)
def print_results(results):
    """
    Prints the evaluation results in a well-structured format with each flag's results in a separate paragraph.

    Args:
        results (dict): Dictionary containing train and test metrics for different flag settings.
    """
    for flag, metrics in results.items():
        print(f"### Results for Tokenization Flag = {flag} ###\n")
        print(f"Train Accuracy:    {metrics['Train Accuracy']:.6f}")
        print(f"Train Precision:   {metrics['Train Precision']:.6f}")
        print(f"Train Recall:      {metrics['Train Recall']:.6f}")
        print(f"Train F1-score:    {metrics['Train F1-score']:.6f}")
        print(f"Train Support:     {metrics['Train Support']}\n")

        print(f"Test Accuracy:     {metrics['Test Accuracy']:.6f}")
        print(f"Test Precision:    {metrics['Test Precision']:.6f}")
        print(f"Test Recall:       {metrics['Test Recall']:.6f}")
        print(f"Test F1-score:     {metrics['Test F1-score']:.6f}")
        print(f"Test Support:      {metrics['Test Support']}\n")
        
        print("-" * 50, "\n")  # Separator for readability

print_results(results_df)
# print(results_df["Train Accuracy"])
# print(results_df["Test Accuracy"])
# print(results_df["Train Precision"])
# print(results_df["Test Precision"])
# print(results_df["Train Recall"])
# print(results_df["Test Recall"])
# print(results_df["Train F1-score"])
# print(results_df["Test F1-score"])


# In[33]:


from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def evaluate_model_title(train_df, test_df, flag, smoothening=1.0):
    """
    Trains the NaiveBayes model with a given flag setting and evaluates its performance on both train and test data.

    Args:
        train_df (pd.DataFrame): Training dataset with tokenized text.
        test_df (pd.DataFrame): Test dataset with tokenized text.
        flag (int): Tokenization flag (0: simple, 1: stopword removal, 2: bigrams).
        smoothening (float): Laplace smoothing parameter.

    Returns:
        dict: Dictionary containing train and test accuracy, precision, recall, F1-score.
    """

    # Tokenize training and test data
    train_df = train_df.copy()
    test_df = test_df.copy()
    train_df["Tokenized Title"] = train_df["Title"].apply(lambda x: tokenize_text(x, flag))
    test_df["Tokenized Title"] = test_df["Title"].apply(lambda x: tokenize_text(x, flag))

    # Train Na誰ve Bayes model
    nb = NaiveBayes()
    nb.fit(train_df, smoothening, text_col="Tokenized Title")

    # Make predictions on train and test data
    nb.predict(train_df, text_col="Tokenized Title")
    nb.predict(test_df, text_col="Tokenized Title")

    # Get true labels and predictions
    y_train_true, y_train_pred = train_df["Class Index"], train_df["Predicted"]
    y_test_true, y_test_pred = test_df["Class Index"], test_df["Predicted"]

    # Compute train metrics
    train_metrics = precision_recall_fscore_support(y_train_true, y_train_pred, average="weighted")
    train_support = len(y_train_true)

    # Compute test metrics
    test_metrics = precision_recall_fscore_support(y_test_true, y_test_pred, average="weighted")
    test_support = len(y_test_true)

    return {
        "Train Accuracy": accuracy_score(y_train_true, y_train_pred),
        "Train Precision": train_metrics[0],
        "Train Recall": train_metrics[1],
        "Train F1-score": train_metrics[2],
        "Train Support": train_support,
        "Test Accuracy": accuracy_score(y_test_true, y_test_pred),
        "Test Precision": test_metrics[0],
        "Test Recall": test_metrics[1],
        "Test F1-score": test_metrics[2],
        "Test Support": test_support
    }


# In[36]:


# Evaluate models with different feature engineering settings
results_title = {
    "Unigram ": evaluate_model_title(train_df, test_df, flag=0),
    "Unigram + Stopword Removal + Stemming ": evaluate_model_title(train_df, test_df, flag=1),
    "Unigram + Bigrams on top of above ": evaluate_model_title(train_df, test_df, flag=2),
}


# In[38]:


results_title_df = pd.DataFrame(results_title)
def print_results(results):
    """
    Prints the evaluation results in a well-structured format with each flag's results in a separate paragraph.

    Args:
        results (dict): Dictionary containing train and test metrics for different flag settings.
    """
    for flag, metrics in results.items():
        print(f"### Results for Tokenization Flag = {flag} ###\n")
        print(f"Train Accuracy:    {metrics['Train Accuracy']:.6f}")
        print(f"Train Precision:   {metrics['Train Precision']:.6f}")
        print(f"Train Recall:      {metrics['Train Recall']:.6f}")
        print(f"Train F1-score:    {metrics['Train F1-score']:.6f}")
        print(f"Train Support:     {metrics['Train Support']}\n")

        print(f"Test Accuracy:     {metrics['Test Accuracy']:.6f}")
        print(f"Test Precision:    {metrics['Test Precision']:.6f}")
        print(f"Test Recall:       {metrics['Test Recall']:.6f}")
        print(f"Test F1-score:     {metrics['Test F1-score']:.6f}")
        print(f"Test Support:      {metrics['Test Support']}\n")
        
        print("-" * 50, "\n")  # Separator for readability

print_results(results_title_df)


# In[15]:


train_df["Tokenized Title 1"] = train_df["Title"].apply(lambda x: tokenize_text(x, 0))
train_df["Tokenized Title 2"] = train_df["Title"].apply(lambda x: tokenize_text(x, 1))

test_df["Tokenized Title 1"] = test_df["Title"].apply(lambda x: tokenize_text(x, 0))
test_df["Tokenized Title 2"] = test_df["Title"].apply(lambda x: tokenize_text(x, 1))


print("WordCloud for Unigram Tokenization on train data")
generate_wordclouds(train_df, text_col="Tokenized Title 1")

print("WordCloud for Unigram Tokenization on test data")
generate_wordclouds(test_df, text_col="Tokenized Title 1")

print("WordCloud for Unigram Tokenization + Stopword Removal + Stemming on train data")
generate_wordclouds(train_df, text_col="Tokenized Title 2")

print("WordCloud for Unigram Tokenization + Stopword Removal + Stemming on test data")
generate_wordclouds(test_df, text_col="Tokenized Title 2")


# In[31]:


def merge_title_description(df):
    """
    Merge the title and description into a single text column.
    """
    df["Merged Text"] = df["Title"] + " " + df["Description"]
    return df

def evaluate_merged_model(train_df, test_df, flag, smoothening=1.0):
    """
    Train and evaluate Na誰ve Bayes model using merged title + description text.

    Args:
        train_df (pd.DataFrame): Training dataset with 'Title' and 'Description' columns.
        test_df (pd.DataFrame): Test dataset with 'Title' and 'Description' columns.
        flag (int): Tokenization flag (0: simple, 1: stopword removal, 2: bigrams).
        smoothening (float): Laplace smoothing parameter.

    Returns:
        dict: Accuracy, Precision, Recall, and F1-score.
    """
    # Merge title and description into one field
    train_df = merge_title_description(train_df)
    test_df = merge_title_description(test_df)

    # Tokenize merged text
    train_df["Tokenized Description"] = train_df["Merged Text"].apply(lambda x: tokenize_text(x, flag))
    test_df["Tokenized Description"] = test_df["Merged Text"].apply(lambda x: tokenize_text(x, flag))

    # Train and Predict
    nb = NaiveBayes()
    nb.fit(train_df, smoothening)
    nb.predict(test_df)

    # Compute Metrics
    y_true = test_df["Class Index"]
    y_pred = test_df["Predicted"]
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="weighted")

    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1
    }

# Evaluate the model
merged_results = evaluate_merged_model(train_df, test_df, flag=2)
print("Merged Model Performance:", merged_results)


# In[27]:


class NaiveBayesSeparate:
    def __init__(self):
        self.class_probs = {}
        self.title_word_probs = {}
        self.desc_word_probs = {}
        self.vocab_title = set()
        self.vocab_desc = set()

    def fit(self, df, smoothening, class_col="Class Index", title_col="Title", desc_col="Description"):
        """
        Train the Na誰ve Bayes model separately for title and description.

        Args:
            df (pd.DataFrame): Training data.
            smoothening (float): Laplace smoothing parameter.
        """
        # Class probabilities
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        self.class_probs = {c: np.log(count / total_docs) for c, count in class_counts.items()}

        # Initialize word counts
        title_word_counts = {c: {} for c in class_counts}
        desc_word_counts = {c: {} for c in class_counts}
        title_total_counts = {c: 0 for c in class_counts}
        desc_total_counts = {c: 0 for c in class_counts}

        # Tokenize and count words
        for _, row in df.iterrows():
            c = row[class_col]
            title_tokens = tokenize_text(row[title_col], 2)
            desc_tokens = tokenize_text(row[desc_col], 2)

            self.vocab_title.update(title_tokens)
            self.vocab_desc.update(desc_tokens)

            for word in title_tokens:
<A NAME="1"></A><FONT color = #00FF00><A HREF="match99-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                title_word_counts[c][word] = title_word_counts[c].get(word, 0) + 1
                title_total_counts[c] += 1

            for word in desc_tokens:
                desc_word_counts[c][word] = desc_word_counts[c].get(word, 0) + 1
</FONT>                desc_total_counts[c] += 1

        # Compute log probabilities with Laplace smoothing
        for c in class_counts:
            vocab_size_title = len(self.vocab_title)
            vocab_size_desc = len(self.vocab_desc)

<A NAME="2"></A><FONT color = #0000FF><A HREF="match99-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.title_word_probs[c] = {
                word: np.log((title_word_counts[c].get(word, 0) + smoothening) /
                             (title_total_counts[c] + smoothening * (vocab_size_title + 1)))
</FONT>                for word in self.vocab_title
            }

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match99-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.desc_word_probs[c] = {
                word: np.log((desc_word_counts[c].get(word, 0) + smoothening) /
                             (desc_total_counts[c] + smoothening * (vocab_size_desc + 1)))
</FONT>                for word in self.vocab_desc
            }

    def predict(self, df, title_col="Title", desc_col="Description", predicted_col="Predicted"):
        """
        Predict class probabilities using separate title and description likelihoods.
        """
        predictions = []
        for _, row in df.iterrows():
            title_tokens = tokenize_text(row[title_col], 2)
            desc_tokens = tokenize_text(row[desc_col], 2)

            best_class = None
            best_log_prob = float("-inf")

            for c in self.class_probs:
                log_prob = self.class_probs[c]

                for word in title_tokens:
                    log_prob += self.title_word_probs[c].get(word, np.log(1e-10))

                for word in desc_tokens:
                    log_prob += self.desc_word_probs[c].get(word, np.log(1e-10))

                if log_prob &gt; best_log_prob:
                    best_log_prob = log_prob
                    best_class = c

            predictions.append(best_class)

        df[predicted_col] = predictions

# Train and evaluate the separate parameter model
nb_sep = NaiveBayesSeparate()
nb_sep.fit(train_df, smoothening=1.0)
nb_sep.predict(test_df)

# Compute metrics
y_true = test_df["Class Index"]
y_pred = test_df["Predicted"]
accuracy_sep = accuracy_score(y_true, y_pred)

precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="weighted")

print("Separate Model Performance:") 
print(f"Accuracy: {accuracy_sep:.6f}")
print(f"Precision: {precision:.6f}")
print(f"Recall: {recall:.6f}")
print(f"F1-score: {f1:.6f}")


# In[30]:


import numpy as np

# Number of classes (AG News has 4 classes)
num_classes = len(train_df["Class Index"].unique())

# (a) Random Guessing Accuracy
random_guess_accuracy = 1 / num_classes

# (b) Majority Class Baseline
most_frequent_class = train_df["Class Index"].value_counts().idxmax()
majority_class_accuracy = (test_df["Class Index"] == most_frequent_class).mean()

# (c) Improvement Over Baselines
best_model_accuracy = 0.902237  # Using the hardcoded flag=2 model

random_improvement = best_model_accuracy - random_guess_accuracy
majority_improvement = best_model_accuracy - majority_class_accuracy

# Print results
print(f"Random Guessing Accuracy: {random_guess_accuracy:.4f}")
print(f"Majority Class Baseline Accuracy: {majority_class_accuracy:.4f}")
print(f"Best Model Accuracy (Flag=2): {best_model_accuracy:.4f}")
print(f"Improvement over Random Guessing: {random_improvement:.4f}")
print(f"Improvement over Majority Class Baseline: {majority_improvement:.4f}")


# In[32]:


import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, class_labels, title):
    """
    Plots a confusion matrix with actual class names instead of numerical indices.

    Args:
        y_true (array-like): True class labels.
        y_pred (array-like): Predicted class labels.
        class_labels (dict): Dictionary mapping class indices to class names.
        title (str): Title of the plot.
    """
    cm = confusion_matrix(y_true, y_pred)
    class_names = [class_labels[i] for i in sorted(class_labels)]  
    
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Reds", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(title)
    plt.show()

# Example class labels dictionary (modify accordingly)
class_labels = {
    1: "World",
    2: "Sports",
    3: "Business",
    4: "Science/Tech"
}

# Plot for best-performing model
best_model = "Title + Description (Flag=2)"
y_true = test_df["Class Index"]
y_pred = test_df["Predicted"]
plot_confusion_matrix(y_true, y_pred, class_labels, title=f"Confusion Matrix: {best_model}")


# ## Sentiment analysis
# 

# In[ ]:


train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")


# In[49]:


from textblob import TextBlob

def get_sentiment(text):
    """
    Analyze the sentiment of a given text using TextBlob.
    
    Args:
        text (str): The input text.
    
    Returns:
        str: The sentiment label ("positive", "negative", or "neutral").
    """
    analysis = TextBlob(text)
    polarity = analysis.sentiment.polarity
    
    if polarity &gt; 0:
        return "positive"
    elif polarity &lt; 0:
        return "negative"
    else:
        return "neutral"

def add_sentiment_column(df, text_col="Description", sentiment_col="Sentiment"):
    """
    Add a sentiment column to the DataFrame based on the text in text_col.
    
    Args:
        df (pd.DataFrame): The input DataFrame.
        text_col (str): The column containing the text to analyze.
        sentiment_col (str): The name of the new sentiment column.
    
    Returns:
        pd.DataFrame: The DataFrame with the added sentiment column.
    """
    df[sentiment_col] = df[text_col].apply(get_sentiment)
    return df

# Example usage:
# Assuming df is your DataFrame with a column "Description" containing text data
test_df = add_sentiment_column(test_df, text_col="Description", sentiment_col="Sentiment")
train_df = add_sentiment_column(train_df, text_col="Description", sentiment_col="Sentiment")


# In[51]:



train_df["Tokenized Description"] = train_df["Description"].apply(lambda x: tokenize_text(x, 2))
test_df["Tokenized Description"] = test_df["Description"].apply(lambda x: tokenize_text(x, 2))


# In[ ]:


import numpy as np
import pandas as pd

class NaiveBayesSentiment:
    def __init__(self):
        self.class_priors = {}  # Stores log P(C)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match99-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.sentiment_priors = {}  # Stores log P(S)
        self.word_counts = {}  # Stores word frequencies per class
        self.sentiment_word_counts = {}  # Stores word frequencies per sentiment
        self.class_word_totals = {}  # Total words per class
        self.sentiment_word_totals = {}  # Total words per sentiment
        self.vocab = set()  # Vocabulary set
        self.smoothening = 1.0
</FONT>        
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description", sentiment_col="Sentiment"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed.

        Args:
            df (pd.DataFrame): The training data containing columns class_col, sentiment_col, and text_col.
                Each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.smoothening = smoothening
        class_counts = df[class_col].value_counts().to_dict()
        sentiment_counts = df[sentiment_col].value_counts().to_dict()
        total_docs = len(df)
        
        
        # Compute prior probabilities P(C) and P(S)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}
        self.sentiment_priors = {s: np.log(count / total_docs) for s, count in sentiment_counts.items()}
        
        # Initialize structures for word counts
        self.word_counts = {cls: {} for cls in class_counts}
        self.sentiment_word_counts = {sent: {} for sent in sentiment_counts}
        self.class_word_totals = {cls: 0 for cls in class_counts}
        self.sentiment_word_totals = {sent: 0 for sent in sentiment_counts}
        
        # Compute word counts per class and per sentiment
        for _, row in df.iterrows():
            cls = row[class_col]
            sent = row[sentiment_col]
            tokens = row[text_col]
            self.class_word_totals[cls] += len(tokens)
            self.sentiment_word_totals[sent] += len(tokens)
            
            for token in tokens:
                if token not in self.word_counts[cls]:
                    self.word_counts[cls][token] = 0
                self.word_counts[cls][token] += 1
                
                if token not in self.sentiment_word_counts[sent]:
                    self.sentiment_word_counts[sent][token] = 0
                self.sentiment_word_counts[sent][token] += 1
                
                self.vocab.add(token)
    
    def predict_class(self, df, text_col="Tokenized Description", predicted_col="Predicted Class"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                Each entry of text_col is a list of tokens.
        """
        predictions = []
        V = len(self.vocab)  # Vocabulary size
        
        for _, row in df.iterrows():
            tokens = row[text_col]
            class_scores = {}
            
            # Compute log-probabilities for each class
            for cls in self.class_priors:
                log_prob = self.class_priors[cls]
                total_words_in_class = self.class_word_totals[cls]
                
                for token in tokens:
                    word_freq = self.word_counts[cls].get(token, 0)
                    log_prob += np.log((word_freq + self.smoothening) / 
                                       (total_words_in_class + self.smoothening * V))
                    
                class_scores[cls] = log_prob
            
            # Assign class with maximum probability
            predictions.append(max(class_scores, key=class_scores.get))
        
        df[predicted_col] = predictions
    
    def predict_sentiment(self, df, text_col="Tokenized Description", predicted_col="Predicted Sentiment"):
        """
        Predict the sentiment of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                Each entry of text_col is a list of tokens.
        """
        predictions = []
        V = len(self.vocab)  # Vocabulary size
        
        for _, row in df.iterrows():
            tokens = row[text_col]
            sentiment_scores = {}
            
            # Compute log-probabilities for each sentiment
            for sent in self.sentiment_priors:
                log_prob = self.sentiment_priors[sent]
                total_words_in_sentiment = self.sentiment_word_totals[sent]
                
                for token in tokens:
                    word_freq = self.sentiment_word_counts[sent].get(token, 0)
                    log_prob += np.log((word_freq + self.smoothening) / 
                                       (total_words_in_sentiment + self.smoothening * V))
                    
                sentiment_scores[sent] = log_prob
            
            # Assign sentiment with maximum probability
            predictions.append(max(sentiment_scores, key=sentiment_scores.get))
        
        df[predicted_col] = predictions

# Example usage:
# Assuming df_train and df_test are your training and testing dataframes respectively
# df_train has columns: 'Class Index', 'Sentiment', 'Tokenized Description'
# df_test has columns: 'Tokenized Description'

nb = NaiveBayesSentiment()
nb.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized Description", sentiment_col="Sentiment")
nb.predict_class(test_df, text_col="Tokenized Description", predicted_col="Predicted Class")
nb.predict_sentiment(test_df, text_col="Tokenized Description", predicted_col="Predicted Sentiment")

y_true_class = test_df["Class Index"]
y_pred_class = test_df["Predicted Class"]






# In[54]:


test_metrics = precision_recall_fscore_support(y_true_class, y_pred_class, average="weighted")
print(test_metrics)
print(accuracy_score(y_true_class, y_pred_class))





#!/usr/bin/env python
# coding: utf-8

# In[7]:


import cvxopt
import numpy as np

class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alpha = None
        self.w = None
        self.b = None
        self.X_train = None
        self.y_train = None
        self.gamma = None
        
    def gaussian_kernel(self,X1, X2, gamma):
        '''
        Compute the Gaussian kernel matrix between X1 and X2 efficiently.
        
        Args:
            X1: np.array of shape (N1, D)
            X2: np.array of shape (N2, D)
            gamma: float, gamma parameter for the Gaussian kernel
            
        Returns:
            Kernel matrix of shape (N1, N2)
        '''
<A NAME="6"></A><FONT color = #00FF00><A HREF="match99-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.gamma = gamma
        # Compute squared Euclidean distances efficiently
        X1_norm = np.sum(X1**2, axis=1).reshape(-1, 1)
        X2_norm = np.sum(X2**2, axis=1).reshape(1, -1)
</FONT>        distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)
        
        # Compute the Gaussian kernel
        K = np.exp(-gamma * distances)
        return K
        
    def fit(self, X, y, kernel = 'linear', C = 1.0, gamma = 0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        N, D = X.shape
        y = y.astype(np.double).reshape(-1, 1) 
        
        # Compute Gram matrix (for linear kernel)
        if kernel == 'linear':
            K = X @ X.T
        elif kernel == 'gaussian':
            K = self.gaussian_kernel(X, X, gamma)
        
        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones((N, 1)))
        G = cvxopt.matrix(np.vstack((-np.eye(N), np.eye(N))))
        h = cvxopt.matrix(np.hstack((np.zeros(N), np.ones(N) * C)))
        A = cvxopt.matrix(y.reshape(1, -1))
        b = cvxopt.matrix(0.0)

        # Solve QP problem
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alpha = np.ravel(solution['x'])
        
        # Select support vectors
        sv = alpha &gt; 1e-5
        self.alpha = alpha[sv]
        self.X_train = X[sv]
        self.y_train = y[sv]

        # Compute w and b for linear kernel
        if kernel == 'linear':
            self.w = np.sum(self.alpha[:, None] * self.y_train * self.X_train, axis=0)
            self.b = np.mean(self.y_train - (self.w @ self.X_train.T))
        elif kernel == 'gaussian':
            # Compute bias term (b) for Gaussian kernel
            sv_indices = np.where(sv)[0]
            self.b = 0
            for i in range(len(self.alpha)):
                self.b += self.y_train[i] - np.sum(self.alpha * self.y_train * K[sv_indices[i], sv])
            self.b /= len(self.alpha)
            
        num_support_vectors = np.sum(sv)
        support_vector_percentage = (num_support_vectors / len(y)) * 100
        print(f"Number of Support Vectors: {num_support_vectors}")
<A NAME="7"></A><FONT color = #0000FF><A HREF="match99-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        print(f"Percentage of Support Vectors: {support_vector_percentage:.2f}%")
        
        
    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
</FONT>                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        
        if self.w is not None:  # Linear Kernel
            print(self.w)
            print(self.b)
            return (X @ self.w + self.b &gt;= 0).astype(int)
        # Gaussian Kernel (optimized computation)
        gamma = self.gamma  # Ensure gamma is set correctly
        X_norm = np.sum(X**2, axis=1).reshape(-1, 1)  # (N_test, 1)
        sv_norm = np.sum(self.X_train**2, axis=1).reshape(1, -1)  # (1, N_train)
        print(self.w)
        print(self.b)
        # Compute Gaussian Kernel correctly (N_test, N_train)
        K = np.exp(-gamma * (X_norm + sv_norm - 2 * np.dot(X, self.X_train.T))) - self.b  # (N_test, N_train)
        
        # Ensure correct broadcasting
        decision_values = np.sum((self.alpha * self.y_train.T) * K, axis=1)  # (N_test,)
        
        return (decision_values &gt;= 0).astype(int)


# In[3]:


import os
import cv2
import numpy as np

def load_images_from_folder(base_path):
    data = []
    labels = []
    
    classes = sorted(os.listdir(base_path))  # Get class labels
    for class_index, class_name in enumerate(classes):
        class_folder = os.path.join(base_path, class_name)
        image_files = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if f.endswith('.jpg')]

        for img_path in image_files:
            img = cv2.imread(img_path)  # Load image
            if img is None:
                continue  # Skip this image

            img = cv2.resize(img, (100, 100))  # Resize to 100x100
            img = img / 255.0  # Normalize to [0, 1]
            img = img.flatten()  # Flatten to 30,000 features
            
            data.append(img)
            labels.append(class_index)

    return np.array(data), np.array(labels)

# Load Train & Test Data
train_data, train_labels = load_images_from_folder("../data/Q2/train")
test_data, test_labels = load_images_from_folder("../data/Q2/test")


# In[5]:


d = 27
class_1 = d % 11
class_2 = (d + 1) % 11

# Filter dataset to keep only two selected classes
binary_train_mask = (train_labels == class_1) | (train_labels == class_2)
binary_test_mask = (test_labels == class_1) | (test_labels == class_2)

X_train, y_train = train_data[binary_train_mask], train_labels[binary_train_mask]
X_test, y_test = test_data[binary_test_mask], test_labels[binary_test_mask]

# Convert labels to {+1, -1} for SVM
y_train = np.where(y_train == class_1, 1, -1)
y_test = np.where(y_test == class_1, 1, -1)


# In[8]:


# Create SVM instance
svm_linear = SupportVectorMachine()

# Train SVM on training data (Only binary classification: class_1 vs class_2)
svm_linear.fit(X_train, y_train, kernel='linear', C=1.0)

# Predict on train data
y_pred_train = svm_linear.predict(X_train)

# Predict on test data
y_pred = svm_linear.predict(X_test)

# Compute accuracy
accuracy_train = np.mean(y_pred_train == (y_train == 1)) * 100
accuracy = np.mean(y_pred == (y_test == 1)) * 100
print(f"Train Accuracy: {accuracy_train:.2f}%")
print(f"Test Accuracy: {accuracy:.2f}%")


# In[21]:


import matplotlib.pyplot as plt

# Get top-5 support vectors (highest alphas)
top_5_indices = np.argsort(svm_linear.alpha)[-5:]

fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for i, idx in enumerate(top_5_indices):
    img = svm_linear.X_train[idx].reshape(100, 100, 3)  # Reshape to image dimensions
    img = (img - img.min()) / (img.max() - img.min())
    axes[i].imshow(img)
    axes[i].set_title(f"Support Vector {i+1}")
    axes[i].axis('off')
plt.show()


# In[9]:


import matplotlib.pyplot as plt

# Reshape the weight vector w to (100, 100, 3)
w_img = svm_linear.w.reshape(100, 100, 3)

# Normalize w_img to [0, 1] for visualization
w_img = (w_img - w_img.min()) / (w_img.max() - w_img.min())

# Plot the weight vector
plt.figure(figsize=(5, 5))
im=plt.imshow(w_img)
plt.colorbar(im, fraction=0.046, pad=0.04)
plt.title("Weight Vector (w)")
plt.axis('off')
plt.show()


# part 2

# In[10]:


# Create SVM instance
svm_gaussian = SupportVectorMachine()

# Train SVM using Gaussian Kernel
svm_gaussian.fit(X_train, y_train, kernel='gaussian', C=1, gamma=0.001)

# Predict on train set
y_pred_train = svm_gaussian.predict(X_train)

# Predict on test set
y_pred_gaussian = svm_gaussian.predict(X_test)

# Compute accuracy
accuracy_train_gaussian = np.mean(y_pred_train == (y_train == 1)) * 100
accuracy_gaussian = np.mean(y_pred_gaussian == (y_test == 1)) * 100
print(f"Train Accuracy (Gaussian Kernel): {accuracy_train_gaussian:.2f}%")
print(f"Test Accuracy (Gaussian Kernel): {accuracy_gaussian:.2f}%")


# In[12]:


import numpy as np

# Retrieve support vectors for both models
sv_linear = svm_linear.X_train  # Support vectors for linear kernel
sv_gaussian = svm_gaussian.X_train  # Support vectors for Gaussian kernel

# Use numpy broadcasting to check common support vectors
common_support_vectors = np.array([np.any(np.all(np.isclose(sv, sv_gaussian, atol=1e-5), axis=1)) for sv in sv_linear])

# Count the number of common support vectors
num_common_sv = np.sum(common_support_vectors)

print(f"Number of Common Support Vectors: {num_common_sv}")


# In[10]:


import matplotlib.pyplot as plt

# Get top-5 support vectors (highest alphas)
top_5_indices = np.argsort(svm_gaussian.alpha)[-5:]

fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for i, idx in enumerate(top_5_indices):
    img = svm_gaussian.X_train[idx].reshape(100, 100, 3)  # Reshape to image dimensions
    axes[i].imshow(img)
    axes[i].set_title(f"Support Vector {i+1}")
    axes[i].axis('off')
plt.show()


# part 3
# 

# In[23]:


from sklearn.svm import SVC
import time

# Train Linear SVM
start_time = time.time()
svm_sklearn_linear = SVC(kernel='linear', C=1.0)
svm_sklearn_linear.fit(X_train, y_train)
end_time = time.time()
train_time_linear = end_time - start_time
print(f"Training Time (Linear Kernel, scikit-learn): {train_time_linear:.4f} sec")

# Train Gaussian (RBF) Kernel SVM
start_time = time.time()
svm_sklearn_gaussian = SVC(kernel='rbf', C=1.0, gamma=0.001)
svm_sklearn_gaussian.fit(X_train, y_train)
end_time = time.time()
train_time_gaussian = end_time - start_time
print(f"Training Time (Gaussian Kernel, scikit-learn): {train_time_gaussian:.4f} sec")


# In[12]:


num_sv_linear_sklearn = len(svm_sklearn_linear.support_)
num_sv_gaussian_sklearn = len(svm_sklearn_gaussian.support_)

print(f"Number of Support Vectors (scikit-learn, Linear): {num_sv_linear_sklearn}")
print(f"Number of Support Vectors (scikit-learn, Gaussian): {num_sv_gaussian_sklearn}")


# In[14]:


# Extract weights and bias for scikit-learn linear SVM
w_sklearn = svm_sklearn_linear.coef_
b_sklearn = svm_sklearn_linear.intercept_

# Extract weights and bias for cvxopt linear SVM
w_cvxopt = svm_linear.w
b_cvxopt = svm_linear.b

print(f"Weight Vector (CVXOPT, Linear): {w_cvxopt}")
print(f"Weight Vector (scikit-learn, Linear): {w_sklearn}")

print(f"Bias Term (CVXOPT, Linear): {b_cvxopt}")
print(f"Bias Term (scikit-learn, Linear): {b_sklearn}")


# In[24]:


# Predict using scikit-learn
y_pred_sklearn_linear = svm_sklearn_linear.predict(X_test)
y_pred_sklearn_gaussian = svm_sklearn_gaussian.predict(X_test)

# Compute accuracy
accuracy_sklearn_linear = np.mean(y_pred_sklearn_linear == y_test) * 100
accuracy_sklearn_gaussian = np.mean(y_pred_sklearn_gaussian == y_test) * 100

print(f"Test Accuracy (scikit-learn, Linear): {accuracy_sklearn_linear:.2f}%")
print(f"Test Accuracy (scikit-learn, Gaussian): {accuracy_sklearn_gaussian:.2f}%")


# part 4
# 

# In[16]:


from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
import time

# Train SVM using SGD
<A NAME="0"></A><FONT color = #FF0000><A HREF="match99-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

sgd_svm = SGDClassifier(loss='hinge', alpha=1e-3, max_iter=1000, tol=1e-3, random_state=42)

start_time = time.time()
sgd_svm.fit(X_train, y_train)  # Train on training data
sgd_train_time = time.time() - start_time
</FONT>
# Predict on test data
y_pred_sgd = sgd_svm.predict(X_test)

# Calculate accuracy
sgd_accuracy = accuracy_score(y_test, y_pred_sgd)

print(f"SGD Training Time: {sgd_train_time:.4f} seconds")
print(f"SGD Test Accuracy: {sgd_accuracy:.4f}")





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import cv2
import numpy as np

def load_images_from_folder(base_path):
    data = []
    labels = []
    
    classes = sorted(os.listdir(base_path))  # Get class labels
    for class_index, class_name in enumerate(classes):
        class_folder = os.path.join(base_path, class_name)
        image_files = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if f.endswith('.jpg')]

        for img_path in image_files:
            img = cv2.imread(img_path)  # Load image
            if img is None:
                continue  # Skip this image

            img = cv2.resize(img, (100, 100))  # Resize to 100x100
            img = img / 255.0  # Normalize to [0, 1]
            img = img.flatten()  # Flatten to 30,000 features
            
            data.append(img)
            labels.append(class_index)

    return np.array(data), np.array(labels)


# In[2]:


# Load full dataset
train_data, train_labels = load_images_from_folder("../data/Q2/train")
test_data, test_labels = load_images_from_folder("../data/Q2/test")


# In[3]:


import numpy as np
from cvxopt import matrix, solvers
from itertools import combinations

class MultiClassSVM:
    def __init__(self, C=1.0, gamma=0.001):
        self.C = C
        self.gamma = gamma
        self.models = {}  # Dictionary to store binary classifiers
        self.classes = None

    def gaussian_kernel(self, X1, X2):
        """Compute the Gaussian (RBF) Kernel."""
        X1_norm = np.sum(X1**2, axis=1).reshape(-1, 1)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match99-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X2_norm = np.sum(X2**2, axis=1).reshape(1, -1)
        distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)
        return np.exp(-self.gamma * distances)

    def train_binary_svm(self, X, y):
</FONT>        """Train a binary SVM using CVXOPT."""
        N = X.shape[0]
        y = y.astype(np.double).reshape(-1, 1)
        
        K = self.gaussian_kernel(X, X)

        P = matrix(np.outer(y, y) * K)
        q = matrix(-np.ones((N, 1)))
        G = matrix(np.vstack((-np.eye(N), np.eye(N))))
        h = matrix(np.hstack((np.zeros(N), np.ones(N) * self.C)))
        A = matrix(y.reshape(1, -1))
        b = matrix(0.0)

        solvers.options['show_progress'] = False
        solution = solvers.qp(P, q, G, h, A, b)
        alpha = np.ravel(solution['x'])

        sv = alpha &gt; 1e-5  # Support vectors
        alpha_sv = alpha[sv]
        X_sv = X[sv]
        y_sv = y[sv]

        # Compute bias term (b)
        b = np.mean(y_sv - np.sum(alpha_sv * y_sv * K[sv][:, sv], axis=1))

        return alpha_sv, X_sv, y_sv, b

    def fit(self, X, y):
        """Train one binary SVM per class pair using One-vs-One (OvO) strategy."""
        self.classes = np.unique(y)
        self.models = {}

        for class_1, class_2 in combinations(self.classes, 2):
            mask = (y == class_1) | (y == class_2)
            X_binary, y_binary = X[mask], y[mask]
            y_binary = np.where(y_binary == class_1, 1, -1)

            alpha, X_sv, y_sv, b = self.train_binary_svm(X_binary, y_binary)

            self.models[(class_1, class_2)] = (alpha, X_sv, y_sv, b)

    def predict(self, X):
        """Predict using one-vs-one voting."""
        votes = np.zeros((X.shape[0], len(self.classes)))

        for (class_1, class_2), (alpha, X_sv, y_sv, b) in self.models.items():
            K = self.gaussian_kernel(X, X_sv)
            decision_values = np.sum(alpha * y_sv.T * K, axis=1) + b

            idx_1 = np.where(self.classes == class_1)[0][0]
            idx_2 = np.where(self.classes == class_2)[0][0]

            votes[:, idx_1] += (decision_values &gt;= 0)
            votes[:, idx_2] += (decision_values &lt; 0)

        return self.classes[np.argmax(votes, axis=1)]


# In[4]:


# Instantiate and train the MultiClassSVM
svm_cvxopt = MultiClassSVM(C=1.0, gamma=0.01)
svm_cvxopt.fit(train_data, train_labels)


# In[5]:


y_pred_cvxopt = svm_cvxopt.predict(test_data)


# In[6]:


from sklearn.metrics import accuracy_score, classification_report

# Calculate accuracy
accuracy_svxopt = accuracy_score(test_labels, y_pred_cvxopt)
print(f"Accuracy: {accuracy_svxopt:.4f}")

# Display detailed classification report
print(classification_report(test_labels, y_pred_cvxopt))


# In[7]:


from sklearn.svm import SVC
import time

# Train One-vs-One Multi-Class SVM using Sklearn
start_time = time.time()

svm_sklearn = SVC(kernel="rbf", C=1.0, gamma=0.001, decision_function_shape='ovo')
svm_sklearn.fit(train_data, train_labels)

train_time_sklearn = time.time() - start_time

# Predict on test set
y_pred_sklearn = svm_sklearn.predict(test_data)

# Compute test accuracy
accuracy_sklearn = np.mean(y_pred_sklearn == test_labels)
print(f"Multi-Class SVM (Sklearn) Test Accuracy: {accuracy_sklearn:.4f}")
print(f"Training Time (Sklearn): {train_time_sklearn:.2f} seconds")


# In[8]:


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Define class labels from the unique test labels
class_labels = np.unique(test_labels)

# Compute confusion matrices
cm_cvxopt = confusion_matrix(test_labels, y_pred_cvxopt)
cm_sklearn = confusion_matrix(test_labels, y_pred_sklearn)

# Plot Confusion Matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(title)
    plt.show()

# Plot confusion matrices
plot_confusion_matrix(cm_cvxopt, "Confusion Matrix - CVXOPT SVM")
plot_confusion_matrix(cm_sklearn, "Confusion Matrix - Sklearn SVM (LIBSVM)")


# In[9]:


misclassified_indices = np.where(y_pred_sklearn != test_labels)[0]

# Select 10 random misclassified examples
num_examples = 10
random_indices = np.random.choice(misclassified_indices, num_examples, replace=False)

# Plot misclassified images
fig, axes = plt.subplots(2, 5, figsize=(12, 6))

for i, idx in enumerate(random_indices):
    ax = axes[i // 5, i % 5]
    img = test_data[idx].reshape(100, 100, 3)  # Reshape to image format
    ax.imshow(img)
    ax.set_title(f"True: {class_labels[test_labels[idx]]}\nPred: {class_labels[y_pred_sklearn[idx]]}")
    ax.axis("off")

plt.tight_layout()
plt.show()


# In[1]:


from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt

# Define the hyperparameter search space
C_values = [1e-5, 1e-3, 1, 5, 10]
gamma = 0.001

# Perform 5-fold cross-validation
cv_accuracies = []
test_accuracies = []

for C in C_values:
    # Create SVM model with RBF kernel
    svm = SVC(C=C, kernel='rbf', gamma=gamma)

    # Perform 5-fold cross-validation
    scores = cross_val_score(svm, train_data, train_labels, cv=5)
    avg_cv_accuracy = np.mean(scores)
    cv_accuracies.append(avg_cv_accuracy)

    # Train on full train data and test on test set
    svm.fit(train_data, train_labels)
    test_accuracy = svm.score(test_data, test_labels)
    test_accuracies.append(test_accuracy)

    print(f"C = {C}, Cross-validation Accuracy = {avg_cv_accuracy:.4f}, Test Accuracy = {test_accuracy:.4f}")

# Store best C based on cross-validation accuracy
best_C = C_values[np.argmax(cv_accuracies)]
print(f"\nBest C found: {best_C}")





import cvxopt
import numpy as np

class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alpha = None
        self.w = None
        self.b = None
        self.X_train = None
        self.y_train = None
        self.gamma = None
        
    def gaussian_kernel(self,X1, X2, gamma):
        '''
        Compute the Gaussian kernel matrix between X1 and X2 efficiently.
        
        Args:
            X1: np.array of shape (N1, D)
            X2: np.array of shape (N2, D)
            gamma: float, gamma parameter for the Gaussian kernel
            
        Returns:
            Kernel matrix of shape (N1, N2)
        '''
        self.gamma = gamma
        # Compute squared Euclidean distances efficiently
        X1_norm = np.sum(X1**2, axis=1).reshape(-1, 1)
        X2_norm = np.sum(X2**2, axis=1).reshape(1, -1)
        distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)
        
        # Compute the Gaussian kernel
        K = np.exp(-gamma * distances)
        return K
        
    def fit(self, X, y, kernel = 'linear', C = 1.0, gamma = 0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        N, D = X.shape
        y = y.astype(np.double).reshape(-1, 1) 
        
        # Compute Gram matrix (for linear kernel)
        if kernel == 'linear':
            K = X @ X.T
        elif kernel == 'gaussian':
            K = self.gaussian_kernel(X, X, gamma)
        
        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones((N, 1)))
        G = cvxopt.matrix(np.vstack((-np.eye(N), np.eye(N))))
        h = cvxopt.matrix(np.hstack((np.zeros(N), np.ones(N) * C)))
        A = cvxopt.matrix(y.reshape(1, -1))
        b = cvxopt.matrix(0.0)

        # Solve QP problem
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alpha = np.ravel(solution['x'])
        
        # Select support vectors
        sv = alpha &gt; 1e-5
        self.alpha = alpha[sv]
        self.X_train = X[sv]
        self.y_train = y[sv]

        # Compute w and b for linear kernel
        if kernel == 'linear':
            self.w = np.sum(self.alpha[:, None] * self.y_train * self.X_train, axis=0)
            self.b = np.mean(self.y_train - (self.w @ self.X_train.T))
        elif kernel == 'gaussian':
            # Compute bias term (b) for Gaussian kernel
            sv_indices = np.where(sv)[0]
            self.b = 0
            for i in range(len(self.alpha)):
                self.b += self.y_train[i] - np.sum(self.alpha * self.y_train * K[sv_indices[i], sv])
            self.b /= len(self.alpha)
            
        num_support_vectors = np.sum(sv)
        support_vector_percentage = (num_support_vectors / len(y)) * 100
        print(f"Number of Support Vectors: {num_support_vectors}")
        print(f"Percentage of Support Vectors: {support_vector_percentage:.2f}%")
        
        
    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        
        if self.w is not None:  # Linear Kernel
            print(self.w)
            print(self.b)
            return (X @ self.w + self.b &gt;= 0).astype(int)
        # Gaussian Kernel (optimized computation)
        gamma = self.gamma  # Ensure gamma is set correctly
        X_norm = np.sum(X**2, axis=1).reshape(-1, 1)  # (N_test, 1)
        sv_norm = np.sum(self.X_train**2, axis=1).reshape(1, -1)  # (1, N_train)
        print(self.w)
        print(self.b)
        # Compute Gaussian Kernel correctly (N_test, N_train)
        K = np.exp(-gamma * (X_norm + sv_norm - 2 * np.dot(X, self.X_train.T))) - self.b  # (N_test, N_train)
        
        # Ensure correct broadcasting
        decision_values = np.sum((self.alpha * self.y_train.T) * K, axis=1)  # (N_test,)
        
        return (decision_values &gt;= 0).astype(int)

</PRE>
</PRE>
</BODY>
</HTML>
