<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_0VNUY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_0VNUY.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    # stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens ]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text_with_bigrams)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy ( bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy ( bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    # stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text_with_bigrams)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text_with_bigrams)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df, text_col="Description"):
    """Generate word clouds for each class based on word frequencies."""
    class_labels = sorted(df["Class Index"].unique())  # Ensure consistent order
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid for 4 classes
    
    for ax, class_label in zip(axes.flatten(), class_labels):
        text = ' '.join(df[df["Class Index"] == class_label][text_col].values)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f"Class {class_label}")
        ax.axis("off")  # Hide axis labels
    
    plt.tight_layout()  # Adjust layout to prevent overlapping
    plt.savefig(f'wordcloud_b_sw.png')

generate_wordclouds(train_df, text_col="Processed Description")
# generate_wordclouds(test_df)



import numpy as np

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value
        
    def fit(self, df, smoothening, class_col = "Class Index", text_col = "Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
<A NAME="2"></A><FONT color = #0000FF><A HREF="match94-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
</FONT>        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col] 
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute log of word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0) 
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    
    def predict(self, df, text_col = "Tokenized Description", predicted_col = "Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col]
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df




#!/usr/bin/env python
# coding: utf-8

# # Naive Bayes Classification
# This notebook replicates the functionality of **q1.py** in a Jupyter Notebook.

# ## Imports
# Here we import our necessary libraries for data manipulation, visualization, and model evaluation.

# In[16]:


get_ipython().system('pip install nltk')


# In[1]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


# ## Naive Bayes Class
# Defines a straightforward Naive Bayes classifier.

# In[2]:


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]
                for word in words:
                    if word in self.word_probs[c]:
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df


# ## Data Loading and Model Training
# Load the training and test datasets, then train the Naive Bayes model.

# In[3]:


train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

nb = NaiveBayes()
nb.fit(train_df, smoothening=1)

# Predict on training and test data
train_df = nb.predict(train_df)
test_df = nb.predict(test_df)

# Evaluate accuracy
train_accuracy = accuracy_score(train_df["Class Index"], train_df["Predicted"])
test_accuracy = accuracy_score(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


# ## Visualizing the Results
# Here, we generate word clouds for each class.

# In[4]:


def generate_wordclouds(df, text_col="Description"):
    """Generate word clouds for each class based on word frequencies."""
    class_labels = sorted(df["Class Index"].unique())  # Ensure consistent order
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid for 4 classes
    
    for ax, class_label in zip(axes.flatten(), class_labels):
        text = ' '.join(df[df["Class Index"] == class_label][text_col].values)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f"Class {class_label}")
        ax.axis("off")  # Hide axis labels
    
    plt.tight_layout()  # Adjust layout to prevent overlapping
    plt.show()
        
generate_wordclouds(train_df, text_col="Description")


# In[5]:


generate_wordclouds(test_df, text_col="Description")


# In[6]:


nltk.download('stopwords')
nltk.download('punkt')
import re


# In[7]:


def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)


# In[8]:


# Apply preprocessing
train_df["Tokenized Description"] = train_df["Description"].apply(preprocess_text)
test_df["Tokenized Description"] = test_df["Description"].apply(preprocess_text)

# Train the model
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Tokenized Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Tokenized Description")
test_df = nb.predict(test_df, text_col="Tokenized Description")

# Evaluate accuracy
train_accuracy = accuracy_score(train_df["Class Index"], train_df["Predicted"])
test_accuracy = accuracy_score(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


# In[9]:


generate_wordclouds(train_df, text_col="Tokenized Description")


# In[10]:


generate_wordclouds(test_df, text_col="Tokenized Description")


# In[11]:


from nltk.util import ngrams


# In[12]:


def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)


# In[13]:



# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text_with_bigrams)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = accuracy_score(train_df["Class Index"], train_df["Predicted"])
test_accuracy = accuracy_score(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")


# In[14]:


generate_wordclouds(train_df, text_col="Processed Description")


# In[15]:


generate_wordclouds(test_df, text_col="Processed Description")


# In[ ]:








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

generate_wordclouds(train_df)
generate_wordclouds(test_df)




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text_with_bigrams)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

generate_wordclouds(train_df)
generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)


def preprocess_desc(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

def preprocess_title(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Combined tokens"] = train_df.apply(lambda row: preprocess_title(row['Title']) + " " + preprocess_desc(row['Description']), 
    axis=1)
test_df["Combined tokens"] = test_df.apply(lambda row: preprocess_title(row['Title']) + " " + preprocess_desc(row['Description']),
    axis=1)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Combined tokens")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Combined tokens")
test_df = nb.predict(test_df, text_col="Combined tokens")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy : {train_accuracy:.4f}")
print(f"Test Accuracy : {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df, text_col="Description"):
    """Generate word clouds for each class based on word frequencies."""
    class_labels = sorted(df["Class Index"].unique())  # Ensure consistent order
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid for 4 classes
    
    for ax, class_label in zip(axes.flatten(), class_labels):
        text = ' '.join(df[df["Class Index"] == class_label][text_col].values)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f"Class {class_label}")
        ax.axis("off")  # Hide axis labels
    
    plt.tight_layout()  # Adjust layout to prevent overlapping
    plt.savefig(f'wordcloud_train.png')

generate_wordclouds(train_df, text_col="Combined tokens")
# generate_wordclouds(test_df, text_col="Combined tokens")
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
<A NAME="0"></A><FONT color = #FF0000><A HREF="match94-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from nltk.util import ngrams

class SeparateNaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs_title = {}  # P(w | C_k) for each word in title
        self.word_probs_desc = {}  # P(w | C_k) for each word in description
        self.title_vocab = set()  # Set of all words in title
        self.desc_vocab = set()  # Set of all words in description
        self.class_counts = {}  # Count of documents per class
        self.title_word_counts = {}  # Word counts per class for title
        self.desc_word_counts = {}  # Word counts per class for description
        self.smoothing = 1  # Default Laplace smoothing value
</FONT>
    def fit(self, df, smoothening, class_col="Class Index", title_col="Processed Title", desc_col="Processed Description"):
        """Train the Naïve Bayes model by computing separate probabilities for title and description."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.title_word_counts[class_label] = {}
            self.desc_word_counts[class_label] = {}

        # Compute word counts per class for title and description
        for _, row in df.iterrows():
            class_label = row[class_col]
            
            # Process title words
            title_words = row[title_col].split()
            for word in title_words:
                if word in self.title_word_counts[class_label]:
                    self.title_word_counts[class_label][word] += 1
                else:
                    self.title_word_counts[class_label][word] = 1
                self.title_vocab.add(word)
            
            # Process description words
            desc_words = row[desc_col].split()
            for word in desc_words:
                if word in self.desc_word_counts[class_label]:
                    self.desc_word_counts[class_label][word] += 1
                else:
                    self.desc_word_counts[class_label][word] = 1
                self.desc_vocab.add(word)
        
        # Compute word probabilities P(w | C_k) for title
        title_vocab_size = len(self.title_vocab)
        self.word_probs_title = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_title_words_in_class = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
            for word in self.title_vocab:
                count = self.title_word_counts[c].get(word, 0)
                self.word_probs_title[c][word] = np.log((count + self.smoothing) / 
                                                       (total_title_words_in_class + self.smoothing * title_vocab_size))
        
        # Compute word probabilities P(w | C_k) for description
        desc_vocab_size = len(self.desc_vocab)
        self.word_probs_desc = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_desc_words_in_class = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
            for word in self.desc_vocab:
                count = self.desc_word_counts[c].get(word, 0)
                self.word_probs_desc[c][word] = np.log((count + self.smoothing) / 
                                                      (total_desc_words_in_class + self.smoothing * desc_vocab_size))

    def predict(self, df, title_col="Processed Title", desc_col="Processed Description", predicted_col="Predicted"):
        """Predict class using separate parameters for title and description."""
        predictions = []
        
        for _, row in df.iterrows():
            title_words = row[title_col].split()
            desc_words = row[desc_col].split()
            class_scores = {}
            
            for c in self.class_counts:
                # Start with class prior
                class_scores[c] = self.class_priors[c]
                
                # Add title word contributions
                title_vocab_size = len(self.title_vocab)
                for word in title_words:
                    if word in self.word_probs_title[c]:
                        class_scores[c] += self.word_probs_title[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_title_words = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_title_words + self.smoothing * title_vocab_size))
                
                # Add description word contributions
                desc_vocab_size = len(self.desc_vocab)
                for word in desc_words:
                    if word in self.word_probs_desc[c]:
                        class_scores[c] += self.word_probs_desc[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_desc_words = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_desc_words + self.smoothing * desc_vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df
    

def preprocess_desc(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

def preprocess_title(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

def prepare_separate_data(df):
    """Prepare data with separate processing for title and description"""
    df['Processed Title'] = df['Title'].apply(preprocess_title)
    df['Processed Description'] = df['Description'].apply(preprocess_desc)
    return df

def evaluate_separate_model(train_df, test_df):
    """Train and evaluate the separate parameters model"""
    # Prepare data
    train_df = prepare_separate_data(train_df)
    test_df = prepare_separate_data(test_df)
    
    # Initialize and train model
    nb = SeparateNaiveBayes()
    nb.fit(train_df, smoothening=1, class_col="Class Index", 
           title_col="Processed Title", desc_col="Processed Description")
    
    # Evaluate on train data
    train_predictions = nb.predict(train_df, 
                                  title_col="Processed Title", 
                                  desc_col="Processed Description")
    train_accuracy = sum(train_predictions["Predicted"] == train_predictions["Class Index"]) / len(train_predictions)
    
    # Evaluate on test data
    test_predictions = nb.predict(test_df,
                                 title_col="Processed Title", 
                                 desc_col="Processed Description")
    test_accuracy = sum(test_predictions["Predicted"] == test_predictions["Class Index"]) / len(test_predictions)
    
    return train_accuracy, test_accuracy


# Evaluate separate parameters model
print("\nEvaluating Separate Parameters Model...")
separate_train_acc, separate_test_acc = evaluate_separate_model(train_df.copy(), test_df.copy())
print(f"Separate Parameters Model - Training Accuracy: {separate_train_acc:.4f}")
print(f"Separate Parameters Model - Test Accuracy: {separate_test_acc:.4f}")

# # Compare results
# print("\nComparison:")
# print(f"Best Description-only Model Test Accuracy: 0.9030")
# print(f"Best Title-only Model Test Accuracy: 0.8732")
# print(f"Separate Parameters Model Test Accuracy: {separate_test_acc:.4f}")



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from collections import Counter


def random_baseline(df, class_col="Class Index"):
    """Compute the accuracy of a random baseline."""
    random_predictions = np.random.choice(df[class_col], len(df))
    return accuracy_score(df[class_col], random_predictions)

def positive_baseline(df, class_col="Class Index"):
    """Compute the accuracy of a positive baseline."""
    class_counts = df["Class Index"].value_counts()
    most_common_class = class_counts.idxmax()
    
    # Predict the most common class for all test samples
    predictions = [most_common_class] * len(test_df)
    
    # Calculate accuracy
    accuracy = sum(predictions == test_df["Class Index"]) / len(test_df)
    return accuracy, most_common_class

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

random_accuracy = random_baseline(test_df)
positive_accuracy, most_common_class = positive_baseline(test_df)

print(f"Random baseline accuracy: {random_accuracy:.4f}")
print(f"Positive baseline accuracy: {positive_accuracy:.4f} (always predicting '{most_common_class}')")




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


class SeparateNaiveBayes:
    def __init__(self):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match94-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.class_priors = {}  # P(C_k) for each class
        self.word_probs_title = {}  # P(w | C_k) for each word in title
        self.word_probs_desc = {}  # P(w | C_k) for each word in description
        self.title_vocab = set()  # Set of all words in title
        self.desc_vocab = set()  # Set of all words in description
        self.class_counts = {}  # Count of documents per class
        self.title_word_counts = {}  # Word counts per class for title
        self.desc_word_counts = {}  # Word counts per class for description
        self.smoothing = 1  # Default Laplace smoothing value
</FONT>
    def fit(self, df, smoothening, class_col="Class Index", title_col="Processed Title", desc_col="Processed Description"):
        """Train the Naïve Bayes model by computing separate probabilities for title and description."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.title_word_counts[class_label] = {}
            self.desc_word_counts[class_label] = {}

        # Compute word counts per class for title and description
        for _, row in df.iterrows():
            class_label = row[class_col]
            
            # Process title words
            title_words = row[title_col].split()
            for word in title_words:
                if word in self.title_word_counts[class_label]:
                    self.title_word_counts[class_label][word] += 1
                else:
                    self.title_word_counts[class_label][word] = 1
                self.title_vocab.add(word)
            
            # Process description words
            desc_words = row[desc_col].split()
            for word in desc_words:
                if word in self.desc_word_counts[class_label]:
                    self.desc_word_counts[class_label][word] += 1
                else:
                    self.desc_word_counts[class_label][word] = 1
                self.desc_vocab.add(word)
        
        # Compute word probabilities P(w | C_k) for title
        title_vocab_size = len(self.title_vocab)
        self.word_probs_title = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_title_words_in_class = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
            for word in self.title_vocab:
                count = self.title_word_counts[c].get(word, 0)
                self.word_probs_title[c][word] = np.log((count + self.smoothing) / 
                                                       (total_title_words_in_class + self.smoothing * title_vocab_size))
        
        # Compute word probabilities P(w | C_k) for description
        desc_vocab_size = len(self.desc_vocab)
        self.word_probs_desc = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_desc_words_in_class = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
            for word in self.desc_vocab:
                count = self.desc_word_counts[c].get(word, 0)
                self.word_probs_desc[c][word] = np.log((count + self.smoothing) / 
                                                      (total_desc_words_in_class + self.smoothing * desc_vocab_size))

    def predict(self, df, title_col="Processed Title", desc_col="Processed Description", predicted_col="Predicted"):
        """Predict class using separate parameters for title and description."""
        predictions = []
        
        for _, row in df.iterrows():
            title_words = row[title_col].split()
            desc_words = row[desc_col].split()
            class_scores = {}
            
            for c in self.class_counts:
                # Start with class prior
                class_scores[c] = self.class_priors[c]
                
                # Add title word contributions
                title_vocab_size = len(self.title_vocab)
                for word in title_words:
                    if word in self.word_probs_title[c]:
                        class_scores[c] += self.word_probs_title[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_title_words = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_title_words + self.smoothing * title_vocab_size))
                
                # Add description word contributions
                desc_vocab_size = len(self.desc_vocab)
                for word in desc_words:
                    if word in self.word_probs_desc[c]:
                        class_scores[c] += self.word_probs_desc[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_desc_words = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_desc_words + self.smoothing * desc_vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df
    

def preprocess_desc(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

def preprocess_title(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

def prepare_separate_data(df):
    """Prepare data with separate processing for title and description"""
    df['Processed Title'] = df['Title'].apply(preprocess_title)
    df['Processed Description'] = df['Description'].apply(preprocess_desc)
    return df

def evaluate_separate_model(train_df, test_df):
    """Train and evaluate the separate parameters model"""
    # Prepare data
    train_df = prepare_separate_data(train_df)
    test_df = prepare_separate_data(test_df)
    
    # Initialize and train model
    nb = SeparateNaiveBayes()
    nb.fit(train_df, smoothening=1, class_col="Class Index", 
           title_col="Processed Title", desc_col="Processed Description")
    
    # Evaluate on train data
    train_predictions = nb.predict(train_df, 
                                  title_col="Processed Title", 
                                  desc_col="Processed Description")
    train_accuracy = sum(train_predictions["Predicted"] == train_predictions["Class Index"]) / len(train_predictions)
    
    # Evaluate on test data
    test_predictions = nb.predict(test_df,
                                 title_col="Processed Title", 
                                 desc_col="Processed Description")
    test_accuracy = sum(test_predictions["Predicted"] == test_predictions["Class Index"]) / len(test_predictions)
    
    
    y_true = test_df["Class Index"]
    y_pred = test_predictions["Predicted"]

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["World", "Sports", "Business", "Science/Tech"])
    disp.plot(cmap=plt.cm.Blues, values_format='d')
    plt.title("Confusion Matrix for Best Model (6b)")
    plt.savefig('confusion_matrix.png')
    plt.show()

    
    return train_accuracy, test_accuracy


# Evaluate separate parameters model
print("\nEvaluating Separate Parameters Model...")
separate_train_acc, separate_test_acc = evaluate_separate_model(train_df.copy(), test_df.copy())
print(f"Separate Parameters Model - Training Accuracy: {separate_train_acc:.4f}")
print(f"Separate Parameters Model - Test Accuracy: {separate_test_acc:.4f}")




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk
import string
from collections import Counter

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams

class EnhancedSeparateNaiveBayes:
    def __init__(self):
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match94-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.class_priors = {}  # P(C_k) for each class
        self.word_probs_title = {}  # P(w | C_k) for each word in title
        self.word_probs_desc = {}  # P(w | C_k) for each word in description
        self.title_vocab = set()  # Set of all words in title
        self.desc_vocab = set()  # Set of all words in description
        self.class_counts = {}  # Count of documents per class
        self.title_word_counts = {}  # Word counts per class for title
</FONT>        self.desc_word_counts = {}  # Word counts per class for description
        self.smoothing = 1  # Default Laplace smoothing value
        
        # Metadata feature distributions
        self.length_bins = 5  # Number of bins for length features
        self.title_length_probs = {}  # P(title_length | C_k)
        self.desc_length_probs = {}  # P(desc_length | C_k)
        self.punc_ratio_probs = {}  # P(punctuation_ratio | C_k)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match94-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.num_ratio_probs = {}  # P(number_ratio | C_k)
        self.spec_char_probs = {}  # P(special_char_presence | C_k)

    def fit(self, df, smoothening, class_col="Class Index", title_col="Processed Title", 
            desc_col="Processed Description", meta_cols=None):
        """Train the Naïve Bayes model by computing separate probabilities for title and description."""
</FONT>        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.title_word_counts[class_label] = {}
            self.desc_word_counts[class_label] = {}

        # Compute word counts per class for title and description
        for _, row in df.iterrows():
            class_label = row[class_col]
            
            # Process title words
            title_words = row[title_col].split()
            for word in title_words:
                if word in self.title_word_counts[class_label]:
                    self.title_word_counts[class_label][word] += 1
                else:
                    self.title_word_counts[class_label][word] = 1
                self.title_vocab.add(word)
            
            # Process description words
            desc_words = row[desc_col].split()
            for word in desc_words:
                if word in self.desc_word_counts[class_label]:
                    self.desc_word_counts[class_label][word] += 1
                else:
                    self.desc_word_counts[class_label][word] = 1
                self.desc_vocab.add(word)
        
        # Compute word probabilities P(w | C_k) for title
        title_vocab_size = len(self.title_vocab)
        self.word_probs_title = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_title_words_in_class = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
            for word in self.title_vocab:
                count = self.title_word_counts[c].get(word, 0)
                self.word_probs_title[c][word] = np.log((count + self.smoothing) / 
                                                       (total_title_words_in_class + self.smoothing * title_vocab_size))
        
        # Compute word probabilities P(w | C_k) for description
        desc_vocab_size = len(self.desc_vocab)
        self.word_probs_desc = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_desc_words_in_class = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
            for word in self.desc_vocab:
                count = self.desc_word_counts[c].get(word, 0)
                self.word_probs_desc[c][word] = np.log((count + self.smoothing) / 
                                                      (total_desc_words_in_class + self.smoothing * desc_vocab_size))
        
        # Compute metadata feature probabilities
        if meta_cols:
            # Create bins for continuous features
            title_length_bins = pd.qcut(df['title_length'], self.length_bins, labels=False, duplicates='drop')
            desc_length_bins = pd.qcut(df['desc_length'], self.length_bins, labels=False, duplicates='drop')
            punc_ratio_bins = pd.qcut(df['punctuation_ratio'], self.length_bins, labels=False, duplicates='drop')
            num_ratio_bins = pd.qcut(df['number_ratio'], self.length_bins, labels=False, duplicates='drop')
            
            # Initialize counters for metadata features
            self.title_length_probs = {c: {} for c in self.class_counts.keys()}
            self.desc_length_probs = {c: {} for c in self.class_counts.keys()}
            self.punc_ratio_probs = {c: {} for c in self.class_counts.keys()}
            self.num_ratio_probs = {c: {} for c in self.class_counts.keys()}
            self.spec_char_probs = {c: {} for c in self.class_counts.keys()}
            
            for c in self.class_counts:
                class_df = df[df[class_col] == c]
                class_size = len(class_df)
                
                # Count occurrences in each bin for each feature
                title_length_counts = Counter(title_length_bins[class_df.index])
                desc_length_counts = Counter(desc_length_bins[class_df.index])
                punc_ratio_counts = Counter(punc_ratio_bins[class_df.index])
                num_ratio_counts = Counter(num_ratio_bins[class_df.index])
                spec_char_counts = Counter(class_df['special_chars_present'])
                
                # Convert counts to log probabilities with smoothing
                for bin_idx in range(self.length_bins):
                    self.title_length_probs[c][bin_idx] = np.log((title_length_counts.get(bin_idx, 0) + self.smoothing) / 
                                                               (class_size + self.smoothing * self.length_bins))
                    self.desc_length_probs[c][bin_idx] = np.log((desc_length_counts.get(bin_idx, 0) + self.smoothing) / 
                                                              (class_size + self.smoothing * self.length_bins))
                    self.punc_ratio_probs[c][bin_idx] = np.log((punc_ratio_counts.get(bin_idx, 0) + self.smoothing) / 
                                                             (class_size + self.smoothing * self.length_bins))
                    self.num_ratio_probs[c][bin_idx] = np.log((num_ratio_counts.get(bin_idx, 0) + self.smoothing) / 
                                                            (class_size + self.smoothing * self.length_bins))
                
                # Binary feature probabilities (special chars present or not)
                for bin_idx in [0, 1]:  # Binary feature: 0 or 1
                    self.spec_char_probs[c][bin_idx] = np.log((spec_char_counts.get(bin_idx, 0) + self.smoothing) / 
                                                            (class_size + self.smoothing * 2))

    def predict(self, df, title_col="Processed Title", desc_col="Processed Description", 
                predicted_col="Predicted", meta_cols=None):
        """Predict class using separate parameters for title and description with metadata features."""
        predictions = []
        
        # Create bins for continuous features if using metadata
        if meta_cols:
            title_length_bins = pd.qcut(df['title_length'], self.length_bins, labels=False, duplicates='drop')
            desc_length_bins = pd.qcut(df['desc_length'], self.length_bins, labels=False, duplicates='drop')
            punc_ratio_bins = pd.qcut(df['punctuation_ratio'], self.length_bins, labels=False, duplicates='drop')
            num_ratio_bins = pd.qcut(df['number_ratio'], self.length_bins, labels=False, duplicates='drop')
        
        for idx, row in df.iterrows():
            title_words = row[title_col].split()
            desc_words = row[desc_col].split()
            class_scores = {}
            
            for c in self.class_counts:
                # Start with class prior
                class_scores[c] = self.class_priors[c]
                
                # Add title word contributions
                title_vocab_size = len(self.title_vocab)
                for word in title_words:
                    if word in self.word_probs_title[c]:
                        class_scores[c] += self.word_probs_title[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_title_words = sum(self.title_word_counts[c].values()) if self.title_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_title_words + self.smoothing * title_vocab_size))
                
                # Add description word contributions
                desc_vocab_size = len(self.desc_vocab)
                for word in desc_words:
                    if word in self.word_probs_desc[c]:
                        class_scores[c] += self.word_probs_desc[c][word]
                    else:
                        # Apply smoothing for unseen words
                        total_desc_words = sum(self.desc_word_counts[c].values()) if self.desc_word_counts[c] else 0
                        class_scores[c] += np.log(self.smoothing / (total_desc_words + self.smoothing * desc_vocab_size))
                
                # Add metadata feature contributions if available
                if meta_cols:
                    # Get bin indices for this row
                    title_len_bin = title_length_bins[idx] if idx in title_length_bins.index else 0
                    desc_len_bin = desc_length_bins[idx] if idx in desc_length_bins.index else 0
                    punc_bin = punc_ratio_bins[idx] if idx in punc_ratio_bins.index else 0
                    num_bin = num_ratio_bins[idx] if idx in num_ratio_bins.index else 0
                    spec_char = row['special_chars_present']
                    
                    # Add log probabilities for metadata features
                    if title_len_bin in self.title_length_probs[c]:
                        class_scores[c] += self.title_length_probs[c][title_len_bin]
                    if desc_len_bin in self.desc_length_probs[c]:
                        class_scores[c] += self.desc_length_probs[c][desc_len_bin]
                    if punc_bin in self.punc_ratio_probs[c]:
                        class_scores[c] += self.punc_ratio_probs[c][punc_bin]
                    if num_bin in self.num_ratio_probs[c]:
                        class_scores[c] += self.num_ratio_probs[c][num_bin]
                    if spec_char in self.spec_char_probs[c]:
                        class_scores[c] += self.spec_char_probs[c][spec_char]
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df


def extract_metadata_features(df):
    """Extract metadata features from title and description"""
    # Title length (number of words)
    df['title_length'] = df['Title'].apply(lambda x: len(x.split()))
    
    # Description length (number of words)
    df['desc_length'] = df['Description'].apply(lambda x: len(x.split()))
    
    # Punctuation ratio in description
    def punctuation_ratio(text):
        punct_count = sum(1 for char in text if char in string.punctuation)
        return punct_count / len(text) if len(text) &gt; 0 else 0
    
    df['punctuation_ratio'] = df['Description'].apply(punctuation_ratio)
    
    # Number ratio in description (useful for business/science articles)
    def number_ratio(text):
        num_count = sum(1 for char in text if char.isdigit())
        return num_count / len(text) if len(text) &gt; 0 else 0
    
    df['number_ratio'] = df['Description'].apply(number_ratio)
    
    # Special characters presence
    special_chars = {'$', '%', '#', '@', '&', '*', '+', '=', '/', '\\'}
    def has_special_chars(text):
        return 1 if any(char in special_chars for char in text) else 0
    
    df['special_chars_present'] = df['Description'].apply(has_special_chars)
    
    return df

def preprocess_desc(text):
    """Tokenizes, removes stopwords, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

def preprocess_title(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

def prepare_enhanced_data(df):
    """Prepare data with separate processing for title and description plus metadata features"""
    df['Processed Title'] = df['Title'].apply(preprocess_title)
    df['Processed Description'] = df['Description'].apply(preprocess_desc)
    df = extract_metadata_features(df)
    return df

def evaluate_enhanced_model(train_df, test_df):
    """Train and evaluate the enhanced model with metadata features"""
    # Prepare data
    train_df = prepare_enhanced_data(train_df)
    test_df = prepare_enhanced_data(test_df)
    
    meta_cols = ['title_length', 'desc_length', 'punctuation_ratio', 
                'number_ratio', 'special_chars_present']
    
    # Initialize and train model
    nb = EnhancedSeparateNaiveBayes()
    nb.fit(train_df, smoothening=1, class_col="Class Index", 
           title_col="Processed Title", desc_col="Processed Description",
           meta_cols=meta_cols)
    
    # Evaluate on train data
    train_predictions = nb.predict(train_df, 
                                  title_col="Processed Title", 
                                  desc_col="Processed Description",
                                  meta_cols=meta_cols)
    train_accuracy = sum(train_predictions["Predicted"] == train_predictions["Class Index"]) / len(train_predictions)
    
    # Evaluate on test data
    test_predictions = nb.predict(test_df,
                                 title_col="Processed Title", 
                                 desc_col="Processed Description",
                                 meta_cols=meta_cols)
    test_accuracy = sum(test_predictions["Predicted"] == test_predictions["Class Index"]) / len(test_predictions)
    
    # Generate confusion matrix
    from sklearn.metrics import confusion_matrix, classification_report
    confusion = confusion_matrix(test_predictions["Class Index"], test_predictions["Predicted"])
    report = classification_report(test_predictions["Class Index"], test_predictions["Predicted"])
    
    return train_df, test_df, train_accuracy, test_accuracy, confusion, report


# Load dataset
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Evaluate enhanced model
print("\nEvaluating Enhanced Model with Metadata Features...")
enhanced_train_df, enhanced_test_df, enhanced_train_acc, enhanced_test_acc, confusion, report = evaluate_enhanced_model(train_df.copy(), test_df.copy())
print(f"Enhanced Model - Training Accuracy: {enhanced_train_acc:.4f}")
print(f"Enhanced Model - Test Accuracy: {enhanced_test_acc:.4f}")

# Print confusion matrix and classification report
print("\nConfusion Matrix:")
print(confusion)
print("\nClassification Report:")
print(report)

# Compare results
print("\nPerformance Improvement:")
print(f"Original Separate Parameters Model Test Accuracy: 0.9114")
print(f"Enhanced Model with Metadata Features Test Accuracy: {enhanced_test_acc:.4f}")
print(f"Improvement: {(enhanced_test_acc - 0.9114) * 100:.2f}%")

# Analyze feature importance
def analyze_feature_importance(model, train_df):
    """Analyze which metadata features contribute most to classification"""
    # Get class distribution
    class_distribution = train_df["Class Index"].value_counts(normalize=True)
    
    feature_importance = {}
    metadata_features = ['title_length', 'desc_length', 'punctuation_ratio', 
                        'number_ratio', 'special_chars_present']
    
    # For each feature, measure how much it separates the classes
    for feature in metadata_features:
        class_means = train_df.groupby("Class Index")[feature].mean()
        overall_mean = train_df[feature].mean()
        
        # Calculate weighted variance of class means from overall mean
        weighted_variance = sum(class_distribution[c] * (class_means[c] - overall_mean)**2 
                                for c in class_means.index)
        feature_importance[feature] = weighted_variance
    
    # Normalize and return
    total = sum(feature_importance.values())
    if total &gt; 0:
        for feature in feature_importance:
            feature_importance[feature] /= total
    
    return feature_importance

# Analyze feature importance
feature_importance = analyze_feature_importance(None, enhanced_train_df)
print("\nFeature Importance:")
for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
    print(f"{feature}: {importance:.4f}")

# Visualize class-specific metadata patterns
def plot_metadata_by_class(train_df):
    """Plot metadata feature distributions by class"""
    metadata_features = ['title_length', 'desc_length', 'punctuation_ratio', 
                        'number_ratio', 'special_chars_present']
    
    fig, axes = plt.subplots(len(metadata_features), 1, figsize=(10, 3*len(metadata_features)))
    
    for i, feature in enumerate(metadata_features):
        for class_idx in sorted(train_df["Class Index"].unique()):
            class_data = train_df[train_df["Class Index"] == class_idx][feature]
            axes[i].hist(class_data, alpha=0.5, bins=20, label=f"Class {class_idx}")
        
        axes[i].set_title(f"{feature} Distribution by Class")
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel("Count")
        axes[i].legend()
    
    plt.tight_layout()
    plt.savefig("metadata_distributions.png")
    plt.close()

# Plot metadata distributions
plot_metadata_by_class(enhanced_train_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    # stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens ]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text_with_bigrams)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy ( bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy ( bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    # stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text_with_bigrams)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text_with_bigrams)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df, text_col="Description"):
    """Generate word clouds for each class based on word frequencies."""
    class_labels = sorted(df["Class Index"].unique())  # Ensure consistent order
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid for 4 classes
    
    for ax, class_label in zip(axes.flatten(), class_labels):
        text = ' '.join(df[df["Class Index"] == class_label][text_col].values)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.set_title(f"Class {class_label}")
        ax.axis("off")  # Hide axis labels
    
    plt.tight_layout()  # Adjust layout to prevent overlapping
    plt.savefig(f'wordcloud_b_st_sw.png')

generate_wordclouds(train_df, text_col="Processed Title")
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.util import ngrams


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = word_tokenize(re.sub(r'[^a-zA-Z]', ' ', text.lower()))
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

def preprocess_text_with_bigrams(text):
    """Tokenizes, removes stopwords, stems words, and generates bigrams."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]

    # Generate bigrams (list of tuples)
    bigrams = list(ngrams(tokens, 2))
    
    # Convert bigrams into strings (e.g., "good food" → "good_food")
    bigram_tokens = ["_".join(bigram) for bigram in bigrams]

    # Combine unigrams and bigrams
    final_tokens = tokens + bigram_tokens
    
    return " ".join(final_tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text_with_bigrams)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text_with_bigrams)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed bigrams): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed bigrams): {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Train the model
nb = NaiveBayes()
nb.fit(train_df, smoothening=1)

# Predict on training and test data
train_df = nb.predict(train_df)
test_df = nb.predict(test_df)

# Evaluate accuracy using custom function
train_accuracy = accuracy_score(train_df["Class Index"], train_df["Predicted"])
test_accuracy = accuracy_score(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    # stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens]
    return " ".join(tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Title"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Title", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Title' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Title"] = train_df["Title"].apply(preprocess_text)
test_df["Processed Title"] = test_df["Title"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Title")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Title")
test_df = nb.predict(test_df, text_col="Processed Title")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Title"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    # stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [stemmer.stem(word) for word in tokens]
    return " ".join(tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import nltk

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # P(C_k) for each class
        self.word_probs = {}  # P(w | C_k) for each word
        self.vocab = set()  # Set of all words in training data
        self.class_counts = {}  # Count of documents per class
        self.word_counts = {}  # Word counts per class
        self.smoothing = 1  # Default Laplace smoothing value

    def fit(self, df, smoothening, class_col="Class Index", text_col="Processed Description"):
        """Train the Naïve Bayes model by computing probabilities."""
        self.smoothing = smoothening
        total_docs = len(df)
        
        unique_classes = df[class_col].unique()
        for class_label in unique_classes:
            self.class_counts[class_label] = len(df[df[class_col] == class_label])
            self.class_priors[class_label] = np.log(self.class_counts[class_label] / total_docs)
            self.word_counts[class_label] = {}

        # Compute word counts per class
        for _, row in df.iterrows():
            class_label = row[class_col]
            words = row[text_col].split()  # Raw split on spaces
            for word in words:
                if word in self.word_counts[class_label]:
                    self.word_counts[class_label][word] += 1
                else:
                    self.word_counts[class_label][word] = 1
                self.vocab.add(word)
        
        # Compute word probabilities P(w | C_k)
        vocab_size = len(self.vocab)
        self.word_probs = {c: {} for c in self.class_counts.keys()}
        
        for c in self.class_counts:
            total_words_in_class = sum(self.word_counts[c].values())
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)  # Get count, default to 0 if not found
                self.word_probs[c][word] = np.log((count + self.smoothing) / (total_words_in_class + self.smoothing * vocab_size))

    def predict(self, df, text_col="Processed Description", predicted_col="Predicted"):
        """Predict the class of each document based on computed probabilities."""
        predictions = []
        
        for _, row in df.iterrows():
            words = row[text_col].split()  # Raw split on spaces
            class_scores = {}
            
            for c in self.class_counts:
                class_scores[c] = self.class_priors[c]  # Start with log(P(C_k))
                for word in words:
                    if word in self.word_probs[c]:  # Only consider words seen in training
                        class_scores[c] += self.word_probs[c][word]
                    else:
                        # Apply smoothing for unseen words
                        vocab_size = len(self.vocab)
                        class_scores[c] += np.log(self.smoothing / (sum(self.word_counts[c].values()) + self.smoothing * vocab_size))
            
            # Assign the class with highest probability
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df

def compute_accuracy(y_true, y_pred):
    """Compute accuracy manually."""
    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    return correct / len(y_true)

# Custom preprocessing using NLTK
def preprocess_text(text):
    """Tokenizes, removes stopwords, and stems words in a given text."""
    stop_words = set(stopwords.words('english'))
    # stemmer = PorterStemmer()
    
    # Tokenize and clean text
    tokens = text.lower().split()
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Load dataset (assuming CSV format with 'Class Index' and 'Description' columns)
train_df = pd.read_csv("../data/Q1/train.csv")
test_df = pd.read_csv("../data/Q1/test.csv")

# Apply preprocessing
train_df["Processed Description"] = train_df["Description"].apply(preprocess_text)
test_df["Processed Description"] = test_df["Description"].apply(preprocess_text)

# Train the model on processed data
nb = NaiveBayes()
nb.fit(train_df, smoothening=1, text_col="Processed Description")

# Predict on training and test data
train_df = nb.predict(train_df, text_col="Processed Description")
test_df = nb.predict(test_df, text_col="Processed Description")

# Evaluate accuracy using custom function
train_accuracy = compute_accuracy(train_df["Class Index"], train_df["Predicted"])
test_accuracy = compute_accuracy(test_df["Class Index"], test_df["Predicted"])
print(f"Training Accuracy (Preprocessed): {train_accuracy:.4f}")
print(f"Test Accuracy (Preprocessed): {test_accuracy:.4f}")


from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(y_true, y_pred):
    """Compute Precision, Recall, and F1-score."""
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    return precision, recall, f1

# Compute metrics for training data
train_precision, train_recall, train_f1 = compute_metrics(train_df["Class Index"], train_df["Predicted"])
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1-score: {train_f1:.4f}")

# Compute metrics for test data
test_precision, test_recall, test_f1 = compute_metrics(test_df["Class Index"], test_df["Predicted"])
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")

# Generate word cloud for each class after preprocessing
def generate_wordclouds(df):
    """Generate word clouds for each class based on word frequencies."""
    for class_label in df["Class Index"].unique():
        text = ' '.join(df[df["Class Index"] == class_label]["Processed Description"])
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f"Class {class_label} Word Cloud (Preprocessed)")
        plt.show()

# generate_wordclouds(train_df)
# generate_wordclouds(test_df)



import numpy as np
from itertools import combinations
from collections import Counter
from svm import SupportVectorMachine  # Import your SVM class

class OneVsOneSVMClassifier:
    def __init__(self, kernel='gaussian', C=1.0, gamma=0.001):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.classifiers = {} 
        self.unique_classes = None

    def fit(self, X, y):
        self.unique_classes = np.unique(y)
        class_pairs = list(combinations(self.unique_classes, 2))

        for class1, class2 in class_pairs:
            binary_mask = (y == class1) | (y == class2)
            X_binary = X[binary_mask]
            y_binary = y[binary_mask]
            y_binary = np.where(y_binary == class1, -1, 1)

            svm = SupportVectorMachine()
            svm.fit(X_binary, y_binary, kernel=self.kernel, C=self.C, gamma=self.gamma)

            self.classifiers[(class1, class2)] = svm

    def predict(self, X):
        votes = np.zeros((X.shape[0], len(self.unique_classes)))
        decision_scores = np.zeros((X.shape[0], len(self.unique_classes)))
        class_to_index = {label: idx for idx, label in enumerate(self.unique_classes)}

        for (class1, class2), model in self.classifiers.items():
            preds = model.predict(X)
            scores = model.project(X)

            for i in range(X.shape[0]):
                if preds[i] == 0:  # Predicted class1
                    votes[i, class_to_index[class1]] += 1
                    decision_scores[i, class_to_index[class1]] += scores[i]
                else:  # Predicted class2
                    votes[i, class_to_index[class2]] += 1
                    decision_scores[i, class_to_index[class2]] += scores[i]

        final_predictions = np.zeros(X.shape[0], dtype=int)

        for i in range(X.shape[0]):
            max_votes = np.max(votes[i])
            candidates = np.where(votes[i] == max_votes)[0]

            if len(candidates) == 1:
                final_predictions[i] = candidates[0]
            else:
                best_candidate = candidates[np.argmax(decision_scores[i, candidates])]
                final_predictions[i] = best_candidate

        return np.array([self.unique_classes[i] for i in final_predictions])

    def accuracy(self, y_true, y_pred):
        return np.mean(y_true == y_pred)





import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from svm import SupportVectorMachine

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

def visualize_support_vectors(svm):
    """ Display the top 5 support vectors """
    top_sv = svm.support_vectors[:5].reshape(-1, 100, 100, 3)
    fig, axes = plt.subplots(1, 5, figsize=(15, 5))
    for i, ax in enumerate(axes):
        ax.imshow(top_sv[i])
        ax.axis('off')
    plt.savefig('linear_support_vectors.png')

def visualize_weight_vector(svm, image_shape=(100,100,3)):
    """
    Visualize the weight vector of an SVM with proper normalization
    
    Args:
        svm: trained SVM model with weight vector w
        image_shape: tuple, original image dimensions (height, width) if applicable
    """
    if svm.w is None:
        print("Weight vector is not available. This might be a non-linear SVM.")
        return
    
    # Get weight vector
    w = svm.w.copy()
    
    # Reshape if image_shape is provided
    if image_shape is not None:
        w = w.reshape(image_shape)
    
    # Normalize for better visualization
    w_min, w_max = w.min(), w.max()
    if w_min == w_max:
        normalized_w = np.zeros_like(w)
    else:
        normalized_w = (w - w_min) / (w_max - w_min)
    
    # Create figure
    plt.figure(figsize=(8, 8))
    plt.title("Weight Vector Visualization")
    
    # Use a colormap that shows pattern better than pure black
    plt.imshow(normalized_w, cmap='viridis')
    plt.colorbar(label='Weight Value')
    plt.tight_layout()
    plt.savefig('linear_weight_vector.png')
    
    # Print stats about the weight vector
    print(f"Weight vector stats:")
    print(f"Min: {w_min:.6f}, Max: {w_max:.6f}")
    print(f"Mean: {w.mean():.6f}, Std: {w.std():.6f}")
    
def main():
    base_train = "../data/Q2/train"
    base_test = "../data/Q2/test"
    class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
    d = 9  # Example last digit of roll number
    classes = [class_folders[d], class_folders[(d + 1) % 11]]
    
    X_train, y_train = prepare_dataset(base_train, classes)
    X_test, y_test = prepare_dataset(base_test, classes)
    
    svm = SupportVectorMachine()
    svm.fit(X_train, y_train, kernel='linear', C=1.0, gamma=0.001)
    
<A NAME="5"></A><FONT color = #FF0000><A HREF="match94-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    num_support_vectors = svm.support_vectors.shape[0]
    percentage_sv = (num_support_vectors / X_train.shape[0]) * 100
    print(f"Support Vectors: {num_support_vectors}")
    print(f"Percentage of Training Samples as Support Vectors: {percentage_sv:.2f}%")
</FONT>    
    # w = svm.w
    b = svm.b
    # print(f"Weight Vector (w) Shape: {w.shape}")
    print(f"Intercept (b): {b}")
    
    y_pred = svm.predict(X_test)
    accuracy = np.mean(y_pred == y_test) * 100
    print(f"Test Accuracy: {accuracy:.2f}%")
    
    visualize_support_vectors(svm)
    # visualize_weight_vector(svm)
    
if __name__ == "__main__":
    main()




import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn import svm as sk_svm
from svm import SupportVectorMachine

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

def visualize_support_vectors(svm_model, sklearn_model, title_prefix):
    """ Display the top 5 support vectors from both models """
    # Custom SVM support vectors
    custom_sv = svm_model.support_vectors[:5].reshape(-1, 100, 100, 3)
    
    # Sklearn SVM support vectors
    sklearn_sv_indices = sklearn_model.support_
    sklearn_sv = X_train[sklearn_sv_indices][:5].reshape(-1, 100, 100, 3)
    
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    
    for i in range(5):
        axes[0, i].imshow(custom_sv[i])
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title(f"{title_prefix} - Custom SVM")
        
        axes[1, i].imshow(sklearn_sv[i])
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title(f"{title_prefix} - Scikit-learn SVM")
    
    plt.tight_layout()
    plt.savefig(f'{title_prefix} support_vectors_comparison.png')

def compare_support_vectors(custom_svm, sklearn_svm, X_train):
    """Compare support vectors between custom and sklearn implementations"""
    custom_sv = custom_svm.support_vectors
    sklearn_sv_indices = sklearn_svm.support_
    sklearn_sv = X_train[sklearn_sv_indices]
    
    # Count matching support vectors
    matching_count = 0
    matched_sklearn_indices = set()  # Keep track of sklearn vectors already matched
    
    for i, custom_vector in enumerate(custom_sv):
        found_match = False
        for j, sklearn_vector in enumerate(sklearn_sv):
            if j not in matched_sklearn_indices and np.allclose(custom_vector, sklearn_vector, rtol=1e-7, atol=1e-9):
                matching_count += 1
                matched_sklearn_indices.add(j)
                found_match = True
                break
    
    print(f"Custom SVM support vectors: {len(custom_sv)}")
    print(f"Scikit-learn SVM support vectors: {len(sklearn_sv)}")
    print(f"Matching support vectors: {matching_count}")
    print(f"Percentage of matching vectors: {(matching_count / min(len(custom_sv), len(sklearn_sv))) * 100:.2f}%")
    
    return custom_sv, sklearn_sv, matching_count

def compare_weights_bias(custom_svm, sklearn_svm):
    """Compare weight vectors and bias terms"""
    # For linear kernel only
    if custom_svm.kernel == 'linear' and sklearn_svm.kernel == 'linear':
        custom_w = custom_svm.w
        custom_b = custom_svm.b
        
        # Scikit-learn stores the coefficients differently
        sklearn_w = sklearn_svm.coef_[0]
        sklearn_b = sklearn_svm.intercept_[0]
        
        # Normalize both weight vectors for comparison
        # (Sign may be flipped depending on implementation)
        custom_w_norm = custom_w / np.linalg.norm(custom_w)
        sklearn_w_norm = sklearn_w / np.linalg.norm(sklearn_w)
        
        dot_product = np.abs(np.dot(custom_w_norm, sklearn_w_norm))
        angle = np.arccos(dot_product) * 180 / np.pi
        
        print(f"Custom SVM weight norm: {np.linalg.norm(custom_w):.4f}")
        print(f"Scikit-learn SVM weight norm: {np.linalg.norm(sklearn_w):.4f}")
        print(f"Cosine similarity between weight vectors: {dot_product:.4f}")
        print(f"Angle between normalized weight vectors: {angle:.2f} degrees")
        print(f"Custom SVM bias: {custom_b:.4f}")
        print(f"Scikit-learn SVM bias: {sklearn_b:.4f}")
        
        return custom_w, sklearn_w, custom_b, sklearn_b
    else:
        print("Weight comparison only available for linear kernel")
        return None, None, None, None

def visualize_weight_vectors(custom_w, sklearn_w):
    """Visualize weight vectors from both implementations"""
    if custom_w is not None and sklearn_w is not None:
        custom_w_image = custom_w.reshape(100, 100, 3)
        sklearn_w_image = sklearn_w.reshape(100, 100, 3)
        
        # Normalize for better visualization
        custom_w_image = (custom_w_image - custom_w_image.min()) / (custom_w_image.max() - custom_w_image.min())
        sklearn_w_image = (sklearn_w_image - sklearn_w_image.min()) / (sklearn_w_image.max() - sklearn_w_image.min())
        
        fig, axes = plt.subplots(1, 2, figsize=(10, 5))
        axes[0].imshow(custom_w_image)
        axes[0].set_title("Custom SVM Weight Vector")
        axes[0].axis('off')
        
        axes[1].imshow(sklearn_w_image)
        axes[1].set_title("Scikit-learn SVM Weight Vector")
        axes[1].axis('off')
        
        plt.tight_layout()
        plt.savefig('weight_vector_comparison.png')

def run_comparison(kernel_type, C=1.0, gamma=0.001):
    """Run full comparison between custom and sklearn SVMs"""
    print(f"\n{'='*50}")
    print(f"Comparing {kernel_type.upper()} KERNEL SVM Implementations")
    print(f"{'='*50}")
    
    # Initialize custom SVM
    custom_start_time = time.time()
    custom_svm = SupportVectorMachine()
    custom_svm.fit(X_train, y_train, kernel=kernel_type, C=C, gamma=gamma)
    custom_train_time = time.time() - custom_start_time
    print(f"Custom SVM training time: {custom_train_time:.4f} seconds")
    
    sklearn_start_time = time.time()
    # Initialize sklearn SVM
    if kernel_type == 'linear':
        sklearn_model = sk_svm.SVC(kernel='linear', C=C)
    else:  # gaussian/rbf
        sklearn_model = sk_svm.SVC(kernel='rbf', C=C, gamma=gamma)
    
    sklearn_model.fit(X_train, y_train.flatten())
    sklearn_train_time = time.time() - sklearn_start_time
    print(f"Scikit-learn SVM training time: {sklearn_train_time:.4f} seconds")
    
    print(f"Training time ratio: {custom_train_time / sklearn_train_time:.4f}")
    
    # Compare support vectors
    print("\nSupport Vector Comparison:")
    custom_sv, sklearn_sv, matching_sv = compare_support_vectors(custom_svm, sklearn_model, X_train)
    
    # Compare weights and bias (linear kernel only)
    if kernel_type == 'linear':
        print("\nWeight and Bias Comparison:")
        custom_w, sklearn_w, custom_b, sklearn_b = compare_weights_bias(custom_svm, sklearn_model)
        visualize_weight_vectors(custom_w, sklearn_w)
    
    # Evaluate and compare accuracy
    custom_pred = custom_svm.predict(X_test)
    custom_accuracy = np.mean(custom_pred == y_test) * 100
    
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_accuracy = np.mean(sklearn_pred == y_test) * 100
    
    print("\nAccuracy Comparison:")
    print(f"Custom SVM test accuracy: {custom_accuracy:.2f}%")
    print(f"Scikit-learn SVM test accuracy: {sklearn_accuracy:.2f}%")
    print(f"Accuracy difference: {abs(custom_accuracy - sklearn_accuracy):.2f}%")
    
    # Visualize support vectors
    visualize_support_vectors(custom_svm, sklearn_model, kernel_type.capitalize())
    
    return custom_svm, sklearn_model, custom_accuracy, sklearn_accuracy

# Main execution
if __name__ == "__main__":
    base_train = "../data/Q2/train"
    base_test = "../data/Q2/test"
    class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
    d = 5  
    classes = [class_folders[d], class_folders[(d + 1) % 11]]
    
    print(f"Comparing classes: {classes[0]} vs {classes[1]}")
    
    # Prepare datasets
    X_train, y_train = prepare_dataset(base_train, classes)
    X_test, y_test = prepare_dataset(base_test, classes)
    
    # Run linear kernel comparison
    custom_linear, sklearn_linear, linear_acc_custom, linear_acc_sklearn = run_comparison('linear')
    
    # Run Gaussian kernel comparison
    custom_gaussian, sklearn_gaussian, gaussian_acc_custom, gaussian_acc_sklearn = run_comparison('gaussian')
    
    # Summary
    print("\n" + "="*50)
    print("SUMMARY")
    print("="*50)
    print(f"Linear Kernel - Custom SVM Accuracy: {linear_acc_custom:.2f}%")
    print(f"Linear Kernel - Scikit-learn SVM Accuracy: {linear_acc_sklearn:.2f}%")
    print(f"Gaussian Kernel - Custom SVM Accuracy: {gaussian_acc_custom:.2f}%")
    print(f"Gaussian Kernel - Scikit-learn SVM Accuracy: {gaussian_acc_sklearn:.2f}%")



import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn import svm as sk_svm
from sklearn.linear_model import SGDClassifier
from svm import SupportVectorMachine

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

def compare_sgd_vs_libsvm(X_train, y_train, X_test, y_test):
    """Compare SGD solver with LIBSVM/LIBLINEAR"""
    print("\n" + "="*50)
    print("SGD vs LIBSVM/LIBLINEAR Comparison")
    print("="*50)
    
    # List of solvers to compare
    solvers = [
        ("SGD", SGDClassifier(loss="hinge", penalty="l2", max_iter=1000, tol=1e-3, random_state=42, alpha=0.001)),
        ("LIBLINEAR", sk_svm.LinearSVC(dual=False, max_iter=1000)),
        ("LIBSVM-Linear", sk_svm.SVC(kernel="linear")),
        ("LIBSVM-RBF", sk_svm.SVC(kernel="rbf", gamma="scale")),
    ]
    
    # Custom SVM with CVXOPT (both linear and Gaussian)
    solvers.extend([
        ("CVXOPT-Linear", SupportVectorMachine()),
        ("CVXOPT-Gaussian", SupportVectorMachine())
    ])
    
    # Initialize result storage
    training_times = []
    prediction_times = []
    accuracies = []
    n_support_vectors = []
    
    # Perform training and testing for each solver
    for name, solver in solvers:
        print(f"\n{name} Solver:")
        
        # Training
        start_time = time.time()
        if name == "CVXOPT-Linear":
            solver.fit(X_train, y_train, kernel="linear")
        elif name == "CVXOPT-Gaussian":
            solver.fit(X_train, y_train, kernel="gaussian", gamma=0.001)
        else:
            solver.fit(X_train, y_train)
        train_time = time.time() - start_time
        training_times.append(train_time)
        print(f"  Training time: {train_time:.4f} seconds")
        
        # Count support vectors where applicable
        if name == "CVXOPT-Linear" or name == "CVXOPT-Gaussian":
            n_sv = len(solver.support_vectors)
        elif "LIBSVM" in name:
            n_sv = len(solver.support_vectors_)
        else:
            n_sv = 0  # SGD and LinearSVC don't have explicit support vectors
        n_support_vectors.append(n_sv)
        
        if n_sv &gt; 0:
            print(f"  Number of support vectors: {n_sv}")
        
        # Prediction
        start_time = time.time()
        if name == "CVXOPT-Linear" or name == "CVXOPT-Gaussian":
            y_pred = solver.predict(X_test)
        else:
            y_pred = solver.predict(X_test)
        pred_time = time.time() - start_time
        prediction_times.append(pred_time)
        print(f"  Prediction time: {pred_time:.4f} seconds")
        
        # Accuracy
        accuracy = np.mean(y_pred == y_test) * 100
        accuracies.append(accuracy)
        print(f"  Test accuracy: {accuracy:.2f}%")
    
    return solvers, training_times, prediction_times, accuracies, n_support_vectors

def plot_comparison(solvers, training_times, prediction_times, accuracies, n_support_vectors):
    """Plot comparison results"""
    solver_names = [s[0] for s in solvers]
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Training time subplot
    axes[0, 0].bar(solver_names, training_times)
    axes[0, 0].set_title("Training Time Comparison")
    axes[0, 0].set_ylabel("Time (seconds)")
    axes[0, 0].set_xticklabels(solver_names, rotation=45, ha="right")
    for i, v in enumerate(training_times):
        axes[0, 0].text(i, v + 0.05, f"{v:.3f}s", ha='center')
    
    # Prediction time subplot
    axes[0, 1].bar(solver_names, prediction_times)
    axes[0, 1].set_title("Prediction Time Comparison")
    axes[0, 1].set_ylabel("Time (seconds)")
    axes[0, 1].set_xticklabels(solver_names, rotation=45, ha="right")
    for i, v in enumerate(prediction_times):
        axes[0, 1].text(i, v + 0.01, f"{v:.3f}s", ha='center')
    
    # Accuracy subplot
    axes[1, 0].bar(solver_names, accuracies)
    axes[1, 0].set_title("Accuracy Comparison")
    axes[1, 0].set_ylabel("Accuracy (%)")
    axes[1, 0].set_xticklabels(solver_names, rotation=45, ha="right")
    for i, v in enumerate(accuracies):
        axes[1, 0].text(i, v - 5, f"{v:.2f}%", ha='center')
    
    # Support vectors subplot
    axes[1, 1].bar(solver_names, n_support_vectors)
    axes[1, 1].set_title("Number of Support Vectors")
    axes[1, 1].set_ylabel("Count")
    axes[1, 1].set_xticklabels(solver_names, rotation=45, ha="right")
    for i, v in enumerate(n_support_vectors):
        if v &gt; 0:  # Only show text for non-zero values
            axes[1, 1].text(i, v + 5, str(v), ha='center')
    
    plt.tight_layout()
    plt.savefig('comparison_4.png')

def compare_sgd_parameters(X_train, y_train, X_test, y_test):
    """Compare different SGD parameters"""
    print("\n" + "="*50)
    print("SGD Parameter Comparison")
    print("="*50)
    
    # Different parameters to try
    alphas = [0.0001, 0.001, 0.01, 0.1]
    max_iters = [100, 500, 1000, 5000]
    
    # Results storage
    results = []
    
    # Alpha comparison
    print("\nComparing alpha values:")
    for alpha in alphas:
        sgd = SGDClassifier(loss="hinge", penalty="l2", alpha=alpha, max_iter=1000, random_state=42)
        
<A NAME="6"></A><FONT color = #00FF00><A HREF="match94-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        start_time = time.time()
        sgd.fit(X_train, y_train)
        train_time = time.time() - start_time
        
        y_pred = sgd.predict(X_test)
        accuracy = np.mean(y_pred == y_test) * 100
</FONT>        
        print(f"  Alpha={alpha}: Accuracy = {accuracy:.2f}%, Training time = {train_time:.4f}s")
        results.append((f"Alpha={alpha}", train_time, accuracy))
    
    # Max iterations comparison
    print("\nComparing max iterations:")
    for max_iter in max_iters:
        sgd = SGDClassifier(loss="hinge", penalty="l2", max_iter=max_iter, random_state=42)
        
        start_time = time.time()
        sgd.fit(X_train, y_train)
        train_time = time.time() - start_time
        
        y_pred = sgd.predict(X_test)
        accuracy = np.mean(y_pred == y_test) * 100
        
        print(f"  Max iterations={max_iter}: Accuracy = {accuracy:.2f}%, Training time = {train_time:.4f}s")
        results.append((f"Iter={max_iter}", train_time, accuracy))
    
    return results

# Main execution
if __name__ == "__main__":
    base_train = "../data/Q2/train"
    base_test = "../data/Q2/test"
    class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
    d = 5  # Example last digit of roll number
    classes = [class_folders[d], class_folders[(d + 1) % 11]]
    
    print(f"Comparing classes: {classes[0]} vs {classes[1]}")
    
    # Prepare datasets
    X_train, y_train = prepare_dataset(base_train, classes)
    X_test, y_test = prepare_dataset(base_test, classes)
    
    # Compare SGD vs LIBSVM/LIBLINEAR
    solvers, training_times, prediction_times, accuracies, n_support_vectors = compare_sgd_vs_libsvm(
        X_train, y_train, X_test, y_test
    )
    
    # Plot comparison results
    plot_comparison(solvers, training_times, prediction_times, accuracies, n_support_vectors)
    
    # Compare different SGD parameters
    sgd_params_results = compare_sgd_parameters(X_train, y_train, X_test, y_test)
    
    # Print final summary
    print("\n" + "="*50)
    print("SUMMARY")
    print("="*50)
    print("SGD vs Other Solvers:")
    for i, (name, _) in enumerate(solvers):
        print(f"{name}: Training time = {training_times[i]:.4f}s, Accuracy = {accuracies[i]:.2f}%")
    
    # Calculate speedup
    liblinear_index = [name for name, _ in solvers].index("LIBLINEAR")
    sgd_index = [name for name, _ in solvers].index("SGD")
    cvxopt_linear_index = [name for name, _ in solvers].index("CVXOPT-Linear")

    liblinear_time = training_times[liblinear_index]
    sgd_time = training_times[sgd_index]
    cvxopt_linear_time = training_times[cvxopt_linear_index]

    print(f"\nSpeedups:")
    print(f"SGD is {liblinear_time/sgd_time:.2f}x faster than LIBLINEAR")
    print(f"SGD is {cvxopt_linear_time/sgd_time:.2f}x faster than CVXOPT-Linear")



# Example usage:
from multi_svm import OneVsOneSVMClassifier
import os
import cv2
import time
import numpy as np

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

base_train = "../data/Q2/train"
base_test = "../data/Q2/test"
class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
classes = [class_folders[d] for d in range(11)]

X_train, y_train = prepare_dataset(base_train, classes)
X_test, y_test = prepare_dataset(base_test, classes)

# Initialize and train the multi-class SVM
multi_svm = OneVsOneSVMClassifier(kernel="gaussian", C=1.0, gamma=0.001)
start = time.time()
multi_svm.fit(X_train, y_train)
print(f"Training time : {time.time() - start}")

# Or you can make predictions directly
y_pred = multi_svm.predict(X_test)
acc = multi_svm.accuracy(y_test, y_pred)
print(f"Test Accuracy: {acc * 100:.2f}%")




import numpy as np
import time
import os
import cv2
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Function to train and evaluate scikit-learn SVM
def train_evaluate_sklearn_svm(X_train, y_train, X_test, y_test):
    # Initialize SVM with Gaussian kernel (RBF) and the same parameters
    sklearn_svm = SVC(kernel='rbf', C=1.0, gamma=0.001)
    
    # Record training start time
    train_start = time.time()
    
    # Train the model
    sklearn_svm.fit(X_train, y_train)
    
    # Record training end time
    train_end = time.time()
    training_time = train_end - train_start
    
    # Make predictions
    y_pred = sklearn_svm.predict(X_test)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    return accuracy, training_time

    
def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

base_train = "../data/Q2/train"
base_test = "../data/Q2/test"
class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
classes = [class_folders[d] for d in range(11)]

X_train, y_train = prepare_dataset(base_train, classes)
X_test, y_test = prepare_dataset(base_test, classes)

# Train and evaluate scikit-learn SVM
accuracy, training_time = train_evaluate_sklearn_svm(X_train, y_train, X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")
print(f"Training Time: {training_time:.4f} seconds")



import numpy as np
import time
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

# Function to train and evaluate scikit-learn SVM
def train_evaluate_sklearn_svm(X_train, y_train, X_test, y_test):
    sklearn_svm = SVC(kernel='rbf', C=1.0, gamma=0.001)

    train_start = time.time()
    sklearn_svm.fit(X_train, y_train)
    train_end = time.time()
    
    training_time = train_end - train_start
    y_pred = sklearn_svm.predict(X_test)
    print(f"prediction time {time.time()-train_end}")
    accuracy = accuracy_score(y_test, y_pred)
    
    return accuracy, training_time, y_test, y_pred

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, flatten, and normalize them. """
    data, labels = [], []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

base_train = "../data/Q2/train"
base_test = "../data/Q2/test"
class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
classes = [class_folders[d] for d in range(11)]

X_train, y_train = prepare_dataset(base_train, classes)
X_test, y_test = prepare_dataset(base_test, classes)

# Train and evaluate scikit-learn SVM
accuracy, training_time, y_test, y_pred = train_evaluate_sklearn_svm(X_train, y_train, X_test, y_test)

print(f"Accuracy: {accuracy:.4f}")
print(f"Training Time: {training_time:.4f} seconds")

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.savefig('confusion_matrix_scikit_svm')




# Example usage:
from multi_svm import OneVsOneSVMClassifier
import os
import cv2
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

base_train = "../data/Q2/train"
base_test = "../data/Q2/test"
class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
classes = [class_folders[d] for d in range(11)]

X_train, y_train = prepare_dataset(base_train, classes)
X_test, y_test = prepare_dataset(base_test, classes)

# Initialize and train the multi-class SVM
multi_svm = OneVsOneSVMClassifier(kernel="gaussian", C=1.0, gamma=0.001)
multi_svm.fit(X_train, y_train)

# Or you can make predictions directly
y_pred = multi_svm.predict(X_test)
acc = multi_svm.accuracy(y_test, y_pred)
print(f"Test Accuracy: {acc * 100:.2f}%")

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import random

# Example usage:
from multi_svm import OneVsOneSVMClassifier
import os
import cv2
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, crop, flatten, and normalize them. """
    data = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data), np.array(labels)

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    return np.vstack(X), np.hstack(y)

base_train = "../data/Q2/train"
base_test = "../data/Q2/test"
class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
classes = [class_folders[d] for d in range(11)]

X_train, y_train = prepare_dataset(base_train, classes)
X_test, y_test = prepare_dataset(base_test, classes)

# Initialize and train the multi-class SVM
multi_svm = OneVsOneSVMClassifier(kernel="gaussian", C=1.0, gamma=0.001)
multi_svm.fit(X_train, y_train)

# Or you can make predictions directly
y_pred = multi_svm.predict(X_test)
acc = multi_svm.accuracy(y_test, y_pred)
print(f"Test Accuracy: {acc * 100:.2f}%")

# Compute confusion matrix
# conf_matrix = confusion_matrix(y_test, y_pred)

# # Plot confusion matrix
# plt.figure(figsize=(10, 8))
# sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
# plt.xlabel("Predicted Label")
# plt.ylabel("True Label")
# plt.title("Confusion Matrix")
# plt.show()

def plot_misclassified_examples(X_test, y_test, y_pred, classes, n_examples=10, img_size=(100, 100)):
    """
    Plot examples of misclassified images with their true and predicted labels.
    
    Parameters:
    -----------
    X_test : numpy array
        Flattened test images
    y_test : numpy array
        True labels
    y_pred : numpy array
        Predicted labels
    classes : list
        List of class names
    n_examples : int
        Number of examples to show
    img_size : tuple
        Size of original images before flattening
    """
    # Find indices of misclassified samples
    misclassified_indices = np.where(y_test != y_pred)[0]
    
    if len(misclassified_indices) == 0:
        print("No misclassified samples found.")
        return
    
    # Select random samples if there are more than n_examples
    if len(misclassified_indices) &gt; n_examples:
        selected_indices = random.sample(list(misclassified_indices), n_examples)
    else:
        selected_indices = misclassified_indices
        n_examples = len(selected_indices)
    
    # Calculate grid dimensions
    n_cols = 3
    n_rows = (n_examples + n_cols - 1) // n_cols
    
    # Create figure
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))
    axes = axes.flatten() if n_examples &gt; 1 else [axes]
    
    # Plot each misclassified example
    for i, idx in enumerate(selected_indices):
        if i &lt; n_examples:
            img = X_test[idx].reshape(img_size)
            true_label = classes[int(y_test[idx])]
            pred_label = classes[int(y_pred[idx])]
            
            axes[i].imshow(img, cmap='gray')
            axes[i].set_title(f"({chr(97+i)}) True: {true_label}, predicted: {pred_label}")
            axes[i].axis('off')
    
    # Hide empty subplots
    for i in range(len(selected_indices), len(axes)):
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig('misclassified_examples.png', dpi=300)
    plt.show()
    
    return selected_indices

# You can also analyze which classes get misclassified most often
def analyze_misclassifications(y_test, y_pred, classes):
    """Print analysis of which classes get misclassified as which."""
    misclass_indices = np.where(y_test != y_pred)[0]
    print(f"Total misclassifications: {len(misclass_indices)}")
    
    # Count misclassifications for each true-predicted pair
    misclass_counts = {}
    for idx in misclass_indices:
        true_class = int(y_test[idx])
<A NAME="7"></A><FONT color = #0000FF><A HREF="match94-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        pred_class = int(y_pred[idx])
        pair = (classes[true_class], classes[pred_class])
        misclass_counts[pair] = misclass_counts.get(pair, 0) + 1
</FONT>    
    # Sort by count and print
    sorted_misclass = sorted(misclass_counts.items(), key=lambda x: x[1], reverse=True)
    print("\nTop misclassifications:")
    for (true_class, pred_class), count in sorted_misclass[:10]:  # Top 10
        print(f"True: {true_class}, Predicted: {pred_class}: {count} instances")

misclassified_indices = plot_misclassified_examples(X_test, y_test, y_pred, classes, 10, (100,100,3))

analyze_misclassifications(y_test, y_pred, classes)



#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().system('pip install decorator')


# In[2]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import time
import os
import cv2


# In[3]:



def custom_k_fold_split(X, y, n_splits=5, shuffle=True, random_state=None):
    """
    Custom implementation of k-fold cross-validation split.
    
    Args:
        X: Features array
        y: Labels array
        n_splits: Number of folds
        shuffle: Whether to shuffle the data before splitting
        random_state: Random seed for reproducibility
    
    Returns:
        folds: List of (train_indices, val_indices) tuples for each fold
    """
    n_samples = len(X)
    indices = np.arange(n_samples)
    
    # Set random seed if provided
    if shuffle and random_state is not None:
        np.random.seed(random_state)
        np.random.shuffle(indices)
    
    # Calculate fold size
    fold_size = n_samples // n_splits
    remainder = n_samples % n_splits
    
    # Generate fold indices
    folds = []
    start = 0
    for i in range(n_splits):
        # Add one extra element to some folds if n_samples is not divisible by n_splits
        end = start + fold_size + (1 if i &lt; remainder else 0)
        
        # Validation indices for this fold
        val_indices = indices[start:end]
        
        # Training indices for this fold (all except validation)
        train_indices = np.concatenate([indices[:start], indices[end:]])
        
        folds.append((train_indices, val_indices))
        start = end
    
    return folds

def ensure_c_contiguous(X):
    """
    Ensure that the array is C-contiguous and double precision.
    Returns the array itself if it's already C-contiguous and double precision,
    otherwise returns a copy with the proper format.
    """
    if not (X.flags.c_contiguous and X.dtype == np.float64):
        return np.ascontiguousarray(X, dtype=np.float64)
    return X

def k_fold_cross_validation(X_train, y_train, X_test, y_test, gamma=0.001):
    """
    Perform 5-fold cross-validation to find the optimal C value.
    Custom implementation without using sklearn's cross_val_score.
    
    Args:
        X_train, y_train: Training data and labels
        X_test, y_test: Test data and labels
        gamma: Gamma parameter for the Gaussian kernel (fixed at 0.001)
    
    Returns:
        best_C: The C value with the highest cross-validation accuracy
        cv_results: Dictionary containing results for each C value
    """
    # Ensure data is C-contiguous and double precision to avoid unnecessary copies
    X_train = ensure_c_contiguous(X_train)
    X_test = ensure_c_contiguous(X_test)
    
    # C values to test
    C_values = [1e-5, 1e-3, 1, 5, 10]
    
    # Generate 5-fold splits
    folds = custom_k_fold_split(X_train, y_train, n_splits=5, shuffle=True, random_state=42)
    
    # Results storage
    cv_results = {
        'C_values': C_values,
        'cv_accuracies': [],
        'test_accuracies': [],
        'training_times': []
    }
    
    # Cache size set to 4000MB (4GB) for 16GB RAM system
    # This is 25% of available RAM, which should be reasonable
    cache_size = 4000
    
    # For each C value
    for C in C_values:
        print(f"\nTesting C = {C}")
        
        # Measure training time
        start_time = time.time()
        
        # Cross-validation scores for this C value
        fold_scores = []
        
        # For each fold
        for fold_idx, (train_indices, val_indices) in enumerate(folds):
            print(f"  Fold {fold_idx+1}/5")
            
            # Split data into training and validation for this fold
            X_fold_train, y_fold_train = X_train[train_indices], y_train[train_indices]
            X_fold_val, y_fold_val = X_train[val_indices], y_train[val_indices]
            
            # Create and train SVM model with increased cache size
            model = SVC(C=C, kernel='rbf', gamma=gamma, cache_size=cache_size)
            model.fit(X_fold_train, y_fold_train)
            
            # Evaluate on validation set
            val_pred = model.predict(X_fold_val)
            val_acc = accuracy_score(y_fold_val, val_pred)
            fold_scores.append(val_acc)
            
            print(f"    Validation accuracy: {val_acc:.4f}")
        
        # Average cross-validation accuracy
        cv_mean = np.mean(fold_scores)
        print(f"  5-fold CV accuracy: {cv_mean:.4f}")
        
        # Train on full training set
        full_model = SVC(C=C, kernel='rbf', gamma=gamma, cache_size=cache_size)
        full_model.fit(X_train, y_train)
        training_time = time.time() - start_time
        
        # Test accuracy
        test_pred = full_model.predict(X_test)
        test_acc = accuracy_score(y_test, test_pred)
        print(f"  Test accuracy: {test_acc:.4f}")
        print(f"  Training time: {training_time:.2f} seconds")
        
        # Store results
        cv_results['cv_accuracies'].append(cv_mean)
        cv_results['test_accuracies'].append(test_acc)
        cv_results['training_times'].append(training_time)
    
    # Find the C value with the highest cross-validation accuracy
    best_idx = np.argmax(cv_results['cv_accuracies'])
    best_C = C_values[best_idx]
    
    return best_C, cv_results

def plot_results(cv_results):
    """
    Plot the cross-validation and test accuracies against C values.
    """
    plt.figure(figsize=(10, 6))
    
    # Use log scale for x-axis
    plt.semilogx(cv_results['C_values'], cv_results['cv_accuracies'], marker='o', linestyle='-', label='5-fold CV Accuracy')
    plt.semilogx(cv_results['C_values'], cv_results['test_accuracies'], marker='s', linestyle='--', label='Test Accuracy')
    
    plt.xlabel('C value (log scale)')
    plt.ylabel('Accuracy')
    plt.title('SVM Performance vs. C Value (Gaussian Kernel, gamma=0.001)')
    plt.legend()
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    
    # Save the plot
    plt.savefig('svm_cv_performance.png')
    plt.show()

def train_final_model(X_train, y_train, X_test, y_test, best_C, gamma=0.001):
    """
    Train the final model using the best C value and evaluate on test set.
    """
    # Ensure data is C-contiguous and double precision
    X_train = ensure_c_contiguous(X_train)
    X_test = ensure_c_contiguous(X_test)
    
    print(f"\nTraining final model with C = {best_C}")
    
    # Create and train the final model with increased cache size
    final_model = SVC(C=best_C, kernel='rbf', gamma=gamma, cache_size=4000)
    final_model.fit(X_train, y_train)
    
    # Evaluate on test set
    test_pred = final_model.predict(X_test)
    test_acc = accuracy_score(y_test, test_pred)
    
    print(f"Final model test accuracy: {test_acc:.4f}")
    
    return final_model, test_acc

def load_images_from_folder(folder, label, img_size=(100, 100)):
    """ Load images, resize, flatten, and normalize them. """
    data, labels = [], []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, img_size)
            img = img / 255.0  # Normalize to [0,1]
            data.append(img.flatten())
            labels.append(label)
    return np.array(data, dtype=np.float64), np.array(labels)  # Ensure float64 dtype

def prepare_dataset(base_path, classes):
    """ Load and prepare dataset for binary classification """
    X, y = [], []
    for idx, class_name in enumerate(classes):
        class_path = os.path.join(base_path, class_name)
        data, labels = load_images_from_folder(class_path, idx)
        X.append(data)
        y.append(labels)
    
    X_combined = np.vstack(X)
    y_combined = np.hstack(y)
    
    # Ensure data is C-contiguous and float64
    return ensure_c_contiguous(X_combined), y_combined


# In[4]:



def main():
    base_train = "../data/Q2/train"
    base_test = "../data/Q2/test"
    class_folders = sorted(os.listdir(base_train))  # Ensure consistent ordering
    classes = [class_folders[d] for d in range(11)]

    # Load and prepare data with C-contiguous arrays
    X_train, y_train = prepare_dataset(base_train, classes)
    X_test, y_test = prepare_dataset(base_test, classes)
    
    # Print data format details
    print(f"X_train shape: {X_train.shape}, dtype: {X_train.dtype}")
    print(f"X_train is C-contiguous: {X_train.flags.c_contiguous}")
    
    # Perform cross-validation
    print("\nPerforming 5-fold cross-validation...")
    best_C, cv_results = k_fold_cross_validation(X_train, y_train, X_test, y_test)
    
    # Plot results
    print("\nPlotting results...")
    plot_results(cv_results)
    
    # Train final model with best C
    print("\nTraining final model...")
    final_model, final_acc = train_final_model(X_train, y_train, X_test, y_test, best_C)
    
    # Report results
    print("\nResults Summary:")
    print(f"Best C value from 5-fold CV: {best_C}")
    print(f"CV accuracy with C = {best_C}: {cv_results['cv_accuracies'][cv_results['C_values'].index(best_C)]:.4f}")
    print(f"Test accuracy with best C: {final_acc:.4f}")
    
    # Compare with previous test accuracy (C=1.0)
    previous_C_idx = cv_results['C_values'].index(1.0)
    previous_test_acc = cv_results['test_accuracies'][previous_C_idx]
    
    print(f"\nComparison with C=1.0:")
    print(f"Test accuracy with C=1.0: {previous_test_acc:.4f}")
    accuracy_diff = final_acc - previous_test_acc
    print(f"Improvement: {accuracy_diff:.4f} ({accuracy_diff*100:.2f}%)")

if __name__ == "__main__":
    main()


# In[ ]:








import cvxopt
import numpy as np

class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alphas = None
        self.kernel = None
        self.b = None
        self.w = None
        self.gamma = None
        self.support_vectors = None
        self.support_vector_labels = None

    def linear_kernel(self, X):
        return np.dot(X, X.T)
    
    def gaussian_kernel(self, X, gamma):
        X_norm = np.sum(X**2, axis=1).reshape(-1, 1)
        return np.exp(-gamma * (X_norm + X_norm.T - 2 * np.dot(X, X.T)))
    
    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        self.kernel = kernel
        self.gamma = gamma
        m, n = X.shape
        y = y.astype(float)
        y[y == 0] = -1  # Convert {0,1} labels to {-1,1}

        # Compute Kernel Matrix
        if kernel == 'linear':
            K = self.linear_kernel(X)
        elif kernel == 'gaussian':
            K = self.gaussian_kernel(X, gamma)
        
        # Quadratic Programming Setup
        P = cvxopt.matrix(np.outer(y, y) * K, tc='d')
        q = cvxopt.matrix(-np.ones(m), tc='d')
        
        G = cvxopt.matrix(np.vstack((-np.eye(m), np.eye(m))), tc='d')
        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * C)), tc='d')
        
        A = cvxopt.matrix(y, (1, m), tc='d')
        b = cvxopt.matrix(0.0, tc='d')
        
        cvxopt.solvers.options['show_progress'] = False
        sol = cvxopt.solvers.qp(P, q, G, h, A, b)
        
        alpha = np.ravel(sol['x'])
        
        # Support Vectors
        sv_indices = alpha &gt; 1e-5
        self.alphas = alpha[sv_indices]
        self.support_vectors = X[sv_indices]
        self.support_vector_labels = y[sv_indices]
        
        if kernel == 'linear':
            self.w = np.sum(self.alphas[:, None] * self.support_vector_labels[:, None] * self.support_vectors, axis=0)
        else:
            self.w = None
        
        # Compute bias term b
        self.b = np.mean(
            self.support_vector_labels - np.sum(self.alphas * self.support_vector_labels * K[sv_indices][:, sv_indices], axis=1)
        )
    
    def project(self, X):
        if self.kernel == 'linear':
            return np.dot(X, self.w) + self.b
        else:
            X_norm = np.sum(X**2, axis=1).reshape(-1, 1)  # Shape (N, 1)
            SV_norm = np.sum(self.support_vectors**2, axis=1).reshape(1, -1)  # Shape (1, M)
            
            # Compute squared distances (N, M)
            pairwise_sq_dists = X_norm + SV_norm - 2 * np.dot(X, self.support_vectors.T)
            
            # Compute the kernel matrix
            K = np.exp(-self.gamma * pairwise_sq_dists)  # (N, M)
            
            return np.dot(K, self.alphas * self.support_vector_labels) + self.b

                 
    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        return (self.project(X) &gt; 0).astype(int)


</PRE>
</PRE>
</BODY>
</HTML>
