<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_AAPWP.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_AAPWP.py<p><PRE>


import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from naive_bayes import NaiveBayes
import re

def tokenize(text):
    if isinstance(text,str):
        text=re.sub(r'[^\w\s]','',text.lower())
        return text.split()
    return []

train_df=pd.read_csv('train.csv')
test_df=pd.read_csv('test.csv')

train_df['Tokenized Description']=train_df['Description'].apply(tokenize)
test_df['Tokenized Description']=test_df['Description'].apply(tokenize)

nb_model=NaiveBayes()
nb_model.fit(train_df,smoothening=1.0)

train_predictions=nb_model.predict(train_df.copy())
train_accuracy=accuracy_score(train_df['Class Index'],train_predictions['Predicted'])
print(f"Training Accuracy: {train_accuracy:.4f}")

test_predictions=nb_model.predict(test_df.copy())
test_accuracy=accuracy_score(test_df['Class Index'],test_predictions['Predicted'])
print(f"Testing Accuracy: {test_accuracy:.4f}")


class_names = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Science/Technology'}
plt.figure(figsize=(20, 15))
for class_idx in range(1, 5):
    class_docs = train_df[train_df['Class Index'] == class_idx]['Description']
    text = ' '.join(class_docs)
    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)
    plt.subplot(2, 2, class_idx)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for {class_names[class_idx]}')
    plt.axis('off')

plt.tight_layout()
plt.savefig('word_clouds.png')
plt.show()




import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from naive_bayes import NaiveBayes
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer


stemmer=PorterStemmer()
stop_words=set(stopwords.words('english'))

def tokenize(text):
    if isinstance(text,str):
        text=re.sub(r'[^\w\s]','',text.lower())
        tokens=text.split()
        processed_tokens=[stemmer.stem(token) for token in tokens if token not in stop_words]
        return processed_tokens
    return []

train_df=pd.read_csv('train.csv')
test_df=pd.read_csv('test.csv')

train_df['Tokenized Description']=train_df['Title'].apply(tokenize)
test_df['Tokenized Description']=test_df['Title'].apply(tokenize)

nb_model=NaiveBayes()
nb_model.fit(train_df,smoothening=1.0)

train_predictions=nb_model.predict(train_df.copy())
train_accuracy=accuracy_score(train_df['Class Index'],train_predictions['Predicted'])
print(f"Training Accuracy: {train_accuracy:.4f}")

test_predictions=nb_model.predict(test_df.copy())
test_accuracy=accuracy_score(test_df['Class Index'],test_predictions['Predicted'])
print(f"Testing Accuracy: {test_accuracy:.4f}")


class_names = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Science/Technology'}
plt.figure(figsize=(20, 15))
for class_idx in range(1, 5):
    class_docs = train_df[train_df['Class Index'] == class_idx]['Title']
    text = ' '.join(class_docs)
    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)
    plt.subplot(2, 2, class_idx)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for {class_names[class_idx]}')
    plt.axis('off')

plt.tight_layout()
plt.savefig('word_clouds_stemmed.png')
plt.show()




import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from naive_bayes import NaiveBayes
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download NLTK resources (uncomment these lines if needed)
# nltk.download('stopwords')
# nltk.download('punkt')

# Initialize stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Function to load the data
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Function to preprocess text: tokenize, remove stopwords, and apply stemming (from part 2)
def preprocess_text(text):
    if not isinstance(text, str):
        return []
    
    # Convert to lowercase and remove punctuation
    text = re.sub(r'[^\w\s]', '', text.lower())
    
    # Tokenize
    tokens = text.split()
    
    # Remove stopwords and apply stemming
    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]
    
    return processed_tokens

# Function to generate bigrams from tokens
def generate_bigrams(tokens):
    bigrams = []
    for i in range(len(tokens) - 1):
        bigram = tokens[i] + " " + tokens[i + 1]
        bigrams.append(bigram)
    return bigrams

# Function to combine unigrams and bigrams
def combine_unigrams_bigrams(unigrams, bigrams):
    return unigrams + bigrams

# Load the data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Apply preprocessing from part 2 (stemming and stopword removal)
train_df['Unigrams'] = train_df['Title'].apply(preprocess_text)
test_df['Unigrams'] = test_df['Title'].apply(preprocess_text)

# Generate bigrams
train_df['Bigrams'] = train_df['Unigrams'].apply(generate_bigrams)
test_df['Bigrams'] = test_df['Unigrams'].apply(generate_bigrams)

# Combine unigrams and bigrams
train_df['Combined_Features'] = train_df.apply(
    lambda row: combine_unigrams_bigrams(row['Unigrams'], row['Bigrams']), axis=1)
test_df['Combined_Features'] = test_df.apply(
    lambda row: combine_unigrams_bigrams(row['Unigrams'], row['Bigrams']), axis=1)

# Train model using only unigrams (similar to part 2)
unigram_model = NaiveBayes()
unigram_model.fit(train_df, smoothening=1.0, text_col="Unigrams")

# Evaluate unigram model on training set
unigram_train_predictions = unigram_model.predict(train_df.copy(), text_col="Unigrams")
unigram_train_accuracy = accuracy_score(train_df['Class Index'], unigram_train_predictions['Predicted'])

# Evaluate unigram model on test set
unigram_test_predictions = unigram_model.predict(test_df.copy(), text_col="Unigrams")
unigram_test_accuracy = accuracy_score(test_df['Class Index'], unigram_test_predictions['Predicted'])

# Train model using combined unigrams and bigrams
combined_model = NaiveBayes()
combined_model.fit(train_df, smoothening=1.0, text_col="Combined_Features")

# Evaluate combined model on training set
combined_train_predictions = combined_model.predict(train_df.copy(), text_col="Combined_Features")
combined_train_accuracy = accuracy_score(train_df['Class Index'], combined_train_predictions['Predicted'])

# Evaluate combined model on test set
combined_test_predictions = combined_model.predict(test_df.copy(), text_col="Combined_Features")
combined_test_accuracy = accuracy_score(test_df['Class Index'], combined_test_predictions['Predicted'])

# Print results
print("Model with Unigrams Only:")
print(f"Training Accuracy: {unigram_train_accuracy:.4f}")
print(f"Test Accuracy: {unigram_test_accuracy:.4f}")
print("\nModel with Unigrams and Bigrams:")
print(f"Training Accuracy: {combined_train_accuracy:.4f}")
print(f"Test Accuracy: {combined_test_accuracy:.4f}")
print("\nImprovement:")
print(f"Training Accuracy Improvement: {combined_train_accuracy - unigram_train_accuracy:.4f}")
print(f"Test Accuracy Improvement: {combined_test_accuracy - unigram_test_accuracy:.4f}")

# Print example of how text processing works
sample_text = "Machine learning is fascinating technology"
print("\nExample Processing:")
print(f"Original text: {sample_text}")
unigrams = preprocess_text(sample_text)
print(f"After preprocessing (unigrams): {unigrams}")
bigrams = generate_bigrams(unigrams)
print(f"Bigrams: {bigrams}")
combined = combine_unigrams_bigrams(unigrams, bigrams)
print(f"Combined features: {combined}")



import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from naive_bayes import NaiveBayes
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
import time

# Initialize stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Function to load the data
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Basic tokenization function (no stopword removal or stemming)
def basic_tokenize(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    return text.split()

# Tokenize with stopword removal but no stemming
def remove_stopwords(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [token for token in tokens if token not in stop_words]

# Tokenize with stemming but no stopword removal
def apply_stemming(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [stemmer.stem(token) for token in tokens]

# Full preprocessing: tokenize, remove stopwords, and apply stemming
def full_preprocess(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [stemmer.stem(token) for token in tokens if token not in stop_words]

# Function to generate bigrams
def generate_bigrams(tokens):
    return [tokens[i] + " " + tokens[i+1] for i in range(len(tokens)-1)]

# Function to calculate all evaluation metrics
def evaluate_model(true_labels, predictions, class_names):
    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')
    
    # Class-wise metrics
    class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(true_labels, predictions, average=None)
    
    # Confusion matrix
    cm = confusion_matrix(true_labels, predictions)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'class_precision': class_precision,
        'class_recall': class_recall,
        'class_f1': class_f1,
        'confusion_matrix': cm
    }

# Function to train and evaluate different models
def run_model_evaluation(train_df, test_df, preprocessing_functions, feature_types):
    results = []
    
    for preprocessing_name, preprocessing_func in preprocessing_functions.items():
        for feature_type in feature_types:
            print(f"Evaluating: {preprocessing_name} with {feature_type}")
            
            # Prepare features based on preprocessing and feature type
            train_df['Processed'] = train_df['Title'].apply(preprocessing_func)
            test_df['Processed'] = test_df['Title'].apply(preprocessing_func)
            
            if feature_type == 'unigrams':
                features_train = train_df['Processed']
                features_test = test_df['Processed']
            elif feature_type == 'bigrams':
                train_df['Bigrams'] = train_df['Processed'].apply(generate_bigrams)
                test_df['Bigrams'] = test_df['Processed'].apply(generate_bigrams)
                features_train = train_df['Bigrams']
                features_test = test_df['Bigrams']
            else:  # combined
                train_df['Bigrams'] = train_df['Processed'].apply(generate_bigrams)
                test_df['Bigrams'] = test_df['Processed'].apply(generate_bigrams)
                features_train = train_df.apply(lambda x: x['Processed'] + x['Bigrams'], axis=1)
                features_test = test_df.apply(lambda x: x['Processed'] + x['Bigrams'], axis=1)
            
            # Train model
            start_time = time.time()
            model = NaiveBayes()
            train_df_copy=train_df.copy()
            train_df_copy['features_train'] = features_train
            model.fit(train_df_copy, smoothening=1.0, text_col='features_train')
            training_time = time.time() - start_time
            
            # Predict and evaluate
            test_df_copy = test_df.copy()
            test_df_copy['features_train'] = features_test
            predictions = model.predict(test_df_copy, text_col='features_train')['Predicted']
            
            # Calculate metrics
            metrics = evaluate_model(test_df['Class Index'], predictions, 
                                    ['World', 'Sports', 'Business', 'Science/Tech'])
            
            # Calculate vocabulary size
            if feature_type == 'unigrams':
                vocab_size = len(set(word for doc in features_train for word in doc))
            elif feature_type == 'bigrams':
                vocab_size = len(set(bigram for doc in features_train for bigram in doc))
            else:
                unigram_vocab = set(word for doc in train_df['Processed'] for word in doc)
                bigram_vocab = set(bigram for doc in train_df['Bigrams'] for bigram in doc)
                vocab_size = len(unigram_vocab) + len(bigram_vocab)
            
            # Store results
            results.append({
                'preprocessing': preprocessing_name,
                'feature_type': feature_type,
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1'],
                'class_precision': metrics['class_precision'],
                'class_recall': metrics['class_recall'],
                'class_f1': metrics['class_f1'],
                'confusion_matrix': metrics['confusion_matrix'],
                'training_time': training_time,
                'vocab_size': vocab_size
            })
    
    return results

# Load data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Define preprocessing functions
preprocessing_functions = {
    'Basic': basic_tokenize,
    'StopwordRemoval': remove_stopwords,
    'Stemming': apply_stemming,
    'Full': full_preprocess
}

# Define feature types
feature_types = ['unigrams', 'bigrams', 'combined']

# Run the evaluation
results = run_model_evaluation(train_df, test_df, preprocessing_functions, feature_types)

# Convert results to DataFrame for easier analysis
results_df = pd.DataFrame([
    {
        'preprocessing': r['preprocessing'],
        'feature_type': r['feature_type'],
        'accuracy': r['accuracy'],
        'precision': r['precision'],
        'recall': r['recall'],
        'f1': r['f1'],
        'training_time': r['training_time'],
        'vocab_size': r['vocab_size']
    } for r in results
])

# Sort by accuracy
results_df = results_df.sort_values('accuracy', ascending=False)

# Display top 5 models
print("Top 5 Models by Accuracy:")
print(results_df.head(5))

# Create visualization comparing all models
plt.figure(figsize=(12, 8))
sns.barplot(x='preprocessing', y='accuracy', hue='feature_type', data=results_df)
plt.title('Model Accuracy by Preprocessing Method and Feature Type')
plt.xlabel('Preprocessing Method')
plt.ylabel('Accuracy')
plt.tight_layout()
plt.savefig('model_comparison.png')

# Analyze the best model in more detail
best_model_idx = results_df['accuracy'].idxmax()
best_model = results[best_model_idx]

print("\nBest Model Analysis:")
<A NAME="5"></A><FONT color = #FF0000><A HREF="match218-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

print(f"Preprocessing: {best_model['preprocessing']}")
print(f"Feature Type: {best_model['feature_type']}")
print(f"Accuracy: {best_model['accuracy']:.4f}")
print(f"Precision: {best_model['precision']:.4f}")
print(f"Recall: {best_model['recall']:.4f}")
print(f"F1 Score: {best_model['f1']:.4f}")
print(f"Vocabulary Size: {best_model['vocab_size']}")
print(f"Training Time: {best_model['training_time']:.2f} seconds")

# Plot confusion matrix for best model
plt.figure(figsize=(10, 8))
</FONT>sns.heatmap(best_model['confusion_matrix'], annot=True, fmt='d', cmap='Blues',
            xticklabels=['World', 'Sports', 'Business', 'Science/Tech'],
            yticklabels=['World', 'Sports', 'Business', 'Science/Tech'])
plt.title(f'Confusion Matrix - {best_model["preprocessing"]} with {best_model["feature_type"]}')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.savefig('best_model_confusion_matrix.png')

# Calculate class-wise metrics for best model
class_metrics = pd.DataFrame({
    'Class': ['World', 'Sports', 'Business', 'Science/Tech'],
    'Precision': best_model['class_precision'],
    'Recall': best_model['class_recall'],
    'F1 Score': best_model['class_f1']
})

print("\nClass-wise Performance for Best Model:")
print(class_metrics)

# Analyze the impact of preprocessing and feature type
preprocessing_impact = results_df.groupby('preprocessing')[['accuracy', 'precision', 'recall', 'f1']].mean()
feature_impact = results_df.groupby('feature_type')[['accuracy', 'precision', 'recall', 'f1']].mean()

print("\nAverage Impact of Preprocessing Methods:")
print(preprocessing_impact)

print("\nAverage Impact of Feature Types:")
print(feature_impact)



import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from naive_bayes import NaiveBayes
import re
import time

# Initialize stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Function to load the data
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Full preprocessing: tokenize, remove stopwords, and apply stemming
# Assuming this was the best preprocessing approach from previous analysis
def full_preprocess(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [stemmer.stem(token) for token in tokens if token not in stop_words]

def remove_stopwords(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [token for token in tokens if token not in stop_words]

# Function to generate bigrams
def generate_bigrams(tokens):
    return [tokens[i] + " " + tokens[i+1] for i in range(len(tokens)-1)]

# Function to combine unigrams and bigrams
# Assuming combined features performed best in previous analysis
def get_combined_features(text):
    unigrams = full_preprocess(text)
    bigrams = generate_bigrams(unigrams)
    return unigrams + bigrams

def get_combined_features_stopwordonly(text):
    unigrams=remove_stopwords(text)
    bigrams = generate_bigrams(unigrams)
    return unigrams + bigrams

# Function to evaluate model performance
def evaluate_model(true_labels, predictions):
    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Load data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Process title and description separately
print("Processing title and description features...")
train_df['Title_Features'] = train_df['Title'].apply(get_combined_features)
train_df['Description_Features'] = train_df['Description'].apply(get_combined_features_stopwordonly)
test_df['Title_Features'] = test_df['Title'].apply(get_combined_features)
test_df['Description_Features'] = test_df['Description'].apply(get_combined_features_stopwordonly)

# Create combined features (concatenate title and description)
print("Creating combined features...")
train_df['Combined_Features'] = train_df.apply(lambda x: x['Title_Features'] + x['Description_Features'], axis=1)
test_df['Combined_Features'] = test_df.apply(lambda x: x['Title_Features'] + x['Description_Features'], axis=1)

# Train model using only title features
print("Training title-only model...")
title_model = NaiveBayes()
title_model.fit(train_df, smoothening=1.0, text_col="Title_Features")

# Evaluate title model
title_predictions = title_model.predict(test_df.copy(), text_col="Title_Features")
title_metrics = evaluate_model(test_df['Class Index'], title_predictions['Predicted'])

# Train model using only description features
print("Training description-only model...")
desc_model = NaiveBayes()
desc_model.fit(train_df, smoothening=1.0, text_col="Description_Features")

# Evaluate description model
desc_predictions = desc_model.predict(test_df.copy(), text_col="Description_Features")
desc_metrics = evaluate_model(test_df['Class Index'], desc_predictions['Predicted'])

# Train model using combined features
print("Training combined model...")
combined_model = NaiveBayes()
combined_model.fit(train_df, smoothening=1.0, text_col="Combined_Features")

# Evaluate combined model
combined_predictions = combined_model.predict(test_df.copy(), text_col="Combined_Features")
combined_metrics = evaluate_model(test_df['Class Index'], combined_predictions['Predicted'])

# Print results
print("\nModel Performance Comparison:")
print("-" * 50)
print(f"Title-only Model Accuracy: {title_metrics['accuracy']:.4f}")
print(f"Title-only Model F1 Score: {title_metrics['f1']:.4f}")
print("-" * 50)
print(f"Description-only Model Accuracy: {desc_metrics['accuracy']:.4f}")
print(f"Description-only Model F1 Score: {desc_metrics['f1']:.4f}")
print("-" * 50)
print(f"Combined Model Accuracy: {combined_metrics['accuracy']:.4f}")
print(f"Combined Model F1 Score: {combined_metrics['f1']:.4f}")
print("-" * 50)

# Calculate improvements
title_improvement = combined_metrics['accuracy'] - title_metrics['accuracy']
desc_improvement = combined_metrics['accuracy'] - desc_metrics['accuracy']

print("\nImprovements:")
print(f"Improvement over title-only: {title_improvement:.4f} ({title_improvement*100:.2f}%)")
print(f"Improvement over description-only: {desc_improvement:.4f} ({desc_improvement*100:.2f}%)")

# Create bar chart to visualize accuracy comparison
models = ['Title Only', 'Description Only', 'Combined']
accuracies = [title_metrics['accuracy'], desc_metrics['accuracy'], combined_metrics['accuracy']]
f1_scores = [title_metrics['f1'], desc_metrics['f1'], combined_metrics['f1']]

plt.figure(figsize=(10, 6))
x = np.arange(len(models))
width = 0.35

plt.bar(x - width/2, accuracies, width, label='Accuracy')
plt.bar(x + width/2, f1_scores, width, label='F1 Score')

plt.xlabel('Model Type')
plt.ylabel('Score')
plt.title('Performance Comparison of Different Feature Sets')
plt.xticks(x, models)
plt.ylim(0, 1.0)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)

for i, v in enumerate(accuracies):
    plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')

for i, v in enumerate(f1_scores):
    plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.show()



# Calculating accuracy for NaiveBayesSeparateParams model
import pandas as pd
import numpy as np
import re
from naive_bayes_separate import NaiveBayesSep

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
# Assuming you've already loaded your train_df and test_df with tokenized title and description
# And you've already created and trained your NaiveBayesSeparateParams model as nb_separate



# Initialize stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Function to load the data
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Full preprocessing: tokenize, remove stopwords, and apply stemming
# Assuming this was the best preprocessing approach from previous analysis
def full_preprocess(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [stemmer.stem(token) for token in tokens if token not in stop_words]

def remove_stopwords(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [token for token in tokens if token not in stop_words]

# Function to generate bigrams
def generate_bigrams(tokens):
    return [tokens[i] + " " + tokens[i+1] for i in range(len(tokens)-1)]

# Function to combine unigrams and bigrams
# Assuming combined features performed best in previous analysis
def get_combined_features(text):
    unigrams = full_preprocess(text)
    bigrams = generate_bigrams(unigrams)
    return unigrams + bigrams

def get_combined_features_stopwordonly(text):
    unigrams=remove_stopwords(text)
    bigrams = generate_bigrams(unigrams)
    return unigrams + bigrams

# Function to evaluate model performance

# Load data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Process title and description separately
print("Processing title and description features...")
train_df['Tokenized Title'] = train_df['Title'].apply(get_combined_features)
train_df['Tokenized Description'] = train_df['Description'].apply(get_combined_features_stopwordonly)
test_df['Tokenized Title'] = test_df['Title'].apply(get_combined_features)
test_df['Tokenized Description'] = test_df['Description'].apply(get_combined_features_stopwordonly)


nb_separate=NaiveBayesSep()
nb_separate.fit(train_df, smoothening=1.0, 
                class_col="Class Index", 
                title_col="Tokenized Title", 
                desc_col="Tokenized Description")
# Make predictions
test_predictions = nb_separate.predict(test_df)
train_predictions = nb_separate.predict(train_df)

# Calculate test accuracy
test_correct = sum(test_predictions["Predicted"] == test_df["Class Index"])
test_total = len(test_df)
test_accuracy = test_correct / test_total

# Calculate training accuracy
train_correct = sum(train_predictions["Predicted"] == train_df["Class Index"])
train_total = len(train_df)
train_accuracy = train_correct / train_total

# Print results
print(f"NaiveBayes with Separate Parameters for Title and Description:")
print(f"Training accuracy: {train_accuracy:.4f}")
print(f"Test accuracy: {test_accuracy:.4f}")

# Compare with previous models (assuming you have these values from previous runs)
# Replace these values with your actual previous results
prev_desc_only_accuracy = 0.9047  # Replace with your result from best description-only model
prev_title_only_accuracy = 0.8745  # Replace with your result from best title-only model
prev_concat_accuracy = 0.9125  # Replace with your result from model 6(a) - concatenation

print("\nComparison with previous models:")
print(f"Best description-only model: {prev_desc_only_accuracy:.4f}")
print(f"Best title-only model: {prev_title_only_accuracy:.4f}")
print(f"Concatenated model (6a): {prev_concat_accuracy:.4f}")
print(f"Separate parameters model (6b): {test_accuracy:.4f}")

# Calculate improvements
improvement_over_desc = test_accuracy - prev_desc_only_accuracy
improvement_over_title = test_accuracy - prev_title_only_accuracy
improvement_over_concat = test_accuracy - prev_concat_accuracy

print("\nImprovement over previous models:")
print(f"vs. Best description-only: {improvement_over_desc:.4f}")
print(f"vs. Best title-only: {improvement_over_title:.4f}")
print(f"vs. Concatenated: {improvement_over_concat:.4f}")

# Optional: Calculate precision, recall, F1-score by class
from sklearn.metrics import classification_report

print("\nDetailed classification report:")
print(classification_report(test_df["Class Index"], test_predictions["Predicted"]))




import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from naive_bayes import NaiveBayes
import re

stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))


def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Full preprocessing: tokenize, remove stopwords, and apply stemming
# Assuming this was the best preprocessing approach from previous analysis
def full_preprocess(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [stemmer.stem(token) for token in tokens if token not in stop_words]

def remove_stopwords(text):
    if not isinstance(text, str):
        return []
    text = re.sub(r'[^\w\s]', '', text.lower())
    tokens = text.split()
    return [token for token in tokens if token not in stop_words]

# Function to generate bigrams
def generate_bigrams(tokens):
    return [tokens[i] + " " + tokens[i+1] for i in range(len(tokens)-1)]

# Function to combine unigrams and bigrams
# Assuming combined features performed best in previous analysis
def get_combined_features(text):
    unigrams = full_preprocess(text)
    bigrams = generate_bigrams(unigrams)
    return unigrams + bigrams


def add_category_keyword_features(df, text_col="Tokenized Description", title_col="Tokenized Title"):
    """
    Add category-specific keyword presence features to improve classification
    
    Args:
        df (pd.DataFrame): DataFrame containing tokenized text data
        text_col (str): Column name for tokenized description
        title_col (str): Column name for tokenized title
        
    Returns:
        pd.DataFrame: DataFrame with additional keyword features
    """
    # Define strong indicator keywords for each category
    business_keywords = ['market', 'stock', 'price', 'economy', 'investment', 'profit', 'revenue', 
                         'dollar', 'finance', 'bank', 'company', 'business', 'corporate', 'trade', 
                         'economic', 'financial', 'industry', 'commercial', 'sale', 'merger']
    
    tech_keywords = ['technology', 'science', 'researcher', 'study', 'discovery', 'scientist', 
                     'computer', 'software', 'digital', 'internet', 'data', 'device', 'innovation', 
                     'engineering', 'laboratory', 'experiment', 'tech', 'scientific', 'research', 'patent']
    
    sports_keywords = ['team', 'player', 'game', 'match', 'win', 'score', 'season', 'championship', 
                       'tournament', 'coach', 'league', 'athlete', 'sport', 'competitor', 'victory', 
                       'defeat', 'ball', 'stadium', 'race', 'olympic']
    
    world_keywords = ['country', 'government', 'president', 'minister', 'international', 'nation', 
                      'war', 'policy', 'foreign', 'election', 'political', 'diplomat', 'military', 
                      'leader', 'global', 'crisis', 'conflict', 'treaty', 'agreement', 'peace']
    
    # Create a copy of the dataframe to avoid modifying the original
    df_new = df.copy()
    
    # Function to count keyword occurrences and create binary features
    def count_keywords(tokens, keyword_list):
        if not tokens:
            return 0
        # Convert all tokens to lowercase for case-insensitive matching
        lowercase_tokens = [t.lower() for t in tokens]
        # Count how many keywords from the list appear in the tokens
        count = sum(1 for kw in keyword_list if kw in lowercase_tokens)
        return count
    
    # Add keyword features for description
    df_new['desc_business_kw'] = df_new[text_col].apply(lambda x: count_keywords(x, business_keywords))
    df_new['desc_tech_kw'] = df_new[text_col].apply(lambda x: count_keywords(x, tech_keywords))
    df_new['desc_sports_kw'] = df_new[text_col].apply(lambda x: count_keywords(x, sports_keywords))
    df_new['desc_world_kw'] = df_new[text_col].apply(lambda x: count_keywords(x, world_keywords))
    
    # Add keyword features for title (titles often contain very strong category signals)
    if title_col in df_new.columns:
        df_new['title_business_kw'] = df_new[title_col].apply(lambda x: count_keywords(x, business_keywords))
        df_new['title_tech_kw'] = df_new[title_col].apply(lambda x: count_keywords(x, tech_keywords))
        df_new['title_sports_kw'] = df_new[title_col].apply(lambda x: count_keywords(x, sports_keywords))
        df_new['title_world_kw'] = df_new[title_col].apply(lambda x: count_keywords(x, world_keywords))
    
    return df_new

class EnhancedNaiveBayes(NaiveBayes):
    def __init__(self):
        super().__init__()
        self.num_feature_means = {}
        self.num_feature_vars = {}
        self.num_features = []
    
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description", num_features=None):
        """
        Enhanced fit method that handles both text and numerical features
        
        Args:
            df (pd.DataFrame): The training data
            smoothening (float): Laplace smoothing parameter
            class_col (str): Column containing class labels
            text_col (str): Column containing tokenized text
            num_features (list): List of numerical feature column names
        """
        # First, fit the text features using the parent class method
        super().fit(df, smoothening, class_col, text_col)
        
        # If no numerical features specified, we're done
        if num_features is None or len(num_features) == 0:
            return
        
        self.num_features = num_features
        
        # For each class, calculate mean and variance of numerical features
        # We'll use Gaussian Naive Bayes for numerical features
        for cls in range(1, self.num_classes + 1):
            class_data = df[df[class_col] == cls]
            
            # Calculate means and variances for each numerical feature
            means = {}
            vars = {}
            
            for feature in num_features:
                means[feature] = class_data[feature].mean()
                # Add small epsilon to variance to avoid division by zero
                vars[feature] = class_data[feature].var() + 1e-10
            
            self.num_feature_means[cls] = means
            self.num_feature_vars[cls] = vars
    
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Enhanced predict method that handles both text and numerical features
        
        Args:
            df (pd.DataFrame): The testing data
            text_col (str): Column containing tokenized text
            predicted_col (str): Column name for predicted class
            
        Returns:
            pd.DataFrame: DataFrame with predictions
        """
        predictions = []
        
        for _, row in df.iterrows():
            tokens = row[text_col]
            class_score = {}
            
            for cls in range(1, self.num_classes + 1):
                # Start with class prior
                score = self.class_priors[cls]
                
                # Add text feature scores (from parent class method)
                for word in tokens:
                    if word in self.word_probs[cls]:
                        score += self.word_probs[cls][word]
                    else:
                        score += self.default_probs[cls]
                
                # Add numerical feature scores using Gaussian Naive Bayes
                for feature in self.num_features:
                    if feature in row and not pd.isna(row[feature]):
                        value = row[feature]
                        mean = self.num_feature_means[cls][feature]
                        var = self.num_feature_vars[cls][feature]
                        
                        # Log probability of Gaussian distribution
                        log_prob = -0.5 * np.log(2 * np.pi * var) - 0.5 * ((value - mean) ** 2) / var
                        score += log_prob
                
                class_score[cls] = score
            
            predicted_class = max(class_score, key=class_score.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df
    
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Process title and description separately
print("Processing title and description features...")
train_df['Tokenized Title'] = train_df['Title'].apply(get_combined_features)
train_df['Tokenized Description'] = train_df['Description'].apply(get_combined_features)
test_df['Tokenized Title'] = test_df['Title'].apply(get_combined_features)
test_df['Tokenized Description'] = test_df['Description'].apply(get_combined_features)


# Create combined features (concatenate title and description)
# Process your data to add keyword features
train_df_enhanced = add_category_keyword_features(train_df, "Tokenized Description", "Tokenized Title")
test_df_enhanced = add_category_keyword_features(test_df, "Tokenized Description", "Tokenized Title")

# List of numerical features we added
num_features = [
    'desc_business_kw', 'desc_tech_kw', 'desc_sports_kw', 'desc_world_kw',
    'title_business_kw', 'title_tech_kw', 'title_sports_kw', 'title_world_kw'
]

# Train the enhanced model
model = EnhancedNaiveBayes()
model.fit(train_df_enhanced, smoothening=1.0, class_col="Class Index", 
          text_col="Tokenized Description", num_features=num_features)

# Make predictions
test_df_enhanced = model.predict(test_df_enhanced, text_col="Tokenized Description", predicted_col="Predicted")

# Calculate accuracy
accuracy = (test_df_enhanced["Predicted"] == test_df_enhanced["Class Index"]).mean()
print(f"Enhanced model accuracy: {accuracy:.4f}")



import numpy as np
import pandas as pd
class NaiveBayes:
    def __init__(self):
        self.class_priors=None
        self.word_probs=None
        self.vocabulary=None
        self.num_classes=None
        self.default_probs=None
        pass
        
    def fit(self, df, smoothening, class_col = "Class Index", text_col = "Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """

        self.num_classes=df[class_col].max()

        class_counts=df[class_col].value_counts()
        total_samples=len(df)
        self.class_priors={cls: np.log(count/total_samples) for cls,count in class_counts.items()}
        all_words=[]
        for tokens in df[text_col]:
            all_words.extend(tokens)
        self.vocabulary=set(all_words)
        vocab_size=len(self.vocabulary)
        self.word_probs={}
        self.default_probs={}
        
        for cls in range(1,self.num_classes+1):
            class_docs=df[df[class_col]==cls][text_col]
            word_counts={}
            for tokens in class_docs:
                for word in tokens:
                    word_counts[word] = word_counts.get(word, 0) + 1    
            total_words=sum(word_counts.values())
            class_word_prob={}
            for word in self.vocabulary:
                prob=(word_counts.get(word,0)+smoothening)/(total_words+smoothening*vocab_size)
                class_word_prob[word]=np.log(prob)
            self.word_probs[cls]=class_word_prob
            self.default_probs[cls]=np.log(smoothening/(total_words+smoothening*vocab_size)) 



        pass
    
    def predict(self, df, text_col = "Tokenized Description", predicted_col = "Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
<A NAME="2"></A><FONT color = #0000FF><A HREF="match218-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        predictions=[]
        for _,row in df.iterrows():
            tokens=row[text_col]
</FONT>            class_score={}
            for cls in range(1,self.num_classes+1):
                score=self.class_priors[cls]
                for word in tokens:
                    if word in self.word_probs[cls]:
                        score+=self.word_probs[cls][word]
                    else:
                        score+=self.default_probs[cls]
                class_score[cls]=score
            predicted_class=max(class_score,key=class_score.get)
            predictions.append(predicted_class)
        df[predicted_col]=predictions
        return df
        pass



import numpy as np
import pandas as pd
from collections import Counter

class NaiveBayesSep:
    def __init__(self):
        self.class_priors = None
        self.title_word_probs = None
        self.desc_word_probs = None
        self.title_vocabulary = None
        self.desc_vocabulary = None
        self.num_classes = None
        self.title_default_probs = None
        self.desc_default_probs = None
        
    def fit(self, df, smoothening, class_col="Class Index", title_col="Tokenized Title", desc_col="Tokenized Description"):
        """Learn separate parameters for title and description from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col, title_col, and desc_col.
                Each entry of title_col and desc_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.num_classes = df[class_col].max()

        # Calculate class priors
        class_counts = df[class_col].value_counts()
        total_samples = len(df)
        self.class_priors = {cls: np.log(count/total_samples) for cls, count in class_counts.items()}
        
        # Process title vocabulary
        all_title_words = []
        for tokens in df[title_col]:
            all_title_words.extend(tokens)
        self.title_vocabulary = set(all_title_words)
        title_vocab_size = len(self.title_vocabulary)
        
        # Process description vocabulary
        all_desc_words = []
        for tokens in df[desc_col]:
            all_desc_words.extend(tokens)
        self.desc_vocabulary = set(all_desc_words)
        desc_vocab_size = len(self.desc_vocabulary)
        
        # Initialize probability dictionaries
        self.title_word_probs = {}
        self.desc_word_probs = {}
        self.title_default_probs = {}
        self.desc_default_probs = {}
        
        # Calculate word probabilities for each class
        for cls in range(1, self.num_classes+1):
            class_docs = df[df[class_col] == cls]
            
            # Process title words
            title_word_counts = Counter()
            for tokens in class_docs[title_col]:
                title_word_counts.update(tokens)
            total_title_words = sum(title_word_counts.values())
            
            title_class_word_prob = {}
            for word in self.title_vocabulary:
                prob = (title_word_counts[word] + smoothening) / (total_title_words + smoothening * title_vocab_size)
                title_class_word_prob[word] = np.log(prob)
            self.title_word_probs[cls] = title_class_word_prob
            self.title_default_probs[cls] = np.log(smoothening / (total_title_words + smoothening * title_vocab_size))
            
            # Process description words
            desc_word_counts = Counter()
            for tokens in class_docs[desc_col]:
                desc_word_counts.update(tokens)
            total_desc_words = sum(desc_word_counts.values())
            
            desc_class_word_prob = {}
            for word in self.desc_vocabulary:
                prob = (desc_word_counts[word] + smoothening) / (total_desc_words + smoothening * desc_vocab_size)
                desc_class_word_prob[word] = np.log(prob)
            self.desc_word_probs[cls] = desc_class_word_prob
            self.desc_default_probs[cls] = np.log(smoothening / (total_desc_words + smoothening * desc_vocab_size))
    
    def predict(self, df, title_col="Tokenized Title", desc_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.
        Uses both title and description with separate parameters.

        Args:
            df (pd.DataFrame): The testing data containing columns title_col and desc_col.
                Each entry of title_col and desc_col is a list of tokens.
        """
        predictions = []
        for _, row in df.iterrows():
            title_tokens = row[title_col]
            desc_tokens = row[desc_col]
            class_score = {}
            
            for cls in range(1, self.num_classes+1):
                # Start with class prior
                score = self.class_priors[cls]
                
                # Add log probabilities from title
                for word in title_tokens:
                    if word in self.title_word_probs[cls]:
                        score += self.title_word_probs[cls][word]
                    else:
                        score += self.title_default_probs[cls]
                
                # Add log probabilities from description
                for word in desc_tokens:
                    if word in self.desc_word_probs[cls]:
                        score += self.desc_word_probs[cls][word]
                    else:
                        score += self.desc_default_probs[cls]
                
                class_score[cls] = score
            
            predicted_class = max(class_score, key=class_score.get)
            predictions.append(predicted_class)
        
        df[predicted_col] = predictions
        return df



import os
import numpy as np
import cv2
from glob import glob
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from svm import SupportVectorMachine  # Import the SVM class we just created

def preprocess_image(image_path):
    """
    Preprocess a single image according to the requirements:
    1. Resize to 100x100 pixels
    2. Center crop (if needed)
    3. Flatten to 1D vector
    4. Normalize to [0, 1]
    """
    # Read the image
    img = cv2.imread(image_path)
    
    if img is None:
        raise ValueError(f"Could not read image: {image_path}")
    
    # Resize to 100x100 pixels
    img = cv2.resize(img, (100, 100))
    
    # Flatten the image to 1D vector (100x100x3 = 30,000)
    flattened = img.flatten()
    
    # Normalize to [0, 1]
    normalized = flattened / 255.0
    
    return normalized

def load_dataset(base_dir, class_indices):
    """
    Load and preprocess images for specific classes
    base_dir: Path to the dataset
    class_indices: List of class indices to load
    """
    class_names = sorted(os.listdir(base_dir))
    selected_classes = [class_names[i] for i in class_indices]
    
    X = []
    y = []
    
    print(f"Selected classes: {selected_classes}")
    
    for i, class_name in enumerate(selected_classes):
        class_dir = os.path.join(base_dir, class_name)
        image_paths = glob(os.path.join(class_dir, "*.jpg"))
        
        print(f"Loading {len(image_paths)} images from class {class_name}")
        
        for img_path in image_paths:
            try:
                features = preprocess_image(img_path)
                X.append(features)
                y.append(i)  # Binary labels: 0 for first class, 1 for second class
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
    
    return np.array(X), np.array(y), selected_classes

def visualize_results(X_train, svm_model, class_names):
    """
    Visualize the top-5 support vectors and weight vector
    """
    # Get top-5 support vectors with highest alpha values
    top5_indices = svm_model.get_top_support_vectors(5)
    
    # Reshape the support vectors to images
<A NAME="1"></A><FONT color = #00FF00><A HREF="match218-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.figure(figsize=(15, 10))
    for i, idx in enumerate(top5_indices):
        img = X_train[idx].reshape(100, 100, 3)
        plt.subplot(2, 3, i+1)
        plt.imshow(img)
        
        # Get the alpha value and class
        alpha = svm_model.alphas[idx]
</FONT>        class_label = svm_model.y_train[idx]
        
        plt.title(f"SV #{i+1}, Alpha: {alpha:.4f}\nClass: {class_names[class_label]}")
        plt.axis('off')
    
    # Reshape and plot the weight vector
    w = svm_model.get_weights()
    w_img = w.reshape(100, 100, 3)
    
    # Normalize for visualization
    w_min, w_max = np.min(w_img), np.max(w_img)
    w_img = (w_img - w_min) / (w_max - w_min) if w_max &gt; w_min else w_img
    
    plt.subplot(2, 3, 6)
    plt.imshow(w_img)
    plt.title("Weight Vector")
    plt.axis('off')
    
    plt.tight_layout()
    plt.savefig('svm_visualization.png')
    plt.show()

def main():
    # Define class indices based on the last two digits of entry number
    # Change this to your entry number's last two digits
    d = 5
    class_indices = [d, (d + 1) % 11]
    print(f"Using classes with indices {class_indices}")
    
    # Load and preprocess the data
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"
    
    # Load training and test data
    X_train, y_train, class_names = load_dataset(train_dir, class_indices)
    X_test, y_test, _ = load_dataset(test_dir, class_indices)
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    
    # Train SVM using CVXOPT with linear kernel and C=1.0
    print("Training SVM model...")
    svm = SupportVectorMachine()
    svm.fit(X_train, y_train, kernel='linear', C=1.0)
    
    # (a) Count support vectors
    sv_count = svm.get_support_vectors_count()
    sv_percentage = svm.get_support_vectors_percentage()
    print(f"Number of support vectors: {sv_count}")
    print(f"Percentage of support vectors: {sv_percentage:.2f}%")
    
    # (b) Get weight vector and bias term
    w = svm.get_weights()
    b = svm.get_bias()
    print(f"Weight vector: {w}")
    print(f"Bias term: {b:.4f}")
    
    # Calculate test accuracy
    y_pred = svm.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred) * 100
    print(f"Test accuracy: {accuracy:.2f}%")
    
    # (c) Visualize top-5 support vectors and weight vector
    visualize_results(X_train, svm, class_names)

if __name__ == "__main__":
    main()



import os
import numpy as np
import cv2
from glob import glob
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from svm import SupportVectorMachine  # Import the SVM class we just created

def preprocess_image(image_path):
    """
    Preprocess a single image according to the requirements:
    1. Resize to 100x100 pixels
    2. Center crop (if needed)
    3. Flatten to 1D vector
    4. Normalize to [0, 1]
    """
    # Read the image
    img = cv2.imread(image_path)
    
    if img is None:
        raise ValueError(f"Could not read image: {image_path}")
    
    # Resize to 100x100 pixels
    img = cv2.resize(img, (100, 100))
    
    # Flatten the image to 1D vector (100x100x3 = 30,000)
    flattened = img.flatten()
    
    # Normalize to [0, 1]
    normalized = flattened / 255.0
    
    return normalized

def load_dataset(base_dir, class_indices):
    """
    Load and preprocess images for specific classes
    base_dir: Path to the dataset
    class_indices: List of class indices to load
    """
    class_names = sorted(os.listdir(base_dir))
    selected_classes = [class_names[i] for i in class_indices]
    
    X = []
    y = []
    
    print(f"Selected classes: {selected_classes}")
    
    for i, class_name in enumerate(selected_classes):
        class_dir = os.path.join(base_dir, class_name)
        image_paths = glob(os.path.join(class_dir, "*.jpg"))
        
        print(f"Loading {len(image_paths)} images from class {class_name}")
        
        for img_path in image_paths:
            try:
                features = preprocess_image(img_path)
                X.append(features)
                y.append(i)  # Binary labels: 0 for first class, 1 for second class
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
    
    return np.array(X), np.array(y), selected_classes

def visualize_results(X_train, svm_model, class_names, kernel_type):
    """
    Visualize the top-5 support vectors and weight vector
    """
    # Get top-5 support vectors with highest alpha values
    top5_indices = svm_model.get_top_support_vectors(5)
    
    # Reshape the support vectors to images
    plt.figure(figsize=(15, 10))
    for i, idx in enumerate(top5_indices):
        img = X_train[idx].reshape(100, 100, 3)
        plt.subplot(2, 3, i+1)
        plt.imshow(img)
        
        # Get the alpha value and class
        alpha = svm_model.alphas[idx]
        class_label = svm_model.y_train[idx]
        
        plt.title(f"SV #{i+1}, Alpha: {alpha:.4f}\nClass: {class_names[class_label]}")
        plt.axis('off')
    
    # If linear kernel, also visualize the weight vector
    if kernel_type == 'linear':
        # Reshape and plot the weight vector
        w = svm_model.get_weights()
        w_img = w.reshape(100, 100, 3)
        
        # Normalize for visualization
        w_min, w_max = np.min(w_img), np.max(w_img)
        w_img = (w_img - w_min) / (w_max - w_min) if w_max &gt; w_min else w_img
        
        plt.subplot(2, 3, 6)
        plt.imshow(w_img)
        plt.title("Weight Vector")
        plt.axis('off')
    
    plt.tight_layout()
    plt.savefig(f'svm_{kernel_type}_visualization.png')
    plt.show()

def compare_support_vectors(linear_sv_indices, gaussian_sv_indices):
    """Compare support vectors between linear and Gaussian kernel"""
    linear_set = set(linear_sv_indices)
    gaussian_set = set(gaussian_sv_indices)
    
    common_sv = linear_set.intersection(gaussian_set)
    
    print(f"Linear kernel: {len(linear_sv_indices)} support vectors")
    print(f"Gaussian kernel: {len(gaussian_sv_indices)} support vectors")
    print(f"Common support vectors: {len(common_sv)}")
    print(f"Percentage of linear SVs in Gaussian SVs: {len(common_sv)/len(linear_sv_indices)*100:.2f}%")
    print(f"Percentage of Gaussian SVs in linear SVs: {len(common_sv)/len(gaussian_sv_indices)*100:.2f}%")

def main():
    # Define class indices based on the last two digits of entry number
    # Change this to your entry number's last two digits
    d = 2
    class_indices = [d, (d + 1) % 11]
    print(f"Using classes with indices {class_indices}")
    
    # Load and preprocess the data
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"
    
    # Load training and test data
    X_train, y_train, class_names = load_dataset(train_dir, class_indices)
    X_test, y_test, _ = load_dataset(test_dir, class_indices)
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    
    # Part 1: Train SVM using linear kernel
    print("\n--- Linear Kernel SVM ---")
    linear_svm = SupportVectorMachine()
    linear_svm.fit(X_train, y_train, kernel='linear', C=1.0)
    
    # Count support vectors for linear kernel
    linear_sv_count = linear_svm.get_support_vectors_count()
    linear_sv_percentage = linear_svm.get_support_vectors_percentage()
    print(f"Number of support vectors: {linear_sv_count}")
    print(f"Percentage of support vectors: {linear_sv_percentage:.2f}%")
    
    # Calculate test accuracy for linear kernel
    linear_pred = linear_svm.predict(X_test)
    linear_accuracy = accuracy_score(y_test, linear_pred) * 100
    print(f"Test accuracy (Linear): {linear_accuracy:.2f}%")
    
    # # Visualize top-5 support vectors and weight vector for linear kernel
    # visualize_results(X_train, linear_svm, class_names, 'linear')
    
    # Part 2: Train SVM using Gaussian kernel
    print("\n--- Gaussian Kernel SVM ---")
    gaussian_svm = SupportVectorMachine()
    gaussian_svm.fit(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
    
    # Count support vectors for Gaussian kernel
    gaussian_sv_count = gaussian_svm.get_support_vectors_count()
    gaussian_sv_percentage = gaussian_svm.get_support_vectors_percentage()

    print(f"Number of support vectors: {gaussian_sv_count}")
    print(f"Percentage of support vectors: {gaussian_sv_percentage:.2f}%")
    
    # Calculate test accuracy for Gaussian kernel
    gaussian_pred = gaussian_svm.predict(X_test)
    gaussian_accuracy = accuracy_score(y_test, gaussian_pred) * 100
    print(f"Test accuracy (Gaussian): {gaussian_accuracy:.2f}%")
    
    # Visualize top-5 support vectors for Gaussian kernel
    visualize_results(X_train, gaussian_svm, class_names, 'gaussian')
    
    # Compare support vectors between linear and Gaussian kernels
    linear_sv_indices = linear_svm.support_vector_indices
    gaussian_sv_indices = gaussian_svm.support_vector_indices
    compare_support_vectors(linear_sv_indices, gaussian_sv_indices)
    
    # Compare test accuracies
    print("\n--- Performance Comparison ---")
    print(f"Linear kernel accuracy: {linear_accuracy:.2f}%")
    print(f"Gaussian kernel accuracy: {gaussian_accuracy:.2f}%")
    accuracy_diff = gaussian_accuracy - linear_accuracy
    print(f"Accuracy difference (Gaussian - Linear): {accuracy_diff:.2f}%")
    
    if accuracy_diff &gt; 0:
        print("Gaussian kernel performs better")
    elif accuracy_diff &lt; 0:
        print("Linear kernel performs better")
    else:
        print("Both kernels perform equally")

if __name__ == "__main__":
    main()



import os
import numpy as np
import cv2
from glob import glob
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
import time
from svm import SupportVectorMachine  # Your custom SVM implementation
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier

def preprocess_image(image_path):
    """
    Preprocess a single image according to the requirements:
    1. Resize to 100x100 pixels
    2. Center crop (if needed)
    3. Flatten to 1D vector
    4. Normalize to [0, 1]
    """
    # Read the image
    img = cv2.imread(image_path)
    
    if img is None:
        raise ValueError(f"Could not read image: {image_path}")
    
    # Resize to 100x100 pixels
    img = cv2.resize(img, (100, 100))
    
    # Flatten the image to 1D vector (100x100x3 = 30,000)
    flattened = img.flatten()
    
    # Normalize to [0, 1]
    normalized = flattened / 255.0
    
    return normalized

def load_dataset(base_dir, class_indices):
    """
    Load and preprocess images for specific classes
    base_dir: Path to the dataset
    class_indices: List of class indices to load
    """
    class_names = sorted(os.listdir(base_dir))
    selected_classes = [class_names[i] for i in class_indices]
    
    X = []
    y = []
    
    print(f"Selected classes: {selected_classes}")
    
    for i, class_name in enumerate(selected_classes):
        class_dir = os.path.join(base_dir, class_name)
        image_paths = glob(os.path.join(class_dir, "*.jpg"))
        
        print(f"Loading {len(image_paths)} images from class {class_name}")
        
        for img_path in image_paths:
            try:
                features = preprocess_image(img_path)
                X.append(features)
                y.append(i)  # Binary labels: 0 for first class, 1 for second class
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
    
    return np.array(X), np.array(y), selected_classes

def compare_support_vectors(model1, model2, name1, name2):
    """Compare support vectors between two models"""
    if hasattr(model1, 'support_'):
        sv1_indices = model1.support_
    else:
        sv1_indices = model1.support_vector_indices
        
    if hasattr(model2, 'support_'):
        sv2_indices = model2.support_
    else:
        sv2_indices = model2.support_vector_indices
    
    sv1_set = set(sv1_indices)
    sv2_set = set(sv2_indices)
    
    common_sv = sv1_set.intersection(sv2_set)
    
    print(f"{name1}: {len(sv1_indices)} support vectors")
    print(f"{name2}: {len(sv2_indices)} support vectors")
    print(f"Common support vectors: {len(common_sv)}")
    print(f"Percentage of {name1} SVs in {name2}: {len(common_sv)/len(sv1_indices)*100:.2f}%")
    print(f"Percentage of {name2} SVs in {name1}: {len(common_sv)/len(sv2_indices)*100:.2f}%")
    
    return len(common_sv)

def compare_weights_and_bias(custom_svm, sklearn_svm, X_train):
    """Compare weights and bias between custom and sklearn SVM"""
    # Extract weights and bias from custom SVM
<A NAME="0"></A><FONT color = #FF0000><A HREF="match218-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    custom_w = custom_svm.get_weights()
    custom_b = custom_svm.get_bias()
    
    # Extract weights and bias from sklearn SVM
    # For linear kernel, sklearn stores the weights directly
    sklearn_w = sklearn_svm.coef_[0]
    sklearn_b = sklearn_svm.intercept_[0]
    
    # Compare weights
    w_diff = np.linalg.norm(custom_w - sklearn_w)
    w_relative_diff = w_diff / np.linalg.norm(sklearn_w) * 100
</FONT>    
    # Compare bias
    b_diff = abs(custom_b - sklearn_b)
    b_relative_diff = b_diff / abs(sklearn_b) * 100 if sklearn_b != 0 else float('inf')
    
    print("\n--- Weight and Bias Comparison ---")
    print(f"Custom SVM bias: {custom_b:.6f}")
    print(f"Sklearn SVM bias: {sklearn_b:.6f}")
    print(f"Absolute difference in bias: {b_diff:.6f}")
    print(f"Relative difference in bias: {b_relative_diff:.2f}%")
    print(f"Norm of weight difference: {w_diff:.6f}")
    print(f"Relative difference in weights: {w_relative_diff:.2f}%")
    
    # Optional: Visualize weight differences
    plt.figure(figsize=(18, 6))
    
    # Plot custom weights
    plt.subplot(1, 3, 1)
    custom_w_img = custom_w.reshape(100, 100, 3)
    custom_w_img = (custom_w_img - custom_w_img.min()) / (custom_w_img.max() - custom_w_img.min())
    plt.imshow(custom_w_img)
    plt.title("Custom SVM Weights")
    plt.axis('off')
    
    # Plot sklearn weights
    plt.subplot(1, 3, 2)
    sklearn_w_img = sklearn_w.reshape(100, 100, 3)
    sklearn_w_img = (sklearn_w_img - sklearn_w_img.min()) / (sklearn_w_img.max() - sklearn_w_img.min())
    plt.imshow(sklearn_w_img)
    plt.title("Sklearn SVM Weights")
    plt.axis('off')
    
    # Plot difference
    plt.subplot(1, 3, 3)
    diff_img = (custom_w - sklearn_w).reshape(100, 100, 3)
    diff_img = (diff_img - diff_img.min()) / (diff_img.max() - diff_img.min()) if diff_img.max() != diff_img.min() else diff_img
    plt.imshow(diff_img)
    plt.title("Weight Difference")
    plt.axis('off')
    
    plt.tight_layout()
    plt.savefig('weight_comparison.png')
    plt.show()


def compare_sgd_liblinear(X_train, y_train, X_test, y_test):
    results = {}
    
    # LIBLINEAR SVM
    print("\n--- LIBLINEAR SVM ---")
    start_time = time.time()
    liblinear_svm = LinearSVC(C=1.0, max_iter=1000, dual=False)
    liblinear_svm.fit(X_train, y_train)
    liblinear_time = time.time() - start_time
    
    # Calculate test accuracy
    liblinear_pred = liblinear_svm.predict(X_test)
    liblinear_accuracy = accuracy_score(y_test, liblinear_pred) * 100
    print(f"Test accuracy: {liblinear_accuracy:.2f}%")
    print(f"Training time: {liblinear_time:.4f} seconds")
    
    results['liblinear'] = {
        'accuracy': liblinear_accuracy,
        'training_time': liblinear_time
    }
    
    # SGD SVM
    print("\n--- SGD SVM ---")
    start_time = time.time()
    sgd_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1/X_train.shape[0], max_iter=1000, tol=1e-3, random_state=42)
    sgd_svm.fit(X_train, y_train)
    sgd_time = time.time() - start_time
    
    # Calculate test accuracy
    sgd_pred = sgd_svm.predict(X_test)
    sgd_accuracy = accuracy_score(y_test, sgd_pred) * 100
    print(f"Test accuracy: {sgd_accuracy:.2f}%")
    print(f"Training time: {sgd_time:.4f} seconds")
    
    results['sgd'] = {
        'accuracy': sgd_accuracy,
        'training_time': sgd_time
    }
    
    # SGD with early stopping - often faster and better in practice
    print("\n--- SGD SVM with early stopping ---")
    start_time = time.time()
    sgd_early_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1/X_train.shape[0], 
                                 max_iter=1000, tol=1e-3, early_stopping=True, 
                                 validation_fraction=0.1, n_iter_no_change=5, random_state=42)
    sgd_early_svm.fit(X_train, y_train)
    sgd_early_time = time.time() - start_time
    
    # Calculate test accuracy
    sgd_early_pred = sgd_early_svm.predict(X_test)
    sgd_early_accuracy = accuracy_score(y_test, sgd_early_pred) * 100
    print(f"Test accuracy: {sgd_early_accuracy:.2f}%")
    print(f"Training time: {sgd_early_time:.4f} seconds")
    
    results['sgd_early'] = {
        'accuracy': sgd_early_accuracy,
        'training_time': sgd_early_time
    }
    
    # Comparison visualization
    models = ['LIBLINEAR', 'SGD', 'SGD+Early Stopping']
    train_times = [results['liblinear']['training_time'], 
                  results['sgd']['training_time'], 
                  results['sgd_early']['training_time']]
    accuracies = [results['liblinear']['accuracy'],
                 results['sgd']['accuracy'],
                 results['sgd_early']['accuracy']]
    
    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Plot training times
    ax1.bar(models, train_times, color=['blue', 'green', 'orange'])
    ax1.set_ylabel('Training Time (seconds)')
    ax1.set_title('Training Time Comparison')
    for i, time_val in enumerate(train_times):
        ax1.text(i, time_val + 0.05, f"{time_val:.4f}s", ha='center')
    
    # Plot accuracies
    ax2.bar(models, accuracies, color=['blue', 'green', 'orange'])
    ax2.set_ylabel('Test Accuracy (%)')
    ax2.set_title('Accuracy Comparison')
    for i, acc in enumerate(accuracies):
        ax2.text(i, acc + 0.5, f"{acc:.2f}%", ha='center')
    
    plt.tight_layout()
    plt.savefig('sgd_vs_liblinear.png')
    plt.show()
    
    # Calculate speedup
    speedup = results['liblinear']['training_time'] / results['sgd']['training_time']
    print(f"\nSGD is {speedup:.2f}x faster than LIBLINEAR")
    
    early_speedup = results['liblinear']['training_time'] / results['sgd_early']['training_time']
    print(f"SGD with early stopping is {early_speedup:.2f}x faster than LIBLINEAR")
    
    # Calculate accuracy difference
    acc_diff = results['sgd']['accuracy'] - results['liblinear']['accuracy']
    print(f"SGD accuracy is {acc_diff:.2f}% {'higher' if acc_diff &gt; 0 else 'lower'} than LIBLINEAR")
    
    early_acc_diff = results['sgd_early']['accuracy'] - results['liblinear']['accuracy']
    print(f"SGD with early stopping accuracy is {early_acc_diff:.2f}% {'higher' if early_acc_diff &gt; 0 else 'lower'} than LIBLINEAR")
    
    # Summary table
    print("\n--- Summary Table ---")
    print("Model                | Accuracy | Training Time (s) | Speedup vs LIBLINEAR")
    print("---------------------|----------|-------------------|-------------------")
    print(f"LIBLINEAR            | {results['liblinear']['accuracy']:8.2f}% | {results['liblinear']['training_time']:8.4f} | 1.00x")
    print(f"SGD                  | {results['sgd']['accuracy']:8.2f}% | {results['sgd']['training_time']:8.4f} | {speedup:4.2f}x")
    print(f"SGD+Early Stopping   | {results['sgd_early']['accuracy']:8.2f}% | {results['sgd_early']['training_time']:8.4f} | {early_speedup:4.2f}x")
    
    return results

def main():
    # Define class indices based on the last two digits of entry number
    # Change this to your entry number's last two digits
    d = 2
    class_indices = [d, (d + 1) % 11]
    print(f"Using classes with indices {class_indices}")
    
    # Load and preprocess the data
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"
    
    # Load training and test data
    X_train, y_train, class_names = load_dataset(train_dir, class_indices)
    X_test, y_test, _ = load_dataset(test_dir, class_indices)
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    
    
    # results = compare_sgd_liblinear(X_train, y_train, X_test, y_test)
    results = {}
    
    # Part 1: CVXOPT SVM with linear kernel
    print("\n--- CVXOPT Linear Kernel SVM ---")
    start_time = time.time()
    custom_linear_svm = SupportVectorMachine()
    custom_linear_svm.fit(X_train, y_train, kernel='linear', C=1.0)
    cvxopt_linear_time = time.time() - start_time
    
    # Count support vectors
    custom_linear_sv_count = custom_linear_svm.get_support_vectors_count()
    custom_linear_sv_percentage = custom_linear_svm.get_support_vectors_percentage()
    print(f"Number of support vectors: {custom_linear_sv_count}")
    print(f"Percentage of support vectors: {custom_linear_sv_percentage:.2f}%")
    
    # Calculate test accuracy
    custom_linear_pred = custom_linear_svm.predict(X_test)
    custom_linear_accuracy = accuracy_score(y_test, custom_linear_pred) * 100
    print(f"Test accuracy: {custom_linear_accuracy:.2f}%")
    print(f"Training time: {cvxopt_linear_time:.4f} seconds")
    
    results['cvxopt_linear'] = {
        'sv_count': custom_linear_sv_count,
        'accuracy': custom_linear_accuracy,
        'training_time': cvxopt_linear_time
    }
    
    # Part 2: CVXOPT SVM with Gaussian kernel
    print("\n--- CVXOPT Gaussian Kernel SVM ---")
    start_time = time.time()
    custom_gaussian_svm = SupportVectorMachine()
    custom_gaussian_svm.fit(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
    cvxopt_gaussian_time = time.time() - start_time
    
    # Count support vectors
    custom_gaussian_sv_count = custom_gaussian_svm.get_support_vectors_count()
    custom_gaussian_sv_percentage = custom_gaussian_svm.get_support_vectors_percentage()
    print(f"Number of support vectors: {custom_gaussian_sv_count}")
    print(f"Percentage of support vectors: {custom_gaussian_sv_percentage:.2f}%")
    
    # Calculate test accuracy
    custom_gaussian_pred = custom_gaussian_svm.predict(X_test)
    custom_gaussian_accuracy = accuracy_score(y_test, custom_gaussian_pred) * 100
    print(f"Test accuracy: {custom_gaussian_accuracy:.2f}%")
    print(f"Training time: {cvxopt_gaussian_time:.4f} seconds")
    
    results['cvxopt_gaussian'] = {
        'sv_count': custom_gaussian_sv_count,
        'accuracy': custom_gaussian_accuracy,
        'training_time': cvxopt_gaussian_time
    }
    
    # Part 3: Scikit-learn SVM with linear kernel
    print("\n--- Scikit-learn Linear Kernel SVM ---")
    start_time = time.time()
    sklearn_linear_svm = SVC(kernel='linear', C=1.0)
    sklearn_linear_svm.fit(X_train, y_train)
    sklearn_linear_time = time.time() - start_time
    
    # Count support vectors
    sklearn_linear_sv_count = len(sklearn_linear_svm.support_)
    sklearn_linear_sv_percentage = sklearn_linear_sv_count / len(X_train) * 100
    print(f"Number of support vectors: {sklearn_linear_sv_count}")
    print(f"Percentage of support vectors: {sklearn_linear_sv_percentage:.2f}%")
    
    # Calculate test accuracy
    sklearn_linear_pred = sklearn_linear_svm.predict(X_test)
    sklearn_linear_accuracy = accuracy_score(y_test, sklearn_linear_pred) * 100
    print(f"Test accuracy: {sklearn_linear_accuracy:.2f}%")
    print(f"Training time: {sklearn_linear_time:.4f} seconds")
    
    results['sklearn_linear'] = {
        'sv_count': sklearn_linear_sv_count,
        'accuracy': sklearn_linear_accuracy,
        'training_time': sklearn_linear_time
    }
    
    # Part 4: Scikit-learn SVM with Gaussian kernel
    print("\n--- Scikit-learn Gaussian Kernel SVM ---")
    start_time = time.time()
    sklearn_gaussian_svm = SVC(kernel='rbf', C=1.0, gamma=0.001)
    sklearn_gaussian_svm.fit(X_train, y_train)
    sklearn_gaussian_time = time.time() - start_time
    
    # Count support vectors
    sklearn_gaussian_sv_count = len(sklearn_gaussian_svm.support_)
    sklearn_gaussian_sv_percentage = sklearn_gaussian_sv_count / len(X_train) * 100
    print(f"Number of support vectors: {sklearn_gaussian_sv_count}")
    print(f"Percentage of support vectors: {sklearn_gaussian_sv_percentage:.2f}%")
    
    # Calculate test accuracy
    sklearn_gaussian_pred = sklearn_gaussian_svm.predict(X_test)
    sklearn_gaussian_accuracy = accuracy_score(y_test, sklearn_gaussian_pred) * 100
    print(f"Test accuracy: {sklearn_gaussian_accuracy:.2f}%")
    print(f"Training time: {sklearn_gaussian_time:.4f} seconds")
    
    results['sklearn_gaussian'] = {
        'sv_count': sklearn_gaussian_sv_count,
        'accuracy': sklearn_gaussian_accuracy,
        'training_time': sklearn_gaussian_time
    }
    
    # Part a: Compare support vectors
    print("\n--- Support Vector Comparison ---")
    print("Linear kernel comparison:")
    linear_sv_overlap = compare_support_vectors(
        custom_linear_svm, sklearn_linear_svm, 
        "CVXOPT Linear", "Sklearn Linear"
    )
    
    print("\nGaussian kernel comparison:")
    gaussian_sv_overlap = compare_support_vectors(
        custom_gaussian_svm, sklearn_gaussian_svm, 
        "CVXOPT Gaussian", "Sklearn Gaussian"
    )
    
    # Part b: Compare weights and bias for linear kernel
    compare_weights_and_bias(custom_linear_svm, sklearn_linear_svm, X_train)
    
    # Part c: Report test accuracies
    print("\n--- Test Accuracy Comparison ---")
    print(f"CVXOPT Linear: {custom_linear_accuracy:.2f}%")
    print(f"Sklearn Linear: {sklearn_linear_accuracy:.2f}%")
    print(f"CVXOPT Gaussian: {custom_gaussian_accuracy:.2f}%")
    print(f"Sklearn Gaussian: {sklearn_gaussian_accuracy:.2f}%")

    # Part d: Compare computational cost
    print("\n--- Computational Cost Comparison ---")
    print(f"CVXOPT Linear training time: {cvxopt_linear_time:.4f} seconds")
    print(f"Sklearn Linear training time: {sklearn_linear_time:.4f} seconds")
    print(f"CVXOPT Linear is {cvxopt_linear_time/sklearn_linear_time:.2f}x slower than Sklearn Linear")
    
    print(f"CVXOPT Gaussian training time: {cvxopt_gaussian_time:.4f} seconds")
    print(f"Sklearn Gaussian training time: {sklearn_gaussian_time:.4f} seconds")
    print(f"CVXOPT Gaussian is {cvxopt_gaussian_time/sklearn_gaussian_time:.2f}x slower than Sklearn Gaussian")
    
    # Summary table
    print("\n--- Summary Table ---")
    print("Model              | SV Count | Accuracy | Training Time (s)")
    print("--------------------|----------|----------|----------------")
    print(f"CVXOPT Linear      | {results['cvxopt_linear']['sv_count']:8d} | {results['cvxopt_linear']['accuracy']:8.2f}% | {results['cvxopt_linear']['training_time']:8.4f}")
    print(f"Sklearn Linear     | {results['sklearn_linear']['sv_count']:8d} | {results['sklearn_linear']['accuracy']:8.2f}% | {results['sklearn_linear']['training_time']:8.4f}")
    print(f"CVXOPT Gaussian    | {results['cvxopt_gaussian']['sv_count']:8d} | {results['cvxopt_gaussian']['accuracy']:8.2f}% | {results['cvxopt_gaussian']['training_time']:8.4f}")
    print(f"Sklearn Gaussian   | {results['sklearn_gaussian']['sv_count']:8d} | {results['sklearn_gaussian']['accuracy']:8.2f}% | {results['sklearn_gaussian']['training_time']:8.4f}")

if __name__ == "__main__":
    main()



import os
import numpy as np
import cv2
from itertools import combinations

# Import your SVM class
from svm import SupportVectorMachine

def load_and_preprocess_images(directory):
    """
    Load images from directory, resize to 100x100, flatten, and normalize
    """
    X = []
    y = []
    
    # Get all class folders
    class_folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]
    class_folders.sort()  # Sort to ensure consistent class numbering
    
    # Map class names to numerical labels
    class_to_label = {class_name: i for i, class_name in enumerate(class_folders)}
    
    print(f"Found {len(class_folders)} classes: {class_folders}")
    
    # Process each class folder
    for class_folder in class_folders:
        class_path = os.path.join(directory, class_folder)
        class_label = class_to_label[class_folder]
        
        # Get all images in the class folder
        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]
        
        for img_file in image_files:
            img_path = os.path.join(class_path, img_file)
            
            # Load image
            img = cv2.imread(img_path)
            if img is None:
                print(f"Warning: Could not load {img_path}")
                continue
            
            # Convert to RGB if needed
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Resize to 100x100
            img = cv2.resize(img, (100, 100))
            
            # Flatten the image
            img_flat = img.flatten()
            
            # Normalize to [0, 1]
            img_flat = img_flat / 255.0
            
            # Add to dataset
            X.append(img_flat)
            y.append(class_label)
    
    return np.array(X), np.array(y)

def train_multiclass_svm_onevsone(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001):
    """
    Train one-vs-one multiclass SVM using binary classifier
    """
    # Get unique classes
    classes = np.unique(y_train)
    print(f"Found {len(classes)} unique classes")
    
    # Generate all pairs of classes
    class_pairs = list(combinations(classes, 2))
    print(f"Training {len(class_pairs)} binary classifiers")
    
    # Train a binary classifier for each pair
    classifiers = {}
    for i, j in class_pairs:
        print(f"Training classifier for classes {i} vs {j}")
        
        # Get indices for the two classes
        idx = np.where((y_train == i) | (y_train == j))[0]
        X_subset = X_train[idx]
        y_subset = y_train[idx]
        
        # Convert to binary problem (class i -&gt; 0, class j -&gt; 1)
        y_binary = np.where(y_subset == i, 0, 1)
        
        # Train binary SVM
        svm = SupportVectorMachine()
        svm.fit(X_subset, y_binary, kernel=kernel, C=C, gamma=gamma)
        
        # Store the classifier
        classifiers[(i, j)] = svm
    
    return classifiers, classes

def predict_multiclass_svm_onevsone(X_test, classifiers, classes):
    """
    Predict using one-vs-one voting scheme with score-based tie breaking
    """
    n_samples = X_test.shape[0]
    n_classes = len(classes)
    
    # Initialize vote matrix and score matrix
    votes = np.zeros((n_samples, n_classes), dtype=int)
    scores = np.zeros((n_samples, n_classes), dtype=float)
    
    # For each binary classifier
    for (i, j), clf in classifiers.items():
        # Get predictions
        predictions = clf.predict(X_test)
        
        # Calculate decision values (distance from hyperplane)
        y_binary = np.where(clf.y_train == 0, -1, 1)
        if clf.kernel_type == 'linear':
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match218-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            decision_values = np.dot(X_test, clf.w) + clf.b
        else:  # gaussian kernel
            K = clf._compute_kernel_matrix(X_test, clf.X_train)
            alpha_y = clf.alphas * y_binary
</FONT>            sv_indices = clf.support_vector_indices
            decision_values = np.sum(alpha_y[sv_indices].reshape(1, -1) * K[:, sv_indices], axis=1) + clf.b
        
        # Update votes and scores based on predictions
        for sample_idx in range(n_samples):
            class_idx_i = np.where(classes == i)[0][0]
            class_idx_j = np.where(classes == j)[0][0]
            
            if predictions[sample_idx] == 0:  # Predicted class i
                votes[sample_idx, class_idx_i] += 1
                scores[sample_idx, class_idx_i] += abs(decision_values[sample_idx])
            else:  # Predicted class j
                votes[sample_idx, class_idx_j] += 1
                scores[sample_idx, class_idx_j] += abs(decision_values[sample_idx])
    
    # Get predictions handling ties
    y_pred = np.zeros(n_samples, dtype=int)
    for sample_idx in range(n_samples):
        # Find maximum vote count for this sample
        max_votes = np.max(votes[sample_idx])
        
        # Find classes that have the maximum votes (could be multiple in case of tie)
        max_vote_classes = np.where(votes[sample_idx] == max_votes)[0]
        
        if len(max_vote_classes) == 1:
            # No tie - use the class with max votes
            winner_idx = max_vote_classes[0]
        else:
            # Tie - choose the class with highest score among tied classes
            tied_scores = scores[sample_idx, max_vote_classes]
            winner_among_tied = np.argmax(tied_scores)
            winner_idx = max_vote_classes[winner_among_tied]
        
        y_pred[sample_idx] = classes[winner_idx]
    
    return y_pred

def main():
    # Paths to data directories
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"
    
    # Load and preprocess data
    print("Loading and preprocessing training data...")
    X_train, y_train = load_and_preprocess_images(train_dir)
    
    print("Loading and preprocessing test data...")
    X_test, y_test = load_and_preprocess_images(test_dir)
    
    print(f"Training data shape: {X_train.shape}")
    print(f"Test data shape: {X_test.shape}")
    
    # Train one-vs-one multiclass SVM
    print("Training multiclass SVM...")
    classifiers, classes = train_multiclass_svm_onevsone(
        X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001
    )
    
    # Make predictions
    print("Making predictions...")
    y_pred = predict_multiclass_svm_onevsone(X_test, classifiers, classes)
    
    # Calculate accuracy
    accuracy = np.mean(y_pred == y_test)
    print(f"Test accuracy: {accuracy:.4f}")
    
    return accuracy

if __name__ == "__main__":
    main()



import os
import numpy as np
import cv2
from sklearn.preprocessing import MinMaxScaler
from itertools import combinations
from tqdm import tqdm  # For progress bars

# Import your SVM class
from svm import SupportVectorMachine

def load_and_preprocess_images(directory):
    """
    Load images from directory, resize to 100x100, center crop, flatten, and normalize
    
    Args:
        directory: Path to directory containing class folders
        
    Returns:
        X: Image features (flattened and normalized)
        y: Class labels
    """
    X = []
    y = []
    
    # Get all class folders
    class_folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]
    class_folders.sort()  # Sort to ensure consistent class numbering
    
    # Map class names to numerical labels
    class_to_label = {class_name: i for i, class_name in enumerate(class_folders)}
    
    print(f"Found {len(class_folders)} classes: {class_folders}")
    
    # Process each class folder
    for class_folder in tqdm(class_folders, desc="Processing classes"):
        class_path = os.path.join(directory, class_folder)
        class_label = class_to_label[class_folder]
        
        # Get all images in the class folder
        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]
        
        for img_file in tqdm(image_files, desc=f"Processing {class_folder}", leave=False):
            img_path = os.path.join(class_path, img_file)
            
            # Load image
            img = cv2.imread(img_path)
            if img is None:
                print(f"Warning: Could not load {img_path}")
                continue
            
            # Convert to RGB if needed
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Resize to 100x100
            img = cv2.resize(img, (100, 100))
            
            # Center crop if needed
            # (In this case, not needed since we already resized to target size)
            
            # Flatten the image
            img_flat = img.flatten()
            
            # Normalize to [0, 1]
            img_flat = img_flat / 255.0
            
            # Add to dataset
            X.append(img_flat)
            y.append(class_label)
    
    return np.array(X), np.array(y)

def train_multiclass_svm_onevsone(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001):
    """
    Train one-vs-one multiclass SVM using binary classifier
    
    Args:
        X_train: Training data features
        y_train: Training data labels
        kernel: Kernel type to use
        C: Regularization parameter
        gamma: Gamma parameter for Gaussian kernel
        
    Returns:
        classifiers: Dictionary of trained binary classifiers
        classes: Array of unique class labels
    """
    # Get unique classes
    classes = np.unique(y_train)
    print(f"Found {len(classes)} unique classes")
    
    # Generate all pairs of classes
    class_pairs = list(combinations(classes, 2))
    print(f"Training {len(class_pairs)} binary classifiers")
    
    # Train a binary classifier for each pair
    classifiers = {}
    for i, j in tqdm(class_pairs, desc="Training binary classifiers"):
        print(f"Training classifier for classes {i} vs {j}")
        
        # Get indices for the two classes
        idx = np.where((y_train == i) | (y_train == j))[0]
        X_subset = X_train[idx]
        y_subset = y_train[idx]
        
        # Convert to binary problem (class i -&gt; 0, class j -&gt; 1)
        y_binary = np.where(y_subset == i, 0, 1)
        
        # Train binary SVM
        svm = SupportVectorMachine()
        svm.fit(X_subset, y_binary, kernel=kernel, C=C, gamma=gamma)
        
        # Store the classifier
        classifiers[(i, j)] = svm
    
    return classifiers, classes

def predict_multiclass_svm_onevsone(X_test, classifiers, classes):
    """
    Predict using one-vs-one voting scheme with score-based tie breaking
    
    Args:
        X_test: Test data features
        classifiers: Dictionary of trained binary classifiers
        classes: Array of unique class labels
        
    Returns:
        y_pred: Predicted class labels
    """
    n_samples = X_test.shape[0]
    n_classes = len(classes)
    
    # Initialize vote matrix and score matrix
    votes = np.zeros((n_samples, n_classes), dtype=int)
    scores = np.zeros((n_samples, n_classes), dtype=float)
    
    # For each binary classifier
    for (i, j), clf in tqdm(classifiers.items(), desc="Making predictions"):
        # Get predictions
        predictions = clf.predict(X_test)
        
        # Calculate decision values (distance from hyperplane)
        decision_values = np.zeros(n_samples)
        
        # Use kernel trick to calculate decision values
        y_binary = np.where(clf.y_train == 0, -1, 1)
        if clf.kernel_type == 'linear':
            decision_values = np.dot(X_test, clf.w) + clf.b
        else:  # gaussian kernel
            K = clf._compute_kernel_matrix(X_test, clf.X_train)
            alpha_y = clf.alphas * y_binary
            sv_indices = clf.support_vector_indices
            decision_values = np.sum(alpha_y[sv_indices].reshape(1, -1) * K[:, sv_indices], axis=1) + clf.b
        
        # Update votes and scores based on predictions
        for sample_idx in range(n_samples):
            class_idx_i = np.where(classes == i)[0][0]
            class_idx_j = np.where(classes == j)[0][0]
            
            if predictions[sample_idx] == 0:  # Predicted class i
                votes[sample_idx, class_idx_i] += 1
                scores[sample_idx, class_idx_i] += abs(decision_values[sample_idx])
            else:  # Predicted class j
                votes[sample_idx, class_idx_j] += 1
                scores[sample_idx, class_idx_j] += abs(decision_values[sample_idx])
    
    # Get predictions handling ties
    y_pred = np.zeros(n_samples, dtype=int)
    for sample_idx in range(n_samples):
        # Find maximum vote count for this sample
        max_votes = np.max(votes[sample_idx])
        
        # Find classes that have the maximum votes (could be multiple in case of tie)
        max_vote_classes = np.where(votes[sample_idx] == max_votes)[0]
        
        if len(max_vote_classes) == 1:
            # No tie - use the class with max votes
            winner_idx = max_vote_classes[0]
        else:
            # Tie - choose the class with highest score among tied classes
            tied_scores = scores[sample_idx, max_vote_classes]
            winner_among_tied = np.argmax(tied_scores)
            winner_idx = max_vote_classes[winner_among_tied]
        
        y_pred[sample_idx] = classes[winner_idx]
    
    return y_pred

def main():
    # Paths to data directories
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"
    
    # Load and preprocess data
    print("Loading and preprocessing training data...")
    X_train, y_train = load_and_preprocess_images(train_dir)
    
    print("Loading and preprocessing test data...")
    X_test, y_test = load_and_preprocess_images(test_dir)
    
    print(f"Training data shape: {X_train.shape}")
    print(f"Test data shape: {X_test.shape}")
    
    # Train one-vs-one multiclass SVM
    print("Training multiclass SVM...")
    classifiers, classes = train_multiclass_svm_onevsone(
        X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001
    )
    
    # Make predictions
    print("Making predictions...")
    y_pred = predict_multiclass_svm_onevsone(X_test, classifiers, classes)
    
    # Calculate accuracy
    accuracy = np.mean(y_pred == y_test)
    print(f"Test accuracy: {accuracy:.4f}")
    
    # Optional: Print confusion matrix
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)
    
    return accuracy

if __name__ == "__main__":
    main()



import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import time
from tqdm import tqdm

# Function to load and preprocess images
def load_and_preprocess_images(data_path, classes=None):
    if classes is None:
        classes = os.listdir(data_path)
        classes = [c for c in classes if os.path.isdir(os.path.join(data_path, c))]
    
    X = []
    y = []
    
    for class_index, class_name in enumerate(classes):
        class_path = os.path.join(data_path, class_name)
        
        print(f"Loading class {class_name}...")
        
        for image_name in tqdm(os.listdir(class_path)):
            image_path = os.path.join(class_path, image_name)
            
            # Load image
            try:
                img = Image.open(image_path)
                
                # Resize to 100x100
                img = img.resize((100, 100))
                
                # Convert to RGB in case image is grayscale or has alpha channel
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Convert to numpy array
                img_array = np.array(img)
                
                # Flatten to 1D vector
                img_flat = img_array.flatten()
                
                # Min-max scaling (normalize to [0, 1])
                img_normalized = img_flat / 255.0
                
                X.append(img_normalized)
                y.append(class_index)
                
            except Exception as e:
                print(f"Error processing image {image_path}: {e}")
                continue
    
    return np.array(X), np.array(y), classes

# Define paths
train_data_path = "../data/Q2/train"  # Path to training data
test_data_path = "../data/Q2/test"    # Path to test data

# Load and preprocess training data
print("Loading training data...")
X_train, y_train, classes = load_and_preprocess_images(train_data_path)
print(f"Training data loaded: {X_train.shape[0]} samples, {len(classes)} classes")

# Load and preprocess test data
print("Loading test data...")
X_test, y_test, _ = load_and_preprocess_images(test_data_path, classes)
print(f"Test data loaded: {X_test.shape[0]} samples")

# Part (a): Train multi-class SVM with Gaussian kernel
print("Training multi-class SVM with Gaussian kernel (gamma=0.001, C=1.0)...")
start_time = time.time()

# Create and train SVM with Gaussian kernel
print("Unique classes in training:", set(y_train))
print("Unique classes in test:", set(y_test))
svm_model = SVC(kernel='rbf',gamma=0.001,C=1.0)

print("going to fitting")
svm_model.fit(X_train, y_train)
print("fit done")
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Part (a): Evaluate on test set
print("Evaluating on test set...")
start_time = time.time()
y_pred = svm_model.predict(X_test)
test_time = time.time() - start_time

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy: {accuracy:.4f}")
print(f"Prediction time for test set: {test_time:.2f} seconds")



import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.svm import SVC
from svm import SupportVectorMachine
from itertools import combinations

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import time
from tqdm import tqdm
import seaborn as sns

# Function to load and preprocess images
def load_and_preprocess_images(data_path, classes=None):
    if classes is None:
        classes = os.listdir(data_path)
        classes = [c for c in classes if os.path.isdir(os.path.join(data_path, c))]
    
    X = []
    y = []
    image_paths = []  # Store image paths for visualization later
    
    for class_index, class_name in enumerate(classes):
        class_path = os.path.join(data_path, class_name)
        
        print(f"Loading class {class_name}...")
        
        for image_name in tqdm(os.listdir(class_path)):
            image_path = os.path.join(class_path, image_name)
            
            # Load image
            try:
                img = Image.open(image_path)
                
                # Resize to 100x100
                img = img.resize((100, 100))
                
                # Convert to RGB in case image is grayscale or has alpha channel
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Convert to numpy array
                img_array = np.array(img)
                
                # Flatten to 1D vector
                img_flat = img_array.flatten()
                
                # Min-max scaling (normalize to [0, 1])
                img_normalized = img_flat / 255.0
                
                X.append(img_normalized)
                y.append(class_index)
                image_paths.append(image_path)  # Save the path
                
            except Exception as e:
                print(f"Error processing image {image_path}: {e}")
                continue
    
    return np.array(X), np.array(y), classes, image_paths

def train_multiclass_svm_onevsone(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001):
    """
    Train one-vs-one multiclass SVM using binary classifier
    
    Args:
        X_train: Training data features
        y_train: Training data labels
        kernel: Kernel type to use
        C: Regularization parameter
        gamma: Gamma parameter for Gaussian kernel
        
    Returns:
        classifiers: Dictionary of trained binary classifiers
        classes: Array of unique class labels
    """
    # Get unique classes
    classes = np.unique(y_train)
    print(f"Found {len(classes)} unique classes")
    
    # Generate all pairs of classes
    class_pairs = list(combinations(classes, 2))
    print(f"Training {len(class_pairs)} binary classifiers")
    
    # Train a binary classifier for each pair
    classifiers = {}
    for i, j in tqdm(class_pairs, desc="Training binary classifiers"):
        print(f"Training classifier for classes {i} vs {j}")
        
        # Get indices for the two classes
        idx = np.where((y_train == i) | (y_train == j))[0]
        X_subset = X_train[idx]
        y_subset = y_train[idx]
        
        # Convert to binary problem (class i -&gt; 0, class j -&gt; 1)
        y_binary = np.where(y_subset == i, 0, 1)
        
        # Train binary SVM
        svm = SupportVectorMachine()
        svm.fit(X_subset, y_binary, kernel=kernel, C=C, gamma=gamma)
        
        # Store the classifier
        classifiers[(i, j)] = svm
    
    return classifiers, classes

def predict_multiclass_svm_onevsone(X_test, classifiers, classes):
    """
    Predict using one-vs-one voting scheme with score-based tie breaking
    
    Args:
        X_test: Test data features
        classifiers: Dictionary of trained binary classifiers
        classes: Array of unique class labels
        
    Returns:
        y_pred: Predicted class labels
    """
    n_samples = X_test.shape[0]
    n_classes = len(classes)
    
    # Initialize vote matrix and score matrix
    votes = np.zeros((n_samples, n_classes), dtype=int)
    scores = np.zeros((n_samples, n_classes), dtype=float)
    
    # For each binary classifier
    for (i, j), clf in tqdm(classifiers.items(), desc="Making predictions"):
        # Get predictions
        predictions = clf.predict(X_test)
        
        # Calculate decision values (distance from hyperplane)
        decision_values = np.zeros(n_samples)
        
        # Use kernel trick to calculate decision values
        y_binary = np.where(clf.y_train == 0, -1, 1)
        if clf.kernel_type == 'linear':
            decision_values = np.dot(X_test, clf.w) + clf.b
        else:  # gaussian kernel
            K = clf._compute_kernel_matrix(X_test, clf.X_train)
            alpha_y = clf.alphas * y_binary
            sv_indices = clf.support_vector_indices
            decision_values = np.sum(alpha_y[sv_indices].reshape(1, -1) * K[:, sv_indices], axis=1) + clf.b
        
        # Update votes and scores based on predictions
        for sample_idx in range(n_samples):
            class_idx_i = np.where(classes == i)[0][0]
            class_idx_j = np.where(classes == j)[0][0]
            
            if predictions[sample_idx] == 0:  # Predicted class i
                votes[sample_idx, class_idx_i] += 1
                scores[sample_idx, class_idx_i] += abs(decision_values[sample_idx])
            else:  # Predicted class j
                votes[sample_idx, class_idx_j] += 1
                scores[sample_idx, class_idx_j] += abs(decision_values[sample_idx])
    
    # Get predictions handling ties
    y_pred = np.zeros(n_samples, dtype=int)
    for sample_idx in range(n_samples):
        # Find maximum vote count for this sample
        max_votes = np.max(votes[sample_idx])
        
        # Find classes that have the maximum votes (could be multiple in case of tie)
        max_vote_classes = np.where(votes[sample_idx] == max_votes)[0]
        
        if len(max_vote_classes) == 1:
            # No tie - use the class with max votes
            winner_idx = max_vote_classes[0]
        else:
            # Tie - choose the class with highest score among tied classes
            tied_scores = scores[sample_idx, max_vote_classes]
            winner_among_tied = np.argmax(tied_scores)
            winner_idx = max_vote_classes[winner_among_tied]
        
        y_pred[sample_idx] = classes[winner_idx]
    
    return y_pred

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    
    # Normalize confusion matrix
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Create heatmap
    sns.heatmap(cm_norm, annot=cm, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(f"{title.replace(' ', '_')}.png")
    plt.show()
    
    # Find most common misclassifications
    n_classes = len(classes)
    misclass_rates = np.zeros((n_classes, n_classes))
    
    for i in range(n_classes):
        for j in range(n_classes):
            if i != j and cm[i, j] &gt; 0:
                misclass_rates[i, j] = cm[i, j] / cm[i].sum()
    
    # Top 3 misclassifications
    flat_indices = np.argsort(misclass_rates.flatten())[::-1]
    for idx in flat_indices[:5]:
        i, j = idx // n_classes, idx % n_classes
        if i != j and misclass_rates[i, j] &gt; 0:
            print(f"Class '{classes[i]}' is misclassified as '{classes[j]}' {cm[i, j]} times ({misclass_rates[i, j]:.2%})")

# Function to visualize misclassified examples
def visualize_misclassified(X, y_true, y_pred, classes, image_paths, title, max_examples=10):
    # Get indices of misclassified samples
    misclassified_indices = np.where(y_true != y_pred)[0]
    
    if len(misclassified_indices) == 0:
        print("No misclassified examples found.")
        return
    
    # Select up to max_examples random misclassified samples
    if len(misclassified_indices) &gt; max_examples:
        selected_indices = np.random.choice(misclassified_indices, max_examples, replace=False)
    else:
        selected_indices = misclassified_indices
    
    # Create a grid of images
    n_cols = 5
    n_rows = (len(selected_indices) + n_cols - 1) // n_cols
    
    plt.figure(figsize=(15, 3 * n_rows))
    plt.suptitle(f"{title} - Misclassified Examples", fontsize=16)
    
    for i, idx in enumerate(selected_indices):
        # Get the image
        image_path = image_paths[idx]
        img = Image.open(image_path).resize((100, 100))
        
        # Plot
        plt.subplot(n_rows, n_cols, i + 1)
        plt.imshow(img)
        plt.title(f"True: {classes[y_true[idx]]}\nPred: {classes[y_pred[idx]]}")
        plt.axis('off')
    
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(f"{title.replace(' ', '_')}_misclassified.png")
    plt.show()

# Define paths
train_data_path = "../data/Q2/train"  # Path to training data
test_data_path = "../data/Q2/test"    # Path to test data

# Load and preprocess training data
print("Loading training data...")
X_train, y_train, classes, train_paths = load_and_preprocess_images(train_data_path)
print(f"Training data loaded: {X_train.shape[0]} samples, {len(classes)} classes")

# Load and preprocess test data
print("Loading test data...")
X_test, y_test, _, test_paths = load_and_preprocess_images(test_data_path, classes)
print(f"Test data loaded: {X_test.shape[0]} samples")

# # Part (a): Train multi-class SVM with LIBSVM (default in sklearn)
# print("Training multi-class SVM with LIBSVM (Gaussian kernel, gamma=0.001, C=1.0)...")
# start_time = time.time()

# # Create and train SVM with Gaussian kernel
# print("Unique classes in training:", set(y_train))
# print("Unique classes in test:", set(y_test))
# libsvm_model = SVC(kernel='rbf', gamma=0.001, C=1.0)

# print("Fitting LIBSVM model...")
# libsvm_model.fit(X_train, y_train)
# libsvm_training_time = time.time() - start_time
# print(f"LIBSVM training completed in {libsvm_training_time:.2f} seconds")

# # Evaluate LIBSVM on test set
# print("Evaluating LIBSVM on test set...")
# start_time = time.time()
# libsvm_y_pred = libsvm_model.predict(X_test)
# libsvm_test_time = time.time() - start_time

# # Calculate LIBSVM accuracy
# libsvm_accuracy = accuracy_score(y_test, libsvm_y_pred)
# print(f"LIBSVM test accuracy: {libsvm_accuracy:.4f}")
# print(f"LIBSVM prediction time for test set: {libsvm_test_time:.2f} seconds")
# print("\nLIBSVM Classification Report:")
# print(classification_report(y_test, libsvm_y_pred, target_names=classes))

# Part (b): Train multi-class SVM with CVXOPT
try:
    # Try to use custom SVM with CVXOPT if available
    # from sklearn.svm import SVC as CVXOPT_SVC  # Replace this with actual CVXOPT SVM if available
    
    print("\nTraining multi-class SVM with CVXOPT (Gaussian kernel, gamma=0.001, C=1.0)...")
    start_time = time.time()
    
    # In practice, you'd replace this with your CVXOPT implementation
    # This is just a placeholder using standard SVC again
    classifiers, classes = train_multiclass_svm_onevsone(
        X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001
    )
    cvxopt_training_time = time.time() - start_time
    print(f"CVXOPT training completed in {cvxopt_training_time:.2f} seconds")
    
    # Evaluate CVXOPT on test set
    print("Evaluating CVXOPT on test set...")
    start_time = time.time()
    cvxopt_y_pred = predict_multiclass_svm_onevsone(X_test, classifiers, classes)
    cvxopt_test_time = time.time() - start_time
    
    # Calculate CVXOPT accuracy
    cvxopt_accuracy = accuracy_score(y_test, cvxopt_y_pred)
    print(f"CVXOPT test accuracy: {cvxopt_accuracy:.4f}")
    print(f"CVXOPT prediction time for test set: {cvxopt_test_time:.2f} seconds")
    print("\nCVXOPT Classification Report:")
    print(classification_report(y_test, cvxopt_y_pred, target_names=classes))
    
    has_cvxopt = True
except (ImportError, NameError):
    print("\nCVXOPT implementation not available. Skipping CVXOPT evaluation.")
    has_cvxopt = False

# # Create confusion matrices
# print("\nGenerating confusion matrix for LIBSVM...")
# plot_confusion_matrix(y_test, libsvm_y_pred, classes, "LIBSVM Confusion Matrix")

if has_cvxopt:
    print("\nGenerating confusion matrix for CVXOPT...")
    plot_confusion_matrix(y_test, cvxopt_y_pred, classes, "CVXOPT Confusion Matrix")

# # Visualize misclassified examples
# print("\nVisualizing misclassified examples for LIBSVM...")
# visualize_misclassified(X_test, y_test, libsvm_y_pred, classes, test_paths, "LIBSVM", max_examples=10)

if has_cvxopt:
    print("\nVisualizing misclassified examples for CVXOPT...")
    visualize_misclassified(X_test, y_test, cvxopt_y_pred, classes, test_paths, "CVXOPT", max_examples=10)

# # Add analysis section
# print("\n=== Analysis of Results ===")
# print("1. Confusion Matrix Analysis:")
# print("   - The confusion matrix shows which classes are most commonly misclassified.")
# print("   - Darker diagonal elements indicate better classification for those classes.")
# print("   - Off-diagonal elements represent misclassifications between classes.")

# print("\n2. Comparison between LIBSVM and CVXOPT:")
# if has_cvxopt:
#     print(f"   - LIBSVM Accuracy: {libsvm_accuracy:.4f}, Training Time: {libsvm_training_time:.2f}s")
#     print(f"   - CVXOPT Accuracy: {cvxopt_accuracy:.4f}, Training Time: {cvxopt_training_time:.2f}s")
    
#     if libsvm_accuracy &gt; cvxopt_accuracy:
#         print("   - LIBSVM achieves higher accuracy.")
#     elif cvxopt_accuracy &gt; libsvm_accuracy:
#         print("   - CVXOPT achieves higher accuracy.")
#     else:
#         print("   - Both methods achieve similar accuracy.")
    
#     if libsvm_training_time &lt; cvxopt_training_time:
#         print("   - LIBSVM is faster in training.")
#     else:
#         print("   - CVXOPT is faster in training.")
# else:
#     print(f"   - LIBSVM Accuracy: {libsvm_accuracy:.4f}, Training Time: {libsvm_training_time:.2f}s")
#     print("   - CVXOPT implementation not available for comparison.")

# print("\n3. Misclassification Analysis:")
# print("   - The 10 visualized misclassified examples help understand where the model struggles.")
# print("   - Look for patterns in these misclassifications (visual similarities between confused classes).")
# print("   - Consider whether misclassifications make intuitive sense based on visual similarity.")



import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import KFold, cross_val_score
import time
from tqdm import tqdm
import seaborn as sns

# Function to load and preprocess images
def load_and_preprocess_images(data_path, classes=None):
    if classes is None:
        classes = os.listdir(data_path)
        classes = [c for c in classes if os.path.isdir(os.path.join(data_path, c))]
    
    X = []
    y = []
    
    for class_index, class_name in enumerate(classes):
        class_path = os.path.join(data_path, class_name)
        
        print(f"Loading class {class_name}...")
        
        for image_name in tqdm(os.listdir(class_path)):
            image_path = os.path.join(class_path, image_name)
            
            # Load image
            try:
                img = Image.open(image_path)
                
                # Resize to 100x100
                img = img.resize((100, 100))
                
                # Convert to RGB in case image is grayscale or has alpha channel
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Convert to numpy array
                img_array = np.array(img)
                
                # Flatten to 1D vector
                img_flat = img_array.flatten()
                
                # Min-max scaling (normalize to [0, 1])
                img_normalized = img_flat / 255.0
                
                X.append(img_normalized)
                y.append(class_index)
                
            except Exception as e:
                print(f"Error processing image {image_path}: {e}")
                continue
    
    return np.array(X), np.array(y), classes

# Define paths
train_data_path = "../data/Q2/train"  # Path to training data
test_data_path = "../data/Q2/test"    # Path to test data

# Load and preprocess training data
print("Loading training data...")
X_train, y_train, classes = load_and_preprocess_images(train_data_path)
print(f"Training data loaded: {X_train.shape[0]} samples, {len(classes)} classes")

# Load and preprocess test data
print("Loading test data...")
X_test, y_test, _ = load_and_preprocess_images(test_data_path, classes)
print(f"Test data loaded: {X_test.shape[0]} samples")

# Question 8: Hyperparameter tuning using 5-fold cross-validation
def perform_hyperparameter_tuning(X_train, y_train, X_test, y_test):
    # Define the C values to test (10^-5, 10^-3, 1, 5, 10)
    C_values = [1e-5, 1e-3, 1, 5, 10]
    gamma = 0.001  # Fixed gamma value
    k_folds = 5    # 5-fold cross-validation
    
    # Lists to store results
    cv_scores = []  # Cross-validation accuracies
    test_scores = [] # Test set accuracies
    
    print("\n===== Question 8: Hyperparameter Tuning with 5-fold Cross-Validation =====")
    print(f"Fixed gamma = {gamma}, Testing C values: {C_values}")
    
    # (a) Compute cross-validation and test accuracies for each C value
    print("\n(a) Computing 5-fold CV and test accuracies for each C value:")
    
    for C in C_values:
        print(f"\nEvaluating SVM with C={C}, gamma={gamma}")
        
        # Create SVM model with Gaussian kernel
        svm = SVC(kernel='rbf', gamma=gamma, C=C)
        
        # Perform k-fold cross-validation
        start_time = time.time()
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        print("kfold done")
        cv_score = cross_val_score(svm, X_train, y_train, cv=kf, scoring='accuracy').mean()
        cv_time = time.time() - start_time
        cv_scores.append(cv_score)
        
        # Train on full training set and evaluate on test set
        start_time = time.time()
        print("fitting")

        svm.fit(X_train, y_train)
        train_time = time.time() - start_time
        
        start_time = time.time()
        print("predicting")

        test_pred = svm.predict(X_test)
        test_time = time.time() - start_time
        
        test_accuracy = accuracy_score(y_test, test_pred)
        test_scores.append(test_accuracy)
        
        print(f"C={C}:")
        print(f"  5-fold CV Accuracy = {cv_score:.4f} (CV time: {cv_time:.2f}s)")
        print(f"  Test Accuracy = {test_accuracy:.4f} (Train time: {train_time:.2f}s, Test time: {test_time:.2f}s)")
    
    # (b) Plot CV and test accuracies
    print("\n(b) Plotting cross-validation and test accuracies vs C value:")
    
    plt.figure(figsize=(10, 6))
    plt.semilogx(C_values, cv_scores, 'o-', label='5-fold CV Accuracy')
    plt.semilogx(C_values, test_scores, 's-', label='Test Accuracy')
    plt.xlabel('C value (log scale)')
    plt.ylabel('Accuracy')
    plt.title('SVM Performance vs C Value ( = 0.001)')
    plt.legend()
    plt.grid(True)
    
    # Add data points with values
    for i, C in enumerate(C_values):
        plt.annotate(f'{cv_scores[i]:.4f}', 
                     (C, cv_scores[i]), 
                     textcoords="offset points",
                     xytext=(0,10), 
                     ha='center')
        plt.annotate(f'{test_scores[i]:.4f}', 
                     (C, test_scores[i]), 
                     textcoords="offset points",
                     xytext=(0,-15), 
                     ha='center')
    
    plt.tight_layout()
    plt.savefig('hyperparameter_tuning_results.png')
    
    # Find best C value based on CV scores
    best_cv_idx = np.argmax(cv_scores)
    best_C = C_values[best_cv_idx]
    best_cv_score = cv_scores[best_cv_idx]
    
    print(f"\nBest C value from 5-fold CV: {best_C} with CV accuracy: {best_cv_score:.4f}")
    print(f"Test accuracy with C={best_C}: {test_scores[best_cv_idx]:.4f}")
    
    # (c) Train final model with best C value
    print("\n(c) Training final model with best C value:")
    
    # Train baseline model (C=1.0) for comparison
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match218-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    baseline_model = SVC(kernel='rbf', gamma=gamma, C=1.0)
    baseline_model.fit(X_train, y_train)
    baseline_pred = baseline_model.predict(X_test)
    baseline_accuracy = accuracy_score(y_test, baseline_pred)
    
    # Train final model with best C
    final_model = SVC(kernel='rbf', gamma=gamma, C=best_C)
</FONT>    start_time = time.time()
    final_model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    start_time = time.time()
    final_pred = final_model.predict(X_test)
    test_time = time.time() - start_time
    
    final_accuracy = accuracy_score(y_test, final_pred)
    improvement = final_accuracy - baseline_accuracy
    
    print(f"Baseline model (C=1.0) test accuracy: {baseline_accuracy:.4f}")
    print(f"Final model (C={best_C}) test accuracy: {final_accuracy:.4f}")
    print(f"Improvement: {improvement:.4f} ({improvement*100:.2f}%)")
    print(f"Training time: {train_time:.2f}s, Test time: {test_time:.2f}s")
    
    # Additional analysis - classification report
    print("\nDetailed classification report for optimized model:")
    report = classification_report(y_test, final_pred, target_names=classes)
    print(report)
    
    # Return results dictionary
    return {
        'C_values': C_values,
        'cv_scores': cv_scores,
        'test_scores': test_scores,
        'best_C': best_C,
        'best_cv_score': best_cv_score,
        'baseline_accuracy': baseline_accuracy,
        'final_test_accuracy': final_accuracy,
        'improvement': improvement
    }

# Execute hyperparameter tuning
results = perform_hyperparameter_tuning(X_train, y_train, X_test, y_test)

# Observations on results
print("\n===== Observations =====")
print("1. The 5-fold cross-validation shows that the optimal C value is:", results['best_C'])
print(f"2. The test accuracy using this optimal C value is: {results['final_test_accuracy']:.4f}")
print(f"3. Compared to the baseline model (C=1.0), the optimized model {'improved' if results['improvement'] &gt; 0 else 'decreased'} accuracy by {abs(results['improvement'])*100:.2f}%")

# Determine relationship between CV and test accuracy
cv_test_correlation = np.corrcoef(results['cv_scores'], results['test_scores'])[0, 1]
print(f"4. Correlation between CV and test accuracies: {cv_test_correlation:.4f}")

if cv_test_correlation &gt; 0.7:
    print("   This indicates a strong positive correlation, suggesting CV is a reliable predictor of test performance")
elif cv_test_correlation &gt; 0.3:
    print("   This indicates a moderate positive correlation between CV and test performance")
else:
    print("   This indicates a weak correlation, suggesting CV may not be reliably predicting test performance")

# Check if best CV model is also best on test set
cv_best_idx = np.argmax(results['cv_scores'])
test_best_idx = np.argmax(results['test_scores'])
if cv_best_idx == test_best_idx:
    print("5. The C value that gives the best CV accuracy also gives the best test accuracy")
else:
    print(f"5. The C value that gives the best CV accuracy ({results['C_values'][cv_best_idx]}) differs from")
    print(f"   the C value that gives the best test accuracy ({results['C_values'][test_best_idx]})")

print("\n===== Conclusion =====")
print("Based on the hyperparameter tuning results, we can conclude that:")
if results['improvement'] &gt; 0:
    print(f"- Optimizing the C value through cross-validation successfully improved model performance")
    print(f"- The optimal C value of {results['best_C']} performs better than the default C=1.0")
else:
    print(f"- Cross-validation did not lead to improved test performance in this case")
    print(f"- The default C=1.0 might already be near-optimal for this dataset")
    
print("- 5-fold cross-validation provided a reliable method for hyperparameter selection")
plt.show()



import cvxopt
import numpy as np
from cvxopt import matrix, solvers

class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alphas = None
        self.support_vectors = None
        self.support_vector_labels = None
        self.support_vector_indices = None
        self.w = None
        self.b = None
        self.kernel_type = None
        self.gamma = None
        
    def _linear_kernel(self, x1, x2):
        """Compute linear kernel between two vectors"""
        return np.dot(x1, x2)
    
    def _gaussian_kernel(self, x1, x2):
        """Compute Gaussian (RBF) kernel between two vectors"""
        return np.exp(-self.gamma * np.sum((x1 - x2) ** 2))
    
    def _compute_kernel_matrix(self, X, Y=None):
        """Compute the kernel matrix for given data"""
        if Y is None:
            Y = X
        
        if self.kernel_type == 'linear':
        # Linear kernel: K(x,y) = x^T y     
            return np.dot(X, Y.T)
        else:  # gaussian
            # Vectorized computation of Gaussian kernel
            # ||x-y||^2 = ||x||^2 + ||y||^2 - 2*x^T*y
            X2 = np.sum(X**2, axis=1).reshape(-1, 1)
            Y2 = np.sum(Y**2, axis=1).reshape(1, -1)
            XY = np.dot(X, Y.T)
            
            # Compute squared distances
            distances = X2 + Y2 - 2 * XY
            
            # Apply RBF function
            return np.exp(-self.gamma * distances)
        
    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        self.kernel_type = kernel
        self.gamma = gamma
        
        # Store the original data
        self.X_train = X
        self.y_train = y
        
        n_samples, n_features = X.shape
        
        # Convert binary labels (0, 1) to (-1, 1) for SVM
        y_binary = np.where(y == 0, -1, 1)
        
        # Compute the kernel matrix
        K = self._compute_kernel_matrix(X)
        
        # Formulate the quadratic programming problem for CVXOPT
        # The objective function to maximize is:
        # max sum(alpha_i) - 0.5 * sum(alpha_i * alpha_j * y_i * y_j * K(x_i, x_j))
        # For minimization, we negate the objective function
        
        # P matrix: (y_i * y_j * K(x_i, x_j))
        P = matrix(np.outer(y_binary, y_binary) * K)
        
        # q vector: -1 for each sample
        q = matrix(-np.ones(n_samples))
        
        # Inequality constraints: 0 &lt;= alpha_i &lt;= C
        # G and h matrices define the constraints: G * alpha &lt;= h
        # -alpha &lt;= 0  and  alpha &lt;= C
        G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))
        h = matrix(np.hstack((np.zeros(n_samples), C * np.ones(n_samples))))
        
        # Equality constraint: sum(alpha_i * y_i) = 0
        A = matrix(y_binary.reshape(1, -1).astype(float))
        b = matrix(np.zeros(1))
        
        # Solve the quadratic programming problem
        solvers.options['show_progress'] = False
        solution = solvers.qp(P, q, G, h, A, b)
        
        # Extract Lagrange multipliers
        alphas = np.array(solution['x']).flatten()
        
        # Identify support vectors (non-zero alphas)
        sv_threshold = 1e-5
        sv_indices =np.where((alphas &gt; sv_threshold) & (alphas &lt; C))[0]
        
        self.alphas = alphas
        self.support_vector_indices = sv_indices
        self.support_vectors = X[sv_indices]
        self.support_vector_labels = y_binary[sv_indices]
        
        # For linear kernel, compute w and b explicitly
        if kernel == 'linear':
            self.w = np.sum((self.alphas[sv_indices] * y_binary[sv_indices]).reshape(-1, 1) * X[sv_indices], axis=0)
        
        # Compute bias term b
        # b is calculated using the fact that for any support vector x_s: y_s(wx_s + b) = 1
        b_sum = 0
        for i in sv_indices:
            if kernel == 'linear':
                b_sum += y_binary[i] - np.dot(self.w, X[i])
            else:  # gaussian kernel
                s = 0
                for alpha_idx, sv_idx in enumerate(sv_indices):
                    s += alphas[sv_idx] * y_binary[sv_idx] * self._gaussian_kernel(X[sv_idx], X[i])
                b_sum += y_binary[i] - s
        
        self.b = b_sum / len(sv_indices) if len(sv_indices) &gt; 0 else 0
        
        return self

    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        y_binary = np.where(self.y_train == 0, -1, 1)
        if self.kernel_type == 'linear':
            # For linear kernel, we can use w and b directly
            decision_values = np.dot(X, self.w) + self.b
        else:  # gaussian kernel
            # For gaussian kernel, we need to compute the decision function using the kernel trick
            K = self._compute_kernel_matrix(X, self.X_train)
            alpha_y = self.alphas * y_binary
            # Only use support vectors for prediction (where alpha &gt; 0)
            sv_indices = self.support_vector_indices
            decision_values = np.sum(alpha_y[sv_indices].reshape(1, -1) * K[:, sv_indices], axis=1) + self.b
        
        # Convert decision values to class labels (0 or 1)
        predictions = np.where(decision_values &lt; 0, 0, 1)
        
        return predictions
    
    def get_support_vectors_count(self):
        """Return the number of support vectors"""
        return len(self.support_vector_indices)
    
    def get_support_vectors_percentage(self):
        """Return the percentage of training samples that are support vectors"""
        return (len(self.support_vector_indices) / len(self.X_train)) * 100
    
    def get_weights(self):
        """Return the weight vector (for linear kernel only)"""
        if self.kernel_type != 'linear':
            raise ValueError("Weight vector is only available for linear kernel")
        return self.w
    
    def get_bias(self):
        """Return the bias term"""
        return self.b
    
    def get_top_support_vectors(self, n=5):
        """Return the indices of the top n support vectors with highest alpha values"""
        sv_indices = self.support_vector_indices
        alphas = self.alphas[sv_indices]
        top_indices = sv_indices[np.argsort(-alphas)[:n]]
        return top_indices

</PRE>
</PRE>
</BODY>
</HTML>
