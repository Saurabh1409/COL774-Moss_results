<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_BLTHQ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_BLTHQ.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from collections import defaultdict, Counter
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import nltk

# Download required NLTK resources (if not already present)
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer


# 1. Naïve Bayes Classifier Implementation

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}
        self.word_counts = {}         # counts for each class: {class: {word: count}}
        self.total_words = {}         # total word counts for each class
        self.vocab = set()            # global vocabulary
        self.alpha = None             # Laplace smoothing parameter
        self.classes = None           # list of classes

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """
        Learn model parameters from training data.
        """
        self.alpha = smoothening
        self.classes = df[class_col].unique()
        self.class_priors = {}
        self.word_counts = {c: defaultdict(int) for c in self.classes}
<A NAME="2"></A><FONT color = #0000FF><A HREF="match103-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.total_words = {c: 0 for c in self.classes}
        doc_counts = {c: 0 for c in self.classes}
        
        for _, row in df.iterrows():
</FONT>            c = row[class_col]
            doc_counts[c] += 1
            tokens = row[text_col]
            for token in tokens:
                self.word_counts[c][token] += 1
                self.total_words[c] += 1
                self.vocab.add(token)
                
        total_docs = len(df)
        # Using logarithms for priors
        for c in self.classes:
            self.class_priors[c] = np.log(doc_counts[c] / total_docs)
            
        # Precompute log likelihoods for each word in each class
        self.log_likelihoods = {c: {} for c in self.classes}
        V = len(self.vocab)
        for c in self.classes:
            for word in self.vocab:
                count = self.word_counts[c].get(word, 0)
                self.log_likelihoods[c][word] = np.log((count + self.alpha) / (self.total_words[c] + self.alpha * V))
        self.V = V

    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predicts class labels and appends a column with the predictions.
        """
        predictions = []
        for _, row in df.iterrows():
            tokens = row[text_col]
            class_scores = {}
            for c in self.classes:
                score = self.class_priors[c]
                for token in tokens:
                    if token in self.vocab:
                        score += self.log_likelihoods[c][token]
                    else:
                        # Unseen words: use only smoothing term
                        score += np.log(self.alpha / (self.total_words[c] + self.alpha * self.V))
                class_scores[c] = score
            predicted_class = max(class_scores, key=class_scores.get)
            predictions.append(predicted_class)
        df[predicted_col] = predictions
        return df


# 2. Helper Functions for Text Processing

def simple_tokenizer(text):
    """Lowercase, remove punctuation and split on whitespace."""
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)
    tokens = text.split()
    return tokens

def preprocess_tokens(tokens, remove_stopwords=True, stem=True):
    """Removes English stopwords and applies stemming."""
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        tokens = [token for token in tokens if token not in stop_words]
    if stem:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(token) for token in tokens]
    return tokens

def create_bigrams(tokens):
    """Creates bigrams from a list of tokens."""
    return [tokens[i] + '_' + tokens[i+1] for i in range(len(tokens)-1)]

def combine_unigrams_bigrams(tokens):
    """Returns a list containing both unigrams and bigrams."""
    return tokens + create_bigrams(tokens)

def add_pos_tags(tokens):
    """Appends POS tag tokens (prefixed with 'POS_') to the token list."""
    tagged = nltk.pos_tag(tokens)
    pos_tokens = ["POS_" + tag for word, tag in tagged]
    return tokens + pos_tokens


# 3. Evaluation and Plotting Functions

def evaluate_model(df, true_col="Class Index", pred_col="Predicted"):
    y_true = df[true_col]
    y_pred = df[pred_col]
    acc = accuracy_score(y_true, y_pred)
    print("Accuracy: {:.2f}%".format(acc * 100))
    print("Classification Report:")
    print(classification_report(y_true, y_pred))
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6,6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.colorbar()
    tick_marks = np.arange(len(np.unique(y_true)))
    plt.xticks(tick_marks, np.unique(y_true))
    plt.yticks(tick_marks, np.unique(y_true))
    plt.xlabel("Predicted")
    plt.ylabel("True")
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] &gt; thresh else "black")
    plt.show()
    
def plot_wordcloud(freq_dict, title):
    wc = WordCloud(width=400, height=300, background_color='white').generate_from_frequencies(freq_dict)
    plt.figure()
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()


# 4. Data Loading (Assuming CSV files)

def load_data(train_path, test_path):
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    # Tokenize raw text fields (using simple_tokenizer)
    train_df['Tokenized Description'] = train_df['Description'].apply(simple_tokenizer)
    test_df['Tokenized Description'] = test_df['Description'].apply(simple_tokenizer)
    train_df['Tokenized Title'] = train_df['Title'].apply(simple_tokenizer)
    test_df['Tokenized Title'] = test_df['Title'].apply(simple_tokenizer)
    
    return train_df, test_df


# 5. Main Execution: Running Experiments

if __name__ == '__main__':
    train_path = "../data/Q1/train.csv"
    test_path = "../data/Q1/test.csv"
    
    # Load data
    train_df, test_df = load_data(train_path, test_path)
    
    ########
    # Experiment 1: Naïve Bayes on Raw Description
    ########
    print("=== Experiment 1: Raw Description ===")
    nb_raw = NaiveBayes()
    nb_raw.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized Description")
    train_df = nb_raw.predict(train_df, text_col="Tokenized Description", predicted_col="Predicted_Raw")
    test_df = nb_raw.predict(test_df, text_col="Tokenized Description", predicted_col="Predicted_Raw")
    
    print("Training Set Performance (Raw):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Raw")
    print("Test Set Performance (Raw):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Raw")
    
    # Plot word clouds (most frequent words) for each class from raw model counts
    print("Plotting word clouds for raw description per class...")
    for c in sorted(nb_raw.classes):
        freq_dict = dict(nb_raw.word_counts[c])
        plot_wordcloud(freq_dict, f"Word Cloud for Class {c} (Raw)")
    
    ########
    # Experiment 2: Preprocessing (Stopword Removal + Stemming)
    ########
    print("=== Experiment 2: Preprocessed Description ===")
    # Create new token columns with stopword removal and stemming
    train_df['Tokenized Description_Preproc'] = train_df['Description'].apply(
        lambda x: preprocess_tokens(simple_tokenizer(x)))
    test_df['Tokenized Description_Preproc'] = test_df['Description'].apply(
        lambda x: preprocess_tokens(simple_tokenizer(x)))
    
    # Plot word clouds for each class on preprocessed data
    # (compute frequency counts over the preprocessed tokens for each class)
    class_tokens = {}
    for c in sorted(train_df["Class Index"].unique()):
        tokens = []
        for doc in train_df[train_df["Class Index"]==c]['Tokenized Description_Preproc']:
            tokens.extend(doc)
        class_tokens[c] = dict(Counter(tokens))
        plot_wordcloud(class_tokens[c], f"Word Cloud for Class {c} (Preprocessed)")
    
    # Train Naïve Bayes on preprocessed tokens
    nb_preproc = NaiveBayes()
    nb_preproc.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized Description_Preproc")
    train_df = nb_preproc.predict(train_df, text_col="Tokenized Description_Preproc", predicted_col="Predicted_Preproc")
    test_df = nb_preproc.predict(test_df, text_col="Tokenized Description_Preproc", predicted_col="Predicted_Preproc")
    
    print("Training Set Performance (Preprocessed):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Preproc")
    print("Test Set Performance (Preprocessed):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Preproc")
    
    ########
    # Experiment 3: Unigrams + Bigrams
    ########
    print("=== Experiment 3: Unigrams + Bigrams ===")
    # Create new column combining unigrams and bigrams (based on preprocessed tokens)
    train_df['Tokenized_Desc_UniBi'] = train_df['Tokenized Description_Preproc'].apply(combine_unigrams_bigrams)
    test_df['Tokenized_Desc_UniBi'] = test_df['Tokenized Description_Preproc'].apply(combine_unigrams_bigrams)
    
    nb_unibi = NaiveBayes()
    nb_unibi.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Desc_UniBi")
    train_df = nb_unibi.predict(train_df, text_col="Tokenized_Desc_UniBi", predicted_col="Predicted_UniBi")
    test_df = nb_unibi.predict(test_df, text_col="Tokenized_Desc_UniBi", predicted_col="Predicted_UniBi")
    
    print("Training Set Performance (Unigrams+Bigrams):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_UniBi")
    print("Test Set Performance (Unigrams+Bigrams):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_UniBi")
    
    ########
    # Experiment 5: Title Features Only
    ########
    print("=== Experiment 5a: Raw Title Features ===")
    # Use raw tokenized title (no preprocessing)
    train_df['Tokenized_Title_Raw'] = train_df['Title'].apply(simple_tokenizer)
    test_df['Tokenized_Title_Raw'] = test_df['Title'].apply(simple_tokenizer)
    
    nb_title_raw = NaiveBayes()
    nb_title_raw.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Title_Raw")
    train_df = nb_title_raw.predict(train_df, text_col="Tokenized_Title_Raw", predicted_col="Predicted_Title_Raw")
    test_df = nb_title_raw.predict(test_df, text_col="Tokenized_Title_Raw", predicted_col="Predicted_Title_Raw")
    
    print("Training Set Performance (Raw Title):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Title_Raw")
    print("Test Set Performance (Raw Title):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Title_Raw")
    
    
    print("=== Experiment 5b: Preprocessed Title Features ===")
    # Preprocess title texts (stopword removal and stemming)
    train_df['Tokenized_Title_Preproc'] = train_df['Title'].apply(
        lambda x: preprocess_tokens(simple_tokenizer(x)))
    test_df['Tokenized_Title_Preproc'] = test_df['Title'].apply(
        lambda x: preprocess_tokens(simple_tokenizer(x)))
    
    nb_title_preproc = NaiveBayes()
    nb_title_preproc.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Title_Preproc")
    train_df = nb_title_preproc.predict(train_df, text_col="Tokenized_Title_Preproc", predicted_col="Predicted_Title_Preproc")
    test_df = nb_title_preproc.predict(test_df, text_col="Tokenized_Title_Preproc", predicted_col="Predicted_Title_Preproc")
    
    print("Training Set Performance (Preprocessed Title):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Title_Preproc")
    print("Test Set Performance (Preprocessed Title):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Title_Preproc")
    
    
    print("=== Experiment 5c: Title Unigrams + Bigrams ===")
    # Build on preprocessed tokens: combine unigrams and bigrams for title
    train_df['Tokenized_Title_UniBi'] = train_df['Tokenized_Title_Preproc'].apply(combine_unigrams_bigrams)
    test_df['Tokenized_Title_UniBi'] = test_df['Tokenized_Title_Preproc'].apply(combine_unigrams_bigrams)
    
    nb_title_unibi = NaiveBayes()
    nb_title_unibi.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Title_UniBi")
    train_df = nb_title_unibi.predict(train_df, text_col="Tokenized_Title_UniBi", predicted_col="Predicted_Title_UniBi")
    test_df = nb_title_unibi.predict(test_df, text_col="Tokenized_Title_UniBi", predicted_col="Predicted_Title_UniBi")
    
    print("Training Set Performance (Title Unigrams+Bigrams):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Title_UniBi")
    print("Test Set Performance (Title Unigrams+Bigrams):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Title_UniBi")

    ########
    # Experiment 6: Combining Title and Description
    ########
    print("=== Experiment 6a: Combined Unigram + Bigram Features ===")
    # Concatenate preprocessed title and description tokens, then generate unigrams + bigrams
    train_df['Tokenized_Combined_UniBi'] = train_df.apply(
        lambda row: combine_unigrams_bigrams(row['Tokenized_Title_Preproc'] + row['Tokenized Description_Preproc']), axis=1)
    test_df['Tokenized_Combined_UniBi'] = test_df.apply(
        lambda row: combine_unigrams_bigrams(row['Tokenized_Title_Preproc'] + row['Tokenized Description_Preproc']), axis=1)
    
    nb_combined_unibi = NaiveBayes()
    nb_combined_unibi.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Combined_UniBi")
    train_df = nb_combined_unibi.predict(train_df, text_col="Tokenized_Combined_UniBi", predicted_col="Predicted_Combined_UniBi")
    test_df = nb_combined_unibi.predict(test_df, text_col="Tokenized_Combined_UniBi", predicted_col="Predicted_Combined_UniBi")
    
    print("Training Set Performance (Combined Unigrams+Bigrams):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Combined_UniBi")
    print("Test Set Performance (Combined Unigrams+Bigrams):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Combined_UniBi")
    print("=== Experiment 6b: Separate Parameters for Title & Description ===")
    # Here we implement a separate-parameter model.
    # For clarity, we define a new class:
    class NaiveBayesSeparate:
        def __init__(self):
            self.class_priors = {}
            self.title_word_counts = {}
            self.desc_word_counts = {}
            self.total_title_words = {}
            self.total_desc_words = {}
            self.title_vocab = set()
            self.desc_vocab = set()
            self.alpha = None
            self.classes = None

        def fit(self, df, smoothening, class_col="Class Index",
                title_col="Tokenized_Title_Preproc", desc_col="Tokenized Description_Preproc"):
            self.alpha = smoothening
            self.classes = df[class_col].unique()
            self.class_priors = {}
            self.title_word_counts = {c: defaultdict(int) for c in self.classes}
            self.desc_word_counts = {c: defaultdict(int) for c in self.classes}
            self.total_title_words = {c: 0 for c in self.classes}
            self.total_desc_words = {c: 0 for c in self.classes}
            doc_counts = {c: 0 for c in self.classes}
            for _, row in df.iterrows():
                c = row[class_col]
                doc_counts[c] += 1
                for token in row[title_col]:
                    self.title_word_counts[c][token] += 1
                    self.total_title_words[c] += 1
                    self.title_vocab.add(token)
                for token in row[desc_col]:
                    self.desc_word_counts[c][token] += 1
                    self.total_desc_words[c] += 1
                    self.desc_vocab.add(token)
            total_docs = len(df)
            for c in self.classes:
                self.class_priors[c] = np.log(doc_counts[c] / total_docs)
            V_title = len(self.title_vocab)
            V_desc = len(self.desc_vocab)
            self.V_title = V_title
            self.V_desc = V_desc
            self.title_log_likelihoods = {c: {} for c in self.classes}
            self.desc_log_likelihoods = {c: {} for c in self.classes}
            for c in self.classes:
                for word in self.title_vocab:
                    count = self.title_word_counts[c].get(word, 0)
                    self.title_log_likelihoods[c][word] = np.log((count + self.alpha) / (self.total_title_words[c] + self.alpha * V_title))
                for word in self.desc_vocab:
                    count = self.desc_word_counts[c].get(word, 0)
                    self.desc_log_likelihoods[c][word] = np.log((count + self.alpha) / (self.total_desc_words[c] + self.alpha * V_desc))

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match103-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        def predict(self, df, title_col="Tokenized_Title_Preproc", desc_col="Tokenized Description_Preproc", predicted_col="Predicted_Separate"):
            predictions = []
            for _, row in df.iterrows():
</FONT>                class_scores = {}
                for c in self.classes:
                    score = self.class_priors[c]
                    for token in row[title_col]:
                        if token in self.title_vocab:
                            score += self.title_log_likelihoods[c][token]
                        else:
                            score += np.log(self.alpha / (self.total_title_words[c] + self.alpha * self.V_title))
                    for token in row[desc_col]:
                        if token in self.desc_vocab:
                            score += self.desc_log_likelihoods[c][token]
                        else:
                            score += np.log(self.alpha / (self.total_desc_words[c] + self.alpha * self.V_desc))
                    class_scores[c] = score
                predictions.append(max(class_scores, key=class_scores.get))
            df[predicted_col] = predictions
            return df

    nb_sep = NaiveBayesSeparate()
    nb_sep.fit(train_df, smoothening=1.0, class_col="Class Index",
               title_col="Tokenized_Title_Preproc", desc_col="Tokenized Description_Preproc")
    train_df = nb_sep.predict(train_df, title_col="Tokenized_Title_Preproc", desc_col="Tokenized Description_Preproc", predicted_col="Predicted_Separate")
    test_df = nb_sep.predict(test_df, title_col="Tokenized_Title_Preproc", desc_col="Tokenized Description_Preproc", predicted_col="Predicted_Separate")
    
    print("Training Set Performance (Separate Parameters):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Separate")
    print("Test Set Performance (Separate Parameters):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Separate")
    
    ########
    # Experiment 7: Baseline Comparisons
    ########
    print("=== Experiment 7: Baseline Comparisons ===")
    def random_baseline(df, class_col="Class Index"):
        np.random.seed(42)
        random_preds = np.random.choice(df[class_col].unique(), size=len(df))
        acc = accuracy_score(df[class_col], random_preds)
        print("Random Baseline Accuracy: {:.2f}%".format(acc * 100))
    
    def majority_baseline(df, class_col="Class Index"):
        majority_class = df[class_col].mode()[0]
        preds = [majority_class] * len(df)
        acc = accuracy_score(df[class_col], preds)
        print("Majority Baseline Accuracy: {:.2f}%".format(acc * 100))
    
    print("Validation (Test) Baselines:")
    random_baseline(test_df)
    majority_baseline(test_df)
    
    ########
    # Experiment 9: Additional Feature Engineering (POS Tags)
    ########
    # print("=== Experiment 9: Adding POS Tag Features ===")
    # # Append POS tag tokens to the preprocessed description tokens.
    # train_df['Tokenized_Desc_POS'] = train_df['Tokenized Description_Preproc'].apply(add_pos_tags)
    # test_df['Tokenized_Desc_POS'] = test_df['Tokenized Description_Preproc'].apply(add_pos_tags)
    
    # nb_pos = NaiveBayes()
    # nb_pos.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized_Desc_POS")
    # train_df = nb_pos.predict(train_df, text_col="Tokenized_Desc_POS", predicted_col="Predicted_POS")
    # test_df = nb_pos.predict(test_df, text_col="Tokenized_Desc_POS", predicted_col="Predicted_POS")
    
    # print("Training Set Performance (POS Enhanced):")
    # evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_POS")
    # print("Test Set Performance (POS Enhanced):")
    # evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_POS")
    
    print("=== Experiment 9: Additional Feature Engineering ===")
    # Map existing processed token columns to common names for consistency.
    train_df['Processed Description'] = train_df['Tokenized Description_Preproc']
    test_df['Processed Description'] = test_df['Tokenized Description_Preproc']
    train_df['Processed Title'] = train_df['Tokenized_Title_Preproc']
    test_df['Processed Title'] = test_df['Tokenized_Title_Preproc']

    # Calculate word probabilities from the preprocessed description model.
    # We use the nb_preproc model from Experiment 2.
    nb_model_processed = nb_preproc
    nb_model_processed.word_probs = {}
    for c in nb_model_processed.classes:
        nb_model_processed.word_probs[c] = {}
        for word in nb_model_processed.vocab:
            # Use log likelihoods to recover probabilities.
            nb_model_processed.word_probs[c][word] = nb_model_processed.log_likelihoods[c].get(
                word,
                np.log(nb_model_processed.alpha / 
                       (nb_model_processed.total_words[c] + nb_model_processed.alpha * nb_model_processed.V))
            )

    def get_discriminative_words(nb_model, num_words=10):
        """Get most discriminative words for each class."""
        discriminative_words = {c: [] for c in nb_model.classes}
        for word in nb_model.vocab:
            for c in nb_model.classes:
                # Calculate probability for the word in the current class.
                word_prob_in_class = np.exp(nb_model.word_probs[c][word])
                # Average probability for this word in the other classes.
                other_probs = [np.exp(nb_model.word_probs[other_c][word])
                               for other_c in nb_model.classes if other_c != c]
                word_prob_in_other_classes = np.mean(other_probs) if other_probs else 0
                if word_prob_in_other_classes &gt; 0:
                    discriminative_power = word_prob_in_class / word_prob_in_other_classes
                    discriminative_words[c].append((word, discriminative_power))
        # For each class, sort the words by discriminative power and keep the top num_words.
        for c in nb_model.classes:
            discriminative_words[c] = sorted(discriminative_words[c], key=lambda x: x[1], reverse=True)[:num_words]
        return discriminative_words

    # Get discriminative words from the processed model.
    discriminative_words = get_discriminative_words(nb_model_processed, num_words=10)
    classes = nb_model_processed.classes

    # Add count features for each class based on discriminative words.
    for c in classes:
        top_words = [word for word, _ in discriminative_words[c]]
        feature_name = f'Class_{c}_KeywordCount'
        train_df[feature_name] = train_df['Processed Description'].apply(
            lambda tokens: sum(1 for token in tokens if token in top_words)
        )
        test_df[feature_name] = test_df['Processed Description'].apply(
            lambda tokens: sum(1 for token in tokens if token in top_words)
        )

    # Bucket these counts into categories: 0 as 'none', 1-2 as 'low', and 3+ as 'high'.
    for c in classes:
        feature_name = f'Class_{c}_KeywordCount'
        cat_feature = f'{feature_name}_Cat'
        train_df[cat_feature] = train_df[feature_name].apply(
            lambda x: 'none' if x == 0 else ('low' if x &lt;= 2 else 'high')
        )
        test_df[cat_feature] = test_df[feature_name].apply(
            lambda x: 'none' if x == 0 else ('low' if x &lt;= 2 else 'high')
        )

    # Combine all features: processed title tokens + processed description tokens +
    # the categorical discriminative keyword count features.
    def combine_features(row):
        features = row['Processed Title'] + row['Processed Description']
        for c in classes:
            cat_feature = f'Class_{c}_KeywordCount_Cat'
            features.append(f"class{c}_key_{row[cat_feature]}")
        return features

    train_df['Combined_Features'] = train_df.apply(combine_features, axis=1)
    test_df['Combined_Features'] = test_df.apply(combine_features, axis=1)

    # Train and evaluate a new Naïve Bayes model using the enhanced combined features.
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match103-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    nb_model_enhanced = NaiveBayes()
    nb_model_enhanced.fit(train_df, smoothening=1.0, text_col='Combined_Features')
    train_df = nb_model_enhanced.predict(train_df, text_col='Combined_Features', predicted_col='Predicted_Enhanced')
    test_df = nb_model_enhanced.predict(test_df, text_col='Combined_Features', predicted_col='Predicted_Enhanced')
</FONT>    
    print("Training Set Performance (Enhanced Features):")
    evaluate_model(train_df, true_col="Class Index", pred_col="Predicted_Enhanced")
    print("Test Set Performance (Enhanced Features):")
    evaluate_model(test_df, true_col="Class Index", pred_col="Predicted_Enhanced")
    print("=== End of Naïve Bayes' Experiments ===")



import os
import glob
import time
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import cvxopt
from cvxopt import matrix, solvers
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix
import itertools
from sklearn.model_selection import cross_val_score

class SupportVectorMachine:
    '''
    Binary Classifier solved via CVXOPT.
    Supports two kernels: linear and gaussian.
    For linear kernel, computes weight vector w.
    For gaussian kernel (K(x,z)=exp(-gamma*||x-z||^2)), w is not explicitly formed.
    '''
    def __init__(self):
        self.w = None
        self.b = 0
        self.alphas = None
        self.support_vectors = None
        self.support_vector_labels = None
        self.sv_idx = None
        self.kernel = None
        self.gamma = None  # used for gaussian kernel
        
    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        '''
        Train the SVM using the dual formulation.
        For labels, converts 0 -&gt; -1 and 1 -&gt; +1.
        If kernel=='linear', uses K = X X^T.
        If kernel=='gaussian', computes K[i,j] = exp(-gamma * ||x_i - x_j||^2).
        '''
        n_samples = X.shape[0]
        self.kernel = kernel
        self.gamma = gamma
        
        # Convert labels: 0-&gt; -1, 1 -&gt; +1
        y = y.astype(np.float64)
        y = np.where(y == 0, -1, 1)
        
        # Compute kernel matrix
        if kernel == 'linear':
            K = np.dot(X, X.T)
        elif kernel == 'gaussian':
<A NAME="1"></A><FONT color = #00FF00><A HREF="match103-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            K = np.zeros((n_samples, n_samples))
            for i in range(n_samples):
                for j in range(n_samples):
                    diff = X[i] - X[j]
                    K[i, j] = np.exp(-gamma * np.dot(diff, diff))
</FONT>        else:
            raise ValueError("Unknown kernel")
            
        # Set up quadratic program: P, q, G, h, A, b as required by CVXOPT.
        P = matrix(np.outer(y, y) * K, tc='d')
        q = matrix(-np.ones(n_samples), tc='d')
        # A must be 1 x n_samples double matrix.
        A = matrix(y.reshape(1, -1), tc='d')
        b_mat = matrix(0.0, (1, 1), tc='d')
        
        # Inequality constraints: 0 &lt;= alpha_i &lt;= C.
        G_std = -np.eye(n_samples)
        h_std = np.zeros(n_samples)
        G_slack = np.eye(n_samples)
        h_slack = np.ones(n_samples) * C
        G = matrix(np.vstack((G_std, G_slack)), tc='d')
        h = matrix(np.hstack((h_std, h_slack)), tc='d')
        
        solvers.options['show_progress'] = False
        solution = solvers.qp(P, q, G, h, A, b_mat)
        alphas_all = np.ravel(solution['x'])
        
        # Select support vectors (alpha &gt; 1e-5)
        sv = alphas_all &gt; 1e-5
        self.sv_idx = np.where(sv)[0]
        self.alphas = alphas_all[sv]
        self.support_vectors = X[sv]
        self.support_vector_labels = y[sv]
        
        if kernel == 'linear':
            # Compute weight vector: w = sum(alpha_i * y_i * x_i)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match103-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.w = np.sum((self.alphas * self.support_vector_labels)[:, np.newaxis] * self.support_vectors, axis=0)
            # Compute bias as average: b = y_i - w^T x_i over support vectors.
            self.b = np.mean(self.support_vector_labels - np.dot(self.support_vectors, self.w))
</FONT>        elif kernel == 'gaussian':
            # Cannot form w explicitly.
            # Compute bias b = average( y_i - sum_j(alpha_j*y_j*K(x_j, x_i)) ) for support vectors.
            b_sum = 0
            for i in range(len(self.alphas)):
                s = 0
                for alpha, label, sv in zip(self.alphas, self.support_vector_labels, self.support_vectors):
                    diff = self.support_vectors[i] - sv
                    s += alpha * label * np.exp(-gamma * np.dot(diff, diff))
                b_sum += (self.support_vector_labels[i] - s)
            self.b = b_sum / len(self.alphas)
        
        # Reporting support vector information.
        n_sv = len(self.alphas)
        print(f"[{kernel.upper()}] Number of support vectors: {n_sv}")
        print(f"[{kernel.upper()}] Percentage of training samples as support vectors: {n_sv / n_samples * 100:.2f}%")
        
    def project(self, X):
        '''
        Compute decision function f(x)= sum(alpha_i*y_i*K(x_i,x)) + b.
        For linear kernel K(x,z)=x^T z.
        For gaussian kernel, K(x,z)= exp(-gamma*||x-z||^2).
        '''
        if self.kernel == 'linear':
            return np.dot(X, self.w) + self.b
        elif self.kernel == 'gaussian':
            y_predict = []
            for i in range(X.shape[0]):
                s = 0
                for alpha, label, sv in zip(self.alphas, self.support_vector_labels, self.support_vectors):
                    diff = X[i] - sv
                    s += alpha * label * np.exp(-self.gamma * np.dot(diff, diff))
                y_predict.append(s)
            return np.array(y_predict) + self.b
        else:
            raise ValueError("Unknown kernel")
            
    def predict(self, X):
        '''
        Predict class labels: 1 if decision function &gt;= 0, else 0.
        '''
        return np.where(self.project(X) &gt;= 0, 1, 0)

# ------- Image Preprocessing and Data Loading -------
def preprocess_image(image_path):
    try:
        with Image.open(image_path) as img:
            img = img.convert('RGB')
            width, height = img.size
            # If dimensions are at least 100, center crop; otherwise, resize.
            if width &gt;= 100 and height &gt;= 100:
                left = (width - 100) // 2
                top = (height - 100) // 2
                right = left + 100
                bottom = top + 100
                img = img.crop((left, top, right, bottom))
            else:
                img = img.resize((100, 100))
            img_array = np.asarray(img, dtype=np.float32) / 255.0
            return img_array.flatten()
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None

def load_data(data_dir, selected_classes):
    X, y = [], []
    for label, class_name in enumerate(selected_classes):
        class_dir = os.path.join(data_dir, class_name)
        image_paths = glob.glob(os.path.join(class_dir, '*.jpg'))
        for path in image_paths:
            x = preprocess_image(path)
            if x is not None:
                X.append(x)
                y.append(label)
    X = np.array(X)
    y = np.array(y)
    return X, y

def train_binary_svm_for_multiclass(X, y, class1, class2, C=1.0, gamma=0.001):
    idx = np.where((y == class1) | (y == class2))[0]
    X_pair = X[idx]
    new_y = (y[idx] == class2).astype(np.int32)  # class1 -&gt; 0, class2 -&gt; 1
    model = SupportVectorMachine()
    model.fit(X_pair, new_y, kernel='gaussian', C=C, gamma=gamma)
    return model

def multi_class_svm_predict(X, classifiers, num_classes):
    n = X.shape[0]
    votes = np.zeros((n, num_classes), dtype=int)
    scores = np.zeros((n, num_classes), dtype=float)
    for (i, j), model in classifiers.items():
        f = model.project(X)  # decision function values for X on classifier (i,j)
        for idx in range(n):
            if f[idx] &gt;= 0:
                votes[idx, j] += 1
                scores[idx, j] += f[idx]
            else:
                votes[idx, i] += 1
                scores[idx, i] += -f[idx]
    pred = np.zeros(n, dtype=int)
    for idx in range(n):
        max_votes = np.max(votes[idx])
        candidates = np.where(votes[idx] == max_votes)[0]
        if len(candidates) == 1:
            pred[idx] = candidates[0]
        else:
            pred[idx] = candidates[np.argmax(scores[idx, candidates])]
    return pred

def multi_class_svm_experiment(X_train, y_train, X_test, y_test, C=1.0, gamma=0.001):
    num_classes = len(np.unique(y_train))
    classifiers = {}
    for i in range(num_classes):
        for j in range(i+1, num_classes):
            model = train_binary_svm_for_multiclass(X_train, y_train, i, j, C, gamma)
            classifiers[(i, j)] = model
            print(f"Trained classifier for classes {i} vs {j}")
    y_pred = multi_class_svm_predict(X_test, classifiers, num_classes)
    acc = np.mean(y_pred == y_test) * 100
    print(f"\nMulti-class SVM Test set accuracy: {acc:.2f}%")


def plot_confusion_matrix(cm, classes, title='Confusion matrix'):
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] &gt; thresh else "black")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# ----- Visualize Misclassified Examples -----
def visualize_misclassifications(X, y_true, y_pred, classes, title_prefix, num_examples=10):
    mis_idx = np.where(y_true != y_pred)[0]
    if len(mis_idx) == 0:
        print(f"No misclassifications for {title_prefix}")
        return
    np.random.shuffle(mis_idx)
    mis_idx = mis_idx[:num_examples]
    plt.figure(figsize=(15, 5))
    for i, idx in enumerate(mis_idx):
        img = X[idx].reshape((100, 100, 3))
        plt.subplot(2, int(np.ceil(num_examples/2)), i+1)
        plt.imshow(img)
        plt.title(f"True: {classes[y_true[idx]]}\nPred: {classes[y_pred[idx]]}")
        plt.axis('off')
    plt.suptitle(f"{title_prefix} - Misclassified Examples")
    plt.show()

if __name__ == '__main__':
    train_dir = "../data/Q2/train"
    test_dir = "../data/Q2/test"

    # ---------------- Binary Classification Experiments ----------------
    # For binary classification I used two classes based on entry number "03" (i.e. indices 3 and 4)
    all_train_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
    if len(all_train_classes) == 0:
        print("No class subdirectories found in train directory.")
        exit(1)
    d = 3  # corresponds to entry "03"
    num_classes_binary = 11
    if len(all_train_classes) &lt; num_classes_binary:
        print(f"Expected at least {num_classes_binary} classes, but found {len(all_train_classes)}")
        exit(1)
    selected_binary_classes = [all_train_classes[d], all_train_classes[(d+1)%num_classes_binary]]
    print(f"Selected classes for binary classification: {selected_binary_classes}")
    
    print("Loading binary training data...")
    X_train, y_train = load_data(train_dir, selected_binary_classes)
    print(f"Binary Training samples: {X_train.shape[0]}")
    print("Loading binary test data...")
    X_test, y_test = load_data(test_dir, selected_binary_classes)
    print(f"Binary Test samples: {X_test.shape[0]}")
    
    # CVXOPT SVM with Linear Kernel (binary)
    print("\n--- CVXOPT SVM with Linear Kernel (Binary) ---")
    svm_linear = SupportVectorMachine()
    svm_linear.fit(X_train, y_train, kernel='linear', C=1.0)
    y_pred_linear = svm_linear.predict(X_test)
    accuracy_linear = np.mean(y_pred_linear == y_test) * 100
    print(f"Linear SVM Test set accuracy: {accuracy_linear:.2f}%")
    print(f"\nLinear SVM Weight vector (w) norm: {np.linalg.norm(svm_linear.w):.4f}")
    print(f"Linear SVM Bias term (b): {svm_linear.b:.4f}")
    # Visualization for top-5 support vectors (linear)
    top5_idx_linear = np.argsort(-svm_linear.alphas)[:5]
    top5_sv_linear = svm_linear.support_vectors[top5_idx_linear]
    plt.figure(figsize=(12, 3))
    for i, sv in enumerate(top5_sv_linear):
        img = sv.reshape((100, 100, 3))
        plt.subplot(1, 5, i+1)
        plt.imshow(img)
        plt.title(f"LSV {i+1}")
        plt.axis('off')
    plt.suptitle("Top-5 Linear Support Vectors")
    plt.show()
    # plotting weight vector as image
    w_normalized = (svm_linear.w - np.min(svm_linear.w)) / (np.max(svm_linear.w) - np.min(svm_linear.w))
    w_img = w_normalized.reshape((100, 100, 3))

    plt.figure()
    plt.imshow(w_img)
    plt.title("Weight Vector as Image")
    plt.axis('off')
    plt.show()

    # CVXOPT SVM with Gaussian Kernel (binary)
    print("\n--- CVXOPT SVM with Gaussian Kernel (Binary) ---")
    svm_gauss = SupportVectorMachine()
    svm_gauss.fit(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
    y_pred_gauss = svm_gauss.predict(X_test)
    accuracy_gauss = np.mean(y_pred_gauss == y_test) * 100
    print(f"Gaussian SVM Test set accuracy: {accuracy_gauss:.2f}%")
    common_sv = np.intersect1d(svm_linear.sv_idx, svm_gauss.sv_idx)
    print(f"Number of common support vectors between linear and gaussian: {len(common_sv)}")
    top5_idx_gauss = np.argsort(-svm_gauss.alphas)[:5]
    top5_sv_gauss = svm_gauss.support_vectors[top5_idx_gauss]
    plt.figure(figsize=(12, 3))
    for i, sv in enumerate(top5_sv_gauss):
        img = sv.reshape((100, 100, 3))
        plt.subplot(1, 5, i+1)
        plt.imshow(img)
        plt.title(f"GSV {i+1}")
        plt.axis('off')
    plt.suptitle("Top-5 Gaussian Support Vectors")
    plt.show()
    
    # scikit-learn SVM experiments (binary)
    print("\n--- scikit-learn SVM with Linear Kernel (Binary) ---")
    t_start = time.time()
<A NAME="6"></A><FONT color = #00FF00><A HREF="match103-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    sklearn_linear = SVC(kernel='linear', C=1.0)
    sklearn_linear.fit(X_train, y_train)
    t_linear = time.time() - t_start
    y_pred_sklearn_linear = sklearn_linear.predict(X_test)
    accuracy_sklearn_linear = np.mean(y_pred_sklearn_linear == y_test) * 100
</FONT>    n_sv_sklearn_linear = len(sklearn_linear.support_)
    print(f"scikit-learn Linear SVM Test set accuracy: {accuracy_sklearn_linear:.2f}%")
    print(f"scikit-learn Linear SVM number of support vectors: {n_sv_sklearn_linear}")
    print(f"scikit-learn Linear SVM training time: {t_linear:.4f} sec")
    print("\nComparison (Linear Kernel):")
    print("CVXOPT w norm:", np.linalg.norm(svm_linear.w))
    print("scikit-learn w norm:", np.linalg.norm(sklearn_linear.coef_[0]))
    print("CVXOPT bias:", svm_linear.b)
    print("scikit-learn bias:", sklearn_linear.intercept_[0])
    common_linear_count = 0
    for sv in sklearn_linear.support_vectors_:
        diff = np.linalg.norm(svm_linear.support_vectors - sv, axis=1)
        if np.any(diff &lt; 1e-5):
            common_linear_count += 1
    print(f"Common support vectors between CVXOPT linear and scikit-learn linear: {common_linear_count}")
    
    print("\n--- scikit-learn SVM with Gaussian Kernel (Binary) ---")
    t_start = time.time()
    sklearn_gauss = SVC(kernel='rbf', C=1.0, gamma=0.001)
    sklearn_gauss.fit(X_train, y_train)
    t_gauss = time.time() - t_start
    y_pred_sklearn_gauss = sklearn_gauss.predict(X_test)
    accuracy_sklearn_gauss = np.mean(y_pred_sklearn_gauss == y_test) * 100
    n_sv_sklearn_gauss = len(sklearn_gauss.support_)
    print(f"scikit-learn Gaussian SVM Test set accuracy: {accuracy_sklearn_gauss:.2f}%")
    print(f"scikit-learn Gaussian SVM number of support vectors: {n_sv_sklearn_gauss}")
    print(f"scikit-learn Gaussian SVM training time: {t_gauss:.4f} sec")
    common_gauss_count = 0
    for sv in sklearn_gauss.support_vectors_:
        diff = np.linalg.norm(svm_gauss.support_vectors - sv, axis=1)
        if np.any(diff &lt; 1e-5):
            common_gauss_count += 1
    print(f"Common support vectors between CVXOPT gaussian and scikit-learn gaussian: {common_gauss_count}")
    
    print("\n--- Computational Cost Comparison ---")
    print(f"scikit-learn Linear SVM training time: {t_linear:.4f} sec")
    print(f"scikit-learn Gaussian SVM training time: {t_gauss:.4f} sec")
    
    print("\n--- scikit-learn SGD SVM (Hinge Loss) ---")
    t_start = time.time()
    sgd_svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)
    sgd_svm.fit(X_train, y_train)
    t_sgd = time.time() - t_start
    y_pred_sgd = sgd_svm.predict(X_test)
    accuracy_sgd = np.mean(y_pred_sgd == y_test) * 100
    print(f"scikit-learn SGD SVM Test set accuracy: {accuracy_sgd:.2f}%")
    print(f"scikit-learn SGD SVM training time: {t_sgd:.4f} sec")
    print("\nComparison with LIBLINEAR (scikit-learn SVC with Linear Kernel):")
    print(f"LIBLINEAR Linear SVM Test set accuracy: {accuracy_sklearn_linear:.2f}%")
    print(f"LIBLINEAR training time: {t_linear:.4f} sec")
    
    # ---------------- Multi-Class Classification Experiment ----------------
    print("\n--- Multi-Class SVM Experiment using One-vs-One (CVXOPT Gaussian) ---")
    # For multi-class, use all available classes
    multi_train_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
    X_train_multi, y_train_multi = load_data(train_dir, multi_train_classes)
    X_test_multi, y_test_multi = load_data(test_dir, multi_train_classes)
    print(f"Multi-class Training samples: {X_train_multi.shape[0]} (for {len(multi_train_classes)} classes)")
    print(f"Multi-class Test samples: {X_test_multi.shape[0]}")
    multi_class_svm_experiment(X_train_multi, y_train_multi, X_test_multi, y_test_multi, C=1.0, gamma=0.001)


    # ------------------ scikit-learn Multi-Class SVM Experiment ------------------
    print("\n--- scikit-learn Multi-Class SVM with Gaussian Kernel ---")
    # Use all available classes
    multi_train_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
    X_train_multi, y_train_multi = load_data(train_dir, multi_train_classes)
    X_test_multi, y_test_multi = load_data(test_dir, multi_train_classes)
    print(f"Multi-class Training samples: {X_train_multi.shape[0]} (for {len(multi_train_classes)} classes)")
    print(f"Multi-class Test samples: {X_test_multi.shape[0]}")

    t_start = time.time()
    sklearn_multi = SVC(kernel='rbf', C=1.0, gamma=0.001)
    sklearn_multi.fit(X_train_multi, y_train_multi)
    t_multi = time.time() - t_start

    y_pred_sklearn_multi = sklearn_multi.predict(X_test_multi)
    accuracy_sklearn_multi = np.mean(y_pred_sklearn_multi == y_test_multi) * 100
    print(f"scikit-learn Multi-class SVM Test set accuracy: {accuracy_sklearn_multi:.2f}%")
    print(f"scikit-learn Multi-class SVM training time: {t_multi:.4f} sec")

    # ----- Run CVXOPT One-vs-One Multi-Class SVM Experiment and get predictions -----
    print("\n--- CVXOPT Multi-class SVM (One-vs-One, Gaussian) for Confusion Matrix -----")
    # Use all available classes (assumed 11)
    multi_train_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
    X_train_multi, y_train_multi = load_data(train_dir, multi_train_classes)
    X_test_multi, y_test_multi = load_data(test_dir, multi_train_classes)
    num_classes = len(np.unique(y_train_multi))
    cvxopt_classifiers = {}
    for i in range(num_classes):
        for j in range(i+1, num_classes):
            model = train_binary_svm_for_multiclass(X_train_multi, y_train_multi, i, j, C=1.0, gamma=0.001)
            cvxopt_classifiers[(i, j)] = model
            print(f"Trained classifier for classes {i} vs {j}")
    y_pred_cvxopt = multi_class_svm_predict(X_test_multi, cvxopt_classifiers, num_classes)
    cm_cvxopt = confusion_matrix(y_test_multi, y_pred_cvxopt)
    plot_confusion_matrix(cm_cvxopt, classes=multi_train_classes, title='CVXOPT Multi-Class Confusion Matrix')
    plt.show()

    # ----- scikit-learn Multi-Class SVM (Gaussian) Confusion Matrix -----
    print("\n--- scikit-learn Multi-class SVM with Gaussian Kernel for Confusion Matrix -----")
    t_start = time.time()
    sklearn_multi = SVC(kernel='rbf', C=1.0, gamma=0.001)
    sklearn_multi.fit(X_train_multi, y_train_multi)
    t_multi = time.time() - t_start
    y_pred_sklearn_multi = sklearn_multi.predict(X_test_multi)
    cm_sklearn = confusion_matrix(y_test_multi, y_pred_sklearn_multi)
    plot_confusion_matrix(cm_sklearn, classes=multi_train_classes, title='scikit-learn Multi-Class Confusion Matrix')
    plt.show()

    print("\n--- Misclassified Examples (CVXOPT Multi-Class) ---")
    visualize_misclassifications(X_test_multi, y_test_multi, y_pred_cvxopt, multi_train_classes, "CVXOPT")

    print("\n--- Misclassified Examples (scikit-learn Multi-Class) ---")
    visualize_misclassifications(X_test_multi, y_test_multi, y_pred_sklearn_multi, multi_train_classes, "scikit-learn")

    # exoeriment 8

    # ---- Hyperparameter tuning: 5-Fold CV for SVM with Gaussian kernel ----
    print("\n--- 5-Fold Cross-Validation for Hyperparameter Tuning (Gaussian SVM) ---")
    # Use the multi-class training and test data (all available classes)
    multi_train_classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])
<A NAME="0"></A><FONT color = #FF0000><A HREF="match103-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X_train_multi, y_train_multi = load_data(train_dir, multi_train_classes)
    X_test_multi, y_test_multi = load_data(test_dir, multi_train_classes)
    print(f"Multi-class Training samples: {X_train_multi.shape[0]} (for {len(multi_train_classes)} classes)")
    print(f"Multi-class Test samples: {X_test_multi.shape[0]}")

    param_C = [1e-5, 1e-3, 1, 5, 10]
    cv_scores = []
    test_scores = []

    for C_val in param_C:
</FONT>        clf = SVC(kernel='rbf', C=C_val, gamma=0.001)
        # Perform 5-fold cross-validation on training set:
        scores = cross_val_score(clf, X_train_multi, y_train_multi, cv=5)
        cv_acc = np.mean(scores) * 100
        cv_scores.append(cv_acc)
        # Train on the full training set and evaluate test accuracy:
        clf.fit(X_train_multi, y_train_multi)
        test_acc = clf.score(X_test_multi, y_test_multi) * 100
        test_scores.append(test_acc)
        print(f"C: {C_val}, 5-fold CV Accuracy: {cv_acc:.2f}%, Test Accuracy: {test_acc:.2f}%")

    # Plotting the 5-fold CV accuracy and test accuracy vs C
    plt.figure(figsize=(8,6))
    plt.plot(param_C, cv_scores, marker='o', label='5-Fold CV Accuracy')
    plt.plot(param_C, test_scores, marker='s', label='Test Accuracy')
<A NAME="7"></A><FONT color = #0000FF><A HREF="match103-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.xscale('log')
    plt.xlabel("C (log scale)")
    plt.ylabel("Accuracy (%)")
    plt.title("5-Fold CV and Test Accuracy vs C (γ = 0.001)")
    plt.legend()
    plt.grid(True)
    plt.show()

    best_index = np.argmax(cv_scores)
</FONT>    best_C = param_C[best_index]
    print(f"Best C value based on 5-fold CV: {best_C} with CV accuracy: {cv_scores[best_index]:.2f}%")

    # Train an SVM using the best hyper-parameter C on the entire training set
    best_clf = SVC(kernel='rbf', C=best_C, gamma=0.001)
    best_clf.fit(X_train_multi, y_train_multi)
    final_test_acc = best_clf.score(X_test_multi, y_test_multi) * 100
    print(f"Final Test Accuracy with SVM (C = {best_C}): {final_test_acc:.2f}%")
    print("--End of Experiment for SVM--")

</PRE>
</PRE>
</BODY>
</HTML>
