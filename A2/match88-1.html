<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_83PSN.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_DBJMZ.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[5]:


import pandas as pd 
import numpy as np 
from naive_bayes import NaiveBayes, NaiveBayesJoint 
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk import bigrams
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns


# In[22]:


def process_data(df):
    df["Tokenized UD"] = df["Description"].astype(str).apply(lambda x: [word.strip(string.punctuation) for word in x.split() if word.strip(string.punctuation)])
    df["Stop-rem UD"] = df["Tokenized UD"].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])
    df["Stemmed UD"] = df["Tokenized UD"].apply(lambda tokens: [stemmer.stem(word) for word in tokens])
    df["Stop-rem + Stemmed UD"] = df["Stop-rem UD"].apply(lambda tokens: [stemmer.stem(word) for word in tokens])

    df["Tokenized BD"] = df["Tokenized UD"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stop-rem BD"] = df["Stop-rem UD"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stemmed BD"] = df["Stemmed UD"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stop-rem + Stemmed BD"] = df["Stop-rem + Stemmed UD"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    
    df["Tokenized BUD"] = df["Tokenized UD"] + df["Tokenized BD"]
    df["Stop-rem BUD"] = df["Stop-rem UD"] + df["Stop-rem BD"]
    df["Stemmed BUD"] = df["Stemmed UD"] + df["Stemmed BD"]
    df["Stop-rem + Stemmed BUD"] = df["Stop-rem + Stemmed UD"] + df["Stop-rem + Stemmed BD"]

    df["Tokenized UT"] = df["Title"].astype(str).apply(lambda x: [word.strip(string.punctuation) for word in x.split() if word.strip(string.punctuation)])
    df["Stop-rem UT"] = df["Tokenized UT"].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])
    df["Stemmed UT"] = df["Tokenized UT"].apply(lambda tokens: [stemmer.stem(word) for word in tokens])
    df["Stop-rem + Stemmed UT"] = df["Stop-rem UT"].apply(lambda tokens: [stemmer.stem(word) for word in tokens])

    df["Tokenized BT"] = df["Tokenized UT"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stop-rem BT"] = df["Stop-rem UT"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stemmed BT"] = df["Stemmed UT"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    df["Stop-rem + Stemmed BT"] = df["Stop-rem + Stemmed UT"].apply(lambda tokens: [" ".join(b) for b in bigrams(tokens)])
    
    df["Tokenized BUT"] = df["Tokenized UT"] + df["Tokenized BT"]
    df["Stop-rem BUT"] = df["Stop-rem UT"] + df["Stop-rem BT"]
    df["Stemmed BUT"] = df["Stemmed UT"] + df["Stemmed BT"]
    df["Stop-rem + Stemmed BUT"] = df["Stop-rem + Stemmed UT"] + df["Stop-rem + Stemmed BT"]
    return

def plot_word_cloud(df,col):
    for class_label in range(4):
        class_texts = df[df["Class Index"] == class_label + 1][col].reset_index(drop=True)
        sentences = []
        for i in range(len(class_texts)):
            sentences.append(" ".join(class_texts[i]))
<A NAME="5"></A><FONT color = #FF0000><A HREF="match88-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        whole_text = " ".join(sentences)
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(whole_text)
        plt.figure(figsize=(8, 4))
        plt.imshow(wordcloud)
</FONT>        plt.axis("off")
        plt.title(f"Word Cloud for Class {class_label+1}")
        plt.show()

def evaluate_model(train_df, test_df, feature_col):
    model = NaiveBayes()
    model.fit(train_df, smoothening=1.0, text_col=feature_col, class_col="Class Index")
    model.predict(test_df, text_col=feature_col)
    accuracy = accuracy_score(test_df["Class Index"], test_df["Predicted"])
    report = classification_report(test_df["Class Index"], test_df["Predicted"], output_dict=True)

    model.predict(train_df,text_col=feature_col)
    train_accuracy = accuracy_score(train_df["Class Index"],train_df["Predicted"])
    return {
        "Train Accuracy": train_accuracy,
        "Accuracy": accuracy,
        "Precision": report["macro avg"]["precision"],
        "Recall": report["macro avg"]["recall"],
        "F1-score": report["macro avg"]["f1-score"]
    }


# In[14]:


train_df = pd.read_csv('../data/Q1/train.csv')
test_df = pd.read_csv('../data/Q1/test.csv')
process_data(train_df)
process_data(test_df)


# In[15]:


print(train_df.columns)


# In[16]:


feature = "Tokenized UD"
print(evaluate_model(train_df, test_df, feature))


# In[21]:


plot_word_cloud(train_df,"Stop-rem + Stemmed UD")


# In[23]:


train_df_b = train_df[["Class Index", "Stop-rem + Stemmed BUT", "Stop-rem BUD"]].rename(columns={"Stop-rem + Stemmed BUT": "best_title", "Stop-rem BUD": "best_description"})
train_df_b["title + description"] = train_df_b["best_title"] + train_df_b["best_description"]
test_df_b = test_df[["Class Index", "Stop-rem + Stemmed BUT", "Stop-rem BUD"]].rename(columns={"Stop-rem + Stemmed BUT": "best_title", "Stop-rem BUD": "best_description"})
test_df_b["title + description"] = test_df_b["best_title"] + test_df_b["best_description"]


# In[24]:


print(evaluate_model(train_df_b,test_df_b,"title + description"))


# In[26]:


model2 = NaiveBayesJoint()
model2.fit(train_df_b,1,"Class Index","best_description","best_title")
model2.predict(test_df_b,"best_description","best_title")
accuracy = accuracy_score(test_df_b["Class Index"], test_df_b["Predicted"])
report = classification_report(test_df_b["Class Index"], test_df_b["Predicted"], output_dict=True)
model2.predict(train_df_b,"best_description","best_title")
train_accuracy = accuracy_score(train_df_b["Class Index"],train_df_b["Predicted"])
print({
    "Train Accuracy": train_accuracy,
    "Accuracy": accuracy,
    "Precision": report["macro avg"]["precision"],
    "Recall": report["macro avg"]["recall"],
    "F1-score": report["macro avg"]["f1-score"]
})


# In[28]:


cm = confusion_matrix(test_df_b["Class Index"], test_df_b["Predicted"])
class_labels = ["Class 1", "Class 2", "Class 3", "Class 4"] 
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Prediction")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()


# In[38]:


import random

def random_predictor(options):
    return random.choice(options)

options = [1,2,3,4]
predictions = []
for i in range(len(test_df["Class Index"])):
    predictions.append(random_predictor(options))

accuracy = accuracy_score(test_df["Class Index"],predictions)
print(accuracy)


# In[51]:


def filter_short_words(word_list):
    return [word for word in word_list if len(word) &gt;= 3]

train_df_b["new best_title"] = train_df_b["best_title"].apply(filter_short_words)
train_df_b["new best_description"] = train_df_b["best_description"].apply(filter_short_words)
test_df_b["new best_title"] = test_df_b["best_title"].apply(filter_short_words)
test_df_b["new best_description"] = test_df_b["best_description"].apply(filter_short_words)


# In[52]:


model2 = NaiveBayesJoint()
model2.fit(train_df_b,1,"Class Index","new best_description","new best_title")
model2.predict(test_df_b,"new best_description","new best_title")
accuracy = accuracy_score(test_df_b["Class Index"], test_df_b["Predicted"])
report = classification_report(test_df_b["Class Index"], test_df_b["Predicted"], output_dict=True)
model2.predict(train_df_b,"new best_description","new best_title")
train_accuracy = accuracy_score(train_df_b["Class Index"],train_df_b["Predicted"])
print({
    "Train Accuracy": train_accuracy,
    "Accuracy": accuracy,
    "Precision": report["macro avg"]["precision"],
    "Recall": report["macro avg"]["recall"],
    "F1-score": report["macro avg"]["f1-score"]
})





import numpy as np

class NaiveBayes:
    def __init__(self):
        self.phi = []
        self.theta_kl = []
        self.vocabulary = set()
        self.smoothening = 0
        self.unseen = 0
        self.data_classes = 0
        return
        
    def fit(self, df, smoothening, class_col = "Class Index", text_col = "Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        X, y = df[text_col], df[class_col]
        num_classes = len(np.unique(y))
        ik = []
        nk = []
        nkl = []
        for _ in range(num_classes):
            ik.append(0)
            nk.append(0)
            nkl.append({})
            self.phi.append(0)
            self.theta_kl.append({})

        self.vocabulary = set()
        self.smoothening = smoothening
        self.data_classes = num_classes
        m = len(y)

        for i in range(m):
            k = y[i] - 1
            ik[k] += 1
            nk[k] += len(X[i])
            for l in X[i] :
                if l in nkl[k]:
                    nkl[k][l] += 1
                else:
                    nkl[k][l] = 1
                self.vocabulary.add(l)
            
        V = len(self.vocabulary)
        self.unseen = np.log(1/(1+V))

        for k in range(num_classes):
            self.phi[k] = np.log(ik[k]/m)
            for l in self.vocabulary:
                if l in nkl[k]:
<A NAME="2"></A><FONT color = #0000FF><A HREF="match88-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    self.theta_kl[k][l] = np.log((nkl[k][l] + smoothening)/(nk[k] + V*smoothening))
</FONT>                else:
                    self.theta_kl[k][l] = np.log((0 + smoothening)/(nk[k] + V*smoothening))
        
        return
    
    def predict(self, df, text_col = "Tokenized Description", predicted_col = "Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        X = df[text_col]
        m = len(X)
        if predicted_col not in df.columns:
            df[predicted_col] = -1
        for i in range(m):
            y, mpy = 0,float('-inf')
            for k in range(self.data_classes):
                pk = self.phi[k]
                for l in X[i]:
                    if l not in self.vocabulary:
                        pk += self.unseen
                    else:
                        pk += self.theta_kl[k][l]
                
                if pk &gt; mpy:
                    mpy = pk
                    y = k 
            
            df.at[i, predicted_col] = y+1
        return
    
class NaiveBayesJoint:
    def __init__(self):
        self.phi = [0,0,0,0]
        self.dtheta_kl = [{},{},{},{}]
        self.dvocabulary = set()
        self.ttheta_kl = [{},{},{},{}]
        self.tvocabulary = set()
        self.smoothening = 0
        return
        
    def fit(self, df, smoothening, class_col = "Class Index", dtext_col = "Tokenized Description", ttext_col = "Tokenized Title"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        X, y = df[dtext_col], df[class_col]
        ik = [0,0,0,0]
        nk = [0,0,0,0]
        nkl = [{},{},{},{}]
        self.phi = [0,0,0,0]
        self.dtheta_kl = [{},{},{},{}]
        self.dvocabulary = set()
        self.smoothening = smoothening
        m = len(y)

        for i in range(m):
            k = y[i] - 1
            ik[k] += 1
            nk[k] += len(X[i])
            for l in X[i] :
                if l in nkl[k]:
                    nkl[k][l] += 1
                else:
                    nkl[k][l] = 1
                self.dvocabulary.add(l)
            
        V = len(self.dvocabulary)

        for k in range(4):
            self.phi[k] = np.log(ik[k]/m)
            for l in self.dvocabulary:
                if l in nkl[k]:
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match88-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    self.dtheta_kl[k][l] = np.log((nkl[k][l] + smoothening)/(nk[k] + V*smoothening))
</FONT>                else:
                    self.dtheta_kl[k][l] = np.log((0 + smoothening)/(nk[k] + V*smoothening))
        
        X, y = df[ttext_col], df[class_col]
        ik = [0,0,0,0]
        nk = [0,0,0,0]
        nkl = [{},{},{},{}]
        self.ttheta_kl = [{},{},{},{}]
        self.tvocabulary = set()
        m = len(y)

        for i in range(m):
            k = y[i] - 1
            ik[k] += 1
            nk[k] += len(X[i])
            for l in X[i] :
                if l in nkl[k]:
                    nkl[k][l] += 1
                else:
                    nkl[k][l] = 1
                self.tvocabulary.add(l)
            
        V = len(self.tvocabulary)

        for k in range(4):
            for l in self.tvocabulary:
                if l in nkl[k]:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match88-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    self.ttheta_kl[k][l] = np.log((nkl[k][l] + smoothening)/(nk[k] + V*smoothening))
</FONT>                else:
                    self.ttheta_kl[k][l] = np.log((0 + smoothening)/(nk[k] + V*smoothening))
        
        return
    
    def predict(self, df, dtext_col = "Tokenized Description", ttext_col = "Tokenized Description", predicted_col = "Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        dX = df[dtext_col]
        tX = df[ttext_col]
        m = len(dX)
        if predicted_col not in df.columns:
            df[predicted_col] = -1
        for i in range(m):
            y, mpy = 0,float('-inf')
            for k in range(4):
                pk = self.phi[k]
                for l in dX[i]:
                    if l not in self.dvocabulary:
                        continue
                    pk += self.dtheta_kl[k][l]

                pk += self.phi[k]
                for l in tX[i]:
                    if l not in self.tvocabulary:
                        continue
                    pk += self.ttheta_kl[k][l]
                
                if pk &gt; mpy:
                    mpy = pk
                    y = k 
            
            df.at[i, predicted_col] = y+1
        return



#!/usr/bin/env python
# coding: utf-8

# In[5]:


import cv2
import numpy as np
import os
from svm import SupportVectorMachine, MultiClassSVM
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
import time
from sklearn.metrics import confusion_matrix
from sklearn.svm import LinearSVC
import seaborn as sns
import random


# In[3]:


def preprocess_dataset(folder_path):
    X = []
    y = []
    index = 0
<A NAME="0"></A><FONT color = #FF0000><A HREF="match88-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for class_name in sorted(os.listdir(folder_path)):  
        class_folder = os.path.join(folder_path, class_name)
        if os.path.isdir(class_folder): 
            for name in os.listdir(class_folder):
                image_path = os.path.join(class_folder, name)
</FONT>                img = cv2.imread(image_path) 
                if img is None:
                    continue 
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  
                img = cv2.resize(img, (100,100), interpolation=cv2.INTER_AREA)
                img = img.astype(np.float32) / 255.0  
                img_vector = img.flatten()
                X.append(img_vector)
                y.append(index)
            index += 1  

    return np.array(X), np.array(y)

def process_dataset(folder_path,d1,d2):
    X, y = preprocess_dataset(folder_path)
    X1, y1 = [], []
    for i in range(len(y)):
        if y[i] == d1 or y[i] == d2:
            X1.append(X[i])
            y1.append(y[i])

    X1, y1 = np.array(X1), np.array(y1)
    return X,y,X1,y1

def print_info(model,y_pred,y_test_1,X_train_1):
    num_support_vectors = len(model.svx)
    percentage_support_vectors = (len(model.svx) / len(X_train_1)) * 100
    print(f"Number of Support Vectors: {num_support_vectors}")
    print(f"Percentage of Training Samples as Support Vectors: {percentage_support_vectors:.2f}%")
    accuracy = accuracy_score(y_pred,y_test_1)*100
    print(f"Test Set Accuracy: {accuracy:.2f}%")
    sorted_indices = np.argsort(model.alphas) 
    top_5_supports = [sorted_indices[-1], sorted_indices[-2], sorted_indices[-3], sorted_indices[-4], sorted_indices[-5]]
    top_5_svx = model.svx[top_5_supports]
    fig, axes = plt.subplots(1, 5, figsize=(15, 5))
    for i, ax in enumerate(axes):
        ax.imshow(top_5_svx[i].reshape(100, 100, 3))  
        ax.axis('off')
        ax.set_title(f"Support Vector {i+1}")

    plt.suptitle("Top-5 Support Vectors")
    plt.show()

    if model.type == "linear":
        w_image = model.w.reshape(100, 100, 3)
        mini_val = np.min(w_image)
        maxi_val = np.max(w_image)
        norm1 = w_image - mini_val
        norm2 = maxi_val - mini_val
        w_image = norm1/norm2
        plt.figure(figsize=(5, 5))
        plt.imshow(w_image) 
        plt.axis('off')
        plt.title("Weight Vector (w) as Image")
        plt.show()

def sk_comparison(model,X_train_1,y_train_1,X_test_1,y_test_1,type="linear"):
    if type == "linear":
        skmodel = SVC(kernel='linear', C=1.0)
    if type == "gaussian":
        skmodel = SVC(kernel='rbf', C=1.0, gamma=0.001)
    start_time = time.time()
    skmodel.fit(X_train_1, y_train_1)
    print("Computation Time : ", time.time()-start_time)
    sky_pred = skmodel.predict(X_test_1)
    accuracy = accuracy_score(y_test_1, sky_pred) * 100
    sk_svind = set(skmodel.support_)
    common_sv = sk_svind.intersection(model.svind)
    print(f"Test Accuracy (Scikit-Learn {type}): {accuracy:.2f}%")
    print(f"Number of support vectors in (Scikit-Learn {type}): {len(sk_svind)}")
    print(f"Common support vectors with {type} model: {len(common_sv)}")

    if type == "linear":
        w_sklearn, b_sklearn = skmodel.coef_.flatten(), skmodel.intercept_[0]  
        w, b = model.w, model.b  
        w_diff, b_diff = np.linalg.norm(w - w_sklearn), abs(b - b_sklearn)
        w_image = w_sklearn.reshape(100, 100, 3)
        mini_val = np.min(w_image)
        maxi_val = np.max(w_image)
        norm1 = w_image - mini_val
        norm2 = maxi_val - mini_val
        w_image = norm1/norm2 
        plt.figure(figsize=(5, 5))
        plt.imshow(w_image) 
        plt.axis('off')
        plt.title("Weight Vector (w) as Image")
        plt.show()
        print(f"Norm of weight vector difference: {w_diff:.6f}")
        print(f"Bias difference: {b_diff:.6f}")
        print(f"CVXOPT ||w||: {np.linalg.norm(w):.6f}")
        print(f"Scikit-Learn ||w||: {np.linalg.norm(w_sklearn):.6f}")


# In[4]:


X_train, y_train, X_train_1, y_train_1_actual = process_dataset("../data/Q2/train",8,9)
X_test, y_test, X_test_1, y_test_1_actual = process_dataset("../data/Q2/test",8,9)
y_train_1 = np.where(y_train_1_actual == 8, 0, 1)
y_test_1 = np.where(y_test_1_actual == 8,0,1)


# In[13]:


model_linear = SupportVectorMachine()
start_time = time.time()
model_linear.fit(X_train_1,y_train_1,'linear',1)
print("Computation Time : ", time.time() - start_time)
y_pred_linear = model_linear.predict(X_test_1)
print_info(model_linear,y_pred_linear,y_test_1,X_train_1)


# In[14]:


model_gaussian = SupportVectorMachine()
start_time = time.time()
model_gaussian.fit(X_train_1,y_train_1,'gaussian',1,0.001)
print("Computation Time : ", time.time() - start_time)
y_pred_gaussian = model_gaussian.predict(X_test_1)
print_info(model_gaussian,y_pred_gaussian,y_test_1,X_train_1)
num_common_sv = len(model_linear.svind.intersection(model_gaussian.svind))
print(f"Common support vectors with linear model: {num_common_sv}")


# In[15]:


sk_comparison(model_linear,X_train_1,y_train_1,X_test_1,y_test_1,"linear")
sk_comparison(model_gaussian,X_train_1,y_train_1,X_test_1,y_test_1,"gaussian")


# In[ ]:


sgd_svm = SGDClassifier(loss='hinge', alpha=1e-4, max_iter=1000, tol=1e-3, random_state=42)
start_time = time.time()
sgd_svm.fit(X_train_1, y_train_1)
print(f"SGD SVM Training Time: {time.time() - start_time:.4f} seconds")
y_pred_sgd = sgd_svm.predict(X_test_1)
accuracy = accuracy_score(y_test_1, y_pred_sgd) * 100
print(f"SGD SVM Test Accuracy: {accuracy:.2f}%")


# In[5]:


model4 = LinearSVC(C=1.0, max_iter=10000, dual=False)
model4.fit(X_train_1,y_train_1)


# In[6]:


y_pred_linearsvc = model4.predict(X_test_1)
accuracy = accuracy_score(y_test_1, y_pred_linearsvc) * 100
print(f"Linear SVC Test Accuracy: {accuracy:.2f}%")


# In[17]:


model_multi = MultiClassSVM()
model_multi.fit(X_train,y_train)


# In[ ]:


print(len(X_test))


# In[19]:


y_multi_pred = model_multi.predict(X_test)


# In[21]:


accuracy = accuracy_score(y_multi_pred,y_test)
print(accuracy)


# In[6]:


sk_svm = SVC(kernel='rbf', C=1.0, gamma=0.001)  
start_time = time.time()
sk_svm.fit(X_train, y_train)
print(f"Scikit-Learn Multi-Class SVM Training Time: {time.time()-start_time:.4f} seconds")


# In[7]:


y_pred_sk = sk_svm.predict(X_test)
sk_accuracy = accuracy_score(y_test, y_pred_sk) * 100
print(f"Scikit-Learn Multi-Class SVM Test Accuracy: {sk_accuracy:.2f}%")


# In[28]:


cm = confusion_matrix(y_test,y_multi_pred)
class_labels = ["Class 0", "Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10"] 
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Prediction")
plt.ylabel("Actual")
plt.title("Confusion Matrix for CVXOPT")
plt.show()


# In[30]:


cm = confusion_matrix(y_test,y_pred_sk)
class_labels = ["Class 0", "Class 1", "Class 2", "Class 3", "Class 4", "Class 5", "Class 6", "Class 7", "Class 8", "Class 9", "Class 10"] 
plt.figure(figsize=(6,5))
<A NAME="6"></A><FONT color = #00FF00><A HREF="match88-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Prediction")
plt.ylabel("Actual")
plt.title("Confusion Matrix for Scikit")
plt.show()


# In[46]:


misclassified = np.where(y_pred_sk != y_test)[0]
</FONT><A NAME="8"></A><FONT color = #00FFFF><A HREF="match88-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

ind = random.randint(0, len(misclassified) - 1)  
image = X_test[ind].reshape((100,100,3))
plt.imshow(image)
plt.title(f"True Label: {y_test[ind]}, Predicted Label: {y_pred_sk[ind]}")
plt.axis("off")
plt.show()
</FONT>




import cvxopt
import numpy as np

class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alphas = None
        self.w = None
        self.b = None
        self.svx = None
        self.svy = None
        self.type = "linear"
        self.gamma = None
        self.svind = None
        return
    
    def process_y(self,y, flag):
        if flag == 0:
            return np.array([-1 if val == 0 else val for val in y])
        return np.array([0 if val == -1 else val for val in y])
        
    def fit(self, X, y, kernel = 'linear', C = 1.0, gamma = 0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        m,n = X.shape
        if kernel == "linear":
            Kernel = X @ X.T
            self.type = "linear"
        elif kernel == "gaussian":
            X_norm = np.sum(X**2, axis=1) 
            Kernel = np.exp(-gamma * (X_norm[:, np.newaxis] + X_norm - 2 * X @ X.T))  
            self.type = "gaussian"
            self.gamma = gamma

        y = self.process_y(y,0)
<A NAME="7"></A><FONT color = #0000FF><A HREF="match88-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        P = cvxopt.matrix(np.outer(y, y) * Kernel)
        q = cvxopt.matrix(-np.ones((m, 1)))
        G = cvxopt.matrix(np.vstack((-np.eye(m), np.eye(m))))
</FONT>        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * C)))
<A NAME="1"></A><FONT color = #00FF00><A HREF="match88-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        A = cvxopt.matrix(y.reshape(1, -1), tc='d') 
        b = cvxopt.matrix(0.0)
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
</FONT>
        # Select support vectors (alpha &gt; epsilon)
        alphas = np.ravel(solution['x'])
        support_vectors = alphas &gt; 1e-5
        self.alphas = alphas[support_vectors]
        self.svx = X[support_vectors]
        self.svy = y[support_vectors].flatten()
        self.svind = set(np.where(support_vectors)[0])

        if kernel == "linear":
            # Compute w = sum(alphai * yi * xi)
            self.w = np.zeros(self.svx.shape[1])
            for i in range(len(self.alphas)):
                self.w += self.alphas[i] * self.svy[i] * self.svx[i]
            
            # Compute bias term b using the given formula
            w_x = np.dot(self.svx, self.w)
            min_w_x_neg = np.min(w_x[self.svy == -1])  
            max_w_x_pos = np.max(w_x[self.svy == 1])   
            self.b = -0.5 * (min_w_x_neg + max_w_x_pos)

        elif kernel == "gaussian":
            self.w = self.alphas
            svx_norm = np.sum(self.svx**2, axis=1)
            Kernel = np.exp(-gamma * (svx_norm[:, np.newaxis] + svx_norm - 2 * self.svx @ self.svx.T))  
            f_sv = Kernel @ (self.alphas * self.svy) 
            b_pos = f_sv[self.svy == 1]
            b_neg = f_sv[self.svy == -1]  
            self.b = -0.5 * (np.min(b_pos) + np.max(b_neg))  
        return

    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        predictions = None
        if self.type == "linear":
            predictions = np.sign(np.dot(X, self.w) + self.b)  # Compute SVM decision function

        elif self.type == "gaussian":
            X_norm = np.sum(X**2, axis=1)  
            SV_norm = np.sum(self.svx**2, axis=1)  
            Kernel = np.exp(-self.gamma * (SV_norm[:, np.newaxis] + X_norm - 2 * self.svx @ X.T))  
            predictions = np.sign((self.alphas * self.svy) @ Kernel + self.b)
        
        predictions = self.process_y(predictions,1)
        return predictions
        
        
class MultiClassSVM:
    def __init__(self):
        self.models = {}  

    def fit(self, X, y, kernel='gaussian', C=1.0, gamma=0.001):
        for i in range(11):
            for j in range(i + 1, 11):  
                X1, y1_actual = [], []
                for k in range(len(y)):
                    if y[k] == i or y[k] == j:
                        X1.append(X[k])
                        y1_actual.append(y[k])

                X1, y1 = np.array(X1), np.where(np.array(y1_actual) == i, 0, 1)

                if i not in self.models:
                    self.models[i] = {}
                if j not in self.models:
                    self.models[j] = {}

                model = SupportVectorMachine()
                model.fit(X1, y1, kernel, C, gamma)
                self.models[i][j] = model  
        return

    def predict(self, X):
        predictions = []
        counter = 0
        for x in X:
            votes = {i: 0 for i in range(11)}  
            for i in range(11):
                for j in range(i + 1, 11):
                    model = self.models[i][j]
                    pred = model.predict(x.reshape(1, -1))[0] 
                    if pred == 0:
                        votes[i] += 1
                    else:
                        votes[j] += 1
            
            final_prediction = max(votes, key=votes.get)
            predictions.append(final_prediction)
            print(counter)
            counter += 1
        
        return np.array(predictions)

        
        

</PRE>
</PRE>
</BODY>
</HTML>
