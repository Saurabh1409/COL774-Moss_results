<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_BKVDM.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_TWIXY.py<p><PRE>


import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import math
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk import bigrams


nltk.download('stopwords')

class NaiveBayes:
    def __init__(self):
        # Initialize class variables
        self.class_probs = {}  # P(class)
        self.word_probs = defaultdict(dict)  # P(word | class)
        self.classes = None
        self.vocab = set()
        self.word_counts = defaultdict(Counter)  # For word clouds

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """
        Train the Naive Bayes classifier.
        :param df: DataFrame containing the training data.
        :param smoothening: Laplace smoothing parameter.
        :param class_col: Column name for the class labels.
        :param text_col: Column name for the tokenized text.
        """
        # Part 1(a): Train the Naive Bayes Classifier
        self.classes = df[class_col].unique()
        total_samples = len(df)

        # Calculate P(class) for each class
        for c in self.classes:
            class_count = len(df[df[class_col] == c])
            self.class_probs[c] = math.log(class_count / total_samples)

        # Calculate P(word | class) for each word in each class
        for c in self.classes:
            class_samples = df[df[class_col] == c]
            all_words = [word for tokens in class_samples[text_col] for word in tokens]
            word_counts = Counter(all_words)
            total_words = len(all_words)

            # Update vocabulary
<A NAME="0"></A><FONT color = #FF0000><A HREF="match222-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.vocab.update(word_counts.keys())

            # Calculate probabilities with Laplace smoothing
            for word in self.vocab:
                count = word_counts.get(word, 0)
                self.word_probs[c][word] = math.log((count + smoothening) / (total_words + smoothening * len(self.vocab)))
</FONT>
            # Store word counts for word clouds
            self.word_counts[c] = word_counts

    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class for each sample in the DataFrame.
        :param df: DataFrame containing the test data.
        :param text_col: Column name for the tokenized text.
        :param predicted_col: Column name to store predictions.
        :return: DataFrame with predictions.
        """
        predictions = []
        for _, row in df.iterrows():
            tokens = row[text_col]
            max_prob = -float('inf')
            best_class = None

            # Calculate posterior probability for each class
            for c in self.classes:
                log_prob = self.class_probs[c]
                for word in tokens:
                    if word in self.word_probs[c]:
                        log_prob += self.word_probs[c][word]
                    else:
                        # Handle unknown words (ignore or use a small probability)
                        log_prob += math.log(1e-10)

                if log_prob &gt; max_prob:
                    max_prob = log_prob
                    best_class = c

            predictions.append(best_class)

        df[predicted_col] = predictions
        return df

    def generate_word_clouds(self):
        """Generate word clouds for each class based on word frequencies."""
        for label, word_dict in self.word_counts.items():
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'Word Cloud for Class {label}')
            plt.show()


def preprocess_text(df, text_col="Description"):
    """
    Preprocess text data by tokenizing, removing stopwords, and stemming.
    :param df: DataFrame containing the text data.
    :param text_col: Column name for the text.
    :return: DataFrame with a new column for tokenized text.
    """
    # Part 2(a): Perform stemming and stopword removal
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()

    def tokenize_and_stem(text):
        tokens = text.split()
        tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]
        return tokens

    df["Tokenized Description"] = df[text_col].apply(tokenize_and_stem)
    return df


# Load data
train_df = pd.read_csv('../data/q1/train.csv')
test_df = pd.read_csv('../data/q1/test.csv')



#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import math
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk import bigrams


nltk.download('stopwords')

class NaiveBayes:
    def __init__(self):
        # Initialize class variables
        self.class_probs = {}  # P(class)
        self.word_probs = defaultdict(dict)  # P(word | class)
        self.classes = None
        self.vocab = set()
        self.word_counts = defaultdict(Counter)  # For word clouds

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """
        Train the Naive Bayes classifier.
        :param df: DataFrame containing the training data.
        :param smoothening: Laplace smoothing parameter.
        :param class_col: Column name for the class labels.
        :param text_col: Column name for the tokenized text.
        """
        # Part 1(a): Train the Naive Bayes Classifier
        self.classes = df[class_col].unique()
        total_samples = len(df)

        # Calculate P(class) for each class
        for c in self.classes:
            class_count = len(df[df[class_col] == c])
            self.class_probs[c] = math.log(class_count / total_samples)

        # Calculate P(word | class) for each word in each class
        for c in self.classes:
            class_samples = df[df[class_col] == c]
            all_words = [word for tokens in class_samples[text_col] for word in tokens]
            word_counts = Counter(all_words)
            total_words = len(all_words)

            # Update vocabulary
<A NAME="1"></A><FONT color = #00FF00><A HREF="match222-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.vocab.update(word_counts.keys())

            # Calculate probabilities with Laplace smoothing
            for word in self.vocab:
                count = word_counts.get(word, 0)
                self.word_probs[c][word] = math.log((count + smoothening) / (total_words + smoothening * len(self.vocab)))
</FONT>
            # Store word counts for word clouds
            self.word_counts[c] = word_counts

    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class for each sample in the DataFrame.
        :param df: DataFrame containing the test data.
        :param text_col: Column name for the tokenized text.
        :param predicted_col: Column name to store predictions.
        :return: DataFrame with predictions.
        """
        predictions = []
        for _, row in df.iterrows():
            tokens = row[text_col]
            max_prob = -float('inf')
            best_class = None

            # Calculate posterior probability for each class
            for c in self.classes:
                log_prob = self.class_probs[c]
                for word in tokens:
                    if word in self.word_probs[c]:
                        log_prob += self.word_probs[c][word]
                    else:
                        # Handle unknown words (ignore or use a small probability)
                        log_prob += math.log(1e-10)

                if log_prob &gt; max_prob:
                    max_prob = log_prob
                    best_class = c

            predictions.append(best_class)

        df[predicted_col] = predictions
        return df

    def generate_word_clouds(self):
        """Generate word clouds for each class based on word frequencies."""
        for label, word_dict in self.word_counts.items():
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(f'Word Cloud for Class {label}')
            plt.show()


def preprocess_text(df, text_col="Description"):
    """
    Preprocess text data by tokenizing, removing stopwords, and stemming.
    :param df: DataFrame containing the text data.
    :param text_col: Column name for the text.
    :return: DataFrame with a new column for tokenized text.
    """
    # Part 2(a): Perform stemming and stopword removal
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()

    def tokenize_and_stem(text):
        tokens = text.split()
        tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]
        return tokens

    df["Tokenized Description"] = df[text_col].apply(tokenize_and_stem)
    return df


# Load data
train_df = pd.read_csv('../data/q1/train.csv')
test_df = pd.read_csv('../data/q1/test.csv')

# Part 1(a): Train and test the Naive Bayes Classifier on raw description text
train_df_raw = train_df.copy()
test_df_raw = test_df.copy()

# Tokenize raw text (no preprocessing)
train_df_raw["Tokenized Description"] = train_df_raw["Description"].apply(lambda x: x.split())
test_df_raw["Tokenized Description"] = test_df_raw["Description"].apply(lambda x: x.split())

# Train the model
nb_raw = NaiveBayes()
nb_raw.fit(train_df_raw, smoothening=1)

# Predict on training and test sets
train_df_raw = nb_raw.predict(train_df_raw)
test_df_raw = nb_raw.predict(test_df_raw)

# Calculate accuracy
train_accuracy_raw = (train_df_raw["Predicted"] == train_df_raw["Class Index"]).mean()
test_accuracy_raw = (test_df_raw["Predicted"] == test_df_raw["Class Index"]).mean()

print(f"Training Accuracy (Raw Text): {train_accuracy_raw:.4f}")
print(f"Test Accuracy (Raw Text): {test_accuracy_raw:.4f}")

# Part 1(b): Generate word clouds for raw text
nb_raw.generate_word_clouds()

# Part 2(a): Preprocess text (stemming and stopword removal)
train_df_processed = preprocess_text(train_df)
test_df_processed = preprocess_text(test_df)

# Part 2(c): Train and test the Naive Bayes Classifier on preprocessed description text
nb_processed = NaiveBayes()
nb_processed.fit(train_df_processed, smoothening=1)

# Predict on training and test sets
train_df_processed = nb_processed.predict(train_df_processed)
test_df_processed = nb_processed.predict(test_df_processed)

# Calculate accuracy
train_accuracy_processed = (train_df_processed["Predicted"] == train_df_processed["Class Index"]).mean()
test_accuracy_processed = (test_df_processed["Predicted"] == test_df_processed["Class Index"]).mean()

print(f"Training Accuracy (Processed Text): {train_accuracy_processed:.4f}")
print(f"Test Accuracy (Processed Text): {test_accuracy_processed:.4f}")

# Part 2(b): Generate word clouds for processed text
nb_processed.generate_word_clouds()


# In[2]:


# Part 2(c): Train a new model on the transformed data and report validation set accuracy
# Preprocess the validation set (test_df is treated as the validation set here)
test_df_processed = preprocess_text(test_df)

# Train the model on the preprocessed training data
nb_processed = NaiveBayes()
nb_processed.fit(train_df_processed, smoothening=1)

# Predict on the validation set
test_df_processed = nb_processed.predict(test_df_processed)

# Calculate validation set accuracy
validation_accuracy_processed = (test_df_processed["Predicted"] == test_df_processed["Class Index"]).mean()
print(f"Validation Accuracy (Processed Text): {validation_accuracy_processed:.4f}")

# Part 2(d): Compare accuracy over the validation set
print(f"Validation Accuracy (Raw Text): {test_accuracy_raw:.4f}")
print(f"Validation Accuracy (Processed Text): {validation_accuracy_processed:.4f}")

# Comment on observations
if validation_accuracy_processed &gt; test_accuracy_raw:
    print("Observation: The accuracy improved after preprocessing (stemming and stopword removal).")
elif validation_accuracy_processed &lt; test_accuracy_raw:
    print("Observation: The accuracy decreased after preprocessing (stemming and stopword removal).")
else:
    print("Observation: The accuracy remained the same after preprocessing.")


# In[ ]:


from nltk import bigrams
# part 3 

def preprocess_text_with_bigrams(df, text_col="Description"):
    """
    Preprocess text data by tokenizing, removing stopwords, stemming, and generating bigrams.
    :param df: DataFrame containing the text data.
    :param text_col: Column name for the text.
    :return: DataFrame with a new column for tokenized text (unigrams + bigrams).
    """
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()

    def tokenize_and_stem(text):
        tokens = text.split()
        tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]
        return tokens

    def generate_unigrams_and_bigrams(tokens):
        # Generate unigrams
        unigrams = tokens
        # Generate bigrams
        bigram_list = list(bigrams(tokens))
        bigrams_combined = [" ".join(bigram) for bigram in bigram_list]
        # Combine unigrams and bigrams
        return unigrams + bigrams_combined

    df["Tokenized Description"] = df[text_col].apply(tokenize_and_stem)
    df["Tokenized Description"] = df["Tokenized Description"].apply(generate_unigrams_and_bigrams)
    return df


# Preprocess the training and test data to include bigrams
train_df_bigrams = preprocess_text_with_bigrams(train_df)
test_df_bigrams = preprocess_text_with_bigrams(test_df)

# Train the Naive Bayes model with unigrams and bigrams
nb_bigrams = NaiveBayes()
nb_bigrams.fit(train_df_bigrams, smoothening=1)

# Predict on the training and test sets
train_df_bigrams = nb_bigrams.predict(train_df_bigrams)
test_df_bigrams = nb_bigrams.predict(test_df_bigrams)

# Calculate training and test accuracies
train_accuracy_bigrams = (train_df_bigrams["Predicted"] == train_df_bigrams["Class Index"]).mean()
test_accuracy_bigrams = (test_df_bigrams["Predicted"] == test_df_bigrams["Class Index"]).mean()

print(f"Training Accuracy (Unigrams + Bigrams): {train_accuracy_bigrams:.4f}")
print(f"Test Accuracy (Unigrams + Bigrams): {test_accuracy_bigrams:.4f}")

# Compare with the previous model (only unigrams)
print(f"Training Accuracy (Unigrams Only): {train_accuracy_processed:.4f}")
print(f"Test Accuracy (Unigrams Only): {validation_accuracy_processed:.4f}")

# Comment on observations
if test_accuracy_bigrams &gt; validation_accuracy_processed:
    print("Observation: The accuracy improved after adding bigrams as features.")
elif test_accuracy_bigrams &lt; validation_accuracy_processed:
    print("Observation: The accuracy decreased after adding bigrams as features.")
else:
    print("Observation: The accuracy remained the same after adding bigrams as features.")


# In[8]:


from sklearn.metrics import precision_score, recall_score, f1_score
#part 4

def preprocess_text(df, text_col="Description", stemming=True, stopword_removal=True, use_bigrams=False):
    """
    Preprocess text data based on the specified conditions.
    :param df: DataFrame containing the text data.
    :param text_col: Column name for the text.
    :param stemming: Whether to apply stemming.
    :param stopword_removal: Whether to remove stopwords.
    :param use_bigrams: Whether to include bigrams.
    :return: DataFrame with a new column for tokenized text.
    """
    stop_words = set(stopwords.words('english')) if stopword_removal else set()
    stemmer = PorterStemmer() if stemming else None

    def tokenize(text):
        tokens = text.split()
        if stopword_removal:
            tokens = [word for word in tokens if word.lower() not in stop_words]
        if stemming:
            tokens = [stemmer.stem(word) for word in tokens]
        if use_bigrams:
            bigram_list = list(bigrams(tokens))
            bigrams_combined = [" ".join(bigram) for bigram in bigram_list]
            tokens += bigrams_combined
        return tokens

    df["Tokenized Description"] = df[text_col].apply(tokenize)
    return df


def evaluate_model(df, true_col="Class Index", pred_col="Predicted"):
    """
    Evaluate the model using accuracy, precision, recall, and F1-score.
    :param df: DataFrame containing true labels and predictions.
    :param true_col: Column name for true labels.
    :param pred_col: Column name for predicted labels.
    :return: Dictionary of evaluation metrics.
    """
    accuracy = (df[pred_col] == df[true_col]).mean()
    precision = precision_score(df[true_col], df[pred_col], average='weighted')
    recall = recall_score(df[true_col], df[pred_col], average='weighted')
    f1 = f1_score(df[true_col], df[pred_col], average='weighted')
    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1
    }


# Define all cases to evaluate
cases = [
    {"stemming": False, "stopword_removal": False, "use_bigrams": False, "name": "Unigram (Raw Text)"},
    {"stemming": True, "stopword_removal": True, "use_bigrams": False, "name": "Unigram (Processed Text)"},
    {"stemming": True, "stopword_removal": True, "use_bigrams": True, "name": "Unigram + Bigram (Processed Text)"},
    {"stemming": False, "stopword_removal": True, "use_bigrams": False, "name": "Unigram (Stopwords Removed)"},
    {"stemming": True, "stopword_removal": False, "use_bigrams": False, "name": "Unigram (Stemmed)"},
    {"stemming": False, "stopword_removal": False, "use_bigrams": True, "name": "Unigram + Bigram (Raw Text)"},
]

# Evaluate each case
for case in cases:
    print(f"\nEvaluating Case: {case['name']}")
    
    # Preprocess the training and test data
    train_df_processed = preprocess_text(train_df, stemming=case["stemming"], stopword_removal=case["stopword_removal"], use_bigrams=case["use_bigrams"])
    test_df_processed = preprocess_text(test_df, stemming=case["stemming"], stopword_removal=case["stopword_removal"], use_bigrams=case["use_bigrams"])

    # Train the Naive Bayes model
    nb_model = NaiveBayes()
    nb_model.fit(train_df_processed, smoothening=1)

    # Predict on the training and test sets
    train_df_processed = nb_model.predict(train_df_processed)
    test_df_processed = nb_model.predict(test_df_processed)

    # Evaluate the model
    train_metrics = evaluate_model(train_df_processed)
    test_metrics = evaluate_model(test_df_processed)

    print(f"Training Metrics: {train_metrics}")
    print(f"Test Metrics: {test_metrics}")


# In[ ]:


# Analysis
# Best Accuracy:

# The Unigram (Raw Text) model achieves the highest test accuracy (0.8825).

# The Unigram + Bigram (Processed Text) model is a close second with an accuracy of 0.8820.

# Best Precision:

# The Unigram + Bigram (Processed Text) model achieves the highest precision (0.8833).

# This indicates that it has the lowest rate of false positives.

# Best Recall:

# The Unigram (Raw Text) model achieves the highest recall (0.8825).

# This indicates that it has the lowest rate of false negatives.

# Best F1-Score:

# The Unigram (Raw Text) model achieves the highest F1-score (0.8818).

# The F1-score balances precision and recall, making it a good overall metric.


# In[10]:


#part 5 
# Preprocess the title text
def preprocess_title(df, text_col="Title", stemming=True, stopword_removal=True, use_bigrams=False):
    """
    Preprocess title text data based on the specified conditions.
    :param df: DataFrame containing the text data.
    :param text_col: Column name for the title text.
    :param stemming: Whether to apply stemming.
    :param stopword_removal: Whether to remove stopwords.
    :param use_bigrams: Whether to include bigrams.
    :return: DataFrame with a new column for tokenized title text.
    """
    stop_words = set(stopwords.words('english')) if stopword_removal else set()
    stemmer = PorterStemmer() if stemming else None

    def tokenize(text):
        tokens = text.split()
        if stopword_removal:
            tokens = [word for word in tokens if word.lower() not in stop_words]
        if stemming:
            tokens = [stemmer.stem(word) for word in tokens]
        if use_bigrams:
            bigram_list = list(bigrams(tokens))
            bigrams_combined = [" ".join(bigram) for bigram in bigram_list]
            tokens += bigrams_combined
        return tokens

    df["Tokenized Title"] = df[text_col].apply(tokenize)
    return df


# Define all cases to evaluate for title features
cases_title = [
    {"stemming": False, "stopword_removal": False, "use_bigrams": False, "name": "Unigram (Raw Text)"},
    {"stemming": True, "stopword_removal": True, "use_bigrams": False, "name": "Unigram (Processed Text)"},
    {"stemming": True, "stopword_removal": True, "use_bigrams": True, "name": "Unigram + Bigram (Processed Text)"},
    {"stemming": False, "stopword_removal": True, "use_bigrams": False, "name": "Unigram (Stopwords Removed)"},
    {"stemming": True, "stopword_removal": False, "use_bigrams": False, "name": "Unigram (Stemmed)"},
    {"stemming": False, "stopword_removal": False, "use_bigrams": True, "name": "Unigram + Bigram (Raw Text)"},
]

# Evaluate each case for title features
best_title_accuracy = 0
best_title_case = None

for case in cases_title:
    print(f"\nEvaluating Case: {case['name']}")
    
    # Preprocess the training and test data for title features
    train_df_processed = preprocess_title(train_df, stemming=case["stemming"], stopword_removal=case["stopword_removal"], use_bigrams=case["use_bigrams"])
    test_df_processed = preprocess_title(test_df, stemming=case["stemming"], stopword_removal=case["stopword_removal"], use_bigrams=case["use_bigrams"])

    # Train the Naive Bayes model
    nb_model = NaiveBayes()
    nb_model.fit(train_df_processed, smoothening=1, text_col="Tokenized Title")

    # Predict on the training and test sets
    train_df_processed = nb_model.predict(train_df_processed, text_col="Tokenized Title")
    test_df_processed = nb_model.predict(test_df_processed, text_col="Tokenized Title")

    # Evaluate the model
    train_metrics = evaluate_model(train_df_processed)
    test_metrics = evaluate_model(test_df_processed)

    print(f"Training Metrics: {train_metrics}")
    print(f"Test Metrics: {test_metrics}")

    # Track the best accuracy
    if test_metrics["Accuracy"] &gt; best_title_accuracy:
        best_title_accuracy = test_metrics["Accuracy"]
        best_title_case = case["name"]

# Compare with the best accuracy using description features
best_description_accuracy = 0.8825  # From Part 4 (Unigram Raw Text)

print(f"\nBest Accuracy Using Title Features: {best_title_accuracy:.4f} (Case: {best_title_case})")
print(f"Best Accuracy Using Description Features: {best_description_accuracy:.4f}")

# Comment on observations
if best_title_accuracy &gt; best_description_accuracy:
    print("Observation: Title features perform better than description features.")
elif best_title_accuracy &lt; best_description_accuracy:
    print("Observation: Description features perform better than title features.")
else:
    print("Observation: Title and description features perform equally well.")


# In[12]:


# Part 6(a): Concatenation Approach

# Define the best preprocessing approach for description and title
best_desc_case = {"stemming": False, "stopword_removal": False, "use_bigrams": False}  # Unigram (Raw Text)
best_title_case = {"stemming": True, "stopword_removal": True, "use_bigrams": True}  # Unigram + Bigram (Processed Text)

# Preprocess the description and title using the best approaches
train_df_processed = preprocess_text(train_df, **best_desc_case)
test_df_processed = preprocess_text(test_df, **best_desc_case)

train_df_processed = preprocess_title(train_df_processed, **best_title_case)
test_df_processed = preprocess_title(test_df_processed, **best_title_case)

# Concatenate the tokenized title and description
train_df_processed["Tokenized Combined"] = train_df_processed["Tokenized Description"] + train_df_processed["Tokenized Title"]
test_df_processed["Tokenized Combined"] = test_df_processed["Tokenized Description"] + test_df_processed["Tokenized Title"]

# Train the Naive Bayes model on the concatenated text
nb_combined = NaiveBayes()
nb_combined.fit(train_df_processed, smoothening=1, text_col="Tokenized Combined")

# Predict on the training and test sets
train_df_processed = nb_combined.predict(train_df_processed, text_col="Tokenized Combined")
test_df_processed = nb_combined.predict(test_df_processed, text_col="Tokenized Combined")

# Evaluate the model
train_metrics_combined = evaluate_model(train_df_processed)
test_metrics_combined = evaluate_model(test_df_processed)

print("\nConcatenation Approach:")
print(f"Training Metrics: {train_metrics_combined}")
print(f"Test Metrics: {test_metrics_combined}")


# In[13]:


# Part 6(b): Separate Parameters Approach

# Train separate models for title and description
nb_desc = NaiveBayes()
nb_title = NaiveBayes()

# Train the description model
nb_desc.fit(train_df_processed, smoothening=1, text_col="Tokenized Description")

# Train the title model
nb_title.fit(train_df_processed, smoothening=1, text_col="Tokenized Title")

# Predict probabilities for description and title
def predict_proba(df, nb_model, text_col):
    """
    Predict class probabilities for a given model and text column.
    :param df: DataFrame containing the data.
    :param nb_model: Trained NaiveBayes model.
    :param text_col: Column name for the tokenized text.
    :return: DataFrame with predicted probabilities for each class.
    """
    class_probs = []
    for _, row in df.iterrows():
        tokens = row[text_col]
        log_probs = {}
        for c in nb_model.classes:
            log_prob = nb_model.class_probs[c]
            for word in tokens:
                if word in nb_model.word_probs[c]:
                    log_prob += nb_model.word_probs[c][word]
                else:
                    log_prob += math.log(1e-10)
            log_probs[c] = log_prob
        # Convert log probabilities to probabilities
        probs = {c: math.exp(log_prob) for c, log_prob in log_probs.items()}
        class_probs.append(probs)
    return pd.DataFrame(class_probs)

# Predict probabilities for description and title
desc_probs_train = predict_proba(train_df_processed, nb_desc, "Tokenized Description")
title_probs_train = predict_proba(train_df_processed, nb_title, "Tokenized Title")

desc_probs_test = predict_proba(test_df_processed, nb_desc, "Tokenized Description")
title_probs_test = predict_proba(test_df_processed, nb_title, "Tokenized Title")

# Combine probabilities (weighted average)
combined_probs_train = (desc_probs_train + title_probs_train) / 2
combined_probs_test = (desc_probs_test + title_probs_test) / 2

# Predict the class with the highest combined probability
train_df_processed["Predicted"] = combined_probs_train.idxmax(axis=1)
test_df_processed["Predicted"] = combined_probs_test.idxmax(axis=1)

# Evaluate the model
train_metrics_separate = evaluate_model(train_df_processed)
test_metrics_separate = evaluate_model(test_df_processed)

print("\nSeparate Parameters Approach:")
print(f"Training Metrics: {train_metrics_separate}")
print(f"Test Metrics: {test_metrics_separate}")


# In[14]:


# Part 7: Compare Best Model with Baselines

# Best model accuracy (from Part 6)
best_model_accuracy = test_metrics_combined["Accuracy"]  # From Concatenation Approach

# (a) Random Prediction Baseline
# Randomly guess one of the 4 categories for each article
np.random.seed(42)  # For reproducibility
random_predictions = np.random.choice([1, 2, 3, 4], size=len(test_df))
random_accuracy = (random_predictions == test_df["Class Index"]).mean()

# (b) Positive Prediction Baseline
# Predict the majority class for all articles
majority_class = train_df["Class Index"].mode()[0]  # Most frequent class in the training set
positive_predictions = np.full(len(test_df), majority_class)
positive_accuracy = (positive_predictions == test_df["Class Index"]).mean()

# (c) Improvement Over Baselines
improvement_over_random = best_model_accuracy - random_accuracy
improvement_over_positive = best_model_accuracy - positive_accuracy

# Print results
print(f"Best Model Accuracy: {best_model_accuracy:.4f}")
print(f"Random Prediction Accuracy: {random_accuracy:.4f}")
print(f"Positive Prediction Accuracy: {positive_accuracy:.4f}")
print(f"Improvement Over Random Baseline: {improvement_over_random:.4f}")
print(f"Improvement Over Positive Baseline: {improvement_over_positive:.4f}")


# In[15]:


import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Part 8: Confusion Matrix for Best Model

# Get predictions from the best model (Concatenation Approach)
best_model_predictions = test_df_processed["Predicted"]

# True labels
true_labels = test_df_processed["Class Index"]

# Compute the confusion matrix
conf_matrix = confusion_matrix(true_labels, best_model_predictions)

# (a) Draw the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", 
            xticklabels=["World", "Sports", "Business", "Sci/Tech"], 
            yticklabels=["World", "Sports", "Business", "Sci/Tech"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix for Best Model")
plt.show()

# (b) Analyze the confusion matrix
# Find the category with the highest diagonal entry
diagonal_entries = conf_matrix.diagonal()
best_category_index = diagonal_entries.argmax()
best_category = ["World", "Sports", "Business", "Sci/Tech"][best_category_index]
best_category_accuracy = diagonal_entries[best_category_index] / conf_matrix[best_category_index].sum()

print(f"\nCategory with the highest diagonal entry: {best_category}")
print(f"Accuracy for {best_category}: {best_category_accuracy:.4f}")
print("This means the model performs best for this category, with the highest proportion of correct predictions.")


# In[16]:


from sklearn.feature_extraction.text import TfidfVectorizer

# Part 9: Feature Engineering with TF-IDF

# Combine title and description for TF-IDF computation
train_df["Combined Text"] = train_df["Title"] + " " + train_df["Description"]
test_df["Combined Text"] = test_df["Title"] + " " + test_df["Description"]

# Compute TF-IDF features
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit to 1000 features for simplicity
tfidf_train = tfidf_vectorizer.fit_transform(train_df["Combined Text"])
tfidf_test = tfidf_vectorizer.transform(test_df["Combined Text"])

# Convert TF-IDF features to DataFrame
tfidf_train_df = pd.DataFrame(tfidf_train.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
tfidf_test_df = pd.DataFrame(tfidf_test.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Add TF-IDF features to the existing tokenized features
train_df_processed = pd.concat([train_df_processed.reset_index(drop=True), tfidf_train_df.reset_index(drop=True)], axis=1)
test_df_processed = pd.concat([test_df_processed.reset_index(drop=True), tfidf_test_df.reset_index(drop=True)], axis=1)

# Retrain the best-performing model with the new features
nb_updated = NaiveBayes()
nb_updated.fit(train_df_processed, smoothening=1, text_col="Tokenized Combined")

# Predict on the training and test sets
train_df_processed = nb_updated.predict(train_df_processed, text_col="Tokenized Combined")
test_df_processed = nb_updated.predict(test_df_processed, text_col="Tokenized Combined")

# Evaluate the updated model
train_metrics_updated = evaluate_model(train_df_processed)
test_metrics_updated = evaluate_model(test_df_processed)

print("\nUpdated Model with TF-IDF Features:")
print(f"Training Metrics: {train_metrics_updated}")
print(f"Test Metrics: {test_metrics_updated}")

# Compare with the previous model
print("\nComparison with Previous Model:")
print(f"Previous Test Accuracy: {best_model_accuracy:.4f}")
print(f"Updated Test Accuracy: {test_metrics_updated['Accuracy']:.4f}")

# Insights
if test_metrics_updated["Accuracy"] &gt; best_model_accuracy:
    print("Insight: The inclusion of TF-IDF features improved the model's accuracy.")
else:
    print("Insight: The inclusion of TF-IDF features did not improve the model's accuracy.")





import numpy as np
import cvxopt
from itertools import combinations
import cvxopt.solvers
import matplotlib.pyplot as plt
from time import time
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
class SupportVectorMachine:
    def __init__(self):
        self.models = {}
        self.classes = None
        # self.cls_int = {self.classes[i]:i for i in range(len(self.classes))}
        self.sv=None

    def _kernel(self, X1, X2, kernel='linear', gamma=0.001):
        if kernel == 'linear':
            return np.dot(X1, X2.T)
        elif kernel == 'gaussian':
            
            prod1 = np.reshape(np.einsum('ij,ij-&gt;i', X1, X1), (X1.shape[0], 1))
            prod2 = np.reshape(np.einsum('ij,ij-&gt;i', X2, X2), (X2.shape[0], 1))
            prod = prod1 + prod2.T - 2 * np.matmul(X1, X2.T)
            return np.exp(-gamma * prod)
            
        else:
            raise ValueError("Unsupported kernel type")
    
    def _solve_svm(self, X, y, kernel, C, gamma):
        N, D = X.shape
        K = self._kernel(X, X, kernel, gamma)
        
        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones(N))
        
        G = cvxopt.matrix(np.vstack((-np.eye(N), np.eye(N))))
        h = cvxopt.matrix(np.hstack((np.zeros(N), np.ones(N) * C)))
        
        A = cvxopt.matrix(y.reshape(1, -1), (1, N), 'd')
        b = cvxopt.matrix(0.0)
        
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alphas = np.ravel(solution['x'])
        
        support_mask = alphas &gt; 1e-5
        sv_X, sv_y, sv_alphas = X[support_mask], y[support_mask], alphas[support_mask]
        self.sv=sv_X
        w = np.sum(sv_alphas[:, np.newaxis] * sv_y[:, np.newaxis] * sv_X, axis=0)
        b = np.mean(sv_y - np.dot(sv_X, w))
        
        return w, b
    
    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        self.classes = np.unique(y)
        pairs = list(combinations(self.classes, 2))
        
        indices = np.arange(X.shape[0])
        np.random.shuffle(indices)
        X, y = X[indices], y[indices]
        for (c1, c2) in pairs:
            binary_mask = np.logical_or(y == c1, y == c2)
            X_binary, y_binary = X[binary_mask], y[binary_mask]
            y_binary = np.where(y_binary == c1, -1, 1)
            
            w, b = self._solve_svm(X_binary, y_binary, kernel, C, gamma)
            self.models[(c1, c2)] = (w, b)
    
    def predict(self, X):
        votes = {c: np.zeros(X.shape[0]) for c in self.classes}
        
        for (c1, c2), (w, b) in self.models.items():
            pred = np.sign(np.dot(X, w) + b)
            for i, p in enumerate(pred):
                if p == -1:
                    votes[c1][i] += 1
                else:
                    votes[c2][i] += 1
        
        return np.array([max(votes, key=lambda c: votes[c][i]) for i in range(X.shape[0])])
    def accuracy(self, X, y):
        predictions = self.predict(X)
        return np.mean(predictions == y)
    
    def plot_support_vectors(self):
        if self.sv.shape[1] == 2:
            plt.scatter(self.sv[:, 0], self.sv[:, 1], color='red', marker='x', label='Support Vectors')
            plt.legend()
            plt.show()
        else:
            print("Visualization is only supported for 2D data.")




#!/usr/bin/env python
# coding: utf-8

# In[6]:


import numpy as np
import cvxopt
from itertools import combinations
import cvxopt.solvers
import matplotlib.pyplot as plt
from time import time
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
class SupportVectorMachine:
    def __init__(self):
        self.models = {}
        self.classes = None
        # self.cls_int = {self.classes[i]:i for i in range(len(self.classes))}
        self.sv=None

    def _kernel(self, X1, X2, kernel='linear', gamma=0.001):
        if kernel == 'linear':
            return np.dot(X1, X2.T)
        elif kernel == 'gaussian':
            
            prod1 = np.reshape(np.einsum('ij,ij-&gt;i', X1, X1), (X1.shape[0], 1))
            prod2 = np.reshape(np.einsum('ij,ij-&gt;i', X2, X2), (X2.shape[0], 1))
            prod = prod1 + prod2.T - 2 * np.matmul(X1, X2.T)
            return np.exp(-gamma * prod)
            
        else:
            raise ValueError("Unsupported kernel type")
    
    def _solve_svm(self, X, y, kernel, C, gamma):
        N, D = X.shape
        K = self._kernel(X, X, kernel, gamma)
        
        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones(N))
        
        G = cvxopt.matrix(np.vstack((-np.eye(N), np.eye(N))))
        h = cvxopt.matrix(np.hstack((np.zeros(N), np.ones(N) * C)))
        
        A = cvxopt.matrix(y.reshape(1, -1), (1, N), 'd')
        b = cvxopt.matrix(0.0)
        
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alphas = np.ravel(solution['x'])
        
        support_mask = alphas &gt; 1e-5
        sv_X, sv_y, sv_alphas = X[support_mask], y[support_mask], alphas[support_mask]
        self.sv=sv_X
        w = np.sum(sv_alphas[:, np.newaxis] * sv_y[:, np.newaxis] * sv_X, axis=0)
        b = np.mean(sv_y - np.dot(sv_X, w))
        
        return w, b
    
    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        self.classes = np.unique(y)
        pairs = list(combinations(self.classes, 2))
        
        indices = np.arange(X.shape[0])
        np.random.shuffle(indices)
        X, y = X[indices], y[indices]
        for (c1, c2) in pairs:
            binary_mask = np.logical_or(y == c1, y == c2)
            X_binary, y_binary = X[binary_mask], y[binary_mask]
            y_binary = np.where(y_binary == c1, -1, 1)
            
            w, b = self._solve_svm(X_binary, y_binary, kernel, C, gamma)
            self.models[(c1, c2)] = (w, b)
    
    def predict(self, X):
        votes = {c: np.zeros(X.shape[0]) for c in self.classes}
        
        for (c1, c2), (w, b) in self.models.items():
            pred = np.sign(np.dot(X, w) + b)
            for i, p in enumerate(pred):
                if p == -1:
                    votes[c1][i] += 1
                else:
                    votes[c2][i] += 1
        
        return np.array([max(votes, key=lambda c: votes[c][i]) for i in range(X.shape[0])])
    def accuracy(self, X, y):
        predictions = self.predict(X)
        return np.mean(predictions == y)
    
    def plot_support_vectors(self):
        if self.sv.shape[1] == 2:
            plt.scatter(self.sv[:, 0], self.sv[:, 1], color='red', marker='x', label='Support Vectors')
            plt.legend()
            plt.show()
        else:
            print("Visualization is only supported for 2D data.")


# In[5]:


get_ipython().system('pip install cvxopt')


# In[7]:


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import cross_val_score, StratifiedKFold

class LIBSVMSolver:
    def __init__(self, kernel='linear', C=1.0, gamma='scale'):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.model = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma)

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)
        self.support_vectors_ = self.model.support_vectors_
        self.support_vector_indices_ = self.model.support_
        self.n_support_ = self.model.n_support_
    
    def predict(self, X_test):
        return self.model.predict(X_test)

    def accuracy(self, X_test, y_test):
        y_pred = self.predict(X_test)
        return accuracy_score(y_test, y_pred)

    def plot_support_vectors(self, image_shape=(100, 100, 3), top_k=5):
        """Plots the top-K support vectors (assuming images as input)."""
        top_indices = np.argsort(np.linalg.norm(self.support_vectors_, axis=1))[:top_k]
        fig, axes = plt.subplots(1, top_k, figsize=(15, 5))
        for i, idx in enumerate(top_indices):
            sv_image = self.support_vectors_[idx].reshape(image_shape)
            axes[i].imshow(sv_image)
            axes[i].axis("off")
        plt.show()

    def plot_confusion_matrix(self, X_test, y_test, class_labels):
        """Plots the confusion matrix."""
        y_pred = self.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.title("Confusion Matrix")
        plt.show()

    def cross_validate(self, X, y, cv=5):
        """Performs 5-fold cross-validation."""
        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
        scores = cross_val_score(self.model, X, y, cv=skf)
        return np.mean(scores), scores


# In[8]:


train_dir = '../data/q2/train/'
test_dir = '../data/q2/test/'
classes=['dew',
         'fogsmog',
         'frost',
         'glaze',
         'hail',
         'lightning',
         'rain',
         'rainbow',
         'rime',
         'sandstorm',
         'snow']
cls_int = {classes[i]:i for i in range(len(classes))}


# In[9]:


"""
Entry No. 2021CS10102
d = 2
classes = frost , glaze
"""


# In[10]:


import cv2,os
from PIL import Image

def load_data(train_dir,test_dir,classes):
    x_test=[]
    y_test=[]
    x_train=[]
    y_train=[]
    i=0
    for cls in classes:
        folder_path = os.path.join(train_dir,cls)
        temp_x,temp_y=[],[]
        for img in  os.listdir(folder_path):
            img_path = os.path.join(folder_path, img)
            try:
                img_check = Image.open(img_path)
                img_check.verify()
            except (IOError, SyntaxError):
                print(f"Skipping invalid image: {img_path}")
                continue
            image = cv2.imread(os.path.join(folder_path, img))
            
            if image is None:
                print(f"Failed to load image: {img_path}")
                continue
            image = cv2.resize(image, (100,100))
            image = image.reshape(30000)
            image=image/255.0
            temp_x.append(image)
            temp_y.append(i)
        x_train.append(temp_x)
        y_train.append(temp_y)
        i+=1
    i=0
    for cls in classes:
        folder_path = os.path.join(test_dir,cls)
        temp_x,temp_y=[],[]
        for img in  os.listdir(folder_path):
            img_path = os.path.join(folder_path, img)
            try:
                img_check = Image.open(img_path)
                img_check.verify()
            except (IOError, SyntaxError):
                print(f"Skipping invalid image: {img_path}")
                continue
            image = cv2.imread(os.path.join(folder_path, img))
            if image is None:
                print(f"Failed to load image: {img_path}")
                continue
            image = cv2.resize(image, (100,100))
            image = image.reshape(30000)
            image=image/255.0
            temp_x.append(image)
            temp_y.append(i)
        x_test.append(temp_x)
        y_test.append(temp_y)
        i+=1
    
    return x_train,y_train,x_test,y_test


# In[11]:


x_train,y_train,x_test,y_test  = load_data(train_dir,test_dir,classes)


# In[12]:


def combine(L):
    ans=[]
    for x in L:
        ans+=x
    ans = np.array(ans)
    return ans


# In[13]:


X_train = x_train[2]+x_train[3]
Y_train = y_train[2]+y_train[3]
X_test = x_test[2]+x_test[3]
Y_test = y_test[2]+y_test[3]
X_train = np.array(X_train)
Y_train = np.array(Y_train)
X_test = np.array(X_test)
Y_test = np.array(Y_test)


# In[16]:


def plot_top5_support_vectors(support_vectors, image_shape=(100, 100, 3)):
    top_k = min(5, len(support_vectors)) 
    fig, axes = plt.subplots(1, top_k, figsize=(15, 5))
    for i in range(top_k):
        img = support_vectors[i].reshape(image_shape)  
        axes[i].imshow(img)
        axes[i].axis("off")
        axes[i].set_title(f"Support Vector {i+1}")

    plt.show()

def plot_weight_vector(w, image_shape=(100, 100, 3)):
    if w is not None and len(w) == np.prod(image_shape):  
        w_image = w.reshape(image_shape)
        plt.figure(figsize=(5, 5))
        plt.imshow(w_image, cmap="gray")  
        plt.axis("off")
        plt.title("Weight Vector Visualization")
        plt.show()
    else:
        print("Cannot visualize weight vector. Ensure you are using a linear kernel.")


# In[17]:


print('Linear Kernel CVXOPT')
SVM = SupportVectorMachine()
st = time()
SVM.fit(X_train,Y_train)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match222-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

print(time()-st)
Y_train_pred = SVM.predict(X_train)
Y_test_pred = SVM.predict(X_test)
# print(Y_test.shape)
train_acc = np.mean(Y_train_pred==Y_train)
test_acc = np.mean(Y_test_pred==Y_test)
</FONT>print(train_acc,test_acc,len(SVM.sv))
W,b=SVM.models[2,3]
plot_top5_support_vectors(SVM.sv)
plot_weight_vector(W)


# In[18]:


print('Gaussian Kernel CVXOPT')
SVM = SupportVectorMachine()
st = time()
SVM.fit(X_train,Y_train,'gaussian')
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match222-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

print(time()-st)
Y_train_pred = SVM.predict(X_train)
Y_test_pred = SVM.predict(X_test)
# print(Y_test.shape)
train_acc = np.mean(Y_train_pred==Y_train)
test_acc = np.mean(Y_test_pred==Y_test)
</FONT>print(train_acc,test_acc,len(SVM.sv))
W,b=SVM.models[2,3]
plot_top5_support_vectors(SVM.sv)
plot_weight_vector(W)


# In[ ]:


get_ipython().system('pip install libsvm-official')
from libsvm.svmutil import svm_problem,svm_parameter,svm_train,svm_predict
from time import time
def sv_libsvm(x_train,y_train,kernel,gamma):
    prob = svm_problem(y_train,x_train)
    param = svm_parameter('-t {} -c 1'.format(kernel))
    if gamma:
        param = svm_parameter('-t {} -c 1 -g {}'.format(kernel,gamma))
    model = svm_train(prob,param)
    return model
def accuracy_libsvm(model,x_train,y_train,x_test,y_test):
    p_label,p_acc_train,p_val = svm_predict(y_train,x_train,model)
    print("Training Accuracy: ",p_acc_train[0])
    p_label,p_acc_validation,p_val = svm_predict(y_test,x_test,model)
    print("Validation Accuracy: ",p_acc_validation[0])
    return p_acc_train,p_acc_validation[0]

def find_w_b(X,Y,alphas,C=1.0):
    n_samples, n_features = X.shape
    sv_indices = np.array(model.get_sv_indices()) - 1
    X_sv = X[sv_indices]  # Select support vectors
    Y_sv = Y[sv_indices]
    # alphas = alphas[sv_indices]
    w = np.sum(alphas * Y_sv.reshape(-1, 1) * X_sv, axis=0)
    sv_margin = (alphas.flatten() &gt; 1e-5) & (alphas.flatten() &lt; C)
    b = np.mean(Y_sv[sv_margin] - np.dot(X_sv[sv_margin], w))
    return w, b
def no_of_sv(alpha,C=1.0):
    SVs = np.where((alpha &gt; 1e-5) & (alpha &lt; C))[0]
    return len(SVs)
    
print("Linear SVM")
start = time()
model = sv_libsvm(X_train,Y_train,0,0)
print("Time taken for training: ",time()-start)
print("Number of Svs : ",model.get_nr_sv())
alpha = np.array(model.get_sv_coef()).reshape(-1,1)
W_lib_lin,b_lib_lin = find_w_b(X_train,Y_train,alpha)

accuracy_libsvm(model,X_train,Y_train,X_test,Y_test)


print("Gaussian SVM")
start = time()
model = sv_libsvm(X_train,Y_train,2,0.001)
print("Time taken for training: ",time()-start)
print("Number of Svs : ",model.get_nr_sv())
accuracy_libsvm(model,X_train,Y_train,X_test,Y_test)


# In[ ]:


X_train = combine(x_train)
Y_train = combine(y_train)
X_test = combine(x_test)
Y_test = combine(y_test)


# In[ ]:


print('Linear Kernel CVXOPT')
SVM = SupportVectorMachine()
st = time()
SVM.fit(X_train,Y_train)
print(time()-st)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match222-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

Y_train_pred = SVM.predict(X_train)
Y_test_pred = SVM.predict(X_test)
# print(Y_test.shape)
train_acc = np.mean(Y_train_pred==Y_train)
test_acc = np.mean(Y_test_pred==Y_test)
</FONT>print(train_acc,test_acc)
confusion_matrix(Y_test, Y_test_pred)


# In[ ]:


indices = np.where(Y_test != Y_test_pred)[0]
np.random.shuffle(indices)
indices=indices[:10]
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.ravel()
for i, idx in enumerate(indices):
    img = X_test[idx].reshape((100,100,3))
    axes[i].imshow(img)
    axes[i].axis("off")
    axes[i].set_title(f"True: {Y_test[idx]}, Pred: {Y_test_pred[idx]}", fontsize=10)

plt.tight_layout()
plt.show()


# In[ ]:


print('Gaussian Kernel CVXOPT')
SVM = SupportVectorMachine()
st = time()
SVM.fit(X_train,Y_train,'gaussian')
print(time()-st)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match222-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

Y_train_pred = SVM.predict(X_train)
Y_test_pred = SVM.predict(X_test)
# print(Y_test.shape)
train_acc = np.mean(Y_train_pred==Y_train)
test_acc = np.mean(Y_test_pred==Y_test)
</FONT>print(train_acc,test_acc)
confusion_matrix(Y_test, Y_test_pred)


# In[30]:



    
print("Linear SVM LIBSVM")
start = time()
model = sv_libsvm(X_train,Y_train,0,0)
print("Time taken for training: ",time()-start)
# print("Number of Svs : ",model.get_nr_sv())
alpha = np.array(model.get_sv_coef()).reshape(-1,1)
W_lib_lin,b_lib_lin = find_w_b(X_train,Y_train,alpha)

accuracy_libsvm(model,X_train,Y_train,X_test,Y_test)
a1,acc,a2 = svm_predict(Y_test,X_test,model)
confusion_matrix(Y_test, a1)


# In[ ]:



print("Gaussian SVM LIBSVM")
start = time()
model = sv_libsvm(X_train,Y_train,2,0.001)
print("Time taken for training: ",time()-start)
# print("Number of Svs : ",model.get_nr_sv())
accuracy_libsvm(model,X_train,Y_train,X_test,Y_test)
a1,acc,a2 = svm_predict(Y_test,X_test,model)
confusion_matrix(Y_test, a1)


# In[ ]:


def k_fold(X,Y,c,k):
    m = X.shape[0]
    m //= k
    X_test, X_train = np.split(X, [m])
    Y_test, Y_train = np.split(Y, [m])
    accuracy = 0
    for i in range(k):
        clf = SVC(kernel='rbf', gamma=0.001, C=c)
        clf.fit(X_train,Y_train)
        pred = clf.predict(X_test)
        accuracy += np.mean(pred == Y_test)
        if i &lt; k - 1:
            X_train[i * m:(i + 1) * m], X_test = X_test, X_train[i * m:(i + 1) * m].copy()
            Y_train[i * m:(i + 1) * m], Y_test = Y_test, Y_train[i * m:(i + 1) * m].copy()
    return accuracy / k
kf_Acc = []
C_values=[1e-5,1e-3,1,5,10]

for c in C_values:
    k_fold_Accuracy = k_fold(X_train,Y_train,c,5)
    kf_Acc.append(k_fold_Accuracy)
    print("C: ",c,"k_fold Accuracy: ",k_fold_Accuracy)



</PRE>
</PRE>
</BODY>
</HTML>
