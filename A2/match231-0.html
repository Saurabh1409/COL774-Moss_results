<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_D73IC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_D73IC.py<p><PRE>


import numpy as np
from collections import defaultdict
import math
import pandas as pd

class NaiveBayes:
    def __init__(self):
        self.class_probs = {}
        self.word_probs = defaultdict(lambda: defaultdict(float))
        self.vocab = set()
        self.max_length = 0

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
        df (pd.DataFrame): The training data containing columns class_col and text_col.
                           each entry of text_col is a list of tokens.
        smoothening (float): The Laplace smoothening parameter.
        """
        class_counts = df[class_col].value_counts()
        total_samples = len(df)

        for cls in class_counts.index:
            self.class_probs[cls] = math.log(class_counts[cls] / total_samples)

        self.max_length = df[text_col].apply(len).max()

        word_counts = defaultdict(lambda: defaultdict(int))
        for _, row in df.iterrows():
            cls = row[class_col]
            for word in row[text_col]:
                word_counts[cls][word] += 1
                self.vocab.add(word)

        vocab_size = len(self.vocab)
        for cls in class_counts.index:
            total_words = sum(word_counts[cls].values())
            for word in self.vocab:
                self.word_probs[cls][word] = math.log((word_counts[cls][word] + smoothening) / (total_words + smoothening * vocab_size))
                
                
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
        df (pd.DataFrame): The testing data containing column text_col.
                           each entry of text_col is a list of tokens.
        """
        predictions = []
        for _, row in df.iterrows():
            doc = row[text_col]
            class_scores = {}
            for cls in self.class_probs:
                score = self.class_probs[cls]
                for word in doc:
                    if word in self.vocab:
                        score += self.word_probs[cls][word]
                class_scores[cls] = score
            predictions.append(max(class_scores, key=class_scores.get))
        df[predicted_col] = predictions
        return predictions


##########################################
if __name__ == "__main__":
    # Load training and test data
    train_data = pd.read_csv('../data/Q1/train.csv')
    test_data = pd.read_csv('../data/Q1/test.csv')


    def tokenize(text):
        return text.lower().split()

    train_data['Tokenized Description'] = train_data['Description'].apply(tokenize)
    test_data['Tokenized Description'] = test_data['Description'].apply(tokenize)

    nb = NaiveBayes()
    smoothening = 1.0  # Laplace smoothing parameter
    nb.fit(train_data, smoothening, class_col="Class Index", text_col="Tokenized Description")


    train_predictions = nb.predict(train_data, text_col="Tokenized Description", predicted_col="Predicted")
    test_predictions = nb.predict(test_data, text_col="Tokenized Description", predicted_col="Predicted")
    
    print(train_predictions[:10])
    print(test_predictions[:10])

    # Calculate accuracy
    train_accuracy = sum(train_predictions == train_data['Class Index']) / len(train_data)
    test_accuracy = sum(test_predictions == test_data['Class Index']) / len(test_data)

    print(f"Training accuracy: {train_accuracy:.4f}")
    print(f"Test accuracy: {test_accuracy:.4f}")





import pandas as pd
import numpy as np
from collections import defaultdict
import math
<A NAME="1"></A><FONT color = #00FF00><A HREF="match231-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
</FONT>from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import re



nltk.download('stopwords')

class NaiveBayes:
    def __init__(self):
        self.class_probs = {}
        self.word_probs = defaultdict(lambda: defaultdict(float))
        self.vocab = set()
        self.max_length = 0

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        class_counts = df[class_col].value_counts()
        total_samples = len(df)

        for cls in class_counts.index:
            self.class_probs[cls] = math.log(class_counts[cls] / total_samples)

        self.max_length = df[text_col].apply(len).max()

        word_counts = defaultdict(lambda: defaultdict(int))
        for _, row in df.iterrows():
            cls = row[class_col]
            for word in row[text_col]:
                word_counts[cls][word] += 1
                self.vocab.add(word)

        vocab_size = len(self.vocab)
        for cls in class_counts.index:
            total_words = sum(word_counts[cls].values())
            for word in self.vocab:
                self.word_probs[cls][word] = math.log((word_counts[cls][word] + smoothening) / (total_words + smoothening * vocab_size))
                
                
<A NAME="0"></A><FONT color = #FF0000><A HREF="match231-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        predictions = []
        for _, row in df.iterrows():
            doc = row[text_col]
</FONT>            class_scores = {}
            for cls in self.class_probs:
                score = self.class_probs[cls]
                for word in doc:
                    if word in self.vocab:
                        score += self.word_probs[cls][word]
                class_scores[cls] = score
            predictions.append(max(class_scores, key=class_scores.get))
        df[predicted_col] = predictions
        return predictions

# Load and preprocess the data
train_data = pd.read_csv('../data/Q1/train.csv')
test_data = pd.read_csv('../data/Q1/test.csv')

# Tokenize the description

def tokenize(text):
    return text.lower().split()

def preprocess(text):
    stop_words = set(stopwords.words('english'))
    ps = PorterStemmer()
    tokens = text.lower().split()
    return [ps.stem(word) for word in tokens if word not in stop_words]

def generate_bigrams(tokens):
    return [f"{tokens[i]} {tokens[i+1]}" for i in range(len(tokens)-1)]

def tokenize_with_bigrams(text):
    tokens = tokenize(text)
    bigrams = generate_bigrams(tokens)
    return tokens + bigrams

def preprocess_with_bigrams(text):
    tokens = preprocess(text)
    bigrams = generate_bigrams(tokens)
    return tokens + bigrams


# Define functions to remove HTML tags and URLs
def remove_html_tag(text):
    html = re.compile('&lt;.*?&gt;')
    cleaned_text = html.sub('', text)
    return cleaned_text

def url_remove(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', text)


###################################
# BEST MODEL FROM q6.a

## apply tokenise and bigram and url and html tag removal
train_data['combined_features'] = train_data['Description'].apply(lambda x: tokenize_with_bigrams(url_remove(remove_html_tag(x))))
test_data['combined_features'] = test_data['Description'].apply(lambda x: tokenize_with_bigrams(url_remove(remove_html_tag(x))))



# Train the enhanced model
<A NAME="2"></A><FONT color = #0000FF><A HREF="match231-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

nb_enhanced = NaiveBayes()
nb_enhanced.fit(train_data, smoothening=1.0, class_col="Class Index", text_col="combined_features")

# Make predictions
nb_enhanced.predict(test_data, text_col="combined_features", predicted_col="Predicted_Enhanced")
</FONT>
# Evaluate the enhanced model
enhanced_accuracy = (test_data["Class Index"] == test_data["Predicted_Enhanced"]).mean()
print(f"Enhanced model accuracy: {enhanced_accuracy:.4f}")







#!/usr/bin/env python
# coding: utf-8

# In[3]:


import cvxopt
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
import os
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import time
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match231-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from itertools import combinations


# In[4]:



class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.alphas = None
        self.support_vectors = None
</FONT>        self.support_vector_labels = None
        self.b = None
        self.w = None
        self.gamma = None
        
    def fit(self, X, y, kernel = 'linear', C = 1.0, gamma = 0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        n_samples, n_features = X.shape
        self.gamma = gamma
        
        # Gram matrix
        if kernel == 'linear':
            K = X @ X.T
        elif kernel == 'gaussian':
            K = np.zeros((n_samples, n_samples))
            for i in range(n_samples):
                for j in range(n_samples):
                    K[i,j] = np.exp(-gamma * np.linalg.norm(X[i] - X[j])**2)
        else:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match231-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            raise ValueError("Unsupported kernel type")

        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones(n_samples))
        A = cvxopt.matrix(y.reshape(1, -1).astype(np.double))
</FONT>        b = cvxopt.matrix(0.0)
        G = cvxopt.matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))
        h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))

        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alphas = np.ravel(solution['x'])


        # Support vectors have non-zero alphas
        sv = alphas &gt; 1e-5
        self.alphas = alphas[sv]
        self.support_vectors = X[sv]
        self.support_vector_labels = y[sv]

        # Weight vector (for linear kernel)
        if kernel == 'linear':
            self.w = np.zeros(n_features)
            for i in range(len(self.alphas)):
                self.w += self.alphas[i] * self.support_vector_labels[i] * self.support_vectors[i]
        else:
            self.w = None
            
        # Intercept
        self.b = np.mean(
            [self.support_vector_labels[i] - np.dot(self.w, self.support_vectors[i])
            if self.w is not None else
            self.support_vector_labels[i] - np.sum(self.alphas * self.support_vector_labels * K[sv][:,sv][i])
            for i in range(len(self.alphas))]
        )
            
            

    
    

    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        
        if self.w is not None:
            return np.sign(X @ self.w + self.b)
        else:
            y_predict = np.zeros(len(X))
            for i in range(len(X)):
                s = 0
                for alpha, sv_y, sv in zip(self.alphas, self.support_vector_labels, self.support_vectors):
                    s += alpha * sv_y * np.exp(-self.gamma * np.linalg.norm(X[i] - sv)**2)
                y_predict[i] = s
            return np.sign(y_predict + self.b)
        
        


# In[5]:



class OneVsOneSVM:
    def __init__(self, C=1.0, gamma=0.001):
        self.C = C
        self.gamma = gamma
        self.classifiers = {}  # Dictionary to store classifiers for each pair of classes

    def gaussian_kernel(self, x1, x2):
        return np.exp(-self.gamma * np.linalg.norm(x1 - x2)**2)

    def fit(self, X, y, classes):
        # Train binary classifiers for each pair of classes
        for class_pair in combinations(classes, 2):
            class_1, class_2 = class_pair
            print(f"Training classifier for classes: {class_1} vs {class_2}")
            
            # Extract data for the two classes
            indices = np.where((y == class_1) | (y == class_2))[0]
            X_pair = X[indices]
            y_pair = y[indices]
            y_pair = np.where(y_pair == class_1, 1, -1)  # Map labels to +1 and -1
            
            # Train SVM using CVXOPT solver
            svm = SupportVectorMachine()
            svm.fit(X_pair, y_pair, kernel='gaussian', C=self.C, gamma=self.gamma)
            
            # Store the trained classifier
            self.classifiers[class_pair] = svm

    def predict(self, X):
        votes = np.zeros((X.shape[0], len(self.classifiers)))
        
        for i, (class_pair, svm) in enumerate(self.classifiers.items()):
            class_1, class_2 = class_pair
            predictions = svm.predict(X)
            
            # Assign votes based on predictions (+1 or -1)
            votes[:, i] = np.where(predictions == 1, class_1, class_2)
        
        # Perform majority voting
        final_predictions = []
        for row in votes:
            unique_classes, counts = np.unique(row, return_counts=True)
            max_votes_idx = np.argmax(counts)
            final_predictions.append(unique_classes[max_votes_idx])
        
        return np.array(final_predictions)

        


# In[6]:


def load_and_preprocess_images(folder_path):
    images = []
    labels = []
    target_shape = (100, 100, 3)  # Expected shape after flattening: 30,000
    
    # Map class names to numeric labels (0 to 10 for 11 classes)
    class_mapping = {
        'dew': 0,
        'fogsmog': 1,
        'frost': 2,
        'glaze': 3,
        'hail': 4,
        'lightning': 5,
        'rain': 6,
        'rainbow': 7,
        'rime': 8,
        'sandstorm': 9,
        'snow': 10
    }

<A NAME="5"></A><FONT color = #FF0000><A HREF="match231-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for class_name in class_mapping.keys():
        class_path = os.path.join(folder_path, class_name)
        for image_name in os.listdir(class_path):
            image_path = os.path.join(class_path, image_name)
</FONT>            try:
                with Image.open(image_path) as img:
                    # Resize to 100x100
                    img = img.resize((100, 100))
                    
                    # Convert to numpy array and flatten
                    img_array = np.array(img).flatten()
                    
                    # Check if the flattened array has the correct shape
                    if img_array.shape[0] != np.prod(target_shape):
                        print(f"Skipping {image_path}: Incorrect shape")
                        continue
                    
                    # Normalize to [0, 1]
                    img_array = img_array / 255.0
                    
                    images.append(img_array)
                    labels.append(class_mapping[class_name])
            except Exception as e:
                print(f"Error processing {image_path}: {e}")
    
    return np.array(images), np.array(labels)

    


# In[7]:


# Load and preprocess images
train_folder = '../data/Q2/train'
test_folder = '../data/Q2/test'

X_train, y_train = load_and_preprocess_images(train_folder)
X_test, y_test = load_and_preprocess_images(test_folder)


# In[8]:


# Get unique classes
classes = np.unique(y_train)

# Train One-vs-One SVM
ovo_svm = OneVsOneSVM(C=1.0, gamma=0.001)
ovo_svm.fit(X_train, y_train, classes)


# In[9]:


# Predict on test set
y_pred_test = ovo_svm.predict(X_test)

# Report test accuracy
test_accuracy = accuracy_score(y_test, y_pred_test)
print(f"Test accuracy (One-vs-One Multi-Class SVM): {test_accuracy:.4f}")


# In[10]:


print(y_pred_test)
## save it
np.save('y_pred_test_ovo.npy', y_pred_test)
np.save('y_test.npy', y_test)


# In[11]:


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# For CVXOPT
plot_confusion_matrix(y_test, y_pred_test, "Confusion Matrix - CVXOPT")


# In[12]:


class_names = {
    0: 'dew',
    1: 'fogsmog',
    2: 'frost',
    3: 'glaze',
    4: 'hail',
    5: 'lightning',
    6: 'rain',
    7: 'rainbow',
    8: 'rime',
    9: 'sandstorm',
    10: 'snow'
}


# In[13]:


def analyze_misclassifications(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    misclassifications = {}
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i][j] &gt; 0:
                misclassifications[(class_names[i], class_names[j])] = cm[i][j]
    
    sorted_misclassifications = sorted(misclassifications.items(), key=lambda x: x[1], reverse=True)
    print("Most common misclassifications:")
    for (true_class, pred_class), count in sorted_misclassifications[:5]:
        print(f"{true_class} misclassified as {pred_class}: {count} times")

analyze_misclassifications(y_test, y_pred_test, class_names)


# In[14]:


def plot_misclassified_examples(X_test, y_test, y_pred, class_names, n_examples=10):
    misclassified = np.where(y_test != y_pred)[0]
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    for i, idx in enumerate(np.random.choice(misclassified, n_examples, replace=False)):
        ax = axes[i // 5, i % 5]
        ax.imshow(X_test[idx].reshape(100, 100, 3))
        ax.set_title(f"True: {class_names[y_test[idx]]}\nPred: {class_names[y_pred[idx]]}")
        ax.axis('off')
    plt.tight_layout()
    plt.show()

plot_misclassified_examples(X_test, y_test, y_pred_test, class_names)





#!/usr/bin/env python
# coding: utf-8

# In[3]:


import cvxopt
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
import os
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import time
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from itertools import combinations


# In[4]:



def load_and_preprocess_images(folder_path):
    images = []
    labels = []
    target_shape = (100, 100, 3)  # Expected shape after flattening: 30,000
    
    # Map class names to numeric labels (0 to 10 for 11 classes)
    class_mapping = {
'dew': 0,
'fogsmog': 1,
'frost': 2,
'glaze': 3,
'hail': 4,
'lightning': 5,
'rain': 6,
'rainbow': 7,
'rime': 8,
'sandstorm': 9,
'snow': 10
    }
    
    # class_mapping = {
    #     'rainbow': 7,
    #     'rime': 8
    # }

<A NAME="6"></A><FONT color = #00FF00><A HREF="match231-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for class_name in class_mapping.keys():
class_path = os.path.join(folder_path, class_name)
for image_name in os.listdir(class_path):
    image_path = os.path.join(class_path, image_name)
</FONT>    try:
        with Image.open(image_path) as img:
            # Resize to 100x100
            img = img.resize((100, 100))
            
            # Convert to numpy array and flatten
            img_array = np.array(img).flatten()
            
            # Check if the flattened array has the correct shape
            if img_array.shape[0] != np.prod(target_shape):
                print(f"Skipping {image_path}: Incorrect shape")
                continue
            
            # Normalize to [0, 1]
            img_array = img_array / 255.0
            
            images.append(img_array)
            labels.append(class_mapping[class_name])
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
    
    return np.array(images), np.array(labels)

       


# In[5]:


# Load and preprocess images
train_folder = '../data/Q2/train'
test_folder = '../data/Q2/test'

X_train, y_train = load_and_preprocess_images(train_folder)
X_test, y_test = load_and_preprocess_images(test_folder)


# Train multi-class SVM with Gaussian kernel
start_time = time.time()
svm_sklearn = SVC(kernel='rbf', C=1.0, gamma=0.001)
svm_sklearn.fit(X_train, y_train)
training_time = time.time() - start_time

# Make predictions on test set
y_pred = svm_sklearn.predict(X_test)


print(y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Test set accuracy: {accuracy:.4f}")
print(f"Training time: {training_time:.2f} seconds")


# In[6]:


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
# For LIBSVM
plot_confusion_matrix(y_test, y_pred, "Confusion Matrix - LIBSVM")


# In[7]:


class_names = {
    0: 'dew',
    1: 'fogsmog',
    2: 'frost',
    3: 'glaze',
    4: 'hail',
    5: 'lightning',
    6: 'rain',
    7: 'rainbow',
    8: 'rime',
    9: 'sandstorm',
    10: 'snow'
}


# In[8]:


def analyze_misclassifications(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    misclassifications = {}
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm[i][j] &gt; 0:
                misclassifications[(class_names[i], class_names[j])] = cm[i][j]
    
    sorted_misclassifications = sorted(misclassifications.items(), key=lambda x: x[1], reverse=True)
    print("Most common misclassifications:")
    for (true_class, pred_class), count in sorted_misclassifications[:5]:
        print(f"{true_class} misclassified as {pred_class}: {count} times")

analyze_misclassifications(y_test, y_pred, class_names)


# In[9]:


def plot_misclassified_examples(X_test, y_test, y_pred, class_names, n_examples=10):
    misclassified = np.where(y_test != y_pred)[0]
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    for i, idx in enumerate(np.random.choice(misclassified, n_examples, replace=False)):
        ax = axes[i // 5, i % 5]
        ax.imshow(X_test[idx].reshape(100, 100, 3))
        ax.set_title(f"True: {class_names[y_test[idx]]}\nPred: {class_names[y_pred[idx]]}")
        ax.axis('off')
    plt.tight_layout()
    plt.show()

plot_misclassified_examples(X_test, y_test, y_pred, class_names)


# In[11]:


np.save('ypred_svm.npy', y_pred)
np.save('ytest.npy', y_test)



</PRE>
</PRE>
</BODY>
</HTML>
