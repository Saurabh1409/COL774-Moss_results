<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_BKVDM.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_BKVDM.py<p><PRE>


import numpy as np
import pandas as pd

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # Prior probabilities P(y)
        self.word_probs = {}    # Conditional probabilities P(x|y)
        self.vocabulary = set()  # All unique words in the training set
        self.classes = []        # List of all classes
        
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.classes = sorted(df[class_col].unique())
        num_documents = len(df)
        for c in self.classes:
            self.class_priors[c] = np.log(sum(df[class_col] == c) / num_documents)
        
        # Build vocabulary (all unique words in the entire dataset)
        for tokens in df[text_col]:
            self.vocabulary.update(tokens)
        
        # Calculate word probabilities for each class with Laplace smoothing
        for c in self.classes:
            # Filter documents for this class
            class_docs = df[df[class_col] == c]
            
            # Count word occurrences for this class
            word_counts = {}
            total_words = 0
            
            for tokens in class_docs[text_col]:
                for word in tokens:
                    word_counts[word] = word_counts.get(word, 0) + 1
                    total_words += 1
            
            # Calculate probabilities with Laplace smoothing
            self.word_probs[c] = {}
<A NAME="0"></A><FONT color = #FF0000><A HREF="match222-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            vocab_size = len(self.vocabulary)
            
            for word in self.vocabulary:
                # Apply Laplace smoothing: (count + alpha) / (total + alpha * |V|)
                count = word_counts.get(word, 0)
                self.word_probs[c][word] = np.log((count + smoothening) / (total_words + smoothening * vocab_size))
</FONT>    
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        # Create a new column for predictions if it doesn't exist
        if predicted_col not in df.columns:
            df[predicted_col] = None
        
        # Iterate through each document
        for idx, row in df.iterrows():
            tokens = row[text_col]
            
            # Calculate log probabilities for each class
            class_scores = {}
            for c in self.classes:
                # Start with the class prior
                score = self.class_priors[c]
                
                # Add log probabilities of words
                for word in tokens:
                    if word in self.vocabulary:
                        score += self.word_probs[c].get(word, 0)
                
                class_scores[c] = score
            
            # Assign the class with the highest probability
            df.at[idx, predicted_col] = max(class_scores, key=class_scores.get)
        
        return df



#!/usr/bin/env python
# coding: utf-8

# In[7]:


import numpy as np
import pandas as pd

class NaiveBayes:
    def __init__(self):
        self.class_priors = {}  # Prior probabilities P(y)
        self.word_probs = {}    # Conditional probabilities P(x|y)
        self.vocabulary = set()  # All unique words in the training set
        self.classes = []        # List of all classes
        
    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        """Learn the parameters of the model from the training data.
        Classes are 1-indexed

        Args:
            df (pd.DataFrame): The training data containing columns class_col and text_col.
                each entry of text_col is a list of tokens.
            smoothening (float): The Laplace smoothening parameter.
        """
        self.classes = sorted(df[class_col].unique())
        num_documents = len(df)
        for c in self.classes:
            self.class_priors[c] = np.log(sum(df[class_col] == c) / num_documents)
        
        # Build vocabulary (all unique words in the entire dataset)
        for tokens in df[text_col]:
            self.vocabulary.update(tokens)
        
        # Calculate word probabilities for each class with Laplace smoothing
        for c in self.classes:
            # Filter documents for this class
            class_docs = df[df[class_col] == c]
            
            # Count word occurrences for this class
            word_counts = {}
            total_words = 0
            
            for tokens in class_docs[text_col]:
                for word in tokens:
                    word_counts[word] = word_counts.get(word, 0) + 1
                    total_words += 1
            
            # Calculate probabilities with Laplace smoothing
            self.word_probs[c] = {}
<A NAME="1"></A><FONT color = #00FF00><A HREF="match222-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            vocab_size = len(self.vocabulary)
            
            for word in self.vocabulary:
                # Apply Laplace smoothing: (count + alpha) / (total + alpha * |V|)
                count = word_counts.get(word, 0)
                self.word_probs[c][word] = np.log((count + smoothening) / (total_words + smoothening * vocab_size))
</FONT>    
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        """
        Predict the class of the input data by filling up column predicted_col in the input dataframe.

        Args:
            df (pd.DataFrame): The testing data containing column text_col.
                each entry of text_col is a list of tokens.
        """
        # Create a new column for predictions if it doesn't exist
        if predicted_col not in df.columns:
            df[predicted_col] = None
        
        # Iterate through each document
        for idx, row in df.iterrows():
            tokens = row[text_col]
            
            # Calculate log probabilities for each class
            class_scores = {}
            for c in self.classes:
                # Start with the class prior
                score = self.class_priors[c]
                
                # Add log probabilities of words
                for word in tokens:
                    if word in self.vocabulary:
                        score += self.word_probs[c].get(word, 0)
                
                class_scores[c] = score
            
            # Assign the class with the highest probability
            df.at[idx, predicted_col] = max(class_scores, key=class_scores.get)
        
        return df


# In[8]:


import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
from sklearn.metrics import  confusion_matrix, classification_report, f1_score
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from wordcloud import WordCloud
import re
import seaborn as sns
from collections import Counter

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()


# In[9]:


def generate_wordcloud(df, class_index,token_idx):
    """Generate a word cloud for a specific class"""
    class_docs = df[df['Class Index'] == class_index]
    
    all_words = []
    for tokens in class_docs[token_idx]:
        all_words.extend(tokens)

    word_freq = Counter(all_words)
    
    wordcloud = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(word_freq)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for {token_idx} Class {class_index}')
    plt.savefig(f'wordcloud_class_{token_idx}_{class_index}.png')
    plt.close()


# In[10]:


def accuracy_score(y_true, y_pred):
    """Compute accuracy of predictions"""
    return np.mean(y_true == y_pred)


# In[11]:


# Basic tokenization function
def tokenize(text):
    """Simple tokenization by splitting on whitespace"""
    if isinstance(text, str):
        return text.lower().split()
    return []


# In[12]:


# Part 1: Train Naive Bayes using only description text
train_df = pd.read_csv('../data/Q1/train.csv')
test_df = pd.read_csv('../data/Q1/test.csv')

# Apply basic tokenization to description
train_df['Tokenized Description'] = train_df['Description'].apply(tokenize)
test_df['Tokenized Description'] = test_df['Description'].apply(tokenize)

# Part 1: Train Naive Bayes using only description text
print("Part 1: Naive Bayes with description text only")
nb_model = NaiveBayes()
start_time = time.time()
nb_model.fit(train_df, smoothening=1.0)
fit_time = time.time() - start_time

# Predict on training set
train_pred_df = nb_model.predict(train_df.copy())
train_accuracy = accuracy_score(train_df['Class Index'], train_pred_df['Predicted'])

# Predict on test set
test_pred_df = nb_model.predict(test_df.copy())
test_accuracy = accuracy_score(test_df['Class Index'], test_pred_df['Predicted'])

print(f"Training time: {fit_time:.2f} seconds")
print(f"Training accuracy: {train_accuracy:.4f}")
print(f"Test accuracy: {test_accuracy:.4f}")

for class_idx in sorted(train_df['Class Index'].unique()):
    generate_wordcloud(train_df, class_idx,"Tokenized Description")

# Make classification report
y_pred = []
y_true = []
for idx, row in test_pred_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[13]:


def preprocess_text(tokens):
    # Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    
    # Apply stemming
    tokens = [stemmer.stem(token) for token in tokens]
    
    return tokens


# In[15]:


# Apply preprocessing to description
train_df['Preprocessed Description'] = train_df['Tokenized Description'].apply(preprocess_text)
test_df['Preprocessed Description'] = test_df['Tokenized Description'].apply(preprocess_text)

# Train a new model on the preprocessed data
nb_model_preprocessed = NaiveBayes()
start_time = time.time()
nb_model_preprocessed.fit(train_df, smoothening=1.0, text_col='Preprocessed Description')
fit_time = time.time() - start_time

# Predict on training set
train_pred_preprocessed_df = nb_model_preprocessed.predict(train_df.copy(), text_col='Preprocessed Description')
train_accuracy_preprocessed = accuracy_score(train_df['Class Index'], train_pred_preprocessed_df['Predicted'])

# Predict on test set
test_pred_preprocessed_df = nb_model_preprocessed.predict(test_df.copy(), text_col='Preprocessed Description')
test_accuracy_preprocessed = accuracy_score(test_df['Class Index'], test_pred_preprocessed_df['Predicted'])

print(f"Training time: {fit_time:.2f} seconds")
print(f"Training accuracy: {train_accuracy_preprocessed:.4f}")
print(f"Test accuracy: {test_accuracy_preprocessed:.4f}")
    


# In[16]:


def extract_bigrams(tokens):
    bigrams = []
    for i in range(len(tokens) - 1):
        bigram = tokens[i] + " " + tokens[i+1]
        bigrams.append(bigram)
    return tokens + bigrams


# In[ ]:


# Part 3: Using bigrams as features
print("\nPart 3: Naive Bayes with unigrams and bigrams")

# Apply bigram extraction to preprocessed description
train_df['Bigram Description'] = train_df['Preprocessed Description'].apply(extract_bigrams)
test_df['Bigram Description'] = test_df['Preprocessed Description'].apply(extract_bigrams)

# Train a new model using unigrams and bigrams
nb_model_bigrams = NaiveBayes()
start_time = time.time()
nb_model_bigrams.fit(train_df, smoothening=1.0, text_col='Bigram Description')
fit_time = time.time() - start_time

# Predict on training set
train_pred_bigram_df = nb_model_bigrams.predict(train_df.copy(), text_col='Bigram Description')
train_accuracy_bigrams = accuracy_score(train_df['Class Index'], train_pred_bigram_df['Predicted'])

# Predict on test set
test_pred_bigram_df = nb_model_bigrams.predict(test_df.copy(), text_col='Bigram Description')
test_accuracy_bigrams = accuracy_score(test_df['Class Index'], test_pred_bigram_df['Predicted'])

print(f"Training time: {fit_time:.2f} seconds")
print(f"Training accuracy: {train_accuracy_bigrams:.4f}")
print(f"Test accuracy: {test_accuracy_bigrams:.4f}")

# Classification report
y_pred = []
y_true = []
for idx, row in test_pred_bigram_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[ ]:


#Unigram biagram without preprocessing

train_df['Bigram2 Description'] = train_df['Tokenized Description'].apply(extract_bigrams)
test_df['Bigram2 Description'] = test_df['Tokenized Description'].apply(extract_bigrams)

# Train a new model using unigrams and bigrams
nb_model_bigrams2 = NaiveBayes()
start_time = time.time()
nb_model_bigrams2.fit(train_df, smoothening=1.0, text_col='Bigram2 Description')
fit_time = time.time() - start_time

# Predict on training set
train_pred_bigram2_df = nb_model_bigrams2.predict(train_df.copy(), text_col='Bigram2 Description')
train_accuracy_bigrams2 = accuracy_score(train_df['Class Index'], train_pred_bigram2_df['Predicted'])

# Predict on test set
test_pred_bigram2_df = nb_model_bigrams2.predict(test_df.copy(), text_col='Bigram2 Description')
test_accuracy_bigrams2 = accuracy_score(test_df['Class Index'], test_pred_bigram2_df['Predicted'])

print(f"Training time: {fit_time:.2f} seconds")
print(f"Training accuracy: {train_accuracy_bigrams2:.4f}")
print(f"Test accuracy: {test_accuracy_bigrams2:.4f}")

#Classification report
y_pred = []
y_true = []
for idx, row in test_pred_bigram2_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[ ]:



# Part 4: Analyze performance of different models for description text
print("\nPart 4: Performance analysis for description text features")
print("Model comparison:")
print(f"Basic model - Test accuracy: {test_accuracy:.4f}")
print(f"Preprocessed model - Test accuracy: {test_accuracy_preprocessed:.4f}")
print(f"Unigram + Bigram model - Test accuracy: {test_accuracy_bigrams:.4f}")
print(f"Unigram + Bigram without preprocessing model - Test accuracy: {test_accuracy_bigrams2:.4f}")

best_model_name = ""  
best_accuracy = max(test_accuracy, test_accuracy_preprocessed, test_accuracy_bigrams, test_accuracy_bigrams2)

if best_accuracy == test_accuracy:
    best_model = nb_model
    best_model_name = "Basic"
    test_description_pred_df = test_pred_df
elif best_accuracy == test_accuracy_preprocessed:
    best_model = nb_model_preprocessed
    best_model_name = "Preprocessed"
    test_description_pred_df = test_pred_preprocessed_df
elif best_accuracy == test_accuracy_bigrams:
    best_model = nb_model_bigrams
    best_model_name = "Unigram + Bigram"
    test_description_pred_df = test_pred_bigram_df
else:
    best_model = nb_model_bigrams2
    best_model_name = "Unigram + Bigram without preprocessing"
    test_description_pred_df = test_pred_bigram2_df

print(f"\nBest model for description text: {best_model_name} model with accuracy {best_accuracy:.4f}")
y_true = []
y_pred = []
for i in range(len(test_df)):
    y_true.append(test_df['Class Index'].iloc[i])
    y_pred.append(test_description_pred_df['Predicted'].iloc[i])

with open('classification_report.txt', 'w') as f:
    f.write("Classification report for Description text\n\n")
    f.write(classification_report(y_true, y_pred))


# In[ ]:



# Apply basic tokenization to title
train_df['Tokenized Title'] = train_df['Title'].apply(tokenize)
test_df['Tokenized Title'] = test_df['Title'].apply(tokenize)

# Part 1: Train Naive Bayes using only Test text

nb_model = NaiveBayes()
start_time = time.time()
nb_model.fit(train_df, smoothening=1.0, class_col="Class Index", text_col="Tokenized Title")
fit_time = time.time() - start_time

# Predict on training set
train_pred_df = nb_model.predict(train_df.copy(), text_col="Tokenized Title")
train_accuracy = accuracy_score(train_df['Class Index'], train_pred_df['Predicted'])

# Predict on test set
test_pred_df = nb_model.predict(test_df.copy(), text_col="Tokenized Title")
test_accuracy = accuracy_score(test_df['Class Index'], test_pred_df['Predicted'])

print(f"Training time for simple Tokenization: {fit_time:.2f} seconds")
print(f"Training accuracy for simple Tokenization: {train_accuracy:.4f}")
print(f"Test accuracy for simple Tokenization: {test_accuracy:.4f}")  

for class_idx in sorted(train_df['Class Index'].unique()):
    generate_wordcloud(train_df, class_idx,"Tokenized Title")

# Make classification report
y_pred = []
y_true = []
for idx, row in test_pred_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test_title.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[ ]:


# Apply preprocessing to title
train_df['Preprocessed Title'] = train_df['Tokenized Title'].apply(preprocess_text)
test_df['Preprocessed Title'] = test_df['Tokenized Title'].apply(preprocess_text)

# Train a new model on the preprocessed data
nb_model_preprocessed = NaiveBayes()
start_time = time.time()
nb_model_preprocessed.fit(train_df, smoothening=1.0, text_col='Preprocessed Title', class_col="Class Index")
fit_time = time.time() - start_time

# Predict on training set
train_pred_preprocessed_df = nb_model_preprocessed.predict(train_df.copy(), text_col='Preprocessed Title')
train_accuracy_preprocessed = accuracy_score(train_df['Class Index'], train_pred_preprocessed_df['Predicted'])

# Predict on test set
test_pred_preprocessed_df = nb_model_preprocessed.predict(test_df.copy(), text_col='Preprocessed Title')
test_accuracy_preprocessed = accuracy_score(test_df['Class Index'], test_pred_preprocessed_df['Predicted'])

print(f"Training time for Preprocessed Title: {fit_time:.2f} seconds")
print(f"Training accuracy for Preprocessed Title: {train_accuracy_preprocessed:.4f}")
print(f"Test accuracy for Preprocessed Title: {test_accuracy_preprocessed:.4f}")

# Generate word clouds for preprocessed data
for class_idx in sorted(train_df['Class Index'].unique()):
    generate_wordcloud(train_df, class_idx,"Preprocessed Title")

# Make classification report
y_pred = []
y_true = []
for idx, row in test_pred_preprocessed_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[ ]:


# Using bigrams as features

# Apply bigram extraction to preprocessed title
train_df['Bigram Title'] = train_df['Preprocessed Title'].apply(extract_bigrams)
test_df['Bigram Title'] = test_df['Preprocessed Title'].apply(extract_bigrams)

# Train a new model using unigrams and bigrams
nb_model_bigrams = NaiveBayes()
start_time = time.time()
nb_model_bigrams.fit(train_df, smoothening=1.0, text_col='Bigram Title', class_col="Class Index")
fit_time = time.time() - start_time

# Predict on training set
train_pred_bigram_df = nb_model_bigrams.predict(train_df.copy(), text_col='Bigram Title')
train_accuracy_bigrams = accuracy_score(train_df['Class Index'], train_pred_bigram_df['Predicted'])

# Predict on test set
test_pred_bigram_df = nb_model_bigrams.predict(test_df.copy(), text_col='Bigram Title')
test_accuracy_bigrams = accuracy_score(test_df['Class Index'], test_pred_bigram_df['Predicted'])

print(f"Training time for Bigram Title: {fit_time:.2f} seconds")
print(f"Training accuracy for Bigram Title: {train_accuracy_bigrams:.4f}")
print(f"Test accuracy for Bigram Title: {test_accuracy_bigrams:.4f}")

# Classification report
y_pred = []
y_true = []
for idx, row in test_pred_bigram_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))
    


# In[ ]:


#Unigram biagram without preprocessing

#Apply bigram extraction to title
train_df['Bigram2 Title'] = train_df['Tokenized Title'].apply(extract_bigrams)
test_df['Bigram2 Title'] = test_df['Tokenized Title'].apply(extract_bigrams)

# Train a new model using unigrams and bigrams
nb_model_bigrams2 = NaiveBayes()
start_time = time.time()
nb_model_bigrams2.fit(train_df, smoothening=1.0, text_col='Bigram2 Title', class_col="Class Index")
fit_time = time.time() - start_time

# Predict on training set
train_pred_bigram2_df = nb_model_bigrams2.predict(train_df.copy(), text_col='Bigram2 Title')
train_accuracy_bigrams2 = accuracy_score(train_df['Class Index'], train_pred_bigram2_df['Predicted'])

# Predict on test set
test_pred_bigram2_df = nb_model_bigrams2.predict(test_df.copy(), text_col='Bigram2 Title')
test_accuracy_bigrams2 = accuracy_score(test_df['Class Index'], test_pred_bigram2_df['Predicted'])

print(f"Training time for Bigram2 Title: {fit_time:.2f} seconds")
print(f"Training accuracy for Bigram2 Title: {train_accuracy_bigrams2:.4f}")
print(f"Test accuracy for Bigram2 Title: {test_accuracy_bigrams2:.4f}")


# In[ ]:


#Analyzing performance of different models for title text

print("\nPart 4: Performance analysis for Title text features")
print("Model comparison:")
print(f"Basic model - Test accuracy: {test_accuracy:.4f}")
print(f"Preprocessed model - Test accuracy: {test_accuracy_preprocessed:.4f}")
print(f"Unigram + Bigram model - Test accuracy: {test_accuracy_bigrams:.4f}")
print(f"Unigram + Bigram without preprocessing model - Test accuracy: {test_accuracy_bigrams2:.4f}")

best_model_name = ""
best_accuracy = max(test_accuracy, test_accuracy_preprocessed, test_accuracy_bigrams, test_accuracy_bigrams2)

if best_accuracy == test_accuracy:
    best_model = nb_model
    best_model_name = "Basic"
    test_title_pred_df = test_pred_df
elif best_accuracy == test_accuracy_preprocessed:
    best_model = nb_model_preprocessed
    best_model_name = "Preprocessed"
    test_title_pred_df = test_pred_preprocessed_df
elif(best_accuracy == test_accuracy_bigrams):
    best_model = nb_model_bigrams
    best_model_name = "Unigram + Bigram"
    test_title_pred_df = test_pred_bigram_df
else:
    best_model = nb_model_bigrams2
    best_model_name = "Unigram + Bigram without preprocessing"
    test_title_pred_df = test_pred_bigram2_df


print(f"\nBest model for Title text: {best_model_name} model with accuracy {best_accuracy:.4f}")
y_true = []
y_pred = []
for i in range(len(test_df)):
    y_true.append(test_df['Class Index'].iloc[i])
    y_pred.append(test_title_pred_df['Predicted'].iloc[i])

with open('classification_report.txt', 'a') as f:
    f.write("\nClassification report for Title text\n\n")
    f.write(classification_report(y_true, y_pred))


# In[ ]:


#Combine the feature of title and description for bigrams with preprocessing

def combine_features(desc_df,title_df,desc_model,title_model):
    return desc_df[desc_model] + title_df[title_model]

train_df['Combined feature'] = combine_features(train_df,train_df,'Bigram Description','Bigram Title')

test_df['Combined feature'] = combine_features(test_df,test_df,'Bigram Description','Bigram Title')

# Train a new model on combined features
nb_model_combined = NaiveBayes()
start_time = time.time()
nb_model_combined.fit(train_df, smoothening=1.0, text_col='Combined feature', class_col="Class Index")
fit_time = time.time() - start_time

# Predict on training set
train_pred_combined_df = nb_model_combined.predict(train_df.copy(), text_col='Combined feature')
train_accuracy_combined = accuracy_score(train_df['Class Index'], train_pred_combined_df['Predicted'])

# Predict on test set
test_pred_combined_df = nb_model_combined.predict(test_df.copy(), text_col='Combined feature')
test_accuracy_combined = accuracy_score(test_df['Class Index'], test_pred_combined_df['Predicted'])

print(f"Training time for Combined feature: {fit_time:.2f} seconds")
print(f"Training accuracy for Combined feature: {train_accuracy_combined:.4f}")
print(f"Test accuracy for Combined feature: {test_accuracy_combined:.4f}")

# Classification report
y_pred = []
y_true = []
for idx, row in test_pred_combined_df.iterrows():
    y_pred.append(row['Predicted'])
    y_true.append(row['Class Index'])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))
    


# In[ ]:


# Let's learn the model seperately for description and title and then combine the features

# Train a new model on description
nb_model_description = NaiveBayes()
start_time = time.time()
nb_model_description.fit(train_df, smoothening=1.0, text_col='Bigram Description', class_col="Class Index")
fit_time = time.time() - start_time

# Train a new model on title
nb_model_title = NaiveBayes()
start_time = time.time()
nb_model_title.fit(train_df, smoothening=1.0, text_col='Bigram Title', class_col="Class Index")
fit_time += time.time() - start_time

# Make an new prediction code for combined features
def predict_combined(df, desc_model, title_model):
    # Create a new column for predictions if it doesn't exist
    if 'Predicted' not in df.columns:
        df['Predicted'] = None
    
    # Iterate through each document
    for idx, row in df.iterrows():
        desc_tokens = row[desc_model]
        title_tokens = row[title_model]
        
        # Calculate log probabilities for each class
        class_scores = {}
        for c in nb_model_description.classes:
            # Start with the class prior
            score = nb_model_description.class_priors[c]
            
            # Add log probabilities of words
            for word in desc_tokens:
                if word in nb_model_description.vocabulary:
                    score += nb_model_description.word_probs[c].get(word, 0)
            
            class_scores[c] = score

            for word in title_tokens:
                if word in nb_model_title.vocabulary:
                    score += nb_model_title.word_probs[c].get(word, 0)
            class_scores[c] += score
        
        # Assign the class with the highest probability
        df.at[idx, 'Predicted'] = max(class_scores, key=class_scores.get)
    
    return df

# Predict on training set
train_pred_combined_df = predict_combined(train_df, 'Bigram Description', 'Bigram Title')
train_accuracy_combined = accuracy_score(train_df['Class Index'], train_pred_combined_df['Predicted'])

# Predict on test set
test_pred_combined_df = predict_combined(test_df, 'Bigram Description', 'Bigram Title')
test_accuracy_combined = accuracy_score(test_df['Class Index'], test_pred_combined_df['Predicted'])

print(f"Training time for Combined feature: {fit_time:.2f} seconds")
print(f"Training accuracy for Combined feature: {train_accuracy_combined:.4f}")
print(f"Test accuracy for Combined feature: {test_accuracy_combined:.4f}")


# In[ ]:


#part 7: Benchmarking the model we obtained.

unique_classes = train_df['Class Index'].unique()

np.random.seed(42)  # For reproducibility
random_predictions = np.random.choice(unique_classes, size=len(test_df))

# Calculate the actual accuracy
random_accuracy = accuracy_score(test_df['Class Index'], random_predictions)

print(f"Test accuracy with random guessing: {random_accuracy:.4f} or {random_accuracy*100:.2f}%")

# Classification report
y_pred = []
y_true = []
for idx, row in test_df.iterrows():
    y_true.append(row['Class Index'])
    y_pred.append(random_predictions[idx])
print("Classification Report:")
with open('classification_report_test.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))

# Predict each text with same class i.e most frequent class in the training data
most_frequent_class = train_df['Class Index'].value_counts().idxmax()
most_frequent_predictions = np.full(len(test_df), most_frequent_class)

# Calculate the actual accuracy
most_frequent_accuracy = accuracy_score(test_df['Class Index'], most_frequent_predictions)

print(f"Test accuracy with most frequent class: {most_frequent_accuracy:.4f} or {most_frequent_accuracy*100:.2f}%")

# Classification report
y_pred = []
y_true = []
for idx, row in test_df.iterrows():
    y_true.append(row['Class Index'])
    y_pred.append(most_frequent_class)
print("Classification Report:")
with open('classification_report_test_title.txt', 'w') as f:
    f.write(classification_report(y_true, y_pred))


# In[ ]:


classes = sorted(test_df['Class Index'].unique())

y_true = []
y_pred = []
for i in range(len(test_df)):
    y_true.append(test_df['Class Index'].iloc[i])
    y_pred.append(test_description_pred_df['Predicted'].iloc[i])

cm_desc = confusion_matrix(y_true, y_pred, labels=classes)

y_true = []
y_pred = []
for i in range(len(test_df)):
    y_true.append(test_df['Class Index'].iloc[i])
    y_pred.append(test_title_pred_df['Predicted'].iloc[i])

cm_title = confusion_matrix(y_true, y_pred, labels=classes)
cm_normalized_desc = cm_desc.astype('float') / cm_desc.sum(axis=1)[:, np.newaxis]
cm_normalized_title = cm_title.astype('float') / cm_title.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(10, 8))

sns.heatmap(cm_normalized_desc, annot=True, fmt='.2f', cmap='Blues', 
            xticklabels=classes, yticklabels=classes)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for News Category Classification for decription feature')
plt.tight_layout()
plt.savefig('confusion_matrix_desc.png')
plt.show()

plt.figure(figsize=(10, 8))

sns.heatmap(cm_normalized_title, annot=True, fmt='.2f', cmap='Blues',xticklabels=classes, yticklabels=classes)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for News Category Classification for title feature')
plt.tight_layout()
plt.savefig('confusion_matrix_title.png')
plt.show()


# In[17]:


# create a new feature 
def extract_trigrams(tokens):
    trigrams = []
    for i in range(len(tokens) - 2):
        trigram = tokens[i] + " " + tokens[i+1] + " " + tokens[i+2]
        trigrams.append(trigram)
    return trigrams

# Apply trigram extraction to preprocessed description
train_df['Trigram Description'] = train_df['Preprocessed Description'].apply(extract_bigrams)+train_df['Preprocessed Description'].apply(extract_trigrams)
test_df['Trigram Description'] = test_df['Preprocessed Description'].apply(extract_bigrams)+test_df['Preprocessed Description'].apply(extract_trigrams)

# Train a new model using unigrams and trigrams
nb_model_trigrams = NaiveBayes()
start_time = time.time()
nb_model_trigrams.fit(train_df, smoothening=1.0, text_col='Trigram Description', class_col="Class Index")
fit_time = time.time() - start_time

# Predict on training set
train_pred_trigram_df = nb_model_trigrams.predict(train_df.copy(), text_col='Trigram Description')
train_accuracy_trigrams = accuracy_score(train_df['Class Index'], train_pred_trigram_df['Predicted'])

# Predict on test set
test_pred_trigram_df = nb_model_trigrams.predict(test_df.copy(), text_col='Trigram Description')
test_accuracy_trigrams = accuracy_score(test_df['Class Index'], test_pred_trigram_df['Predicted'])

print(f"Training time for Trigram Description: {fit_time:.2f} seconds")
print(f"Training accuracy for Trigram Description: {train_accuracy_trigrams:.4f}")
print(f"Test accuracy for Trigram Description: {test_accuracy_trigrams:.4f}")





import cvxopt
import numpy as np

def linear_kernel( x1, x2):
    return np.dot(x1, x2)
    
def gaussian_kernel( x1, x2,gamma):
    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)

def svm_cvxopt(X: np.ndarray, Y: np.ndarray, c: float, tol: float, find_prod, gamma: float = None):
    shape = Y.shape

    prod = find_prod(X, X, gamma)
    P = cvxopt.matrix(np.matmul(Y, Y.T) * prod)

    q = cvxopt.matrix(-np.ones(shape))

    G = np.identity(shape[0])
    G = cvxopt.matrix(np.append(G, -G, axis=0))

    h = np.ones(shape)
    h = cvxopt.matrix(np.append(c * h, 0 * h, axis=0))

    A = cvxopt.matrix(Y.T, tc='d')

    b = cvxopt.matrix(0.0)

    sol = cvxopt.solvers.qp(P, q, G, h, A, b, options={'show_progress': False})
    if sol['status'] == "unknown":
        return None

    alpha = np.reshape(np.array(sol['x']), shape)
    indices = [i for i in range(shape[0]) if alpha[i] &gt; tol]
    alpha = alpha[indices]
    X, Y = X[indices], Y[indices]
    inner_prod = np.sum(alpha * Y * find_prod(X, X, gamma), 0)

    M = max(range(len(indices)), key=lambda i: -float("inf")
            if Y[i] == 1 or alpha[i] &gt;= c - tol else inner_prod[i])
    m = min(range(len(indices)), key=lambda i: float("inf")
            if Y[i] == -1 or alpha[i] &gt;= c - tol else inner_prod[i])
    b = -(inner_prod[M] + inner_prod[m]) / 2

    return indices, alpha, b




class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.support_vectors = None
        self.support_vector_labels = None
        self.alphas = None
        self.w = None
        self.b = None
        self.kernel_type = None
        self.gamma = None
        self.support_vector_indices = None
        self.K= None
        self.P= None
        self.q= None
        self.G= None
        self.h= None
        self.A= None
        self.b= None
        self.solution= None

    def kernel_function(self, x1, x2):
        if self.kernel_type == 'linear':
            return linear_kernel(x1, x2)
        elif self.kernel_type == 'gaussian':
            return gaussian_kernel(x1, x2,self.gamma)
    
    def kernel_matrix(self, X):
        n_samples = X.shape[0]
        self.K = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(n_samples):
                self.K[i, j] = self.kernel_function(X[i], X[j])

    def QuadraticProgramming(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        n_samples, n_features = X.shape
        self.P = cvxopt.matrix(np.outer(y, y) * self.K)
        self.q = cvxopt.matrix(-np.ones(n_samples))

    def Constraints(self, X,y,C=1.0):
        n_samples, n_features = X.shape
        self.h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
        self.G = cvxopt.matrix(np.vstack((-np.eye(n_samples) , np.eye(n_samples) )))
        self.A = cvxopt.matrix(y.astype(np.double).reshape(1, -1))
        self.b = cvxopt.matrix(np.zeros(1))

    def solve(self):
        # make the visible streaming false
        cvxopt.solvers.options['show_progress'] = False
        solved = cvxopt.solvers.qp(self.P, self.q, self.G, self.h, self.A, self.b)
        self.solution = np.array(solved['x']).flatten()


    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''

        self.kernel_type = kernel
        self.gamma = gamma
        
        y_transformed = np.where(y == 0, -1, 1)
        
        n_samples, n_features = X.shape
        
        self.kernel_matrix(X)
        
        # set the parameters for the quadratic programming
        self.QuadraticProgramming(X, y_transformed, kernel, C, gamma)

        # set the constraints for the quadratic programming
        self.Constraints(X, y_transformed, C)

        # solve the quadratic programming optimization problem
        self.solve()
        
        # Find support vectors (where α &gt; 0)
        sv_threshold = 1e-5  # Threshold for numerical stability
        sv_indices = np.where(self.solution &gt; sv_threshold)[0]

        self.alphas=self.solution[sv_indices]
        self.support_vectors=(X[sv_indices])
        self.support_vector_labels=(y_transformed[sv_indices])
        self.support_vector_indices=(sv_indices)
        
        if kernel == 'linear':
            self.w = np.zeros(n_features)
            for i in range(len(self.alphas)):
                self.w += self.alphas[i] * self.support_vector_labels[i] * self.support_vectors[i]
        
        # Compute b
        if kernel == 'linear':
            indices = np.where((self.solution &gt; sv_threshold) & (self.solution &lt; C - sv_threshold))[0]
            if len(indices) &gt; 0:
                b_sum = 0
                for i in indices:
                    b_sum += y_transformed[i] - np.dot(self.w, X[i])
                self.b = b_sum / len(indices)
            else:
                b_sum = 0
                for i in sv_indices:
                    b_sum += y_transformed[i] - np.dot(self.w, X[i])
                self.b = b_sum / len(sv_indices)
        else:
            indices = np.where((self.solution &gt; sv_threshold) & (self.solution &lt; C - sv_threshold))[0]
            if len(indices) &gt; 0:
                b_sum = 0
                for i in indices:
                    s = 0
                    for alpha_idx, sv_idx in enumerate(sv_indices):
                        s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] * self.kernel_function(X[sv_idx], X[i])
                    b_sum += y_transformed[i] - s
                self.b = b_sum / len(indices)
            else:
                b_sum = 0
                for i in sv_indices:
                    s = 0
                    for alpha_idx, sv_idx in enumerate(sv_indices):
                        s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] * self.kernel_function(X[sv_idx], X[i])
                    b_sum += y_transformed[i] - s
                self.b = b_sum / len(sv_indices)

    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        if self.kernel_type == 'linear':
            # Decision function for linear kernel: sign(w^T x + b)
            decision = np.dot(X, self.w) + self.b
        else:
            # Decision function for non-linear kernel: sign(Σ α_i y_i K(x_i, x) + b)
            decision = np.zeros(X.shape[0])
            for i in range(X.shape[0]):
                s = 0
                for alpha_idx, sv_idx in enumerate(range(len(self.alphas))):
                    s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] * \
                         self.kernel_function(self.support_vectors[sv_idx], X[i])
                decision[i] = s + self.b
        
        # Convert decision function to class labels (0 or 1)
        return np.where(decision &lt; 0, 0, 1)



#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import numpy as np
import cv2
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
import time
from sklearn.linear_model import SGDClassifier


# In[2]:


import cvxopt
import numpy as np

def linear_kernel( x1, x2):
    return np.dot(x1, x2)
    
def gaussian_kernel( x1, x2,gamma):
    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)

def svm_cvxopt(X: np.ndarray, Y: np.ndarray, c: float, tol: float, find_prod, gamma: float = None):
    shape = Y.shape

    prod = find_prod(X, X, gamma)
    P = cvxopt.matrix(np.matmul(Y, Y.T) * prod)

    q = cvxopt.matrix(-np.ones(shape))

    G = np.identity(shape[0])
    G = cvxopt.matrix(np.append(G, -G, axis=0))

    h = np.ones(shape)
    h = cvxopt.matrix(np.append(c * h, 0 * h, axis=0))

    A = cvxopt.matrix(Y.T, tc='d')

    b = cvxopt.matrix(0.0)

    sol = cvxopt.solvers.qp(P, q, G, h, A, b, options={'show_progress': False})
    if sol['status'] == "unknown":
        return None

    alpha = np.reshape(np.array(sol['x']), shape)
    indices = [i for i in range(shape[0]) if alpha[i] &gt; tol]
    alpha = alpha[indices]
    X, Y = X[indices], Y[indices]
    inner_prod = np.sum(alpha * Y * find_prod(X, X, gamma), 0)

    M = max(range(len(indices)), key=lambda i: -float("inf")
            if Y[i] == 1 or alpha[i] &gt;= c - tol else inner_prod[i])
    m = min(range(len(indices)), key=lambda i: float("inf")
            if Y[i] == -1 or alpha[i] &gt;= c - tol else inner_prod[i])
    b = -(inner_prod[M] + inner_prod[m]) / 2

    return indices, alpha, b




class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.support_vectors = None
        self.support_vector_labels = None
        self.alphas = None
        self.w = None
        self.b = None
        self.kernel_type = None
        self.gamma = None
        self.support_vector_indices = None
        self.K= None
        self.P= None
        self.q= None
        self.G= None
        self.h= None
        self.A= None
        self.b= None
        self.solution= None

    def kernel_function(self, x1, x2):
        if self.kernel_type == 'linear':
            return linear_kernel(x1, x2)
        elif self.kernel_type == 'gaussian':
            return gaussian_kernel(x1, x2,self.gamma)
    
    def kernel_matrix(self, X):
        n_samples = X.shape[0]
        self.K = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(n_samples):
                self.K[i, j] = self.kernel_function(X[i], X[j])

    def QuadraticProgramming(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        n_samples, n_features = X.shape
        self.P = cvxopt.matrix(np.outer(y, y) * self.K)
        self.q = cvxopt.matrix(-np.ones(n_samples))

    def Constraints(self, X,y,C=1.0):
        n_samples, n_features = X.shape
        self.h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
        self.G = cvxopt.matrix(np.vstack((-np.eye(n_samples) , np.eye(n_samples) )))
        self.A = cvxopt.matrix(y.astype(np.double).reshape(1, -1))
        self.b = cvxopt.matrix(np.zeros(1))

    def solve(self):
        # make the visible streaming false
        cvxopt.solvers.options['show_progress'] = False
        solved = cvxopt.solvers.qp(self.P, self.q, self.G, self.h, self.A, self.b)
        self.solution = np.array(solved['x']).flatten()


    def fit(self, X, y, kernel='linear', C=1.0, gamma=0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''

        self.kernel_type = kernel
        self.gamma = gamma
        
        y_transformed = np.where(y == 0, -1, 1)
        
        n_samples, n_features = X.shape
        
        self.kernel_matrix(X)
        
        # set the parameters for the quadratic programming
        self.QuadraticProgramming(X, y_transformed, kernel, C, gamma)

        # set the constraints for the quadratic programming
        self.Constraints(X, y_transformed, C)

        # solve the quadratic programming optimization problem
        self.solve()
        
        # Find support vectors (where α &gt; 0)
        sv_threshold = 1e-5  # Threshold for numerical stability
        sv_indices = np.where(self.solution &gt; sv_threshold)[0]

        self.alphas=self.solution[sv_indices]
        self.support_vectors=(X[sv_indices])
        self.support_vector_labels=(y_transformed[sv_indices])
        self.support_vector_indices=(sv_indices)
        
        if kernel == 'linear':
            self.w = np.zeros(n_features)
            for i in range(len(self.alphas)):
                self.w += self.alphas[i] * self.support_vector_labels[i] * self.support_vectors[i]
        
        # Compute b
        if kernel == 'linear':
            indices = np.where((self.solution &gt; sv_threshold) & (self.solution &lt; C - sv_threshold))[0]
            if len(indices) &gt; 0:
                b_sum = 0
                for i in indices:
                    b_sum += y_transformed[i] - np.dot(self.w, X[i])
                self.b = b_sum / len(indices)
            else:
                b_sum = 0
                for i in sv_indices:
                    b_sum += y_transformed[i] - np.dot(self.w, X[i])
                self.b = b_sum / len(sv_indices)
        else:
            indices = np.where((self.solution &gt; sv_threshold) & (self.solution &lt; C - sv_threshold))[0]
            if len(indices) &gt; 0:
                b_sum = 0
                for i in indices:
                    s = 0
                    for alpha_idx, sv_idx in enumerate(sv_indices):
                        s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] * self.kernel_function(X[sv_idx], X[i])
                    b_sum += y_transformed[i] - s
                self.b = b_sum / len(indices)
            else:
                b_sum = 0
                for i in sv_indices:
                    s = 0
                    for alpha_idx, sv_idx in enumerate(sv_indices):
                        s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] * self.kernel_function(X[sv_idx], X[i])
                    b_sum += y_transformed[i] - s
                self.b = b_sum / len(sv_indices)

    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        if self.kernel_type == 'linear':
            # Decision function for linear kernel: sign(w^T x + b)
            decision = np.dot(X, self.w) + self.b
        else:
            # Decision function for non-linear kernel: sign(Σ α_i y_i K(x_i, x) + b)
            decision = np.zeros(X.shape[0])
            for i in range(X.shape[0]):
                s = 0
                for alpha_idx, sv_idx in enumerate(range(len(self.alphas))):
                    s += self.alphas[alpha_idx] * self.support_vector_labels[alpha_idx] *                          self.kernel_function(self.support_vectors[sv_idx], X[i])
                decision[i] = s + self.b
        
        # Convert decision function to class labels (0 or 1)
        return np.where(decision &lt; 0, 0, 1)


# In[3]:


def load_images(folder, label):
    """Loads images, resizes, flattens, and normalizes them."""
    images = []
    labels = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img = cv2.resize(img, (100, 100))  # Resize to 100x100
            img = img.astype(np.float32) / 255.0  # Normalize to [0,1]
            images.append(img.flatten())  # Flatten to 30,000-dim vector
            labels.append(label)
    return (np.array(images), np.array(labels))


# In[6]:


d = 93
# Load images from two classes i.e 5 and 6 that are lightning and rain
class1_folder = f"../data/Q2/train/lightning/"
class2_folder = f"../data/Q2/train/rain/"

X0,y0 = load_images(class1_folder, 0)
X1,y1 = load_images(class2_folder, 1)
X_train = np.vstack((X0, X1))
y_train = np.hstack((y0, y1))


# In[7]:


d = 93
# Load images from two classes i.e 5 and 6 that are lightning and rain
class1_folder = f"../data/Q2/test/lightning/"
class2_folder = f"../data/Q2/test/rain/"

X0,y0 = load_images(class1_folder, 0)
X1,y1 = load_images(class2_folder, 1)
X_test = np.vstack((X0, X1))
y_test = np.hstack((y0, y1))


# In[10]:


linear_svm_model = SupportVectorMachine()
start_time=time.time()
linear_svm_model.fit(X_train, y_train, kernel='linear', C=1.0)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match222-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_time=time.time()-start_time
y_train_pred = linear_svm_model.predict(X_train)
y_test_pred = linear_svm_model.predict(X_test)
train_accuracy = np.mean(y_train_pred == y_train)
test_accuracy = np.mean(y_test_pred == y_test)
</FONT>print(f"Training time: {train_time:.2f} seconds")
print(f"B:{linear_svm_model.b}")
print(f"Number of support vectors: {linear_svm_model.support_vectors.shape[0]}")
print(f"Percentage of support vectors: {linear_svm_model.support_vectors.shape[0] / X_train.shape[0]*100:.2f}%")
print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")


# In[11]:


#Plot top5 support vectors
alphas=[]
for i,alpha in enumerate(linear_svm_model.alphas):
    alphas.append((alpha,linear_svm_model.support_vectors[i],linear_svm_model.support_vector_labels[i]))
alphas.sort(key=lambda x: x[0],reverse=True)
top_5_alphas=alphas[:5]
top_5_support_vectors=np.array([x[1] for x in top_5_alphas])
top_5_support_vectors_labels=[x[2] for x in top_5_alphas]

    
import matplotlib.pyplot as plt
fig, axes = plt.subplots(1, 5, figsize=(20, 20))
for i, ax in enumerate(axes):
    ax.imshow(top_5_support_vectors[i].reshape(100, 100, 3))
    ax.axis('off')
    ax.set_title(f"Class {(top_5_support_vectors_labels[i]+1)//2}")
fig.savefig("top5_support_vectors.png")
plt.show()


# In[12]:


w_img = linear_svm_model.w.reshape(100, 100, 3)  # Reshape w to 100x100x3
plt.imshow(w_img, cmap="seismic")  # Use a diverging colormap
plt.axis('off')
plt.title("Weight Vector w")
plt.colorbar()
plt.savefig("weight_vector.png")
plt.show()


# In[13]:


gaussian_svm_model = SupportVectorMachine()
start_time=time.time()
gaussian_svm_model.fit(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match222-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_time=time.time()-start_time
y_train_pred = gaussian_svm_model.predict(X_train)
y_test_pred = gaussian_svm_model.predict(X_test)
train_accuracy = np.mean(y_train_pred == y_train)
test_accuracy = np.mean(y_test_pred == y_test)
</FONT>print(f"Training time: {train_time:.2f} seconds")
print(f"B:{gaussian_svm_model.b}")
print(f"Number of support vectors: {gaussian_svm_model.support_vectors.shape[0]}")
print(f"Percentage of support vectors: {gaussian_svm_model.support_vectors.shape[0] / X_train.shape[0]*100:.2f}%")
print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")


# In[14]:


#How many support vectors are same for each class?
common_support_vectors = np.intersect1d(linear_svm_model.support_vector_indices, gaussian_svm_model.support_vector_indices)
print(f"Number of common support vectors: {len(common_support_vectors)}")


# In[15]:


#Top 5 support vectors for gaussian kernel
alphas=[]
for i,alpha in enumerate(gaussian_svm_model.alphas):
    alphas.append((alpha,gaussian_svm_model.support_vectors[i],gaussian_svm_model.support_vector_labels[i]))
alphas.sort(key=lambda x: x[0],reverse=True)
top_5_alphas=alphas[:5]
top_5_support_vectors=np.array([x[1] for x in top_5_alphas])
top_5_support_vectors_labels=[x[2] for x in top_5_alphas]

fig, axes = plt.subplots(1, 5, figsize=(20, 20))
for i, ax in enumerate(axes):
    ax.imshow(top_5_support_vectors[i].reshape(100, 100, 3))
    ax.axis('off')
    ax.set_title(f"Class {(top_5_support_vectors_labels[i]+1)//2}")
    
fig.savefig("top5_support_vectors_gaussian.png")
plt.show()


# In[16]:


# Train Linear Kernel SVM
start_time = time.time()
linear_svm = SVC(kernel='linear', C=1.0)
linear_svm.fit(X_train, y_train)
linear_train_time = time.time() - start_time

# Train Gaussian Kernel SVM
start_time = time.time()
gaussian_svm = SVC(kernel='rbf', C=1.0, gamma=0.001)
gaussian_svm.fit(X_train, y_train)
gaussian_train_time = time.time() - start_time


# In[17]:


print(f"Number of support vectors (Linear, Scikit-Learn): {sum(linear_svm.n_support_)}")
print(f"Number of support vectors (Gaussian, Scikit-Learn): {sum(gaussian_svm.n_support_)}")
print(f"Training time (Linear, Scikit-Learn): {linear_train_time:.2f} seconds")
print(f"Training time (Gaussian, Scikit-Learn): {gaussian_train_time:.2f} seconds")
print(f"Number of common support vectors for linear: {np.intersect1d(linear_svm_model.support_vector_indices, linear_svm.support_).shape[0]}")
print(f"Number of common support vectors for gaussian: {np.intersect1d(gaussian_svm_model.support_vector_indices, gaussian_svm.support_).shape[0]}")

# how many support vectors are common for each class?
common_support_vectors = np.intersect1d(linear_svm_model.support_vector_indices, gaussian_svm.support_)
# Extract w and b for Linear Kernel
w_sklearn = linear_svm.coef_.flatten()
b_sklearn = linear_svm.intercept_[0]

print(f"Weight vector (w) from Scikit-Learn: {w_sklearn}")
print(f"Weight vector (w) from Implementation: {linear_svm_model.w}")
print(f"Bias term (b) from Scikit-Learn: {b_sklearn:.5f}")
print(f"Bias term (b) from Implementation: {linear_svm_model.b:.5f}")


# In[19]:


b_guassian = gaussian_svm.intercept_[0]
print(f"Bias term (b) from Scikit-Learn (Gaussian): {b_guassian:.5f}")


# In[18]:




# Predictions on test data
y_pred_linear = linear_svm.predict(X_test)
y_pred_gaussian = gaussian_svm.predict(X_test)

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match222-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_train_pred_linear = linear_svm.predict(X_train)
y_train_pred_gaussian = gaussian_svm.predict(X_train)

# Compute accuracy
linear_accuracy = np.mean(y_pred_linear == y_test)
gaussian_accuracy = np.mean(y_pred_gaussian == y_test)
</FONT>
print(f"Train Accuracy (Linear Kernel, Scikit-Learn): {np.mean(y_train_pred_linear == y_train)}")
print(f"Test Accuracy (Linear Kernel, Scikit-Learn): {linear_accuracy * 100:.2f}%")
print(f"Train Accuracy (Gaussian Kernel, Scikit-Learn): {np.mean(y_train_pred_gaussian == y_train)}")
print(f"Test Accuracy (Gaussian Kernel, Scikit-Learn): {gaussian_accuracy * 100:.2f}%")


# In[21]:




# Train SVM using SGD
start_time = time.time()
sgd_svm = SGDClassifier(loss='hinge', alpha=1.0 / len(X_train), max_iter=1000, tol=1e-3)
sgd_svm.fit(X_train, y_train)
sgd_train_time = time.time() - start_time
print(f"Training time (SGD, Scikit-Learn): {sgd_train_time:.2f} seconds")


# In[22]:


# Predictions on test data
<A NAME="5"></A><FONT color = #FF0000><A HREF="match222-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_pred_sgd = sgd_svm.predict(X_test)
y_train_pred_sgd = sgd_svm.predict(X_train)
# Compute accuracy
sgd_accuracy = np.mean(y_pred_sgd == y_test)
sgd_train_accuracy = np.mean(y_train_pred_sgd == y_train)
</FONT>
print(f"Train Accuracy (SGD SVM): {sgd_train_accuracy}")
print(f"Test Accuracy (SGD SVM): {sgd_accuracy * 100:.2f}%")


# In[4]:


# Let's build multiclass SVM using One vs One approach for 11 files.

# Load all the images

X0_train,y0_train = load_images("../data/Q2/train/dew/", 0)
X1_train,y1_train = load_images("../data/Q2/train/fogsmog/", 1)
X2_train,y2_train = load_images("../data/Q2/train/frost/", 2)
X3_train,y3_train = load_images("../data/Q2/train/glaze/", 3)
X4_train,y4_train = load_images("../data/Q2/train/hail/", 4)
X5_train,y5_train = load_images("../data/Q2/train/lightning/", 5)
X6_train,y6_train = load_images("../data/Q2/train/rain/", 6)
X7_train,y7_train = load_images("../data/Q2/train/rainbow/", 7)
X10_train,y10_train = load_images("../data/Q2/train/snow/", 10)
X8_train,y8_train = load_images("../data/Q2/train/rime/", 8)
X9_train,y9_train = load_images("../data/Q2/train/sandstorm/", 9)

Xtrain= [X0_train,X1_train,X2_train,X3_train,X4_train,X5_train,X6_train,X7_train,X8_train,X9_train,X10_train]
ytrain = [y0_train,y1_train,y2_train,y3_train,y4_train,y5_train,y6_train,y7_train,y8_train,y9_train,y10_train]

X0,y0 = load_images("../data/Q2/test/dew/", 0)
X1,y1 = load_images("../data/Q2/test/fogsmog/", 1)
X2,y2 = load_images("../data/Q2/test/frost/", 2)
X3,y3 = load_images("../data/Q2/test/glaze/", 3)
X4,y4 = load_images("../data/Q2/test/hail/", 4)
X5,y5 = load_images("../data/Q2/test/lightning/", 5)
X6,y6 = load_images("../data/Q2/test/rain/", 6)
X7,y7 = load_images("../data/Q2/test/rainbow/", 7)
X10,y10 = load_images("../data/Q2/test/snow/", 10)
X8,y8 = load_images("../data/Q2/test/rime/", 8)
X9,y9 = load_images("../data/Q2/test/sandstorm/", 9)
Xtest= [X0,X1,X2,X3,X4,X5,X6,X7,X8,X9,X10]
ytest = [y0,y1,y2,y3,y4,y5,y6,y7,y8,y9,y10]


# In[19]:


# Train 11 SVMs using One vs One approach
from itertools import combinations
from collections import Counter
from multiprocessing import Process

#using already developed SVM class
svm_models = []
for i in range(11):
    svm_under = []
    processes = []
    for j in range(i+1,11):
        print(i,j)
        yi = np.zeros(ytrain[i].shape[0])
        yj = np.ones(ytrain[j].shape[0])
        X_train = np.vstack((Xtrain[i], Xtrain[j]))
        y_train = np.hstack((yi, yj))
        svm = SupportVectorMachine()
        svm.fit(X_train, y_train, kernel='gaussian', C=1.0,gamma=0.001)
        print(svm.support_vectors.shape[0])
        svm_under.append(svm)
    svm_models.append(svm_under)


# In[32]:


X_test = np.vstack(Xtest)
y_test = np.hstack(ytest)
votes = []
for i in range(len(svm_models)):
    for j in range(len(svm_models[i])):
        svm = svm_models[i][j]
        print(i,j)
        predictions = svm.predict(X_test)  # Binary SVM prediction

        # Convert predictions back to original class labels
        predicted_classes = np.where(predictions == 0, i, i+j+1)
        votes.append(predicted_classes)

votes = np.array(votes).T


# In[34]:


print(y_test)
final_predictions = []
for i in range(votes.shape[0]):
    final_predictions.append(Counter(votes[i]).most_common(1)[0][0])

final_predictions = np.array(final_predictions)
accuracy = np.mean(final_predictions == y_test)
print(f"Test Accuracy (One vs One): {accuracy * 100:.2f}%")


# In[6]:


X_train = np.vstack(Xtrain)
y_train = np.hstack(ytrain)

start_time = time.time()
auto_ovo_svm = SVC(kernel='rbf', C=1.0, gamma=0.001, decision_function_shape='ovo')
auto_ovo_svm.fit(X_train, y_train)
auto_ovo_train_time = time.time() - start_time


# In[7]:


X_test = np.vstack(Xtest)
y_test = np.hstack(ytest)
# Predict on test data
y_pred_auto_ovo = auto_ovo_svm.predict(X_test)

# Compute accuracy
accuracy_auto_ovo = np.mean(y_pred_auto_ovo == y_test)

print(f"Test Accuracy (OvO Multi-Class SVM, Auto LIBSVM): {accuracy_auto_ovo * 100:.2f}%")


# In[8]:


# classification matrix
from sklearn.metrics import confusion_matrix
y_pred = []
y_true=[]
for i in range(y_pred_auto_ovo.shape[0]):
    y_pred.append(y_pred_auto_ovo[i])
    y_true.append(y_test[i])

cm=confusion_matrix(y_true, y_pred)

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

df_cm = pd.DataFrame(cm, index = [i for i in range(11)],
                    columns = [i for i in range(11)])

with open("confusion_matrix.txt", "w") as f:
    f.write(df_cm.to_string())

plt.figure(figsize = (10,7))
sns.heatmap(df_cm, annot=True)
plt.savefig("confusion_matrix.png")
plt.show()


# In[9]:


#find 10 examples of misclassification
misclassified_indices = np.where(y_pred_auto_ovo != y_test)[0]
misclassified_indices = misclassified_indices[:10]
misclassified_images = X_test[misclassified_indices]
misclassified_labels = y_test[misclassified_indices]
misclassified_predictions = y_pred_auto_ovo[misclassified_indices]

#save theses images in a file
fig, axes = plt.subplots(2, 5, figsize=(20, 10))
for i, ax in enumerate(axes.flatten()):
    ax.imshow(misclassified_images[i].reshape(100, 100, 3))
    ax.axis('off')
    ax.set_title(f"True: {misclassified_labels[i]}, Predicted: {misclassified_predictions[i]}")
fig.savefig("misclassified_images.png")
plt.show()


# In[ ]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.datasets import make_classification
from multiprocessing import Process
from sklearn.preprocessing import StandardScaler

X_train = np.vstack(Xtrain)
y_train = np.hstack(ytrain)

X_test = np.vstack(Xtest)
y_test = np.hstack(ytest)

# Step 4: Define hyperparameter values for C
C_values = [1e-5,1e-3,1,5,10]
cv_accuracies = []
test_accuracies = []

def K_fold(C,X_train,y_train,X_test,y_test):
    model = SVC(kernel='rbf', C=C, gamma=0.001, decision_function_shape='ovo', random_state=42)
    
    # Compute 5-fold cross-validation accuracy
    cv_accuracy = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))
    cv_accuracies.append(cv_accuracy)

    # Train model on full train set & evaluate on test set
    model.fit(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    test_accuracies.append(test_accuracy)

    print(f"C = {C}: CV Accuracy = {cv_accuracy:.4f}, Test Accuracy = {test_accuracy:.4f}")

    


# In[16]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.datasets import make_classification
from multiprocessing import Process
from sklearn.preprocessing import StandardScaler

# Step 6: Plot results
C_values = [1e-5,1e-3,1,5,10]
print(C_values)
cv_accuracies=[0.1693,0.1693,0.6569,0.6707,0.6687]
test_accuracies=[0.1686,0.1686,0.6628,0.6831,0.6868]
fig = plt.figure(figsize=(8, 5))
plt.plot(C_values, cv_accuracies, label="5-Fold CV Accuracy", marker='o', linestyle='--')
plt.plot(C_values, test_accuracies, label="Test Accuracy", marker='s', linestyle='-')

plt.xscale("log")  # Log scale for better visualization
plt.xlabel("C Value (log scale)")
plt.ylabel("Accuracy")
plt.title("5-Fold CV vs. Test Accuracy for Different C Values")
plt.legend()
plt.grid()
plt.show()
fig.savefig("cv_vs_test_accuracy.png")
plt.show()

# # Step 7: Train the best model on the entire training set
# best_C = C_values[np.argmax(cv_accuracies)]
# final_model = SVC(kernel='rbf', C=best_C, gamma=0.001, decision_function_shape='ovo', random_state=42)
# final_model.fit(X_train, y_train)

# # Step 8: Final model test accuracy
# final_test_accuracy = final_model.score(X_test, y_test)
# print(f"Final Model with C={best_C}: Test Accuracy = {final_test_accuracy:.4f}")



</PRE>
</PRE>
</BODY>
</HTML>
