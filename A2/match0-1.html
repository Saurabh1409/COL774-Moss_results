<HTML>
<HEAD>
<TITLE>./A2_processed_new_hash_2/combined_AAPWP.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A2_processed_new_hash_2/combined_XU0MN.py<p><PRE>


import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from sklearn.metrics import precision_score, recall_score, f1_score


nltk.download('punkt')
nltk.download('stopwords')

class NaiveBayesClassifier:
    def __init__(self):
        self.class_priors = {}
        self.word_likelihoods = {}
        self.vocab = set()
        self.title_likelihoods = {}
        self.desc_likelihoods = {}
        self.vocab_title = set()
        self.vocab_desc = set()
        
    def calculate_random_guess_accuracy(self,df, class_col="Class Index"):
        
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        class_probs = np.array([count / total_docs for count in class_counts.values()])
        random_accuracy = np.sum(class_probs**2) 
        return random_accuracy

    def fit(self, df, smoothening, class_col="Class Index", text_col="Tokenized Description"):
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}

        word_counts = {c: {} for c in class_counts.keys()}
        class_word_totals = {c: 0 for c in class_counts.keys()}

        for _, row in df.iterrows():
            c = row[class_col]
            words = row[text_col]
            
            for word in words:
                if word not in word_counts[c]:
                    word_counts[c][word] = 0
                word_counts[c][word] += 1
                class_word_totals[c] += 1
                self.vocab.add(word)

        vocab_size = len(self.vocab)
        self.word_likelihoods = {c: {} for c in class_counts.keys()}
        
        for c in class_counts:
            for word in self.vocab:
                count = word_counts[c].get(word, 0)
                self.word_likelihoods[c][word] = np.log((count + smoothening) / 
                                                        (class_word_totals[c] + smoothening * vocab_size))

    def fit_with_title_desc(self, df, smoothening, class_col="Class Index", title_col="Tokenized Title", desc_col="Tokenized Description"):
        
        # Compute class priors based on full dataset
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}

        self.title_likelihoods = {c: {} for c in class_counts.keys()}
        self.desc_likelihoods = {c: {} for c in class_counts.keys()}

        # Count word occurrences separately for title and description
        title_word_counts = {c: {} for c in class_counts.keys()}
        desc_word_counts = {c: {} for c in class_counts.keys()}
        class_word_totals_title = {c: 0 for c in class_counts.keys()}
        class_word_totals_desc = {c: 0 for c in class_counts.keys()}

        for _, row in df.iterrows():
            c = row[class_col]
            title_words = row[title_col]
            desc_words = row[desc_col]

            for word in title_words:
                if word not in title_word_counts[c]:
                    title_word_counts[c][word] = 0
                title_word_counts[c][word] += 1
                class_word_totals_title[c] += 1
                self.vocab_title.add(word)

            for word in desc_words:
                if word not in desc_word_counts[c]:
                    desc_word_counts[c][word] = 0
                desc_word_counts[c][word] += 1
                class_word_totals_desc[c] += 1
                self.vocab_desc.add(word)

        # Compute likelihoods with Laplace smoothing
        vocab_size_title = len(self.vocab_title)
        vocab_size_desc = len(self.vocab_desc)

        for c in class_counts:
            for word in self.vocab_title:
                count = title_word_counts[c].get(word, 0)
                self.title_likelihoods[c][word] = np.log((count + smoothening) /
                                                         (class_word_totals_title[c] + smoothening * vocab_size_title))

            for word in self.vocab_desc:
                count = desc_word_counts[c].get(word, 0)
                self.desc_likelihoods[c][word] = np.log((count + smoothening) /
                                                        (class_word_totals_desc[c] + smoothening * vocab_size_desc))

    def fit_with_tfidf(self, df, smoothening, class_col="Class Index", 
                        title_col="Tokenized Title", desc_col="Tokenized Description",
                        tfidf_title_col="TF-IDF Title", tfidf_desc_col="TF-IDF Description"):
        
        class_counts = df[class_col].value_counts().to_dict()
        total_docs = len(df)
        self.class_priors = {c: np.log(count / total_docs) for c, count in class_counts.items()}

        self.title_likelihoods = {c: {} for c in class_counts.keys()}
        self.desc_likelihoods = {c: {} for c in class_counts.keys()}
        self.tfidf_title = {c: {} for c in class_counts.keys()}
        self.tfidf_desc = {c: {} for c in class_counts.keys()}

        # Count occurrences for Naïve Bayes likelihoods
        title_word_counts = {c: {} for c in class_counts.keys()}
        desc_word_counts = {c: {} for c in class_counts.keys()}
        class_word_totals_title = {c: 0 for c in class_counts.keys()}
        class_word_totals_desc = {c: 0 for c in class_counts.keys()}

        for _, row in df.iterrows():
            c = row[class_col]
            title_words = row[title_col]
            desc_words = row[desc_col]
            title_tfidf = row[tfidf_title_col]
            desc_tfidf = row[tfidf_desc_col]

            # Store TF-IDF separately for weighting
            self.tfidf_title[c].update(title_tfidf)
            self.tfidf_desc[c].update(desc_tfidf)

            for word in title_words:
                if word not in title_word_counts[c]:
                    title_word_counts[c][word] = 0
                title_word_counts[c][word] += 1
                class_word_totals_title[c] += 1
                self.vocab_title.add(word)

            for word in desc_words:
                if word not in desc_word_counts[c]:
                    desc_word_counts[c][word] = 0
                desc_word_counts[c][word] += 1
                class_word_totals_desc[c] += 1
                self.vocab_desc.add(word)

        # Compute likelihoods with Laplace smoothing
        vocab_size_title = len(self.vocab_title)
        vocab_size_desc = len(self.vocab_desc)

        for c in class_counts:
            for word in self.vocab_title:
                count = title_word_counts[c].get(word, 0)
                self.title_likelihoods[c][word] = np.log((count + smoothening) / 
                                                        (class_word_totals_title[c] + smoothening * vocab_size_title))

            for word in self.vocab_desc:
                count = desc_word_counts[c].get(word, 0)
                self.desc_likelihoods[c][word] = np.log((count + smoothening) / 
                                                        (class_word_totals_desc[c] + smoothening * vocab_size_desc))
                
    def predict(self, df, text_col="Tokenized Description", predicted_col="Predicted"):
        predictions = []
        for _, row in df.iterrows():
            words = row[text_col]
            class_scores = {c: self.class_priors[c] for c in self.class_priors}
            for c in self.class_priors:
                for word in words:
                    if word in self.word_likelihoods[c]:
                        class_scores[c] += self.word_likelihoods[c][word]
            predictions.append(max(class_scores, key=class_scores.get))
        df[predicted_col] = predictions
        return df
    
    def predict_with_title_desc(self, df, title_col="Tokenized Title", desc_col="Tokenized Description", predicted_col="Predicted"):
        predictions = []
        
        for _, row in df.iterrows():
            title_words = row[title_col]
            desc_words = row[desc_col]

            class_scores = {c: self.class_priors[c] for c in self.class_priors}
            
            for c in self.class_priors:
                # Compute log probabilities from title words
                for word in title_words:
                    if word in self.title_likelihoods[c]:
                        class_scores[c] += self.title_likelihoods[c][word]

                # Compute log probabilities from description words
                for word in desc_words:
                    if word in self.desc_likelihoods[c]:
                        class_scores[c] += self.desc_likelihoods[c][word]

            predictions.append(max(class_scores, key=class_scores.get))

        df[predicted_col] = predictions
        return df
    def predict_with_tfidf(self, df, title_col="Tokenized Title", desc_col="Tokenized Description",
                            tfidf_title_col="TF-IDF Title", tfidf_desc_col="TF-IDF Description", predicted_col="Predicted"):
        predictions = []
        
        for _, row in df.iterrows():
            title_words = row[title_col]
            desc_words = row[desc_col]
            title_tfidf = row[tfidf_title_col]
            desc_tfidf = row[tfidf_desc_col]

            class_scores = {c: self.class_priors[c] for c in self.class_priors}

            for c in self.class_priors:
                for word in title_words:
                    if word in self.title_likelihoods[c]:
                        class_scores[c] += self.title_likelihoods[c][word]  

                    # Adjust using TF-IDF if word is present
                    if word in title_tfidf and word in self.tfidf_title[c]:
                        class_scores[c] += title_tfidf[word] * self.tfidf_title[c][word]  

                for word in desc_words:
                    if word in self.desc_likelihoods[c]:
                        class_scores[c] += self.desc_likelihoods[c][word]  

                    # Adjust using TF-IDF if word is present
                    if word in desc_tfidf and word in self.tfidf_desc[c]:
                        class_scores[c] += desc_tfidf[word] * self.tfidf_desc[c][word]  

            predictions.append(max(class_scores, key=class_scores.get))

        df[predicted_col] = predictions
        return df
    
    def calculate_accuracy(self, df, class_col="Class Index", predicted_col="Predicted"):
        correct_predictions = (df[class_col] == df[predicted_col]).sum()
        total_predictions = len(df)
        accuracy = correct_predictions / total_predictions
        return accuracy

    def generate_wordcloud(self, df, text_col="Tokenized Description"):
        all_words = " ".join([" ".join(words) for words in df[text_col]])
        stopwords = set(STOPWORDS)
        wordcloud = WordCloud(background_color="white", max_words=2000, stopwords=stopwords).generate(all_words)
        
        plt.figure(figsize=(24, 15))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.savefig("wordcloud.png")
        plt.show()

def tokenize_text(text):
    return word_tokenize(text.lower())

def remove_stopwords(df, text_col="Tokenized Description"):
    stop_words = set(stopwords.words('english'))
    df[text_col] = df[text_col].apply(lambda words: [word for word in words if word not in stop_words])
    return df

def add_bigrams(df, text_col="Tokenized Description"):
    def generate_bigrams(words):
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words) - 1)]
        return words + bigrams  

    df[text_col] = df[text_col].apply(generate_bigrams)
    return df

def calculate_precision(self, df, class_col="Class Index", predicted_col="Predicted"):
    true_labels = df[class_col]
    predicted_labels = df[predicted_col]
    precision = precision_score(true_labels, predicted_labels, average="weighted")  # Use "macro" or "micro" as needed
    return precision

def calculate_recall(self, df, class_col="Class Index", predicted_col="Predicted"):
    true_labels = df[class_col]
    predicted_labels = df[predicted_col]
    recall = recall_score(true_labels, predicted_labels, average="weighted")  # Use "macro" or "micro" as needed
    return recall

def calculate_f1_score(self, df, class_col="Class Index", predicted_col="Predicted"):
    true_labels = df[class_col]
    predicted_labels = df[predicted_col]
    f1 = f1_score(true_labels, predicted_labels, average="weighted")  # Use "macro" or "micro" if needed
    return f1

def compute_confusion_matrix(self, df, class_col="Class Index", predicted_col="Predicted", output_csv="confusion_matrix.csv"):
    true_labels = df[class_col]
    predicted_labels = df[predicted_col]
    class_labels = [1, 2, 3, 4]
    confusion_matrix = np.zeros((len(class_labels), len(class_labels)))
    
    for true_label, predicted_label in zip(true_labels, predicted_labels):
        confusion_matrix[true_label - 1][predicted_label - 1] += 1
    
    df_cm = pd.DataFrame(confusion_matrix , index= [f"Actual {i}" for i in class_labels], columns=[f"Predicted {i}" for i in class_labels])
    df_cm.to_csv(output_csv)
    print(df_cm)

def compute_tf_idf(df, title_col="Tokenized Title", desc_col="Tokenized Description", vocab_size=5000):
    
    # Flatten all words and count document frequency (DF)
    title_word_counts, desc_word_counts = {}, {}
    
    for doc in df[title_col]:
        for word in set(doc):  # Count unique occurrences per document
            title_word_counts[word] = title_word_counts.get(word, 0) + 1

    for doc in df[desc_col]:
        for word in set(doc):
            desc_word_counts[word] = desc_word_counts.get(word, 0) + 1

    # Select top vocab_size words based on frequency
    top_title_words = sorted(title_word_counts, key=title_word_counts.get, reverse=True)[:vocab_size]
    top_desc_words = sorted(desc_word_counts, key=desc_word_counts.get, reverse=True)[:vocab_size]

    title_vocab_set, desc_vocab_set = set(top_title_words), set(top_desc_words)

    # Compute IDF for top words
    N = len(df)
    title_idf = {word: np.log(N / (title_word_counts[word] + 1)) for word in title_vocab_set}
    desc_idf = {word: np.log(N / (desc_word_counts[word] + 1)) for word in desc_vocab_set}

    # Compute TF-IDF for each document
    tfidf_title_features, tfidf_desc_features = [], []

    for title_doc, desc_doc in zip(df[title_col], df[desc_col]):
        # Compute TF for words in the document
        title_tf = {word: title_doc.count(word) / len(title_doc) for word in title_doc if word in title_vocab_set}
        desc_tf = {word: desc_doc.count(word) / len(desc_doc) for word in desc_doc if word in desc_vocab_set}

        # Compute TF-IDF
        tfidf_title = {word: title_tf[word] * title_idf[word] for word in title_tf}
        tfidf_desc = {word: desc_tf[word] * desc_idf[word] for word in desc_tf}

        tfidf_title_features.append(tfidf_title)
        tfidf_desc_features.append(tfidf_desc)

    # Store TF-IDF as dictionary columns
    df["TF-IDF Title"] = tfidf_title_features
    df["TF-IDF Description"] = tfidf_desc_features
    
    return df




if __name__ == "__main__":
    train_file = "../data/Q1/train.csv"
    test_file = "../data/Q1/test.csv"

    # print("Training only on Discription column")
    # train_df = pd.read_csv(train_file)
    # train_df["Tokenized Description"] = train_df["Description"].apply(tokenize_text)

    # test_df = pd.read_csv(test_file)
    # test_df["Tokenized Description"] = test_df["Description"].apply(tokenize_text)
    
    
    # print("Training only on Title column")
    # train_df = pd.read_csv(train_file)
    # train_df["Tokenized Description"] = train_df["Title"].apply(tokenize_text)

    # test_df = pd.read_csv(test_file)
    # test_df["Tokenized Description"] = test_df["Title"].apply(tokenize_text)
    
# ---------------------------------------------------------------------------------------------------------------------
    # print("Training on both Title and Description columns(concatation)")
    # train_df = pd.read_csv(train_file)
    # train_df["Tokenized Description"] = train_df["Title"].str.cat(train_df["Description"], sep=" ", na_rep="").apply(tokenize_text)
    
    # test_df = pd.read_csv(test_file)
    # test_df["Tokenized Description"] = test_df["Title"].str.cat(test_df["Description"], sep=" ", na_rep="").apply(tokenize_text)
    
    # print("stemming")
    # train_df = remove_stopwords(train_df)
    # test_df = remove_stopwords(test_df)

    # print("added bigrams")
    # train_df = add_bigrams(train_df)
    # test_df = add_bigrams(test_df)

    # nb = NaiveBayesClassifier()
    # nb.fit(train_df, smoothening=1.0)
    
    # train_predictions = nb.predict(train_df)
    # test_predictions = nb.predict(test_df)
# ---------------------------------------------------------------------------------------------------------------------
    print("Training on both Title and Description columns(Learning different parameters)")
    train_df = pd.read_csv(train_file)
    train_df["Tokenized Description"] = train_df["Description"].apply(tokenize_text)
    train_df["Tokenized Title"] = train_df["Title"].apply(tokenize_text)
    
    test_df = pd.read_csv(test_file)
    test_df["Tokenized Description"] = test_df["Description"].apply(tokenize_text)
    test_df["Tokenized Title"] = test_df["Title"].apply(tokenize_text)

    print("stemming")
    train_df = remove_stopwords(train_df)
    test_df = remove_stopwords(test_df)

    print("added bigrams")
    train_df = add_bigrams(train_df)
    test_df = add_bigrams(test_df)
    
    print("TFIDF")
    train_df = compute_tf_idf(train_df, "Tokenized Title", "Tokenized Description", vocab_size= 50000)
    test_df = compute_tf_idf(test_df, "Tokenized Title", "Tokenized Description", vocab_size= 50000)
    
    print("Training")
    nb = NaiveBayesClassifier()
    nb.fit_with_tfidf(train_df, smoothening=1.0)
    
    train_predictions = nb.predict_with_tfidf(train_df)
    test_predictions = nb.predict_with_tfidf(test_df)
# ----------------------------------------------------------------------------------------------------------------
    
    train_accuracy = nb.calculate_accuracy(train_predictions)
    test_accuracy = nb.calculate_accuracy(test_predictions)
    f1_score = calculate_f1_score(nb, test_predictions)
    precision = calculate_precision(nb, test_predictions)
    recall = calculate_recall(nb, test_predictions)
    random_choose_probablity = nb.calculate_random_guess_accuracy(test_predictions)
    
    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print(f"F1 Score: {f1_score:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Random Guess Accuracy: {random_choose_probablity:.4f}")
    
    compute_confusion_matrix(nb, test_predictions)
    # nb.generate_wordcloud(test_df)





import re
import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
import cvxopt
import cvxopt.solvers
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from itertools import combinations
import time
import gc

def load_images(data_path, target_class, img_size = (100,100)):
    X , y = [], []
    for label,class_name in enumerate(target_class):
        class_folder = os.path.join(data_path, class_name)
        for filename in os.listdir(class_folder):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match0-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            img_path = os.path.join(class_folder, filename)
            img = cv2.imread(img_path)
            if img is None:
                print(f"Invalid image: {img_path}")
                continue
            img = cv2.resize(img, img_size)
</FONT>            img = img/255.0
            X.append(img.flatten())
            y.append(label)
            
    return np.array(X), np.array(y)


class SupportVectorMachine:
    '''
    Binary Classifier using Support Vector Machine
    '''
    def __init__(self):
        self.w = None
        self.b = None
        self.alpha = None
        self.sv_X = None
        self.sv_y = None
        self.kernel = None
        
    def _compute_kernel(self, X1, X2, kernel='linear', gamma=0.001):
        """ Computes kernel matrix based on the specified kernel type. """
        if kernel == 'linear':
            return np.dot(X1, X2.T)
        elif kernel == 'gaussian':
            sq_dists = np.sum(X1**2, axis=1).reshape(-1,1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
            return np.exp(-gamma * sq_dists)
        else:
            raise ValueError("Unsupported kernel. Use 'linear' or 'gaussian'.")

        
    def fit(self, X, y, kernel = 'linear', C = 1.0, gamma = 0.001):
        '''
        Learn the parameters from the given training data
        Classes are 0 or 1
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
            y: np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the ith sample
                
            kernel: str
                The kernel to be used. Can be 'linear' or 'gaussian'
                
            C: float
                The regularization parameter
                
            gamma: float
                The gamma parameter for gaussian kernel, ignored for linear kernel
        '''
        start_time = time.time()
        
        y = np.where(y == 0, -1, 1)
        N, D = X.shape
        
        K = self._compute_kernel(X, X, kernel=kernel, gamma=gamma)
        
        P = cvxopt.matrix(np.outer(y, y) * K, tc = 'd')
        q = cvxopt.matrix(-np.ones(N), tc = 'd')
        
        G = cvxopt.matrix(np.vstack([-np.eye(N), np.eye(N)]), tc = 'd')
        h = cvxopt.matrix(np.hstack([np.zeros(N), np.ones(N) * C]), tc = 'd')
        
        A = cvxopt.matrix(y.astype(float).reshape(1, -1), tc = 'd')
        b = cvxopt.matrix(0.0, tc = 'd')
        
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alpha = np.array(solution['x']).flatten()
        
        sv_indices = alpha &gt; 1e-5
        self.alpha = alpha[sv_indices]
        self.sv_X = X[sv_indices]
        self.sv_y = y[sv_indices]
        
        self.kernel = kernel
        
        self.w = np.sum(self.alpha[:, None] * self.sv_y[:, None] * self.sv_X, axis = 0)
        if kernel == 'linear':
            self.b = np.mean(self.sv_y - np.dot(self.sv_X, self.w))
        elif kernel == 'gaussian':
            self.b = np.mean(self.sv_y - np.sum(self.alpha * self.sv_y * K[sv_indices][:, sv_indices], axis = 1))
            
        training_time = time.time() - start_time
        print(f"Training Time CVXOPT - {kernel} kernel: {training_time:.2f} seconds")
        
                        
    def predict(self, X):
        '''
        Predict the class of the input data
        
        Args:
            X: np.array of shape (N, D) 
                where N is the number of samples and D is the flattened dimension of each image
                
        Returns:
            np.array of shape (N,)
                where N is the number of samples and y[i] is the class of the
                ith sample (0 or 1)
        '''
        decision_values = None
        if self.kernel == 'linear':
            decision_values = np.dot(X, self.w) + self.b
        elif self.kernel == 'gaussian':
            K_test = self._compute_kernel(X, self.sv_X, kernel=self.kernel, gamma=0.001)
            decision_values = np.sum(self.alpha * self.sv_y * K_test, axis = 1) + self.b
        return np.where(decision_values &gt; 0, 1, 0)
    
    
    
def fit_libsvm( X_train, y_train, kernel, C=1.0, gamma=0.001):

    y_train = np.where(y_train == 0, -1, 1)
    clf = None
    if kernel == 'gaussian':
        clf = SVC(kernel='rbf', C=C, gamma=gamma)
    elif kernel == 'linear':
        sk_kernel = 'linear'
        clf = SVC(kernel='linear', C=C)
    
    start_time = time.time()
    clf.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    print(f"Training Time LIBSVM - {kernel} kernel: {training_time:.2f} seconds")
    return clf

def fit_sgd_svm(X_train , y_train, C = 1.0, max_iter= 1000, tol = 1e-3):
    
    y_train = np.where(y_train == 0, -1, 1)
    sgd_svm = SGDClassifier(loss = 'hinge', alpha= 1.0/C , max_iter = max_iter, tol = tol)
    
    start_time = time.time()
    sgd_svm.fit(X_train, y_train)
    training_time = time.time() - start_time
    print(f"Training Time SGD SVM: {training_time:.2f} seconds")
    
    return sgd_svm

def fit_multiclass_svm_cdxopt(X_train, y_train, C=1.0, gamma=0.001):
    classifiers = {}
    unique_classes = np.unique(y_train)
    
    for class1, class2 in list(combinations(unique_classes, 2)):
        print(f"Fitting SVM for classes {class1} and {class2}")
        
        # Get indices for the two classes
        idx = np.where((y_train == class1) | (y_train == class2))[0]
        if len(idx) &gt; 5000:  # Arbitrary threshold, adjust based on your system
            # Extract data for these classes but don't keep full copies in memory
            X_subset = X_train[idx].copy()  # Force a copy to avoid reference issues
            y_subset = y_train[idx].copy()
            
            # Convert to binary problem
            y_binary = np.where(y_subset == class1, 0, 1)
            
            # Train binary SVM (original functionality)
            svm = SupportVectorMachine()
            svm.fit(X_subset, y_binary, kernel='gaussian', C=C, gamma=gamma)
            
            # Store the classifier
            classifiers[(class1, class2)] = svm
            
            # Explicitly delete temporary variables to free memory
            del X_subset, y_subset, y_binary
<A NAME="6"></A><FONT color = #00FF00><A HREF="match0-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        else:
            # Original code for smaller datasets
            X_subset = X_train[idx]
            y_subset = y_train[idx]
            y_binary = np.where(y_subset == class1, 0, 1)
            
            # Train binary SVM
            svm = SupportVectorMachine()
</FONT>            svm.fit(X_subset, y_binary, kernel='gaussian', C=C, gamma=gamma)
            
            # Store the classifier
            classifiers[(class1, class2)] = svm
        
        # Force garbage collection after each classifier
        gc.collect()
        
    return classifiers, unique_classes

from sklearn.svm import SVC
import time

def fit_multiclass_svm_libsvm(X_train, y_train, C=1.0, gamma=0.001):
    
    # Initialize SVM model with Gaussian kernel and one-vs-one multi-class strategy
    model = SVC(kernel='rbf', C=C, gamma=gamma, decision_function_shape='ovo')

    # Measure training time
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time

    return model, training_time

def predict_multiclass_svm_libsvm(model, X_test):
    return model.predict(X_test)

def predict_libsvm( model , X_test):
    y_pred = model.predict(X_test)
    return y_pred
    
def predict_sgd_svm( model , X_test):
    y_pred = model.predict(X_test)
    return np.where(y_pred == -1, 0, 1)

def predict_multiclass_svm_cdxopt(classifiers, X_test, unique_classes):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match0-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    n_samples = X_test.shape[0]
    n_classes = len(unique_classes)
    
    # Initialize vote matrix and score matrix
    votes = np.zeros((n_samples, n_classes), dtype=int)
    scores = np.zeros((n_samples, n_classes), dtype=float)
    
    # For each binary classifier
    for (i, j), clf in classifiers.items():
</FONT>        print(f"Predicting for classes {i} and {j}")
        # Get predictions
        predictions = clf.predict(X_test)
        
        # Calculate decision values (distance from hyperplane)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match0-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        decision_values = np.zeros(n_samples)
        
        # Use kernel trick to calculate decision values
        # y_binary = np.where(clf.y_train == 0, -1, 1)
        y_binary = np.where(clf.sv_y == 0, -1, 1)
        if clf.kernel == 'linear':
            decision_values = np.dot(X_test, clf.w) + clf.b
        else:  # gaussian kernel
            K = clf._compute_kernel(X_test, clf.sv_X, kernel='gaussian', gamma=0.001)
</FONT>            alpha_y = clf.alpha * y_binary
            sv_index = np.arange(len(clf.alpha))
<A NAME="1"></A><FONT color = #00FF00><A HREF="match0-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_3.gif" ALT="other" BORDER="0" ALIGN=left></A>

            decision_values = np.sum(alpha_y[sv_index].reshape(1, -1) * K[:, sv_index], axis=1) + clf.b
            # K = clf._compute_kernel_matrix(X_test, clf.X_train)
            # alpha_y = clf.alphas * y_binary
            # sv_indices = clf.support_vector_indices
            # decision_values = np.sum(alpha_y[sv_indices].reshape(1, -1) * K[:, sv_indices], axis=1) + clf.b
        
        # Update votes and scores based on predictions
        for sample_idx in range(n_samples):
            class_idx_i = np.where(unique_classes == i)[0][0]
            class_idx_j = np.where(unique_classes == j)[0][0]
            
            if predictions[sample_idx] == 0:  # Predicted class i
                votes[sample_idx, class_idx_i] += 1
                scores[sample_idx, class_idx_i] += abs(decision_values[sample_idx])
            else:  # Predicted class j
                votes[sample_idx, class_idx_j] += 1
                scores[sample_idx, class_idx_j] += abs(decision_values[sample_idx])
    
    # Get predictions handling ties
    y_pred = np.zeros(n_samples, dtype=int)
    for sample_idx in range(n_samples):
        # Find maximum vote count for this sample
        max_votes = np.max(votes[sample_idx])
        
        # Find classes that have the maximum votes (could be multiple in case of tie)
        max_vote_classes = np.where(votes[sample_idx] == max_votes)[0]
        
        if len(max_vote_classes) == 1:
            # No tie - use the class with max votes
            winner_idx = max_vote_classes[0]
        else:
            # Tie - choose the class with highest score among tied classes
            tied_scores = scores[sample_idx, max_vote_classes]
            winner_among_tied = np.argmax(tied_scores)
            winner_idx = max_vote_classes[winner_among_tied]
        
        y_pred[sample_idx] = unique_classes[winner_idx]
    
    return y_pred
</FONT>
def evaluate_multiclass_svm_libsvm(model, X_test, y_test):
    
    y_pred = predict_multiclass_svm_libsvm(model, X_test)
    accuracy = np.mean(y_pred == y_test)
    print(f"One-vs-One Multi-Class SVM (LIBSVM) Accuracy: {accuracy:.2f}")
    return accuracy


def evaluate_multiclass_svm_cdxopt(classifiers, X_test, y_test, unique_classes):
    y_pred = predict_multiclass_svm_cdxopt(classifiers, X_test, unique_classes)
    accuracy = np.mean(y_pred == y_test)
    print(f"Test Accuracy CDXOPT: {accuracy:.2f}")
    return accuracy
    
def compare_sgd_liblinear(sgd_model, liblinear_model, X_test, y_test):
    acc_sgd = np.mean(predict_sgd_svm(sgd_model, X_test) == y_test)
    acc_liblinear = np.mean(predict_libsvm(liblinear_model, X_test) == y_test)
    
    print(f"Test Accuracy SGD SVM: {acc_sgd:.2f}")
    print(f"Test Accuracy LIBLINEAR: {acc_liblinear:.2f}")
    
    
def get_support_vectors(svm,X_train):  
    total_samples = X_train.shape[0]
    num_sv = svm.sv_X.shape[0]
    sv_percentage = (num_sv / total_samples) * 100
    return num_sv, sv_percentage

def classify_test_data(svm, X_test, y_test, kernel='linear', gamma=0.001):
    y_pred = svm.predict( X_test)
    accuracy = np.mean(y_pred == y_test)
    print(f"Test Accuracy CVXOPT - {kernel} kernel: {accuracy:.2f}")
    return accuracy

def classify_test_data_libsvm(model, X_test, y_test, kernel='linear'):
    y_pred = predict_libsvm(model, X_test)
    accuracy = np.mean(y_pred == y_test)
    print(f"Test Accuracy LIBSVM - {kernel} : {accuracy:.2f}")
    return accuracy

def plot_top_support_vectors(svm, num_images = 5, img_size = (100, 100, 3)):
    top_indices = np.argsort(svm.alpha)[-num_images:]
    fig, axes = plt.subplots(1, num_images, figsize = (20, 7))
    
    for i, idx in enumerate(top_indices):
        img = svm.sv_X[idx].reshape(img_size)
        axes[i].imshow(img)
        axes[i].axis('off')
        axes[i].set_title(f"SV # {idx +1}")
        
    plt.title(f"Top {num_images} Support Vectors")
    plt.savefig(f"top_support_vectors_{svm.kernel}.png")
    # plt.show()
    
def plot_weight_vector(svm, img_size = (100, 100, 3)):
    w = svm.w.copy()
    w_normalized = (w - w.min()) / (w.max() - w.min())

    try:
        w_img = w_normalized.reshape(img_size)
    except ValueError:
        print(f"Weight vector size {w.shape[0]} doesn't match expected size {np.prod(img_size)}")
        return
    
    plt.figure(figsize=(7, 7))
    plt.imshow(w_img)
    plt.colorbar()  # Add colorbar to understand the scale
    plt.title("SVM Weight Vector")
    plt.savefig("weight_vector.png")
    # plt.show()
    
    if img_size[2] == 3:
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        channel_names = ['Red', 'Green', 'Blue']
        for i, (ax, name) in enumerate(zip(axes, channel_names)):
            ax.imshow(w_img[:,:,i], cmap='viridis')
            ax.set_title(f"{name} Channel")
            ax.axis('off')
        plt.tight_layout()
        plt.savefig("weight_vector_channels.png")
        # plt.show()
    
def compare_support_vectors(svm_linear, svm_gaussian):
    num_sv_linear = svm_linear.sv_X.shape[0]
    num_sv_gaussian = svm_gaussian.sv_X.shape[0]
    common_sv = np.intersect1d(svm_linear.sv_X, svm_gaussian.sv_X).shape[0]
    
    print(f"Number of Support Vectors (Linear): {num_sv_linear}")
    print(f"Number of Support Vectors (Gaussian): {num_sv_gaussian}")
    print(f"Number of Common Support Vectors: {common_sv}")
    
def compare_support_vectors_libsvm(svm_cvxopt_linear, svm_cvxopt_gaussian, libsvm_linear, libsvm_gaussian):
    
    num_sv_cvxopt_linear = svm_cvxopt_linear.sv_X.shape[0]
    num_sv_cvxopt_gaussian = svm_cvxopt_gaussian.sv_X.shape[0]
    num_sv_libsvm_linear = libsvm_linear.support_vectors_.shape[0]
    num_sv_libsvm_gaussian = libsvm_gaussian.support_vectors_.shape[0]
    
    sv_cvxopt_linear_set = set(map(tuple, svm_cvxopt_linear.sv_X))
    sv_cvxopt_gaussian_set = set(map(tuple, svm_cvxopt_gaussian.sv_X))
    sv_libsvm_linear_set = set(map(tuple, libsvm_linear.support_vectors_))
    sv_libsvm_gaussian_set = set(map(tuple, libsvm_gaussian.support_vectors_))

    common_l_cv_libsvm = len(sv_cvxopt_linear_set.intersection(sv_libsvm_linear_set))
    common_g_cv_libsvm = len(sv_cvxopt_gaussian_set.intersection(sv_libsvm_gaussian_set))
    # common_cv_l_g = len(sv_cvxopt_linear_set.intersection(sv_cvxopt_gaussian_set))
    common_cv_l_g = np.intersect1d(svm_cvxopt_linear.sv_X, svm_cvxopt_gaussian.sv_X).shape[0]
    common_libsvm_l_g = np.intersect1d(libsvm_linear.support_vectors_, libsvm_gaussian.support_vectors_).shape[0]
    # common_libsvm_l_g = len(sv_libsvm_linear_set.intersection(sv_libsvm_gaussian_set))


    print(f"Support Vectors (CVXOPT - Linear): {num_sv_cvxopt_linear}")
    print(f"Support Vectors (CVXOPT - Gaussian): {num_sv_cvxopt_gaussian}")
    print(f"Support Vectors (LIBSVM - Linear): {num_sv_libsvm_linear}")
    print(f"Support Vectors (LIBSVM - Gaussian): {num_sv_libsvm_gaussian}")
    
    print(f'Common SVs (CVXOPT - Linear, LIBSVM - Linear): {common_l_cv_libsvm}')
    print(f'Common SVs (CVXOPT - Gaussian, LIBSVM - Gaussian): {common_g_cv_libsvm}')
    print(f'Common SVs (CVXOPT - Linear, CVXOPT - Gaussian): {common_cv_l_g}')
    print(f'Common SVs (LIBSVM - Linear, LIBSVM - Gaussian): {common_libsvm_l_g}')

def compare_weights_bias(svm_cvxopt_linear, libsvm_linear):
    
    print(f"CVXOPT Linear SVM - Bias (b): {svm_cvxopt_linear.b:.4f}")
<A NAME="2"></A><FONT color = #0000FF><A HREF="match0-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_2.gif" ALT="other" BORDER="0" ALIGN=left></A>

    print(f"LIBSVM Linear SVM - Bias (b): {libsvm_linear.intercept_[0]:.4f}")

    # if hasattr(libsvm_linear, "coef_"):
    #     print(f"CVXOPT Linear SVM - Weight Vector Shape: {svm_cvxopt_linear.w.shape}")
    #     print(f"LIBSVM Linear SVM - Weight Vector Shape: {libsvm_linear.coef_.shape}")

def perform_hyperparameter_tuning(X_train, y_train, X_test, y_test):
    # Define the C values to test (10^-5, 10^-3, 1, 5, 10)
    C_values = [1e-5, 1e-3, 1, 5, 10]
    gamma = 0.001  # Fixed gamma value
    k_folds = 5    # 5-fold cross-validation
    
    # Lists to store results
    cv_scores = []  # Cross-validation accuracies
    test_scores = [] # Test set accuracies
    
    print("\n===== Question 8: Hyperparameter Tuning with 5-fold Cross-Validation =====")
    print(f"Fixed gamma = {gamma}, Testing C values: {C_values}")
    
    # (a) Compute cross-validation and test accuracies for each C value
    print("\n(a) Computing 5-fold CV and test accuracies for each C value:")
    
    for C in C_values:
        print(f"\nEvaluating SVM with C={C}, gamma={gamma}")
        
        # Create SVM model with Gaussian kernel
        svm = SVC(kernel='rbf', gamma=gamma, C=C)
        
        # Perform k-fold cross-validation
        start_time = time.time()
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        print("kfold done")
        cv_score = cross_val_score(svm, X_train, y_train, cv=kf, scoring='accuracy').mean()
        cv_time = time.time() - start_time
        cv_scores.append(cv_score)
        
        # Train on full training set and evaluate on test set
        start_time = time.time()
        print("fitting")

        svm.fit(X_train, y_train)
        train_time = time.time() - start_time
        
        start_time = time.time()
        print("predicting")

        test_pred = svm.predict(X_test)
        test_time = time.time() - start_time
        
        # test_accuracy = accuracy_score(y_test, test_pred)
        test_accuracy = np.mean(test_pred == y_test)
</FONT><A NAME="0"></A><FONT color = #FF0000><A HREF="match0-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_5.gif" ALT="other" BORDER="0" ALIGN=left></A>

        test_scores.append(test_accuracy)
        
        print(f"C={C}:")
        print(f"  5-fold CV Accuracy = {cv_score:.4f} (CV time: {cv_time:.2f}s)")
        print(f"  Test Accuracy = {test_accuracy:.4f} (Train time: {train_time:.2f}s, Test time: {test_time:.2f}s)")
    
    # (b) Plot CV and test accuracies
    print("\n(b) Plotting cross-validation and test accuracies vs C value:")
    
    plt.figure(figsize=(10, 6))
    plt.semilogx(C_values, cv_scores, 'o-', label='5-fold CV Accuracy')
    plt.semilogx(C_values, test_scores, 's-', label='Test Accuracy')
    plt.xlabel('C value (log scale)')
    plt.ylabel('Accuracy')
    plt.title('SVM Performance vs C Value (γ = 0.001)')
    plt.legend()
    plt.grid(True)
    
    # Add data points with values
    for i, C in enumerate(C_values):
        plt.annotate(f'{cv_scores[i]:.4f}', 
                     (C, cv_scores[i]), 
                     textcoords="offset points",
                     xytext=(0,10), 
                     ha='center')
        plt.annotate(f'{test_scores[i]:.4f}', 
                     (C, test_scores[i]), 
                     textcoords="offset points",
                     xytext=(0,-15), 
                     ha='center')
    
    plt.tight_layout()
    plt.savefig('hyperparameter_tuning_results.png')
    
    # Find best C value based on CV scores
    best_cv_idx = np.argmax(cv_scores)
    best_C = C_values[best_cv_idx]
    best_cv_score = cv_scores[best_cv_idx]
    
    print(f"\nBest C value from 5-fold CV: {best_C} with CV accuracy: {best_cv_score:.4f}")
    print(f"Test accuracy with C={best_C}: {test_scores[best_cv_idx]:.4f}")
    
    # (c) Train final model with best C value
    print("\n(c) Training final model with best C value:")
    
    # Train baseline model (C=1.0) for comparison
    baseline_model = SVC(kernel='rbf', gamma=gamma, C=1.0)
    baseline_model.fit(X_train, y_train)
    baseline_pred = baseline_model.predict(X_test)
    baseline_accuracy = accuracy_score(y_test, baseline_pred)
    
    # Train final model with best C
    final_model = SVC(kernel='rbf', gamma=gamma, C=best_C)
    start_time = time.time()
    final_model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    start_time = time.time()
    final_pred = final_model.predict(X_test)
    test_time = time.time() - start_time
    
    final_accuracy = accuracy_score(y_test, final_pred)
    improvement = final_accuracy - baseline_accuracy
    
    print(f"Baseline model (C=1.0) test accuracy: {baseline_accuracy:.4f}")
    print(f"Final model (C={best_C}) test accuracy: {final_accuracy:.4f}")
    print(f"Improvement: {improvement:.4f} ({improvement*100:.2f}%)")
    print(f"Training time: {train_time:.2f}s, Test time: {test_time:.2f}s")
    
    # Additional analysis - classification report
    print("\nDetailed classification report for optimized model:")
    report = classification_report(y_test, final_pred, target_names=classes)
    print(report)
    
    # Return results dictionary
    return {
        'C_values': C_values,
        'cv_scores': cv_scores,
        'test_scores': test_scores,
        'best_C': best_C,
        'best_cv_score': best_cv_score,
        'baseline_accuracy': baseline_accuracy,
        'final_test_accuracy': final_accuracy,
        'improvement': improvement
</FONT>    }

    
def main():
    dataset_path_train = "../data/Q2/train/"
    dataset_path_test = "../data/Q2/test/"
    # target_classes = [ "lightning", "rain"]   # for binary classification
    target_classes = [ "dew" , "fogsmog" , "frost", "glaze", "hail", "lightning", "rain", "rainbow", "rime", "sandstorm", "snow"]  # for multiclass classification

    X_train, y_train = load_images(dataset_path_train, target_classes)
    X_test, y_test = load_images(dataset_path_test, target_classes)
    
    
    svm_linear = SupportVectorMachine()
    svm_linear.fit(X_train, y_train, kernel='linear', C=1.0)
    
    

#----------------------------------------------------------------------------------------
    # Q1 (a Number of support vectors
    # num_sv, sv_percentage = get_support_vectors(svm_linear, X_train)
    # print(f"Number of Support Vectors: {num_sv}")
    # print(f"Percentage of Support Vectors: {sv_percentage:.2f}%")

    # # Q1 (b) Classify test data
    # classify_test_data(svm_linear, X_test, y_test)

    # print(f"Bias (b): {svm_linear.b:.4f}")
    # Q1 (c) Plot top-5 support vectors
    # plot_top_support_vectors(svm_linear)

    # Q1 (c) Plot weight vector
    # plot_weight_vector(svm_linear)
    
#----------------------------------------------------------------------------------------

    # # Q2
    # svm_gaussian = SupportVectorMachine()
    # svm_gaussian.fit(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
    
    # print("Class Distribution:", np.bincount(y_train))
    
    # #Q1 (a) Number of support vectors
    # compare_support_vectors(svm_linear, svm_gaussian)

    # print(f"Bias (b): {svm_gaussian.b:.4f}")
    # # Q2 (b) (d) accuracy
    # acc_linear = classify_test_data(svm_linear, X_test, y_test, kernel='linear')
    # acc_gaussian = classify_test_data(svm_gaussian, X_test, y_test, kernel='gaussian', gamma=0.001)
    
    # Q2 (c) Plot top-5 support vectors
    # plot_top_support_vectors(svm_gaussian)
#----------------------------------------------------------------------------------------
#     # Q3 (b) training time 
    # svm_linear_libsvm = fit_libsvm(X_train, y_train, kernel='linear', C=1.0)
    # svm_gaussian_libsvm = fit_libsvm(X_train, y_train, kernel='gaussian', C=1.0, gamma=0.001)
    
    # # Q3 (a) Number of support vectors
    # compare_support_vectors_libsvm(svm_linear, svm_gaussian, svm_linear_libsvm, svm_gaussian_libsvm)
    
    # # Q3 (c) Accuracy
    # acc_linear_libsvm = classify_test_data_libsvm(svm_linear_libsvm, X_test, y_test, kernel='linear')
    # acc_gaussian_libsvm = classify_test_data_libsvm(svm_gaussian_libsvm, X_test, y_test, kernel='gaussian')
    # print(f"Accuracy LIBSVM - Linear: {acc_linear_libsvm:.2f}")
    # print(f"Accuracy LIBSVM - Gaussian: {acc_gaussian_libsvm:.2f}")
    
    # # Q3 (b) compare weight and bias
    # compare_weights_bias(svm_linear, svm_linear_libsvm)
#----------------------------------------------------------------------------------------
#     # Q4
    # sgd_svm = fit_sgd_svm(X_train, y_train, C=1.0)
    # compare_sgd_liblinear(sgd_svm, svm_linear_libsvm, X_test, y_test)
#----------------------------------------------------------------------------------------
    # # Q5
    print("Multiclass Classification")
    classifiers, unique_classes = fit_multiclass_svm_cdxopt(X_train, y_train, C=1.0, gamma=0.001)
    evaluate_multiclass_svm_cdxopt(classifiers, X_test, y_test, unique_classes)
# ----------------------------------------------------------------------------------------
    # # Q6
    # X_train, y_train = load_images(dataset_path_train, target_classes)  # Load all 11 classes
    # X_test, y_test = load_images(dataset_path_test, target_classes)
    
    # # Train One-vs-One Multi-Class SVM (LIBSVM)
    # libsvm_model, libsvm_time = fit_multiclass_svm_libsvm(X_train, y_train, C=1.0, gamma=0.001)
    # libsvm_acc = evaluate_multiclass_svm_libsvm(libsvm_model, X_test, y_test)

    # print(f"Number of Support Vectors (LIBSVM): {libsvm_model.support_vectors_.shape[0]}")
    # print(f"Time taken to train LIBSVM: {libsvm_time:.2f}s")
# ----------------------------------------------------------------------------------------

    # #Q8
    # results = perform_hyperparameter_tuning(X_train, y_train, X_test, y_test)
    # print(f"Best C value :", results['best_C'])
    # print(f"Test accuracy for best C:{results['final_test_accuracy']:.4f}" )
    # print(f"Improvement over baseline: {results['improvement']:.4f}")
    
    # cv_test_correlation = np.corrcoef(results['cv_scores'], results['test_scores'])[0, 1]
    # print(f"4. Correlation between CV and test accuracies: {cv_test_correlation:.4f}")
    
# ----------------------------------------------------------------------------------------

if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
