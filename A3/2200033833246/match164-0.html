<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_G6YHS.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_G6YHS.py<p><PRE>


import math
from collections import Counter
import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

def H(Y):
    P = Counter(Y)
    tot = len(Y)
    for yy in P:
        P[yy] /= tot
    ans = 0.0
    for yy in P:
        if P[yy] &gt; 0:
            ans += -P[yy] * np.log2(P[yy])
    return ans 

def I(X, Y, is_num):
    n = len(X)
    mi = H(Y)
    if is_num:
        v = X.median()
        Xy = [X &lt;= v, X &gt; v]
        Yy = [None, None]
        for i in range(2):
            Yy[i] = Y[Xy[i]]
        for i in range(2):
            if len(Yy[i]) &gt; 0:
                mi -= (len(Yy[i]) / n) * H(Yy[i])     
        return mi
    cc = Counter(zip(X, Y))
    mi += H(X)
    for (x, y), f in cc.items():
        pxy = f / n
        if pxy &gt; 0:
            mi -= pxy * np.log2(pxy)
    return mi 

def one_hot_encode(train_df, valid_df, test_df, label_col):
    y_train = train_df[label_col]
    y_valid = valid_df[label_col]
    y_test  = test_df[label_col]

    X_train = train_df.drop(columns=[label_col])
    X_valid = valid_df.drop(columns=[label_col])
    X_test  = test_df.drop(columns=[label_col])

    comb = pd.concat([X_train, X_valid, X_test], keys=['train', 'valid', 'test'])
    comb_en = pd.get_dummies(comb)

    X_train_en = comb_en.xs('train')
    X_valid_en = comb_en.xs('valid')
    X_test_en  = comb_en.xs('test')

    return X_train_en, y_train, X_test_en, y_test, X_valid_en, y_valid

class DecisionTree:
    def __init__(self, max_depth, class_label, min_samples_split=1):
        self.prediction = None
        self.split_val = None
        self.split_attr = None
        self.children = {}
        self.left = None
        self.right = None
        self.max_depth = max_depth
        self.class_label = class_label
        self.min_samples_split = min_samples_split
        self.is_num = False
        self.is_cat = False
        self.is_pruned = False 

    def build(self, X, y, cat, num):
        Yc = Counter(y)
        mx = 0 
        for yy in Yc:
            if Yc[yy] &gt; mx:
                mx = Yc[yy]
                self.prediction = yy
        if self.max_depth == 0 or len(Yc) == 1:    
            return 

        best_mi = float('-inf')
        num_cnd = False

        for attr in cat:
            mi = I(X[attr], y, False)
            if mi &gt; best_mi:
                best_mi = mi 
                self.split_attr = attr
                num_cnd = False

        for attr in num:  
            mi = I(X[attr], y, True)
            if mi &gt; best_mi:
                best_mi = mi 
                self.split_attr = attr
                num_cnd = True    

        if num_cnd:
            v = X[self.split_attr]
            self.split_val = v.median()
            l_data = X[v &lt;= self.split_val]
            r_data = X[v &gt; self.split_val]
            y_l = y[v &lt;= self.split_val]
            y_r = y[v &gt; self.split_val]
            if len(l_data) &gt;= self.min_samples_split and len(r_data) &gt;= self.min_samples_split:
                self.is_num  = True 
                self.is_cat = False
                self.left = DecisionTree(self.max_depth - 1, self.class_label, self.min_samples_split)
                self.right = DecisionTree(self.max_depth - 1, self.class_label, self.min_samples_split)
                self.left.parent = self 
                self.right.parent = self 
                self.left.build(l_data, y_l, cat, num)
                self.right.build(r_data, y_r, cat, num)
            else:
                self.split_attr = None
                self.split_val = None
        else:
            cnt = 0 
            for x, g in X.groupby(self.split_attr):
                if len(g) &gt;= self.min_samples_split:
                    cnt += 1
            if cnt &gt; 1:
                for x, g in X.groupby(self.split_attr):
                    if len(g) &gt;= self.min_samples_split:
                        self.is_cat = True
                        self.is_num = False  
                        y_g = y[g.index]
                        self.children[x] = DecisionTree(self.max_depth - 1, self.class_label, self.min_samples_split)
                        self.children[x].build(g, y_g, cat, num)
            else:
                self.split_attr = None
                self.split_val = None

    def predict_one(self, row):
        if self.is_pruned:
            return self.prediction            
        if self.split_attr is None:
            # assert(self.is_num == False)
            # assert(self.is_cat == False)
            # assert(self.children == {})
            # assert(self.left is None)
            # assert(self.right is None)
            return self.prediction 
        val = row[self.split_attr]
        if self.is_num:
            # assert(self.left is not None)
            # assert(self.right is not None)
            if val &lt;= self.split_val:
                return self.left.predict_one(row)
            else:
                return self.right.predict_one(row)
        else:
            # assert(self.is_cat)
            if val in self.children:
                return self.children[val].predict_one(row)
            return self.prediction


    def predict_all(self, X):
        return X.apply(self.predict_one, axis=1)

    def eval(self, X, y):
        y_pred = self.predict_all(X)
        return accuracy_score(y, y_pred)

def dfs(root, X_valid_en, y_valid):
    print("traversal started..")
    stack = [root]
    mx = root.eval(X_valid_en, y_valid)
    ans = None
    L = []
    while stack:
        curr = stack.pop()
        if curr is None:
            continue
        L.append(curr)
        if curr.right:
            stack.append(curr.right)
        if curr.left:
            stack.append(curr.left)
    print(len(L))
    if len(L) &gt; 0:
        for i in range(len(L)):
            if (i % 100 == 0):
                print(f"We are at i = {i} out of {len(L)}")
            L[i].is_pruned = True 
            acc = L[i].eval(X_valid_en, y_valid)
            L[i].is_pruned = False 

            if (acc &gt; mx):
                mx = acc 
                ans = L[i]
    print("traversal done..")
    if ans is not None:
        ans.left = None
        ans.right = None
        return True
    return False    

def fn_a(train_df, valid_df, test_df, salary_label, depths):

    train__ = []
    test__ = []
    valid__ = []
    
    y_outp = None

    y_train = train_df[salary_label]
    X_train = train_df.drop(columns=[salary_label])
    
    y_test = test_df[salary_label]
    X_test = test_df.drop(columns=[salary_label])

    y_valid = valid_df[salary_label]
    X_valid = valid_df.drop(columns=[salary_label])

    cat = []
    num = []
    for col in X_train.columns:
        if train_df[col].dtype == 'object':
            cat.append(col)
        else:
            num.append(col)

    mx = -5

    for d in depths:
        print(f"We are at depth d = {d}")

        model = DecisionTree(max_depth=d, class_label=salary_label, min_samples_split=1)    
        model.build(X_train, y_train, cat, num)
        
        train_pred = model.predict_all(X_train)
        test_pred = model.predict_all(X_test)
        valid_pred = model.predict_all(X_valid)

        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        valid_acc = accuracy_score(y_valid, valid_pred)
        
        if test_acc &gt; mx:
            mx = test_acc 
            y_outp = test_pred

        train__.append(train_acc)
        test__.append(test_acc)
        valid__.append(valid_acc)

        print(f"Depth={d}: Train Acc={train_acc * 100:.4f}%, Test Acc={test_acc * 100:.4f}%, Valid Acc={valid_acc * 100:.4f}%")
    
    # plt.figure(figsize=(10, 6))
    # plt.plot(depths, train__, marker='o', label='Train Accuracy')
    # plt.plot(depths, test__, marker='s', label='Test Accuracy')
    # plt.plot(depths, valid__, marker='^', label='Validation Accuracy')
    # plt.title('Decision Tree Accuracy vs Depth')
    # plt.xlabel('Maximum Depth')
    # plt.ylabel('Accuracy')
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    return y_outp

def fn_b(train_df, valid_df, test_df, label_col, depths):

    train__ = []
    test__ = []
    valid__ = []
    
    X_train_en, y_train, X_test_en, y_test, X_valid_en, y_valid = one_hot_encode(train_df, valid_df, test_df, label_col)

    y_outp = None

    cat = []
    num = []
    for col in X_train_en.columns:
        if X_train_en[col].dtype == 'object':
            cat.append(col)
        else:
            num.append(col)

    assert(len(cat) == 0)

    mx = -5

    for d in depths:
        print(f"We are at depth d = {d}")

        model = DecisionTree(max_depth=d, class_label=label_col, min_samples_split=1)    
        model.build(X_train_en, y_train, cat, num)

        train_pred = model.predict_all(X_train_en)
        test_pred = model.predict_all(X_test_en)
        valid_pred = model.predict_all(X_valid_en)

        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        valid_acc = accuracy_score(y_valid, valid_pred)

        if test_acc &gt; mx:
            mx = test_acc
            y_outp = test_pred

        train__.append(train_acc)
        test__.append(test_acc)
        valid__.append(valid_acc)

        print(f"Depth={d}: Train Acc={train_acc * 100:.4f}%, Test Acc={test_acc * 100:.4f}%, Valid Acc={valid_acc * 100:.4f}%")

    # plt.figure(figsize=(10, 6))
    # plt.plot(depths, train__, marker='o', label='Train Accuracy')
    # plt.plot(depths, test__, marker='s', label='Test Accuracy')
    # plt.plot(depths, valid__, marker='^', label='Validation Accuracy')
    # plt.title('Decision Tree Accuracy vs Depth')
    # plt.xlabel('Maximum Depth')
    # plt.ylabel('Accuracy')
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    return y_outp

def fn_c(train_df, valid_df, test_df, label_col, depths):
    train__ = []
    test__ = []
    valid__ = []
    
    X_train_en, y_train, X_test_en, y_test, X_valid_en, y_valid = one_hot_encode(train_df, valid_df, test_df, label_col)

    y_outp = None

    cat = []
    num = []
    for col in X_train_en.columns:
        if X_train_en[col].dtype == 'object':
            cat.append(col)
        else:
            num.append(col)

    # assert(len(cat) == 0)

    mx = -5

    for d in depths:
        print(f"We are at depth d = {d}")

        model = DecisionTree(max_depth=d, class_label=label_col, min_samples_split=1)    
        model.build(X_train_en, y_train, cat, num)

        # print("model is built")

        while dfs(model, X_valid_en, y_valid) == True:
            print("a node got pruned ...")

        train_pred = model.predict_all(X_train_en)
        test_pred = model.predict_all(X_test_en)
        valid_pred = model.predict_all(X_valid_en)

        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        valid_acc = accuracy_score(y_valid, valid_pred)
        
        if test_acc &gt; mx:
            mx = test_acc 
            y_outp = test_pred 
        
        train__.append(train_acc)
        test__.append(test_acc)
        valid__.append(valid_acc)

        print(f"Depth={d}: Train Acc={train_acc * 100:.4f}%, Test Acc={test_acc * 100:.4f}%, Valid Acc={valid_acc * 100:.4f}%")

    # plt.figure(figsize=(10, 6))
    # plt.plot(depths, train__, marker='o', label='Train Accuracy')
    # plt.plot(depths, test__, marker='s', label='Test Accuracy')
    # plt.plot(depths, valid__, marker='^', label='Validation Accuracy')
    # plt.title('Decision Tree Accuracy vs Depth')
    # plt.xlabel('Maximum Depth')
    # plt.ylabel('Accuracy')
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    return y_outp

def fn_d(train_df, valid_df, test_df, depths, label_col):

    train__ = []
    test__ = []
    valid__ = []

    X_train_en, y_train, X_test_en, y_test, X_valid_en, y_valid = one_hot_encode(train_df, valid_df, test_df, label_col)

    y_outp = None

    cat = []
    num = []
    for col in X_train_en.columns:
        if X_train_en[col].dtype == 'object':
            cat.append(col)
        else:
            num.append(col)

    # assert(len(cat) == 0)

    for d in depths:
        print(f"We are at depth d = {d}")

<A NAME="1"></A><FONT color = #00FF00><A HREF="match164-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        model = DecisionTreeClassifier(criterion='entropy', max_depth=d)
        model.fit(X_train_en, y_train)

        train_acc = accuracy_score(y_train, model.predict(X_train_en))
        test_acc = accuracy_score(y_test, model.predict(X_test_en))
</FONT>        valid_acc = accuracy_score(y_valid, model.predict(X_valid_en))

        train__.append(train_acc)
        test__.append(test_acc)
        valid__.append(valid_acc)

        print(f"[max_depth={d}] Train: {train_acc:.4f}, Test: {test_acc:.4f}, Validation: {valid_acc:.4f}")

    # plt.figure(figsize=(10, 6))
    # plt.plot(depths, train__, marker='o', label='Train Accuracy')
    # plt.plot(depths, test__, marker='s', label='Test Accuracy')
    # plt.plot(depths, valid__, marker='^', label='Validation Accuracy')
    # plt.title('Accuracy vs max_depth (criterion=entropy)')
    # plt.xlabel('max_depth')
    # plt.ylabel('Accuracy')
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()

    best_val = depths[0]
    mx = valid__[0]
    for i in range(len(depths)):
        if(valid__[i] &gt; mx):
            mx = valid__[i]
            best_val = depths[i]

    print(f"Best max_depth based on validation accuracy: {best_val}")

    ccps = [0.001, 0.01, 0.1, 0.2]
    train__c = []
    test__c = []
    valid__c = []

    mx = -10

<A NAME="2"></A><FONT color = #0000FF><A HREF="match164-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for ccp in ccps:
        model = DecisionTreeClassifier(criterion='entropy', ccp_alpha=ccp)
        model.fit(X_train_en, y_train)

        train_acc = accuracy_score(y_train, model.predict(X_train_en))
</FONT>        test_acc = accuracy_score(y_test, model.predict(X_test_en))
        valid_acc = accuracy_score(y_valid, model.predict(X_valid_en))

        if (test_acc &gt; mx):
            mx = test_acc 
            y_outp = model.predict(X_test_en)

        train__c.append(train_acc)  
        test__c.append(test_acc)
        valid__c.append(valid_acc)

        print(f"[ccp_alpha={ccp}] Train: {train_acc:.4f}, Test: {test_acc:.4f}, Validation: {valid_acc:.4f}")
    
    # Plot accuracies vs ccp_alpha
    # plt.figure(figsize=(10, 6))
    # plt.plot(ccps, train__c, marker='o', label='Train Accuracy')
    # plt.plot(ccps, test__c, marker='s', label='Test Accuracy')
    # plt.plot(ccps, valid__c, marker='^', label='Validation Accuracy')
    # plt.title('Accuracy vs ccp_alpha (full tree, criterion=entropy)')
    # plt.xlabel('ccp_alpha')
    # plt.ylabel('Accuracy')
    # plt.xscale('log')
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()

    best_ccp = ccps[0]
    mx = valid__c[0]
    for i in range(len(ccps)):
        if (valid__c[i] &gt; mx):
           mx = valid__c[i]
           best_ccp = ccps[i] 
    
    print(f"Best ccp_alpha based on validation accuracy: {best_ccp}")
    
    return y_outp    
    


def fn_e(train_df, valid_df, test_df, label_col):

    X_train_en, y_train, X_test_en, y_test, X_valid_en, y_valid = one_hot_encode(train_df, valid_df, test_df, label_col)
    
    n_est = [50, 150, 250, 350]
    mx_f = [0.1, 0.3, 0.5, 0.7, 1.0]
    min_ss = [2, 4, 6, 8, 10]

    best_oob = 0.0
    best_model = None
    best_params = None

    for n in n_est:
        for mx in mx_f:
            for min_s in min_ss:
                model = RandomForestClassifier(
                    n_estimators=n,
                    max_features=mx,
                    min_samples_split=min_s,
                    criterion='entropy',
                    oob_score=True,
                    bootstrap=True,
                    random_state=42,
                    n_jobs=-1
                )

                model.fit(X_train_en, y_train)
                oob = model.oob_score_

                print(f"Params: n={n}, max_features={mx}, min_samples_split={min_s} | OOB Score: {oob:.4f}")

                if oob &gt; best_oob:
                    best_oob = oob
                    best_model = model
                    best_params = [n, mx, min_s]

    print(best_params)
    print(f"OOB Score: {best_oob:.4f}")
        
    # val_acc = accuracy_score(y_valid, best_model.predict(X_valid_en))
    # test_acc = accuracy_score(y_test, best_model.predict(X_test_en))
    # train_acc = accuracy_score(y_train, best_model.predict(X_train_en))

    # print(f"Training Accuracy: {train_acc:.4f}")
    # print(f"Validation Accuracy: {val_acc:.4f}")
    # print(f"Test Accuracy: {test_acc:.4f}")

    return best_model.predict(X_test_en)

def main():

    train_path , valid_path , test_path, out_dir, part = sys.argv[1 : ]
    part = part.lower()

    # part = sys.argv[1 : ]
    # part = part[0].lower()

    # train_path = 'data/train.csv' 
    # valid_path = 'data/valid.csv'
    # test_path = 'data/test.csv'
    
    train_df = pd.read_csv(train_path)
    train_df.columns = train_df.columns.str.strip()
    train_df = train_df.map(lambda x: x.strip() if isinstance(x, str) else x)

    valid_df = pd.read_csv(valid_path)
    valid_df.columns = train_df.columns.str.strip()
    valid_df = valid_df.map(lambda x: x.strip() if isinstance(x, str) else x)

    test_df = pd.read_csv(test_path)
    test_df.columns = test_df.columns.str.strip()
    test_df = test_df.map(lambda x: x.strip() if isinstance(x, str) else x)

    label_col = 'income'.strip()
    os.makedirs(out_dir, exist_ok=True)

    if part == 'a':
        print(f"running a ...")
        D = [5, 10, 15, 20]
        ya = fn_a(train_df, valid_df, test_df, label_col, D)
        ya = ya.to_frame(name="prediction")
        ya.to_csv(os.path.join(out_dir, "prediction_a.csv"), index=False)


    if part == 'b':
        print(f"running b ...")
        D = [25, 35, 45, 55]
        yb = fn_b(train_df, valid_df, test_df, label_col, D)
        yb = yb.to_frame(name="prediction")
        yb.to_csv(os.path.join(out_dir, "prediction_b.csv"), index=False)

    
    if part == 'c':
        print(f"running c ...")
        D = [25, 35, 45, 55]
        yc = fn_c(train_df, valid_df, test_df, label_col, D)
        yc = yc.to_frame(name="prediction")
        yc.to_csv(os.path.join(out_dir, "prediction_c.csv"), index=False)


    if part == 'd':
        print(f"running d ...")
        D = [25, 35, 45, 55]
        yd = fn_d(train_df, valid_df, test_df, D, label_col)
        
        yd = pd.DataFrame({'prediction': yd})
        
        yd.to_csv(os.path.join(out_dir, "prediction_d.csv"), index=False)

    if part == 'e':
        print(f"running e ...")
        ye = fn_e(train_df, valid_df, test_df, label_col)
        
        ye = pd.DataFrame({'prediction': ye})

        ye.to_csv(os.path.join(out_dir, "prediction_e.csv"), index=False)


if __name__ == "__main__":
    main()



import numpy as np
import os
import cv2
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import classification_report, accuracy_score, f1_score
import matplotlib.pyplot as plt
import math
from sklearn.neural_network import MLPClassifier
import sys
# import multiprocessing
# from multiprocessing import Pool, cpu_count
# np.seterr(over='raise', under='warn', divide='raise', invalid='raise')

def sigmoid(z):
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))    

def sigmoid_deriv(a):
    return a * (1 - a)

def relu(z):
    return np.maximum(0, z)

def relu_derivative(a):
    return (a &gt; 0).astype(float)

def softmax(z):
    z = z - np.max(z, axis=0, keepdims=True)
    exp_z = np.exp(z)
    sum_exp = np.sum(exp_z, axis=0, keepdims=True)
    return exp_z / sum_exp

class NeuralNetwork:
    def __init__(self, M, n, L_Arch, r, mod):
        self.L = len(L_Arch) + 1
        self.sz = [n] + L_Arch + [r]
        self.M = M
        self.mod = mod
        if mod == 'relu':
            self.afn = relu 
            self.ad = relu_derivative
            self.W = [np.random.randn(self.sz[i], self.sz[i - 1]) * np.sqrt(2 / self.sz[i - 1]) 
                      for i in range(1, self.L + 1)]
        else:
            assert(self.mod == 'sigmoid')
            self.afn = sigmoid
            self.ad = sigmoid_deriv
            self.W = [np.random.randn(self.sz[i], self.sz[i - 1]) * np.sqrt(1 / self.sz[i - 1]) 
                      for i in range(1, self.L + 1)]

        self.b = [np.zeros((self.sz[i], 1)) for i in range(1, self.L + 1)]

    def forward(self, x):
        a = [x]
        z = [None]
        for i in range(1, self.L + 1):
            z_i = self.W[i - 1] @ a[i - 1] + self.b[i - 1]
            z.append(z_i)
            if i == self.L:
                a_i = softmax(z_i)
            else:
                a_i = self.afn(z_i)
            a.append(a_i)
        return a, z

    def backward(self, a, z, y):
        m = y.shape[1]
        dW = [None] * self.L
        db = [None] * self.L
        dZ = [None] * (self.L + 1)
        dZ[self.L] = a[self.L] - y
        dW[self.L - 1] = (1 / m) * dZ[self.L] @ a[self.L - 1].T
        db[self.L - 1] = (1 / m) * np.sum(dZ[self.L], axis=1, keepdims=True)

        for l in range(self.L - 1, 0, -1):
            da = self.W[l].T @ dZ[l + 1]
            dZ[l] = da * self.ad(a[l])
            dW[l - 1] = (1 / m) * dZ[l] @ a[l - 1].T
            db[l - 1] = (1 / m) * np.sum(dZ[l], axis=1, keepdims=True)
        return dW, db
    
    def delta_(self, dW, db, lr):
        for i in range(self.L):
            self.W[i] -= lr * dW[i]
            self.b[i] -= lr * db[i]

    def loss_fn(self, y_pred, y_true):
            y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)
            m = y_true.shape[1]
            return -np.sum(y_true * np.log(y_pred)) / m

    def predict(self, x):
        a, _ = self.forward(x)
        return np.argmax(a[-1], axis=0)

    def train(self, X_train, y_train, epochs, lr, tol=1e-4, adaptive_lr=False):
        B = self.M
        m = X_train.shape[1]

        St = 30
        if adaptive_lr:
            St += 30
        # St = 0

        for _ in range(St):
            idx = np.random.permutation(m)
            Xr = X_train[:, idx]
            yr = y_train[:, idx]

            for i in range(0, m, B):
                j = min(i + B, m)
                xb = Xr[:, i:j]
                yb = yr[:, i:j]

                a, z = self.forward(xb)
                dw, db = self.backward(a, z, yb)
                self.delta_(dw, db, lr)

        for ep in range(epochs):
            eta = lr / np.sqrt(1 + ep) if adaptive_lr else lr
            idx = np.random.permutation(m)
            Xr = X_train[:, idx]
            yr = y_train[:, idx]

            total_loss = 0
            count = 0

            for i in range(0, m, B):
                j = min(i + B, m)
                xb = Xr[:, i:j]
                yb = yr[:, i:j]

                a, z = self.forward(xb)
                dw, db = self.backward(a, z, yb)
                self.delta_(dw, db, eta)

                total_loss += self.loss_fn(a[-1], yb)
                count += 1

            avg_loss = total_loss / count

            if avg_loss &lt; tol:
                print(f"Converged at Epoch {ep}/{epochs} with Loss: {avg_loss:.4f}")
                return

            if ep % 10 == 0:
                print(f"Epoch {ep}/{epochs} - Avg Loss: {avg_loss:.4f}")


# ====================================================== ALL PARTS HERE =============================================================================== #

def fn_b(X_train, y_train, X_test, y_test):
    print(f"Running part (b)...")

    M = 32
    n = 2352
    r = 43  
    L = [1, 5, 10, 50, 100]
    lr = 0.01
    epochs = 500

    assert y_train.shape[0] == r
    assert X_train.shape[0] == n

    test_mac = []
    train_mac = []

    y_train_labels = np.argmax(y_train, axis=0)
    y_test_labels = np.argmax(y_test, axis=0) 

    best_f1 = -1
    best_pred = None

    for sz in L:
        print(f"\nTraining with hidden layer size = {sz}")
        
        model = NeuralNetwork(M, n, [sz], r, mod='sigmoid')
        model.train(X_train, y_train, epochs, lr)
        
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # print("training....")
        # print(classification_report(y_train_labels, y_pred_train, digits=3))
        trainf1 = f1_score(y_train_labels, y_pred_train, average='macro')
        train_mac.append(trainf1)

        # print("testing....")
        # print(classification_report(y_test_labels, y_pred_test, digits=3))
        testf1 = f1_score(y_test_labels, y_pred_test, average='macro')
        test_mac.append(testf1)

        if testf1 &gt; best_f1:
            best_f1 = testf1
            best_pred = y_pred_test
    
    # plt.figure(figsize=(10, 6)) 
    # plt.plot(L, train_mac, label='Train F1', marker='o', color='green')
    # plt.plot(L, test_mac, label='Test F1', marker='o', color='blue')
    # plt.title("Average F1 Score vs Hidden Layer Size")
    # plt.xlabel("Number of Hidden Units in Hidden Layer")
    # plt.ylabel("Average F1 Score")
    # plt.xticks(L)
    # plt.grid(True)
    # plt.legend()
    # plt.show()

    return best_pred

def fn_c(X_train, y_train, X_test, y_test):
    print(f"Running part (c)...")

    M = 32   
    n = 2352 
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match164-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    r = 43 
    L = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]] 
    lr = 0.01
</FONT>    epochs = 1000    
    
    assert y_train.shape[0] == r
    assert X_train.shape[0] == n

    test_mac = []
    train_mac = []

    y_train_labels = np.argmax(y_train, axis=0)
    y_test_labels = np.argmax(y_test, axis=0)

    best_f1 = -1
    best_pred = None

    for sz in L:
        print(f"\nTraining with hidden layer size = {sz}")

        model = NeuralNetwork(M, n, sz, r, mod='sigmoid')
        model.train(X_train, y_train, epochs, lr)

        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # print("training...")
        # print(classification_report(y_train_labels, y_pred_train, digits=3))
        trainf1 = f1_score(y_train_labels, y_pred_train, average='macro')
        train_mac.append(trainf1)

        # print("testing...")
        # print(classification_report(y_test_labels, y_pred_test, digits=3))
        testf1 = f1_score(y_test_labels, y_pred_test, average='macro')
        test_mac.append(testf1)

        if testf1 &gt; best_f1:
            best_f1 = testf1
            best_pred = y_pred_test


    # labels = [str(sz) for sz in L]  
    
    # plt.figure(figsize=(10, 6))
    # plt.plot(labels, train_mac, label='Train F1', marker='o', color='green')
    # plt.plot(labels, test_mac, label='Test F1', marker='o', color='blue')
    # plt.title("Average F1 Score vs Hidden Layer Size")
    # plt.xlabel("Hidden Layer Architecture")
    # plt.ylabel("Average F1 Score")
    # plt.xticks(labels)
    # plt.grid(True)
    # plt.legend()
    # plt.show()

    return best_pred


def fn_d(X_train, y_train, X_test, y_test):
    print(f"Running part (d)...")

    M = 32
    n = 2352
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match164-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    r = 43
    L = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    lr = 0.01
</FONT>    epochs = 1000
    
    assert y_train.shape[0] == r
    assert X_train.shape[0] == n

    test_mac = []
    train_mac = []

    y_train_labels = np.argmax(y_train, axis=0)
    y_test_labels = np.argmax(y_test, axis=0)

    best_f1 = -1
    best_pred = None

    for sz in L:
        print(f"\nTraining with hidden layer size = {sz}")
        
        model = NeuralNetwork(M, n, sz, r, mod='sigmoid')
        model.train(X_train, y_train, epochs, lr, adaptive_lr = True)

        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # print("training...")
        # print(classification_report(y_train_labels, y_pred_train, digits=3))
        trainf1 = f1_score(y_train_labels, y_pred_train, average='macro')
        train_mac.append(trainf1)

        # print("testing...")
        # print(classification_report(y_test_labels, y_pred_test, digits=3))
        testf1 = f1_score(y_test_labels, y_pred_test, average='macro')
        test_mac.append(testf1)

        if testf1 &gt; best_f1:
            best_f1 = testf1
            best_pred = y_pred_test

    # labels = [str(sz) for sz in L]

    # plt.figure(figsize=(10, 6))
    # plt.plot(labels, avg_f1_scores_train, label='Train Avg F1', marker='o', color='green')
    # plt.plot(labels, avg_f1_scores_test, label='Test Avg F1', marker='o', color='blue')
    # plt.title("Average F1 Score vs Hidden Layer Sizes (Part d)")
    # plt.xlabel("Hidden Layer Configuration")
    # plt.ylabel("Average F1 Score")
    # plt.grid(True)
    # plt.legend()
    # plt.show()

    return best_pred


def fn_e(X_train, y_train, X_test, y_test):
    print(f"Running part (e)...")

    M = 32
    n = 2352
<A NAME="5"></A><FONT color = #FF0000><A HREF="match164-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    r = 43
    L = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    lr = 0.01
</FONT>    epochs = 1000
        
    assert y_train.shape[0] == r
    assert X_train.shape[0] == n

    test_mac = []
    train_mac = []

    y_train_labels = np.argmax(y_train, axis=0)
    y_test_labels = np.argmax(y_test, axis=0)
    
    best_f1 = -1
    best_pred = None

    for sz in L:
        print(f"\nTraining with hidden layer size = {sz}")
        
        model = NeuralNetwork(M, n, sz, r, mod='relu')
        model.train(X_train, y_train, epochs, lr, adaptive_lr = True)

        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # print("training...")
        # print(classification_report(y_train_labels, y_pred_train, digits=3))
        trainf1 = f1_score(y_train_labels, y_pred_train, average='macro')
        train_mac.append(trainf1)

        # print("testing...")
        # print(classification_report(y_test_labels, y_pred_test, digits=3))
        testf1 = f1_score(y_test_labels, y_pred_test, average='macro')
        test_mac.append(testf1)
        
        if testf1 &gt; best_f1:
            best_f1 = testf1
            best_pred = y_pred_test
    
    # labels = [str(sz) for sz in L]

    # plt.figure(figsize=(10, 6))
    # plt.plot(labels, avg_f1_scores_train, label='Train F1', marker='o', color='green')
    # plt.plot(labels, avg_f1_scores_test, label='Test F1', marker='o', color='blue')
    # plt.title("Average F1 Score vs Hidden Layer Sizes (Part e)")
    # plt.xlabel("Hidden Layer Configuration")
    # plt.ylabel("Average F1 Score")
    # plt.grid(True)
    # plt.legend()
    # plt.show()

    return best_pred


def fn_f(X_train, y_train, X_test, y_test):
    print(f"Running part (f)...")

    M = 32
    n = 2352
    r = 43
    L = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    lr = 0.01
    epochs = 1000

    assert X_train.shape[0] == n
    assert y_train.shape[0] == r

    train_mac = []
    test_mac = []

    y_train_labels = np.argmax(y_train, axis=0)
    y_test_labels = np.argmax(y_test, axis=0)

    X_train_T = X_train.T
    X_test_T = X_test.T

    best_f1 = -1
    best_pred = None

    for sz in L:
        print(f"\nTraining with hidden layer size = {sz}")          

        model = MLPClassifier(
            hidden_layer_sizes=tuple(sz),
            activation='relu',
            solver='sgd',
            alpha=0.0,
            batch_size=M,
            learning_rate='invscaling',
            learning_rate_init=lr,
            max_iter=epochs,
            early_stopping=True,
            n_iter_no_change=10,
            validation_fraction=0.1
        )
        model.fit(X_train_T, y_train_labels)

        y_pred_train = model.predict(X_train_T)
        y_pred_test = model.predict(X_test_T)
        
        # print("training...")
        # print(classification_report(y_train_labels, y_pred_train, digits=3))
        trainf1 = f1_score(y_train_labels, y_pred_train, average='macro')
        train_mac.append(trainf1)
        
        # print("testing...")
        # print(classification_report(y_test_labels, y_pred_test, digits=3))
        testf1 = f1_score(y_test_labels, y_pred_test, average='macro')
        test_mac.append(testf1)

        if testf1 &gt; best_f1:
            best_f1 = testf1
            best_pred = y_pred_test
    
    # labels = [str(sz) for sz in L]

    # plt.figure(figsize=(10, 6))
    # plt.plot(labels, train_mac, label='Train F1', marker='o', color='green')
    # plt.plot(labels, test_mac, label='Test F1', marker='o', color='blue')
    # plt.title("Average F1 Score vs Hidden Layer Sizes (Part f)")
    # plt.xlabel("Hidden Layer Configuration")
    # plt.ylabel("Average F1 Score")
    # plt.grid(True)
    # plt.legend()
    # plt.show()

    return best_pred

# ====================================================== MAIN RUNNER IS BELOW =============================================================================== #
def load_train_data(train_dir):
    images = []
    labels = []
    for label in range(43):
        folder = os.path.join(train_dir, f"{label:05d}")
        if not os.path.exists(folder):
            continue
        for file in os.listdir(folder):
            if file.endswith(".jpg") or file.endswith(".png"):
                img_path = os.path.join(folder, file)
                img = cv2.imread(img_path) #(28, 28, 3)
                if img is None:
                    continue

                img = img.reshape(-1)
                img = img.astype('float') / 255.0

                images.append(img)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match164-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                labels.append(label)
    return np.array(images), np.array(labels)

def load_test_data(test_dir, label_file):
    df = pd.read_csv(label_file)
    images = []
    labels = []
    for _, row in df.iterrows():
        img_name = row['Filename'] if 'Filename' in df.columns else row['image']
        label = row['ClassId'] if 'ClassId' in df.columns else row['label']
        img_path = os.path.join(test_dir, img_name)
</FONT>        if not os.path.exists(img_path):
            continue
        img = cv2.imread(img_path)
        if img is None:
            continue
 
        img = img.reshape(-1)
        img = img.astype('float') / 255.0

        images.append(img)
        labels.append(label)
    return np.array(images), np.array(labels)

def conv(y_train, y_test):
    nc = 43
    y_train_onehot = np.zeros((y_train.shape[0], nc))
    y_test_onehot = np.zeros((y_test.shape[0], nc))
    for i in range(y_train.shape[0]):
        y_train_onehot[i, y_train[i]] = 1
    for i in range(y_test.shape[0]):
        y_test_onehot[i, y_test[i]] = 1
    return y_train_onehot.T, y_test_onehot.T
    
def main():
    # train_path = 'data/train'
    # test_path = 'data/test'
    # label_path = 'data/test_labels.csv'
    
    # part = sys.argv[1 : ]
    # part = part[0].lower()

    train_path, test_path, out_dir, part = sys.argv[1 : ]
    part = part.lower()
    
    X_train, y_train = load_train_data(train_path)
    X_test, y_test = load_train_data(test_path)
    # X_test, y_test = load_test_data(test_path, label_path)

    X_train = X_train.T    
    X_test = X_test.T

    y_train , y_test = conv(y_train, y_test)
    
    if part == 'b':
        yy = fn_b(X_train, y_train, X_test, y_test)
        save_path = os.path.join(out_dir, f"prediction_b.csv")
    elif part == 'c':
        yy = fn_c(X_train, y_train, X_test, y_test)
        save_path = os.path.join(out_dir, f"prediction_c.csv")
    elif part == 'd':
        yy = fn_d(X_train, y_train, X_test, y_test)
        save_path = os.path.join(out_dir, f"prediction_d.csv")
    elif part == 'e':
        yy = fn_e(X_train, y_train, X_test, y_test)
        save_path = os.path.join(out_dir, f"prediction_e.csv")
    elif part == 'f':
        yy = fn_f(X_train, y_train, X_test, y_test)
        save_path = os.path.join(out_dir, f"prediction_f.csv")

    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    df = pd.DataFrame({'prediction': yy})
    df.to_csv(save_path, index=False)


if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
