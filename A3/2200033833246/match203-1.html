<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_0A0UU.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_M4DGO.py<p><PRE>


import sys
from Decision_Trees.dt_a import evaluate_a
from Decision_Trees.dt_b import evaluate_b
from Decision_Trees.dt_c import evaluate_c
from Decision_Trees.dt_d import evaluate_d
from Decision_Trees.dt_e import evaluate_e

def main():
<A NAME="0"></A><FONT color = #FF0000><A HREF="match203-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    if len(sys.argv) != 6:
        print("Usage: python decision_tree.py &lt;train_data_path&gt; &lt;valid_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        print("question_part should be one of: 'a', 'b', 'c', 'd', 'e'")
        sys.exit(1)

    train_data_path = sys.argv[1]
    valid_data_path = sys.argv[2]
    test_data_path = sys.argv[3]
</FONT>    output_folder_path = sys.argv[4]
    question_part = sys.argv[5].lower()

    valid_parts = ['a', 'b', 'c', 'd', 'e']
    if question_part not in valid_parts:
        print("Error: question_part must be one of 'a', 'b', 'c', 'd', 'e'")
        sys.exit(1)

    if question_part == 'a':
        evaluate_a(train_data_path, test_data_path, output_folder_path)
    elif question_part == 'b':
        evaluate_b(train_data_path, test_data_path, output_folder_path)
    elif question_part == 'c':
        evaluate_c(train_data_path, valid_data_path, test_data_path, output_folder_path)
    elif question_part == 'd':
        evaluate_d(train_data_path, valid_data_path, test_data_path, output_folder_path)
    elif question_part == 'e':
        evaluate_e(train_data_path, valid_data_path, test_data_path, output_folder_path)
    else:
        print("Error: Invalid question_part specified.")
        sys.exit(1)

if __name__ == "__main__":
    main()



import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'Neural_Networks'))
from Neural_Networks.nn_b import evaluate_neural_network_b
from Neural_Networks.nn_c import evaluate_neural_network_c
from Neural_Networks.nn_d import evaluate_neural_network_d
from Neural_Networks.nn_e import evaluate_neural_network_e
from Neural_Networks.nn_f import evaluate_neural_network_f

def main():
<A NAME="1"></A><FONT color = #00FF00><A HREF="match203-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    if len(sys.argv) != 5:
        print("Usage: python neural_network.py &lt;train_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        print("question_part should be one of: 'b', 'c', 'd', 'e', 'f'")
        sys.exit(1)

    train_data_path = sys.argv[1]
    test_data_path = sys.argv[2]
    output_folder_path = sys.argv[3]
</FONT>    question_part = sys.argv[4].lower()

    valid_parts = ['b', 'c', 'd', 'e', 'f']
    if question_part not in valid_parts:
        print("Error: question_part must be one of 'b', 'c', 'd', 'e', 'f'")
        sys.exit(1)

    if question_part == 'b':
        evaluate_neural_network_b(train_data_path, test_data_path, output_folder_path)
    elif question_part == 'c':
        evaluate_neural_network_c(train_data_path, test_data_path, output_folder_path)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match203-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    elif question_part == 'd':
        evaluate_neural_network_d(train_data_path, test_data_path, output_folder_path)
    elif question_part == 'e':
        evaluate_neural_network_e(train_data_path, test_data_path, output_folder_path)
    elif question_part == 'f':
        evaluate_neural_network_f(train_data_path, test_data_path, output_folder_path)
    else:
        print("Error: Invalid question_part specified.")
</FONT>        sys.exit(1)

if __name__ == "__main__":
    main()



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

continuous_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']
categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']

def entropy(y):
    proportions = np.bincount(y) / len(y)
    return -np.sum(proportions * np.log2(proportions + 1e-10))

def mutual_info_continuous(X, y, attribute):
    median = X[attribute].median()
    left_mask = X[attribute] &lt;= median
    right_mask = X[attribute] &gt; median
    left_y, right_y = y[left_mask], y[right_mask]
    if len(left_y) == 0 or len(right_y) == 0:
        return 0
    H_y = entropy(y)
    H_y_given_a = (len(left_y) / len(y)) * entropy(left_y) + (len(right_y) / len(y)) * entropy(right_y)
    return H_y - H_y_given_a

def mutual_info_categorical(X, y, attribute):
    H_y = entropy(y)
    unique_values = X[attribute].unique()
    H_y_given_a = 0
    for value in unique_values:
        mask = X[attribute] == value
        subset_y = y[mask]
        if len(subset_y) &gt; 0:
            H_y_given_a += (len(subset_y) / len(y)) * entropy(subset_y)
    return H_y - H_y_given_a

class Node:
    def __init__(self, depth=0):
        self.depth = depth
        self.children = {}
        self.is_leaf = False
        self.prediction = None
        self.attribute = None
        self.threshold = None

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.root = None
    
    def fit(self, X, y):
        self.root = self._build_tree(X, y, 0)
    
    def _build_tree(self, X, y, depth):
        node = Node(depth)
        unique_classes, counts = np.unique(y, return_counts=True)
        node.prediction = unique_classes[np.argmax(counts)]
        
        if len(unique_classes) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth) or len(y) &lt;= 1:
            node.is_leaf = True
            return node
        
        best_gain = -1
        best_attr = None
        best_threshold = None
        
        for attr in X.columns:
            if attr in continuous_cols:
                gain = mutual_info_continuous(X, y, attr)
                if gain &gt; best_gain:
                    best_gain = gain
                    best_attr = attr
                    best_threshold = X[attr].median()
            elif attr in categorical_cols:
                gain = mutual_info_categorical(X, y, attr)
                if gain &gt; best_gain:
                    best_gain = gain
                    best_attr = attr
        
        if best_gain &lt;= 0:
            node.is_leaf = True
            return node
        
        node.attribute = best_attr
        
        if best_attr in continuous_cols:
            node.threshold = best_threshold
            left_mask = X[best_attr] &lt;= best_threshold
            right_mask = X[best_attr] &gt; best_threshold
            if left_mask.sum() &gt; 0:
                node.children['left'] = self._build_tree(X[left_mask], y[left_mask], depth + 1)
            if right_mask.sum() &gt; 0:
                node.children['right'] = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        else:
            for value in X[best_attr].unique():
                mask = X[best_attr] == value
                if mask.sum() &gt; 0:
                    node.children[value] = self._build_tree(X[mask], y[mask], depth + 1)
        
        return node
    
    def predict(self, X):
        return np.array([self._predict_single(row) for _, row in X.iterrows()])
    
    def _predict_single(self, row):
        node = self.root
        while not node.is_leaf:
            attr = node.attribute
            if attr in continuous_cols:
                if row[attr] &lt;= node.threshold:
                    next_node = node.children.get('left', node)
                    if next_node == node:
                        break
                    node = next_node
                else:
                    next_node = node.children.get('right', node)
                    if next_node == node:
                        break
                    node = next_node
            else:
                value = row[attr]
                next_node = node.children.get(value, node)
                if next_node == node:
                    break
                node = next_node
        return node.prediction

def evaluate_a(train_file, test_file, output_folder):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    
    X_train = train_df.drop(columns=['income'])
    y_train = train_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1})
    
    if 'income' in test_df.columns:
        X_test = test_df.drop(columns=['income'])
    else:
        X_test = test_df

    depth = 20
    
    dt = DecisionTree(max_depth=depth)
    dt.fit(X_train, y_train)
    
    y_pred = dt.predict(X_test)
    
    y_pred_labels = ['&lt;=50K' if pred == 0 else '&gt;50K' for pred in y_pred]
    
    output_df = pd.DataFrame({
        'prediction': y_pred_labels
    })
    
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    output_path = os.path.join(output_folder, 'prediction_a.csv')
    output_df.to_csv(output_path, index=False)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder


categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']
continuous_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

def entropy(y):
    proportions = np.bincount(y) / len(y)
    return -np.sum(proportions * np.log2(proportions + 1e-10))

def mutual_info(X, y, attribute):
    median = X[attribute].median()
    left_mask = X[attribute] &lt;= median
    right_mask = X[attribute] &gt; median
    left_y, right_y = y[left_mask], y[right_mask]
    if len(left_y) == 0 or len(right_y) == 0:
        return 0
    H_y = entropy(y)
    H_y_given_a = (len(left_y) / len(y)) * entropy(left_y) + (len(right_y) / len(y)) * entropy(right_y)
    return H_y - H_y_given_a

def preprocess_all(X_train, X_test):
    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
    encoder.fit(X_train[categorical_cols])
    feature_names = encoder.get_feature_names_out(categorical_cols)
    
    X_train_encoded = pd.DataFrame(encoder.transform(X_train[categorical_cols]), 
                                  columns=feature_names, index=X_train.index)
    X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_cols]), 
                                 columns=feature_names, index=X_test.index)
    
    X_train_final = pd.concat([X_train[continuous_cols], X_train_encoded], axis=1)
    X_test_final = pd.concat([X_test[continuous_cols], X_test_encoded], axis=1)
    
    return X_train_final, X_test_final

class Node:
    def __init__(self, depth=0):
        self.depth = depth
        self.children = {}
        self.is_leaf = False
        self.prediction = None
        self.attribute = None
        self.threshold = None

class DecisionTrees:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.root = None

    def fit(self, X, y):
        self.root = self._build_tree(X, y, 0)
    
    def _build_tree(self, X, y, depth):
        node = Node(depth)

        unique_classes, counts = np.unique(y, return_counts=True)
        node.prediction = unique_classes[np.argmax(counts)]

        if len(unique_classes) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth) or len(y) &lt;= 1:
            node.is_leaf = True
            return node
        
        best_gain = -1
        best_attr = None
        best_threshold = None

        for attr in X.columns:
            gain = mutual_info(X, y, attr)

            if gain &gt; best_gain:
                best_gain = gain
                best_attr = attr
                best_threshold = X[attr].median()
        
        if best_gain &lt;= 0:
            node.is_leaf = True
            return node
        
        node.attribute = best_attr
        node.threshold = best_threshold
        left_mask = X[best_attr] &lt;= best_threshold
        right_mask = X[best_attr] &gt; best_threshold
        if left_mask.sum() &gt; 0:
            node.children['left'] = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        if right_mask.sum() &gt; 0:
            node.children['right'] = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        return node
    
    def predict(self, X):
        return np.array([self._predict_single(x) for _,x in X.iterrows()])
    
    def _predict_single(self, row):
        node = self.root
        while not node.is_leaf:
            if row[node.attribute] &lt;= node.threshold:
                node = node.children['left']
            else:
                node = node.children['right']
        return node.prediction
    
def evaluate_b(train_file, test_file, output_folder):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    
    X_train = train_df.drop(columns=['income'])
    y_train = train_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1})
    
    if 'income' in test_df.columns:
        X_test = test_df.drop(columns=['income'])
    else:
        X_test = test_df
    
    X_train_final, X_test_final = preprocess_all(X_train, X_test)

    depth = 55
    
    dt = DecisionTrees(max_depth=depth)
    dt.fit(X_train_final, y_train)
    
    y_pred = dt.predict(X_test_final)
    
    y_pred_labels = ['&lt;=50K' if pred == 0 else '&gt;50K' for pred in y_pred]
    
    output_df = pd.DataFrame({
        'prediction': y_pred_labels
    })
    
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    output_path = os.path.join(output_folder, 'prediction_b.csv')
    output_df.to_csv(output_path, index=False)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder

categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']
continuous_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

def entropy(y):
    proportions = np.bincount(y) / len(y)
    return -np.sum(proportions * np.log2(proportions + 1e-10))

def mutual_info(X, y, attribute):
    median = X[attribute].median()
    left_mask = X[attribute] &lt;= median
    right_mask = X[attribute] &gt; median
    left_y, right_y = y[left_mask], y[right_mask]
    if len(left_y) == 0 or len(right_y) == 0:
        return 0
    H_y = entropy(y)
    H_y_given_a = (len(left_y) / len(y)) * entropy(left_y) + (len(right_y) / len(y)) * entropy(right_y)
    return H_y - H_y_given_a

def preprocess_all(X_train, X_test, X_valid):
    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
    encoder.fit(X_train[categorical_cols])
    feature_names = encoder.get_feature_names_out(categorical_cols)
    
    X_train_encoded = pd.DataFrame(encoder.transform(X_train[categorical_cols]), columns=feature_names, index=X_train.index)
    X_valid_encoded = pd.DataFrame(encoder.transform(X_valid[categorical_cols]), columns=feature_names, index=X_valid.index)
    X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_cols]), columns=feature_names, index=X_test.index)
    
    X_train_final = pd.concat([X_train[continuous_cols], X_train_encoded], axis=1)
    X_valid_final = pd.concat([X_valid[continuous_cols], X_valid_encoded], axis=1)
    X_test_final = pd.concat([X_test[continuous_cols], X_test_encoded], axis=1)
    
    return X_train_final, X_valid_final, X_test_final

class Node:
    def __init__(self, depth=0):
        self.depth = depth
        self.children = {}
        self.is_leaf = False
        self.prediction = None
        self.attribute = None
        self.threshold = None

class DecisionTrees:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.root = None
        self.non_leaf_nodes = []

    def fit(self, X, y):
        self.root = self._build_tree(X, y, 0)
    
    def _build_tree(self, X, y, depth):
        node = Node(depth)
        unique_classes, counts = np.unique(y, return_counts=True)
        node.prediction = unique_classes[np.argmax(counts)]

        if len(unique_classes) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth) or len(y) &lt;= 1:
            node.is_leaf = True
            return node
        
        best_gain = -1
        best_attr = None
        best_threshold = None

        for attr in X.columns:
            gain = mutual_info(X, y, attr)
            if gain &gt; best_gain:
                best_gain = gain
                best_attr = attr
                best_threshold = X[attr].median()
        
        if best_gain &lt;= 0:
            node.is_leaf = True
            return node
        
        self.non_leaf_nodes.append(node)
        node.attribute = best_attr
        node.threshold = best_threshold
        left_mask = X[best_attr] &lt;= best_threshold
        right_mask = X[best_attr] &gt; best_threshold
        if left_mask.sum() &gt; 0:
            node.children['left'] = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        if right_mask.sum() &gt; 0:
            node.children['right'] = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        return node
    
    def predict(self, X):
        return np.array([self._predict_single(x) for _, x in X.iterrows()])
    
    def _predict_single(self, row):
        node = self.root
        while not node.is_leaf:
            if row[node.attribute] &lt;= node.threshold:
                node = node.children.get('left', node)  # Default to current node if child missing
            else:
                node = node.children.get('right', node)
        return node.prediction

    def count_nodes(self):
        def _count(node):
            if node.is_leaf:
                return 1
            return 1 + sum(_count(child) for child in node.children.values())
        return _count(self.root)

    def prune_tree(self, X_valid, y_valid, accuracy, depth):
        for node in self.non_leaf_nodes:
            if node.is_leaf or node.depth &lt; depth:
                continue
            node.is_leaf = True
            preds = self.predict(X_valid)
            new_accuracy = np.mean(preds == y_valid)
            if new_accuracy &lt; accuracy:
                node.is_leaf = False
            else:
                self.prune_node(node)

    def prune_node(self, node):
        if node and not node.is_leaf:
            for n in node.children.values():
                self.prune_node(n)
            node.is_leaf = True
            self.non_leaf_nodes.remove(node)

def evaluate_c(train_file, validation_file, test_file, output_folder):
    
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match203-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    train_df = pd.read_csv(train_file)
    valid_df = pd.read_csv(validation_file)
    test_df = pd.read_csv(test_file)
    
    X_train = train_df.drop(columns=['income'])
</FONT>    y_train = train_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1})
    if 'income' in test_df.columns:
        X_test = test_df.drop(columns=['income'])
    else:
        X_test = test_df
    X_valid = valid_df.drop(columns=['income'])
    y_valid = valid_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1})

    X_train_final, X_valid_final, X_test_final = preprocess_all(X_train, X_test, X_valid)

    depth = 55

    clf = DecisionTrees(max_depth=depth)
    clf.fit(X_train_final, y_train)

    valid_accs = []

    valid_preds = clf.predict(X_valid_final)
    valid_accs.append(np.mean(valid_preds == y_valid))
    prune_depth = clf.max_depth -5

    while prune_depth &gt; 4:
        print(f"Pruning depth: {prune_depth}")
        clf.prune_tree(X_valid_final, y_valid, valid_accs[-1], prune_depth)
        prune_depth -= 5
        train_preds = clf.predict(X_train_final)
        valid_preds = clf.predict(X_valid_final)
        valid_accs.append(np.mean(valid_preds == y_valid))
    
    y_pred = clf.predict(X_test_final)
    y_pred_labels = ['&lt;=50K' if pred == 0 else '&gt;50K' for pred in y_pred]
    
    output_df = pd.DataFrame({
        'prediction': y_pred_labels
    })
    
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    output_path = os.path.join(output_folder, 'prediction_c.csv')
    output_df.to_csv(output_path, index=False)



import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.tree import DecisionTreeClassifier

categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']
continuous_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

def preprocess_all(X_train, X_test, X_valid):
    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
    encoder.fit(X_train[categorical_cols])
    feature_names = encoder.get_feature_names_out(categorical_cols)
    
    X_train_encoded = pd.DataFrame(encoder.transform(X_train[categorical_cols]), 
                                  columns=feature_names, index=X_train.index)
    X_valid_encoded = pd.DataFrame(encoder.transform(X_valid[categorical_cols]), 
                                  columns=feature_names, index=X_valid.index)
    X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_cols]), 
                                 columns=feature_names, index=X_test.index)
    
    X_train_final = pd.concat([X_train[continuous_cols], X_train_encoded], axis=1)
    X_valid_final = pd.concat([X_valid[continuous_cols], X_valid_encoded], axis=1)
    X_test_final = pd.concat([X_test[continuous_cols], X_test_encoded], axis=1)
    
    return X_train_final, X_valid_final, X_test_final

def evaluate_d(train_file, valid_file, test_file, output_folder):

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match203-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    valid_df = pd.read_csv(valid_file)


    X_train = train_df.drop(columns=['income'])
</FONT>    y_train = train_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1}).values
    if 'income' in test_df.columns:
        X_test = test_df.drop(columns=['income'])
    else:
        X_test = test_df
    X_valid = valid_df.drop(columns=['income'])
    y_valid = valid_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1}).values
    
    X_train_final, X_valid_final, X_test_final = preprocess_all(X_train, X_test, X_valid)

    depths = [25, 35, 45, 55]
    valid_accuracies = []

    for depth in depths:
        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth)
        clf.fit(X_train_final, y_train)
        valid_accuracy = clf.score(X_valid_final, y_valid)
        
    depth = depths[np.argmax(valid_accuracy)]
    ccp_alphas = [0.001, 0.01, 0.1, 0.2]
    
    valid_accuracies = []

    for ccp_alpha in ccp_alphas:
        clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=ccp_alpha)
        clf.fit(X_train_final, y_train)
        valid_accuracies.append(clf.score(X_valid_final, y_valid))
        
    ccp_alpha = ccp_alphas[np.argmax(valid_accuracies)]
    clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth, ccp_alpha=ccp_alpha)
    clf.fit(X_train_final, y_train)
    
    y_pred_test = clf.predict(X_test_final)
    
    y_pred_labels = ['&lt;=50K' if pred == 0 else '&gt;50K' for pred in y_pred_test]
    
    output_df = pd.DataFrame({
        'prediction': y_pred_labels
    })
    
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    output_path = os.path.join(output_folder, 'prediction_d.csv')
    output_df.to_csv(output_path, index=False)




import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier

categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']
continuous_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

def preprocess_all(X_train, X_test, X_valid):
    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
    encoder.fit(X_train[categorical_cols])
    feature_names = encoder.get_feature_names_out(categorical_cols)
    
    X_train_encoded = pd.DataFrame(encoder.transform(X_train[categorical_cols]), 
                                  columns=feature_names, index=X_train.index)
    X_valid_encoded = pd.DataFrame(encoder.transform(X_valid[categorical_cols]), 
                                  columns=feature_names, index=X_valid.index)
    X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_cols]), 
                                 columns=feature_names, index=X_test.index)
    
    X_train_final = pd.concat([X_train[continuous_cols], X_train_encoded], axis=1)
    X_valid_final = pd.concat([X_valid[continuous_cols], X_valid_encoded], axis=1)
    X_test_final = pd.concat([X_test[continuous_cols], X_test_encoded], axis=1)
    
    return X_train_final, X_valid_final, X_test_final


def evaluate_e(train_file, valid_file, test_file, output_folder):
    
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    valid_df = pd.read_csv(valid_file)

    X_train = train_df.drop(columns=['income'])
    y_train = train_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1}).values
    if 'income' in test_df.columns:
        X_test = test_df.drop(columns=['income'])
    else:
        X_test = test_df
    X_valid = valid_df.drop(columns=['income'])
    y_valid = valid_df['income'].str.strip().map({'&lt;=50K': 0, '&gt;50K': 1}).values

    X_train_final, X_valid_final, X_test_final = preprocess_all(X_train, X_test, X_valid)
    
    X_train_final = np.concatenate((X_train_final, X_valid_final))
    y_train = np.concatenate((y_train, y_valid))

    n_estimators = [i for i in range(50, 351, 100)]
    max_features = [i*0.2 for i in range(1, 6, 1)]
    min_samples_split = [i for i in range(2, 11, 2)]

    max_score = -1
    n_estimator_optimal = -1
    max_feature_optimal = -1
    min_sample_optimal = -1

    for n_estimator in n_estimators:
        for max_feature in max_features:
            for min_sample in min_samples_split:
                print(n_estimator, max_feature, min_sample)
                rf = RandomForestClassifier(n_estimators=n_estimator, max_features=max_feature, min_samples_split=min_sample, bootstrap=True, oob_score=True, random_state=42)
                rf.fit(X_train_final, y_train)
                score = rf.oob_score_
                if score &gt; max_score:
                    max_score = score
                    n_estimator_optimal = n_estimator
                    max_feature_optimal = max_feature
                    min_sample_optimal = min_sample
    
    final_rf = RandomForestClassifier(n_estimators=n_estimator_optimal, max_features=max_feature_optimal, min_samples_split=min_sample_optimal, bootstrap=True, oob_score=True, random_state=42)
    final_rf.fit(X_train_final, y_train)
    y_pred_test = final_rf.predict(X_test_final)
    y_pred_labels = ['&lt;=50K' if pred == 0 else '&gt;50K' for pred in y_pred_test]
    
    output_df = pd.DataFrame({
        'prediction': y_pred_labels
    })
    
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    output_path = os.path.join(output_folder, 'prediction_e.csv')
    output_df.to_csv(output_path, index=False)



import numpy as np

class NeuralNetwork:
    
    def __init__(self, batch_size, n_features, hidden_sizes, n_classes, learning_rate = 0.01):
        self.n_features = n_features
        self.hidden_sizes = hidden_sizes
        self.n_classes = n_classes
        self.batch_size = batch_size
        self.learning_rate = learning_rate

        self.activations = []
        self.z_values = []
        
        self.weights = []
        self.biases = []
        
        self.weights.append(np.random.randn(n_features, hidden_sizes[0]) * 0.01)
        self.biases.append(np.zeros(hidden_sizes[0]))
        
        for i in range(len(hidden_sizes) - 1):
            self.weights.append(np.random.randn(hidden_sizes[i], hidden_sizes[i+1]) * 0.01)
            self.biases.append(np.zeros(hidden_sizes[i+1]))
        self.weights.append(np.random.randn(hidden_sizes[-1], n_classes) * 0.01)
        self.biases.append(np.zeros(n_classes))
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        self.activations = [X]
        self.z_values = []
        
        for i in range(len(self.hidden_sizes)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            a = self.sigmoid(z)
            self.activations.append(a)
        
        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]
        self.z_values.append(z)
        output = self.softmax(z)
        self.activations.append(output)
        
        return output
    
    def backward(self, X, y, output):
        m = X.shape[0]
        deltas = []
        
        error = output.copy()
        error[range(m), y] -= 1  # ∂J/∂net = (ok - 1) if k = true label, ok otherwise
        deltas.insert(0, error)
        
        for i in range(len(self.hidden_sizes) - 1, -1, -1):
            delta_next = deltas[0]
            a = self.activations[i + 1]
            w = self.weights[i + 1]
            z = self.z_values[i]
            
            error = np.dot(delta_next, w.T) * self.sigmoid_derivative(z)
            deltas.insert(0, error)
            
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, deltas[i]) / m
            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0) / m
    
    def train(self, X, y, epochs, verbose=True, stopping_criteria=0.001):
        n_samples = X.shape[0]
        loss_history = []
    
        for epoch in range(epochs):
            indices = np.random.permutation(n_samples)
            for i in range(0, n_samples, self.batch_size):
                batch_idx = indices[i:min(i + self.batch_size, n_samples)]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
            
                output = self.forward(X_batch)
                self.backward(X_batch, y_batch, output)
        
            if verbose and epoch % 10 == 0:
                loss = self.compute_loss(X, y)
                loss_history.append(loss)
            
                if len(loss_history) &gt;= 10:
                    last_10_avg = np.mean(loss_history[-10:])
                    if last_10_avg &lt; loss:
                        print(f"Epoch {epoch}, Loss: {loss:.4f}")
                        print("Average of last 10 losses ({:.4f}) is less than current loss ({:.4f}), stopping training.".format(last_10_avg, loss))
                        return
                    
                if len(loss_history) &gt; 100:
                    loss_history = loss_history[-100:]
                
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
                if loss &lt; stopping_criteria:
                    print("Stopping criteria met.")
                    return
    
    def compute_loss(self, X, y):
        output = self.forward(X)
        return -np.mean(np.log(output[range(len(y)), y] + 1e-10))
    
    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)




import numpy as np

class NeuralNetwork:
    
    def __init__(self, batch_size, n_features, hidden_sizes, n_classes, learning_rate=0.01):
        self.n_features = n_features
        self.hidden_sizes = hidden_sizes
        self.n_classes = n_classes
        self.batch_size = batch_size
        self.learning_rate = learning_rate

        self.activations = []
        self.z_values = []
        
        self.weights = []
        self.biases = []
        
        self.weights.append(np.random.randn(n_features, hidden_sizes[0]) * 0.01)
        self.biases.append(np.zeros(hidden_sizes[0]))
        
        for i in range(len(hidden_sizes) - 1):
            self.weights.append(np.random.randn(hidden_sizes[i], hidden_sizes[i+1]) * 0.01)
            self.biases.append(np.zeros(hidden_sizes[i+1]))
            
        self.weights.append(np.random.randn(hidden_sizes[-1], n_classes) * 0.01)
        self.biases.append(np.zeros(n_classes))
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        self.activations = [X]
        self.z_values = []
        
        for i in range(len(self.hidden_sizes)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            a = self.sigmoid(z)
            self.activations.append(a)
        
        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]
        self.z_values.append(z)
        output = self.softmax(z)
        self.activations.append(output)
        
        return output
    
    def backward(self, X, y, output, current_lr):
        m = X.shape[0]
        deltas = []
        
        error = output.copy()
        error[range(m), y] -= 1
        deltas.insert(0, error)
        
        for i in range(len(self.hidden_sizes) - 1, -1, -1):
            delta_next = deltas[0]
            a = self.activations[i + 1]
            w = self.weights[i + 1]
            z = self.z_values[i]
            
            error = np.dot(delta_next, w.T) * self.sigmoid_derivative(z)
            deltas.insert(0, error)
            
        for i in range(len(self.weights)):
            self.weights[i] -= current_lr * np.dot(self.activations[i].T, deltas[i]) / m
            self.biases[i] -= current_lr * np.sum(deltas[i], axis=0) / m
    
    def train(self, X, y, epochs, verbose=True, stopping_criteria=0.001):
        n_samples = X.shape[0]
        loss_history = []
        eta_0 = self.learning_rate
    
        for epoch in range(epochs):
            current_lr = eta_0 / np.sqrt(epoch + 1)
            indices = np.random.permutation(n_samples)
            for i in range(0, n_samples, self.batch_size):
                batch_idx = indices[i:min(i + self.batch_size, n_samples)]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
            
                output = self.forward(X_batch)
                self.backward(X_batch, y_batch, output, current_lr)
        
            if verbose and epoch % 10 == 0:
                loss = self.compute_loss(X, y)
                loss_history.append(loss)
            
                if len(loss_history) &gt;= 10:
                    last_10_avg = np.mean(loss_history[-10:])
                    if last_10_avg &lt; loss:
                        print(f"Epoch {epoch}, Loss: {loss:.4f}")
                        print("Average of last 10 losses ({:.4f}) is less than current loss ({:.4f}), stopping training.".format(last_10_avg, loss))
                        return
                    
                if len(loss_history) &gt; 100:
                    loss_history = loss_history[-100:]
                
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
                if loss &lt; stopping_criteria:
                    print("Stopping criteria met.")
                    return
    
    def compute_loss(self, X, y):
        output = self.forward(X)
        return -np.mean(np.log(output[range(len(y)), y] + 1e-10))
    
    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)



import numpy as np

class NeuralNetwork:
    
    def __init__(self, batch_size, n_features, hidden_sizes, n_classes, learning_rate=0.01):
        self.n_features = n_features
        self.hidden_sizes = hidden_sizes
        self.n_classes = n_classes
        self.batch_size = batch_size
        self.learning_rate = learning_rate

        self.activations = []
        self.z_values = []
        
        self.weights = []
        self.biases = []
        
        self.weights.append(np.random.randn(n_features, hidden_sizes[0]) * 0.01)
        self.biases.append(np.zeros(hidden_sizes[0]))
        
        for i in range(len(hidden_sizes) - 1):
            self.weights.append(np.random.randn(hidden_sizes[i], hidden_sizes[i+1]) * 0.01)
            self.biases.append(np.zeros(hidden_sizes[i+1]))
            
        self.weights.append(np.random.randn(hidden_sizes[-1], n_classes) * 0.01)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match203-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.biases.append(np.zeros(n_classes))
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return np.where(x &gt; 0, 1, 0)
</FONT>    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def forward(self, X):
        self.activations = [X]
        self.z_values = []
        
        for i in range(len(self.hidden_sizes)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            a = self.relu(z)
            self.activations.append(a)
        
        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]
        self.z_values.append(z)
        output = self.softmax(z)
        self.activations.append(output)
        
        return output
    
    def backward(self, X, y, output, current_lr):
        m = X.shape[0]
        deltas = []
        
        error = output.copy()
        error[range(m), y] -= 1
        deltas.insert(0, error)
        
        for i in range(len(self.hidden_sizes) - 1, -1, -1):
            delta_next = deltas[0]
            a = self.activations[i + 1]
            w = self.weights[i + 1]
            z = self.z_values[i]
            
            error = np.dot(delta_next, w.T) * self.relu_derivative(z)
            deltas.insert(0, error)
            
        for i in range(len(self.weights)):
            self.weights[i] -= current_lr * np.dot(self.activations[i].T, deltas[i]) / m
            self.biases[i] -= current_lr * np.sum(deltas[i], axis=0) / m
    
    def train(self, X, y, epochs, verbose=True, stopping_criteria=0.001):
        n_samples = X.shape[0]
        loss_history = []
        eta_0 = self.learning_rate
    
        for epoch in range(epochs):
            current_lr = eta_0
            indices = np.random.permutation(n_samples)
            for i in range(0, n_samples, self.batch_size):
                batch_idx = indices[i:min(i + self.batch_size, n_samples)]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
            
                output = self.forward(X_batch)
                self.backward(X_batch, y_batch, output, current_lr)
        
            if verbose and epoch % 10 == 0:
                loss = self.compute_loss(X, y)
                loss_history.append(loss)
            
                if len(loss_history) &gt;= 10:
                    last_10_avg = np.mean(loss_history[-10:])
                    if last_10_avg &lt; loss:
                        print(f"Epoch {epoch}, Loss: {loss:.4f}")
                        print("Average of last 10 losses ({:.4f}) is less than current loss ({:.4f}), stopping training.".format(last_10_avg, loss))
                        return
                    
                if len(loss_history) &gt; 100:
                    loss_history = loss_history[-100:]
                
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
                if loss &lt; stopping_criteria:
                    print("Stopping criteria met.")
                    return
    
    def compute_loss(self, X, y):
        output = self.forward(X)
        return -np.mean(np.log(output[range(len(y)), y] + 1e-10))
    
    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)



import os
import pandas as pd
from preprocessing import load_images_from_folders
from preprocessing import load_test_images
from nn_a import NeuralNetwork

def evaluate_neural_network_b(train_folder, test_folder, output_folder_path):
    X, y = load_images_from_folders(train_folder)
    X_test = load_test_images(test_folder)

    hidden_layer_size = [100]
    batch_size = 32
    n_features = 2352
    n_classes = 43
    learning_rate = 0.01

    ann = NeuralNetwork(batch_size, n_features, hidden_layer_size, n_classes, learning_rate)
    ann.train(X, y, epochs=4000)
    y_pred = ann.predict(X_test)
    
    output_df = pd.DataFrame({
        'prediction': y_pred
    })
    
    os.makedirs(output_folder_path, exist_ok=True)
    
    output_file = os.path.join(output_folder_path, 'prediction_b.csv')

    output_df.to_csv(output_file, index=False)




import os
import pandas as pd
from preprocessing import load_images_from_folders
from preprocessing import load_test_images
from nn_a import NeuralNetwork

def evaluate_neural_network_c(train_folder, test_folder, output_folder_path):
    X, y = load_images_from_folders(train_folder)
    X_test = load_test_images(test_folder)

    hidden_layer_size = [512, 256, 128, 64]
    batch_size = 32
    n_features = 2352
    n_classes = 43
    learning_rate = 0.01

    ann = NeuralNetwork(batch_size, n_features, hidden_layer_size, n_classes, learning_rate)
    ann.train(X, y, epochs=4000)
    y_pred = ann.predict(X_test)
    
    output_df = pd.DataFrame({
        'prediction': y_pred
    })
    
    os.makedirs(output_folder_path, exist_ok=True)
    
    output_file = os.path.join(output_folder_path, 'prediction_c.csv')

    output_df.to_csv(output_file, index=False)




import os
import pandas as pd
from preprocessing import load_images_from_folders
from preprocessing import load_test_images
from nn_a_adaptive_learning_rate import NeuralNetwork

def evaluate_neural_network_d(train_folder, test_folder, output_folder_path):
    X, y = load_images_from_folders(train_folder)
    X_test = load_test_images(test_folder)

    hidden_layer_size = [512, 256, 128, 64]
    batch_size = 32
    n_features = 2352
    n_classes = 43
    learning_rate = 0.01

    ann = NeuralNetwork(batch_size, n_features, hidden_layer_size, n_classes, learning_rate)
    ann.train(X, y, epochs=4000)
    y_pred = ann.predict(X_test)
    
    output_df = pd.DataFrame({
        'prediction': y_pred
    })
    
    os.makedirs(output_folder_path, exist_ok=True)
    
    output_file = os.path.join(output_folder_path, 'prediction_d.csv')

    output_df.to_csv(output_file, index=False)



import os
import pandas as pd
from preprocessing import load_images_from_folders
from preprocessing import load_test_images
from nn_a_relu import NeuralNetwork

def evaluate_neural_network_e(train_folder, test_folder, output_folder_path):
    X, y = load_images_from_folders(train_folder)
    X_test = load_test_images(test_folder)

    hidden_layer_size = [512, 256, 128, 64]
    batch_size = 32
    n_features = 2352
    n_classes = 43
    learning_rate = 0.01

    ann = NeuralNetwork(batch_size, n_features, hidden_layer_size, n_classes, learning_rate)
    ann.train(X, y, epochs=4000)
    y_pred = ann.predict(X_test)
    
    output_df = pd.DataFrame({
        'prediction': y_pred
    })
    
    os.makedirs(output_folder_path, exist_ok=True)
    
    output_file = os.path.join(output_folder_path, 'prediction_e.csv')

    output_df.to_csv(output_file, index=False)



import os
import pandas as pd
from preprocessing import load_images_from_folders
from preprocessing import load_test_images
from sklearn.neural_network import MLPClassifier

def evaluate_neural_network_f(train_folder, test_folder, output_folder_path):
    X, y = load_images_from_folders(train_folder)
    X_test = load_test_images(test_folder)
    
    hidden_layer_size = (512, 256, 128, 64)
    activation = 'relu'
    solver = 'sgd'
    alpha = 0
    batch_size = 32
    learning_rate = 'invscaling'

    model = MLPClassifier(hidden_layer_sizes=hidden_layer_size, activation=activation, solver=solver, alpha=alpha,
                           batch_size=batch_size, learning_rate=learning_rate, learning_rate_init=0.01, max_iter=2500, random_state=42, verbose=True)
    
    model.fit(X, y)
    y_pred = model.predict(X_test)
    
    output_df = pd.DataFrame({
        'prediction': y_pred
    })
    
    os.makedirs(output_folder_path, exist_ok=True)
    
    output_file = os.path.join(output_folder_path, 'prediction_f.csv')

    output_df.to_csv(output_file, index=False)



import os
import numpy as np
from PIL import Image

def load_images_from_folders(train_folder):
    X = []
    y = []
    
    for folder_name in range(43):
        folder_str = f"{folder_name:05d}"
        folder_path = os.path.join(train_folder, folder_str)
        
        for filename in os.listdir(folder_path):
            if filename.lower().endswith('.jpg'):
                img_path = os.path.join(folder_path, filename)
                img = Image.open(img_path)
                img_array = np.array(img, dtype=np.float32)
                img_array = img_array / 255.0
                img_array = img_array.reshape(2352)
                X.append(img_array)
                y.append(folder_name)
    
    X = np.array(X)
    y = np.array(y)
    
    return X, y

def load_test_images(test_folder):
    X_test = []
    
    image_files = sorted([f for f in os.listdir(test_folder) if f.lower().endswith('.jpg')])
    
    for filename in image_files:
        img_path = os.path.join(test_folder, filename)
        img = Image.open(img_path)
        img_array = np.array(img, dtype=np.float32)
        img_array = img_array / 255.0
        img_array = img_array.reshape(2352)
        X_test.append(img_array)
    
    X_test = np.array(X_test)
    
    return X_test

</PRE>
</PRE>
</BODY>
</HTML>
