<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_0VNUY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_5R0DC.py<p><PRE>


import pandas as pd
import math
import sys
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

label = "income"
label_0 = " &lt;=50K"
label_1 = " &gt;50K"
categorical = None
continuous = None
one_hot_categorical = None

class Tree:
    def __init__(self, attribute, parent_attribute, parent_val, median, cat_val_list, child_list, label, level):
        self.attribute = attribute
        self.parent_attribute = parent_attribute
        self.parent_val = parent_val
        self.median = median
        self.cat_val_list = cat_val_list
        self.child_list = child_list
        self.label = label
        self.level = level

    
    def add_child_node(self, child):
        self.child_list.append(child)



# returns a list of continuous and categorical attributes separately
def separate_attributes(df):
    global continuous, categorical
    continuous = df.select_dtypes(include=['number']).columns.tolist()
    categorical = df.select_dtypes(include=['object', 'category']).columns.tolist()
    categorical.remove(label)
    return continuous, categorical

def entropy(df):

    count_0 = len(df[df[label] == label_0])
    count_1 = len(df[df[label] == label_1])
    prob_0 = float(count_0)/(count_0 + count_1)
    prob_1 = float(count_1)/(count_0 + count_1)
    log_0 = 0
    log_1 = 0
    if(prob_0 != 0):
        log_0 = prob_0*math.log(prob_0)
    if(prob_1 != 0):
        log_1 = prob_1*math.log(prob_1)   
    entropy = -(log_0 + log_1)
    return entropy

def choose_attribute(df):
    # H(Y)
    H_y = entropy(df)
    I_gain_max = 0
    split_attr = ""
    median_max = -sys.maxsize
    count = len(df)

    for attribute in categorical:
        attr_val = df[attribute].unique()
        H_y_val = 0
        for val in attr_val:
            df_attr = df[df[attribute] == val]
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr)        
        I_gain = H_y - H_y_val
        if I_gain &gt; I_gain_max:
            split_attr = attribute
            I_gain_max = I_gain

    for attribute in continuous:
        H_y_val = 0
        median = df[attribute].median()
        
        df_attr = df[df[attribute] &lt;= median]
        if len(df_attr) != 0:
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr)     
        
        df_attr = df[df[attribute] &gt; median]
        if len(df_attr) != 0:
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr) 
        
        I_gain = H_y - H_y_val
        if I_gain &gt; I_gain_max:
            split_attr = attribute
            I_gain_max = I_gain
            median_max = median

    return split_attr, median_max

def grow_tree(df, depth, parent_attribute = "", parent_val = -1, current_level = 1):

    df_label_0 = df[df[label] == label_0]
    df_label_1 = df[df[label] == label_1]
    if len(df_label_0) == 0 or len(df_label_1) == 0 or current_level &gt;= depth:
        if current_level &gt;= depth:
            if len(df_label_0) &gt; len(df_label_1):
                label_val = label_0
            elif len(df_label_0) &lt; len(df_label_1):
                label_val = label_1
            else:
                label_val = label_0
        elif len(df_label_0) == 0:
            label_val = label_1
        elif len(df_label_1) == 0:
            label_val = label_0
        return Tree(attribute = "", parent_attribute = parent_attribute, parent_val = parent_val, median = -sys.maxsize, cat_val_list = [], child_list = [], label = label_val, level = current_level)

    attribute, median = choose_attribute(df)

    # Creating Decision Tree node
    node = Tree(attribute = attribute, parent_attribute = parent_attribute, parent_val = parent_val, median = None, cat_val_list = [], child_list = [], label = -1, level = current_level)
    # Store majority label at each node
    if len(df_label_0) &gt; len(df_label_1):
        label_val = label_0
    elif len(df_label_0) &lt; len(df_label_1):
        label_val = label_1
    else:
        label_val = label_0
    node.label = label_val

    if attribute in categorical:
        cat_val_list = df[attribute].unique()
        node.cat_val_list = cat_val_list
        for val in cat_val_list:
            child_df = df[df[attribute] == val]
            child_node = grow_tree(child_df, depth, attribute, val, current_level + 1)
            if child_node is not None:
                node.add_child_node(child_node)
    elif attribute in continuous:
        node.median = median
        # Left child has values less or equal to median
        left_child_df = df[df[attribute] &lt;= median]
        left_child = grow_tree(left_child_df, depth, attribute, "left", current_level + 1)
        if left_child is not None:
            node.add_child_node(left_child)
        # Right child has values greater than median
        right_child_df = df[df[attribute] &gt; median]
        right_child = grow_tree(right_child_df, depth, attribute, "right", current_level + 1)
        if right_child is not None:
            node.add_child_node(right_child)
    return node

def predict(df, dec_tree):
    df["predicted"] = df.apply(lambda row: predict_label(row, dec_tree), axis=1)
    return df

def predict_label(row, current_node):
    if current_node.attribute == "":
        return current_node.label

    attribute = current_node.attribute

    if attribute in categorical:
        attr_val = row[attribute]
        for child in current_node.child_list:
            if child.parent_attribute == attribute and child.parent_val == attr_val:
                return predict_label(row, child)

    if attribute in continuous:
        attr_val = row[attribute]
        median = current_node.median
        if attr_val &lt;= median:
            for child in current_node.child_list:
                if child.parent_val == "left":
                    return predict_label(row, child)
        else:
            for child in current_node.child_list:
                if child.parent_val == "right":
                    return predict_label(row, child)

    return current_node.label

def accuracy(df, label_col= label):    
    correct = (df[label_col].astype(str).str.strip().str.lower() == df["predicted"].astype(str).str.strip().str.lower()).sum()
    return float(correct)/len(df)

def one_hot_encoding(df, categorical):
    global one_hot_categorical
    one_hot_categorical = []
    for attribute in categorical:
        attr_val = df[attribute].unique()
        for val in attr_val:
            df[attribute + "_" + str(val)] = df[attribute].apply(lambda x: 1 if x == val else 0)
            one_hot_categorical.append(attribute + "_" + str(val))
    return one_hot_categorical

def choose_attribute_one_hot(df):
    # H(Y)
    H_y = entropy(df)
    I_gain_max = 0
    split_attr = ""
    median_max = -sys.maxsize
    count = len(df)

    for attribute in one_hot_categorical:
        attr_val = df[attribute].unique()
        H_y_val = 0
        for val in attr_val:
            df_attr = df[df[attribute] == val]
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr)        
        I_gain = H_y - H_y_val
        if I_gain &gt; I_gain_max:
            split_attr = attribute
            I_gain_max = I_gain

    for attribute in continuous:
        H_y_val = 0
        median = df[attribute].median()
        
        df_attr = df[df[attribute] &lt;= median]
        if len(df_attr) != 0:
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr)     
        
        df_attr = df[df[attribute] &gt; median]
        if len(df_attr) != 0:
            prob_attr = float(len(df_attr))/count
            H_y_val += prob_attr*entropy(df_attr) 
        
        I_gain = H_y - H_y_val
        if I_gain &gt; I_gain_max:
            split_attr = attribute
            I_gain_max = I_gain
            median_max = median

    return split_attr, median_max

def grow_tree_one_hot(df, depth, parent_attribute = "", parent_val = -1, current_level = 1):

    df_label_0 = df[df[label] == label_0]
    df_label_1 = df[df[label] == label_1]
    if len(df_label_0) == 0 or len(df_label_1) == 0 or current_level &gt;= depth:
        if current_level &gt;= depth:
            if len(df_label_0) &gt; len(df_label_1):
                label_val = label_0
            elif len(df_label_0) &lt; len(df_label_1):
                label_val = label_1
            else:
                label_val = label_0
        elif len(df_label_0) == 0:
            label_val = label_1
        elif len(df_label_1) == 0:
            label_val = label_0
        return Tree(attribute = "", parent_attribute = parent_attribute, parent_val = parent_val, median = -sys.maxsize, cat_val_list = [], child_list = [], label = label_val, level = current_level)

    attribute, median = choose_attribute_one_hot(df)

    # Creating Decision Tree node
    node = Tree(attribute = attribute, parent_attribute = parent_attribute, parent_val = parent_val, median = None, cat_val_list = [], child_list = [], label = -1, level = current_level)
    # Store majority label at each node
    if len(df_label_0) &gt; len(df_label_1):
        label_val = label_0
    elif len(df_label_0) &lt; len(df_label_1):
        label_val = label_1
    else:
        label_val = label_0
    node.label = label_val

    if attribute in one_hot_categorical:
        cat_val_list = df[attribute].unique()
        node.cat_val_list = cat_val_list
        for val in cat_val_list:
            child_df = df[df[attribute] == val]
            child_node = grow_tree_one_hot(child_df, depth, attribute, val, current_level + 1)
            if child_node is not None:
                node.add_child_node(child_node)
    elif attribute in continuous:
        node.median = median
        # Left child has values less or equal to median
        left_child_df = df[df[attribute] &lt;= median]
        left_child = grow_tree_one_hot(left_child_df, depth, attribute, "left", current_level + 1)
        if left_child is not None:
            node.add_child_node(left_child)
        # Right child has values greater than median
        right_child_df = df[df[attribute] &gt; median]
        right_child = grow_tree_one_hot(right_child_df, depth, attribute, "right", current_level + 1)
        if right_child is not None:
            node.add_child_node(right_child)
    return node       

def predict_one_hot(df, dec_tree):
    df["predicted"] = df.apply(lambda row: predict_label_one_hot(row, dec_tree), axis=1)
    return df

def predict_label_one_hot(row, current_node):
    if current_node.attribute == "" or current_node.attribute not in row.index:
        return current_node.label

    attribute = current_node.attribute

    if attribute in one_hot_categorical:
        attr_val = row[attribute]
        for child in current_node.child_list:
            if child.parent_attribute == attribute and child.parent_val == attr_val:
                return predict_label_one_hot(row, child)

    if attribute in continuous:
        attr_val = row[attribute]
        median = current_node.median
        if attr_val &lt;= median:
            for child in current_node.child_list:
                if child.parent_val == "left":
                    return predict_label_one_hot(row, child)
        else:
            for child in current_node.child_list:
                if child.parent_val == "right":
                    return predict_label_one_hot(row, child)

        return current_node.label
    
def grow_tree_one_hot_fully(df, parent_attribute="", parent_val=-1, current_level=1):
    df_label_0 = df[df[label] == label_0]
    df_label_1 = df[df[label] == label_1]
    if len(df_label_0) == 0 or len(df_label_1) == 0:
        if len(df_label_0) == 0:
            label_val = label_1
        else:
            label_val = label_0
        return Tree(attribute="", parent_attribute=parent_attribute, parent_val=parent_val,
                         median=-sys.maxsize, cat_val_list=[], child_list=[], label=label_val, level=current_level)

    attribute, median = choose_attribute_one_hot(df)

    node = Tree(attribute=attribute, parent_attribute=parent_attribute, parent_val=parent_val,
                     median=None, cat_val_list=[], child_list=[], label=-1, level=current_level)
    node.label = label_0 if len(df_label_0) &gt;= len(df_label_1) else label_1

    if attribute in one_hot_categorical:
        cat_val_list = df[attribute].unique()
        node.cat_val_list = cat_val_list
        for val in cat_val_list:
            child_df = df[df[attribute] == val]
            child_node = grow_tree_one_hot_fully(child_df, attribute, val, current_level + 1)
            if child_node is not None:
                node.add_child_node(child_node)
    elif attribute in continuous:
        node.median = median
        left_child_df = df[df[attribute] &lt;= median]
        right_child_df = df[df[attribute] &gt; median]
        left_child = grow_tree_one_hot_fully(left_child_df, attribute, "left", current_level + 1)
        right_child = grow_tree_one_hot_fully(right_child_df, attribute, "right", current_level + 1)
        if left_child is not None:
            node.add_child_node(left_child)
        if right_child is not None:
            node.add_child_node(right_child)
    return node

def prune_tree_nodes(df_valid, dec_tree, node, accuracy_max, depth_nodes, node_set):
    
    for depth in range(len(depth_nodes), 1, -1):
        nodes = depth_nodes[depth]
        # print(depth, len(nodes))
        for node in nodes:
            attr_org = node.attribute
            node.attribute = "" # make the child node a leaf, i.e, being pruned
            df_valid = predict_one_hot(df_valid, dec_tree)
            new_accuracy = accuracy(df_valid)
            # print(depth, new_accuracy, accuracy_max)
            if new_accuracy &gt;= accuracy_max:
                accuracy_max = new_accuracy
                nodes.remove(node)
                node_set.remove(node)
            else:
                node.attribute = attr_org
    return accuracy_max, dec_tree, df_valid, node_set



# Greedy because as soon as we see a decrease in accuracy we break out completely
def prune_tree(df_valid, dec_tree, depth_nodes, node_set, df_train, df_test):
    
    passes = 0
    accuracy_max_by_pass = []
    train_accuracy_list = []
    test_accuracy_list = []
    valid_accuracy_list = []
    nodes_num = []
    prune_accuracy = 0
<A NAME="2"></A><FONT color = #0000FF><A HREF="match202-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    df_train = predict_one_hot(df_train, dec_tree)
    df_valid = predict_one_hot(df_valid, dec_tree)
    df_test = predict_one_hot(df_test, dec_tree)
    train_accuracy = accuracy(df_train)
    train_accuracy_list.append(train_accuracy)
    valid_accuracy = accuracy(df_valid)
    valid_accuracy_list.append(valid_accuracy)
</FONT>    test_accuracy = accuracy(df_test)
    test_accuracy_list.append(test_accuracy)
    nodes_num.append(len(node_set))
    print("Len: ", len(node_set))
    while True:
        passes = passes + 1
        print("Pass: ", passes)
        accuracy_max =  accuracy(df_valid) # before pruning decision tree
        prune_accuracy, dec_tree, df_valid, node_set= prune_tree_nodes(df_valid, dec_tree, dec_tree, accuracy_max, depth_nodes, node_set) # after pruning tree
        print(prune_accuracy, accuracy_max)
        if prune_accuracy &gt; accuracy_max:
            accuracy_max = prune_accuracy
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match202-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            df_train = predict_one_hot(df_train, dec_tree)
            df_valid = predict_one_hot(df_valid, dec_tree)
            df_test = predict_one_hot(df_test, dec_tree)
            train_accuracy = accuracy(df_train)
            train_accuracy_list.append(train_accuracy)
            valid_accuracy = accuracy(df_valid)
            valid_accuracy_list.append(valid_accuracy)
</FONT>            test_accuracy = accuracy(df_test)
            test_accuracy_list.append(test_accuracy)
            nodes_num.append(len(node_set))
        else:
            break
        if passes &gt; 1 and abs(accuracy_max_by_pass[-1] - accuracy_max) &lt; 5e-3:
            break
        else:
            accuracy_max_by_pass.append(accuracy_max)
    return dec_tree, train_accuracy_list, valid_accuracy_list, test_accuracy_list, nodes_num

def dfs(node, depth, depth_nodes, node_set):
    if depth not in depth_nodes:
        depth_nodes[depth] = []
    depth_nodes[depth].append(node)
    node_set.add(node)
    for child in node.child_list:
        dfs(child, depth + 1, depth_nodes, node_set)
    return depth_nodes, node_set

# part d

def update_remaining_one_hot_cols(df, one_hot_categorical):
    for attribute in one_hot_categorical:
        if attribute not in df.columns:
            df[attribute] = 0

def part_d(df_trn, df_val, df_tst, output_folder):

    df_train = df_trn.copy(deep=True)
    df_valid = df_val.copy(deep=True)
    df_test = df_tst.copy(deep=True)
    separate_attributes(df_train)
    global continuous, categorical
    one_hot_categorical = one_hot_encoding(df_train, categorical)
    one_hot_encoding(df_valid, categorical)
    one_hot_encoding(df_test, categorical)

    # Remove categorical string type values
    df_train = df_train.drop(columns=categorical)
    df_valid = df_valid.drop(columns=categorical)
    df_test  = df_test.drop(columns=categorical)
    update_remaining_one_hot_cols(df_valid, one_hot_categorical)
    update_remaining_one_hot_cols(df_test, one_hot_categorical)

    # Generating sklearn format for training
    one_hot_attribute = [attribute for attribute in df_train.columns if attribute != label]
    X_train = df_train[one_hot_attribute]
    X_valid = df_valid[one_hot_attribute]
    X_test = df_test[one_hot_attribute]
<A NAME="0"></A><FONT color = #FF0000><A HREF="match202-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y_train = df_train[label]
    y_valid = df_valid[label]
    y_test = df_test[label]

    # d-i: Varying max_depth
    max_depth = [25, 35, 45, 55]
    train_accuracy = []
    valid_accuracy = []
    test_accuracy = []

    for depth in max_depth:
</FONT>        clf = DecisionTreeClassifier(max_depth=depth, criterion='entropy')
        clf.fit(X_train, y_train)
        y_train_predicted = clf.predict(X_train)
        y_valid_predicted = clf.predict(X_valid)
        y_test_predicted = clf.predict(X_test)
        train_accuracy.append(accuracy_score(y_train, y_train_predicted))
        valid_accuracy.append(accuracy_score(y_valid, y_valid_predicted))
        test_accuracy.append(accuracy_score(y_test, y_test_predicted))

    # Accuracy v/s Max Depth
    plt.figure()
    plt.plot(max_depth, train_accuracy, label="Train Accuracy", marker='o')
    plt.plot(max_depth, valid_accuracy, label="Validation Accuracy", marker='o')
    plt.plot(max_depth, test_accuracy, label="Test Accuracy", marker='o')
    plt.xlabel("Max Depth")
    plt.ylabel("Accuracy")
    plt.title("Decision Tree Accuracy vs Max Depth (part d)")
    plt.legend()
    plt.grid(True)
    plt.show()

    max_acc = max(valid_accuracy)
    max_index = valid_accuracy.index(max_acc)
    depth_max_acc = max_depth[max_index]
    print("Train accuracy: ", train_accuracy)
    print("Valid accuracy: ", valid_accuracy)
    print("Test accuracy: ", test_accuracy)
    print("Depth for max validation accuracy", depth_max_acc)

    # Part (ii): Varying ccp_alpha
    ccp_alpha = [0.001, 0.01, 0.1, 0.2]
    alpha_train_accuracy = []
    alpha_valid_accuracy = []
    alpha_test_accuracy = []

    for alpha in ccp_alpha:
        clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha)
        clf.fit(X_train, y_train)
        y_train_predicted = clf.predict(X_train)
        y_valid_predicted = clf.predict(X_valid)
        y_test_predicted = clf.predict(X_test)
        alpha_train_accuracy.append(accuracy_score(y_train, y_train_predicted))
        alpha_valid_accuracy.append(accuracy_score(y_valid, y_valid_predicted))
        alpha_test_accuracy.append(accuracy_score(y_test, y_test_predicted))

    # Accuracy v/s ccp_alpha
    plt.figure()
    plt.plot(ccp_alpha, alpha_train_accuracy, label="Train Accuracy", marker='o')
    plt.plot(ccp_alpha, alpha_valid_accuracy, label="Validation Accuracy", marker='o')
    plt.plot(ccp_alpha, alpha_test_accuracy, label="Test Accuracy", marker='o')
    plt.xlabel("ccp_alpha")
    plt.ylabel("Accuracy")
    plt.title("Decision Tree Accuracy vs CCP Alpha")
    plt.xscale('log')
    plt.legend()
    plt.grid(True)
    plt.show()

    max_acc =  max(alpha_valid_accuracy)
    max_index = alpha_valid_accuracy.index(max_acc)
    ccp_max = ccp_alpha[max_index]
    print("Train accuracy: ", alpha_train_accuracy)
    print("Valid accuracy: ", alpha_valid_accuracy)
    print("Test accuracy: ", alpha_test_accuracy)

    clf = DecisionTreeClassifier(max_depth=depth_max_acc, criterion='entropy', ccp_alpha=ccp_max)
    clf.fit(X_train, y_train)
    y_train_predicted = clf.predict(X_train)
    y_valid_predicted = clf.predict(X_valid)
    y_test_predicted = clf.predict(X_test)
    print(ccp_max)
    print("Train accuracy for best params: ", accuracy_score(y_train, y_train_predicted))
    print("Validation accuracy for best params: ", accuracy_score(y_valid, y_valid_predicted))
    print("Test accuracy for best params: ",accuracy_score(y_test, y_test_predicted))
    
    df_predicted = pd.DataFrame(y_test_predicted, columns=['predicted'])
    df_predicted.to_csv(f"{output_folder}/predicted_d.csv", index=False)

def part_e(df_trn, df_val, df_tst, output_folder):

    df_train = df_trn.copy(deep=True)
    df_valid = df_val.copy(deep=True)
    df_test = df_tst.copy(deep=True)
    separate_attributes(df_train)
    global continuous, categorical
    one_hot_categorical = one_hot_encoding(df_train, categorical)
    one_hot_encoding(df_valid, categorical)
    one_hot_encoding(df_test, categorical)

    # Remove categorical string type values
    df_train = df_train.drop(columns=categorical)
    df_valid = df_valid.drop(columns=categorical)
    df_test  = df_test.drop(columns=categorical)

    # Set absent one_hot_train columns to 0 in df_valid and df_test
    update_remaining_one_hot_cols(df_valid, one_hot_categorical)
    update_remaining_one_hot_cols(df_test, one_hot_categorical)

    # Generating sklearn format for training
    sklearn_attribute = [attribute for attribute in df_train.columns if attribute != label]
    X_train = df_train[sklearn_attribute]
    X_valid = df_valid[sklearn_attribute]
    X_test = df_test[sklearn_attribute]
    y_train = df_train[label]
    y_valid = df_valid[label]
    y_test = df_test[label]


    parameter_grid = {
        "n_estimators": [50, 150, 250, 350],
        "max_features": [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
        "min_samples_split": [2, 4, 6, 8, 10]
    }

    grid = list(ParameterGrid(parameter_grid))

    oob_accuracy_max = 0
    optimal_model = None
    optimal_parameters = None
    output = []

    for parameters in grid:
        clf = RandomForestClassifier(
            criterion="entropy",
            n_estimators=parameters["n_estimators"],
            max_features=parameters["max_features"],
            min_samples_split=parameters["min_samples_split"],
            oob_score=True,
            bootstrap=True,
            n_jobs=-1,
        )

        clf.fit(X_train, y_train)
        y_train_predicted = clf.predict(X_train)
        y_valid_predicted = clf.predict(X_valid)
        y_test_predicted = clf.predict(X_test)
        train_accuracy = accuracy_score(y_train, y_train_predicted)
        oob_accuracy = clf.oob_score_
        valid_accuracy = accuracy_score(y_valid, y_valid_predicted)
        test_accuracy = accuracy_score(y_test, y_test_predicted)

        output.append({
            "params": parameters,
            "train_acc": train_accuracy,
            "oob_acc": oob_accuracy,
            "valid_acc": valid_accuracy,
            "test_acc": test_accuracy
        })

        if oob_accuracy &gt; oob_accuracy_max:
            oob_accuracy_max = oob_accuracy
            optimal_model = clf
            optimal_parameters = parameters

    y_train_predicted = optimal_model.predict(X_train)
    y_valid_predicted = optimal_model.predict(X_valid)
    y_test_predicted = optimal_model.predict(X_test)
    print("Optimal Parameters:", optimal_parameters)
    print("Training Accuracy:", accuracy_score(y_train, y_train_predicted))
    print("OOB Accuracy:", optimal_model.oob_score_)
    print("Validation Accuracy:", accuracy_score(y_valid, y_valid_predicted))
    print("Test Accuracy:", accuracy_score(y_test, y_test_predicted))

    df_predicted = pd.DataFrame(y_test_predicted, columns=['predicted'])
    df_predicted.to_csv(f"{output_folder}/predicted_e.csv", index=False)


def main():
    
    global categorical
    path_train = sys.argv[1]
    path_valid = sys.argv[2]
    path_test  = sys.argv[3]
    output_folder = sys.argv[4]
    q_part = sys.argv[5].lower()

    df_train = pd.read_csv(path_train)
    df_valid = pd.read_csv(path_valid)
    df_test  = pd.read_csv(path_test)
    separate_attributes(df_train)

    if q_part == "a":

        depths = [1, 2, 3, 4, 5, 10, 15, 20]
        train_accuracy_list = []
        test_accuracy_list = []

        for depth in depths:
            flag = False
            if depth == 20:
                flag = True
            decision_tree = grow_tree(df=df_train, depth=depth)
            df_train = predict(df_train, decision_tree)
            df_valid = predict(df_valid, decision_tree)
            df_test = predict(df_test, decision_tree)
            train_accuracy = accuracy(df_train)
            test_accuracy = accuracy(df_test)
            train_accuracy_list.append(train_accuracy)
            test_accuracy_list.append(test_accuracy)
            print("Depth: {0}, train accuracy: {1}, test accuracy: {2}".format(depth, train_accuracy, test_accuracy))
            if flag:
                df_test["predicted"].to_csv(f"{output_folder}/predicted_a.csv", index=False)

        # Accuracy v/s Max Depth
        plt.figure()
        plt.plot(depths, train_accuracy_list, label="Train Accuracy", marker='o')
        plt.plot(depths, test_accuracy_list, label="Test Accuracy", marker='o')
        plt.xlabel("Max Depth")
        plt.ylabel("Accuracy")
        plt.title("Decision Tree (part a): Accuracy vs Max Depth")
        plt.legend()
        plt.grid(True)
        plt.show()    

    elif q_part == "b":
        one_hot_encoding(df_train, categorical)
        one_hot_encoding(df_valid, categorical)
        one_hot_encoding(df_test, categorical)

        depths = [5, 10, 15, 20, 25, 35, 45, 55]
        train_accuracy_list = []
        test_accuracy_list = []

        for depth in depths:
            flag = False
            if depth == 55:
                flag = True
            decision_tree = grow_tree_one_hot(df=df_train, depth=depth)
            df_train = predict_one_hot(df_train, decision_tree)
            df_valid = predict_one_hot(df_valid, decision_tree)
            df_test = predict_one_hot(df_test, decision_tree)
            train_accuracy = accuracy(df_train)
            test_accuracy = accuracy(df_test)
            train_accuracy_list.append(train_accuracy)
            test_accuracy_list.append(test_accuracy)
            print("Depth: {0}, train accuracy: {1}, test accuracy: {2}".format(depth, train_accuracy, test_accuracy))
            if flag:
                df_test["predicted"].to_csv(f"{output_folder}/predicted_b.csv", index=False)

        # Accuracy v/s Max Depth
        plt.figure()
        plt.plot(depths, train_accuracy_list, label="Train Accuracy", marker='o')
        plt.plot(depths, test_accuracy_list, label="Test Accuracy", marker='o')
        plt.xlabel("Max Depth")
        plt.ylabel("Accuracy")
        plt.title("Decision Tree (part b): Accuracy vs Max Depth")
        plt.legend()
        plt.grid(True)
        plt.show() 

    elif q_part == "c":
        one_hot_encoding(df_train, categorical)
        one_hot_encoding(df_valid, categorical)
        one_hot_encoding(df_test, categorical)

        depths = [5, 10, 15, 20, 25, 35, 45, 55]

        for depth in depths:
            flag = False
            if depth == 55:
                flag = True
            print("Depth: {0}".format(depth))
            decision_tree = grow_tree_one_hot(df=df_train, depth=depth)
            depth_nodes, node_set = dfs(decision_tree, 1, depth_nodes={}, node_set=set())
            decision_tree, train_accuracy_list, valid_accuracy_list, test_accuracy_list,  node_num = prune_tree(df_valid, decision_tree, depth_nodes, node_set, df_train, df_test)       
            df_train = predict_one_hot(df_train, decision_tree)
            df_valid = predict_one_hot(df_valid, decision_tree)
            df_test = predict_one_hot(df_test, decision_tree)
            train_accuracy = accuracy(df_train)
            valid_accuracy = accuracy(df_valid)
            test_accuracy = accuracy(df_test)
            print("Train accuracy: ", train_accuracy)
            print("Valid accuracy: ", valid_accuracy)
            print("Test accuracy: ", test_accuracy)
            # Accuracy v/s Number of nodes
            plt.figure()
            plt.plot(node_num, train_accuracy_list, label="Train Accuracy", marker='o')
            plt.plot(node_num, valid_accuracy_list, label="Validation Accuracy", marker='o')
            plt.plot(node_num, test_accuracy_list, label="Test Accuracy", marker='o')
            plt.xlabel("Number of nodes in tree")
            plt.ylabel("Accuracy")
            plt.title("Decision Tree (part c): Accuracy vs Nodes for depth: {0}".format(depth))
            plt.legend()
            plt.grid(True)
            plt.show()
            if flag:
                df_test["predicted"].to_csv(f"{output_folder}/predicted_c.csv", index=False)
    elif q_part == "d":
        part_d(df_trn=df_train, df_val=df_valid, df_tst=df_test, output_folder = output_folder)
    elif q_part == "e":
        part_e(df_trn=df_train, df_val=df_valid, df_tst=df_test, output_folder = output_folder)        

if __name__ == "__main__":
    main()

# depth_nodes, node_set = dfs(decision_tree, 1, depth_nodes={}, node_set=set())

# print(len(depth_nodes))

# print("Pruning tree...")
# accuracy_max = accuracy(df_valid)
# t1 = time.time()
# decision_tree, train_accuracy_list, valid_accuracy_list, test_accuracy_list, nodes_num = prune_tree(df_valid, decision_tree, depth_nodes, node_set, df_train, df_test)
# t2 = time.time()
# print("Tree pruned in {}".format(t2 - t1))
# df_train = predict_one_hot(df_train, decision_tree)
# df_valid = predict_one_hot(df_valid, decision_tree)
# df_test = predict_one_hot(df_test, decision_tree)
# train_accuracy = accuracy(df_train)
# valid_accuracy = accuracy(df_valid)
# test_accuracy = accuracy(df_test)
# print("Train accuracy: ", train_accuracy)
# print("Valid acuracy: ", valid_accuracy)
# print("Test accuracy: ", test_accuracy)

# # Accuracy v/s Nodes in decision tree
# plt.figure()
# plt.plot(nodes_num, train_accuracy_list, label="Train Accuracy", marker='o')
# plt.plot(nodes_num, valid_accuracy_list, label="Validation Accuracy", marker='o')
# plt.plot(nodes_num, test_accuracy_list, label="Test Accuracy", marker='o')
# plt.xlabel("Nodes")
# plt.ylabel("Accuracy")
# plt.title("Decision Tree Accuracy vs Nodes")
# plt.legend()
# plt.grid(True)
# plt.show()

# one_hot_categorical = one_hot_encoding(df_train, categorical)
# one_hot_encoding(df_valid, categorical)
# one_hot_encoding(df_test, categorical)

# df_train.to_csv("one_hot_train.csv", index=False)

# print(one_hot_categorical)

# for attribute in one_hot_categorical:
#     df_attr = df_train[attribute]
#     print(df_attr[df_attr == 1])


# decision_tree = grow_tree_one_hot(df_train, depth = 25)  

# print("Training full decision tree...")
# t1 = time.time()
# # decision_tree = grow_tree_one_hot(df_train, depth=25) 
# decision_tree = grow_tree_one_hot_fully(df_train)         
# t2 = time.time()
# print("Training time: ", t2 - t1)

# print("Predicting...")
# df_train = predict_one_hot(df_train, decision_tree)
# df_valid = predict_one_hot(df_valid, decision_tree)
# df_test = predict_one_hot(df_test, decision_tree)

# accuracy(df_train)
# accuracy(df_valid)
# accuracy(df_test)



import numpy as np
import pandas as pd
import sys
import cv2
import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
import time

# Preprocessing function: normalize and flatten
def pre_process_image(image):
    return (image.flatten() / 255.0)

# Read and preprocess test data
def read_images_test(path):
    path_test_image = os.path.join(path, "test")
    path_test_label = os.path.join(path, "test_labels.csv")
    X_test, y_test = read_images_class_test(path_test_image, path_test_label)
    return X_test, y_test

def read_images_class_test(image_path, label_path):
    df_test = pd.read_csv(label_path)
    array_list = []
    label_list = []
    corrupted_images = []

    for image in os.listdir(image_path):
        image_full_path = os.path.join(image_path, image)
        img = cv2.imread(image_full_path)
        if img is not None:
            x = pre_process_image(img)
            array_list.append(x)
            label = df_test.loc[df_test["image"] == image, "label"].values[0]
            label_list.append(label)
        else:
            corrupted_images.append(image)

    X = np.array(array_list)
    y = np.array(label_list).reshape(-1, 1)

    encoder = OneHotEncoder(sparse_output=False)
    y_one_hot = encoder.fit_transform(y)

    return X, y_one_hot

# Read and preprocess training images
def read_images_train(path):
    X_train = []
    y_train = []
    corrupted_images_all = []

    path_train = os.path.join(path, "train")
    labels = sorted(os.listdir(path_train), key=lambda x: int(x))  # ensure label order is consistent

    for label in labels:
        label_path = os.path.join(path_train, label)
        X_label, corrupted = read_images_class(label_path)
        y_label = int(label) * np.ones(X_label.shape[0])
        X_train.append(X_label)
        y_train.append(y_label)
        corrupted_images_all.extend(corrupted)

    X_train = np.concatenate(X_train, axis=0)
    y_train = np.concatenate(y_train, axis=0).reshape(-1, 1)

    encoder = OneHotEncoder(sparse_output=False)
    y_one_hot = encoder.fit_transform(y_train)

    return X_train, y_one_hot

def read_images_class(path):
    array_list = []
    corrupted_images = []

    for image in os.listdir(path):
        image_path = os.path.join(path, image)
        img = cv2.imread(image_path)
        if img is not None:
            array_list.append(pre_process_image(img))
        else:
            corrupted_images.append(image_path)

    return np.array(array_list), corrupted_images


class NeuralNetwork:
    def __init__(self, hidden_layers, activation, mini_batch, features, classes, learning_rate, threshold = 1e-3, epochs = 300, isAdaptive = False, leaky_relu_slope = 0.01):
        print("Initializing...")
        hidden_layers.insert(0, features)
        hidden_layers.append(classes)
        self.layers = hidden_layers
        self.activation = activation
        self.W = {}
        self.b = {}
        self.mini_batch = mini_batch
        self.derivatives = {}
        self.learning_rate = learning_rate
        self.threshold = threshold
        self.epochs = epochs
        self.isAdaptive = isAdaptive
        self.leaky_relu_slope = leaky_relu_slope


    def sigmoid(self, X):
        output = 1/(1 + np.exp(-X))
        derivative = output * (1 - output)
        return (output, derivative)
    
    def relu(self,x):
        output = np.maximum(x, 0)
        derivative = np.where(x&gt;0, 1, 0)
        return (output, derivative)

    def leaky_relu(self, X):
        output = np.maximum(X, X*self.leaky_relu_slope)
        derivative = np.where(X&gt;0, 1, self.leaky_relu_slope)
        return (output, derivative)
    
    def softmax(self, X):
        output = np.exp(X)/np.exp(X).sum(axis = 0)
        # print("Softmax: ", output.shape)
        return output
    
    def learning_rate_function(self, epoch):
        if self.isAdaptive:
            return self.learning_rate/np.sqrt(epoch)
        else:
            return self.learning_rate

    def cross_entropy(self, y, y_predicted):
        # print("Cross entropy: ")
        loss = -np.sum(y * np.log(y_predicted + 1e-12))/y.shape[0]
        return loss
        
    def initialize(self):
        for i in range(1, len(self.layers), 1):
            self.W["W_" + str(i)] = (1/np.sqrt(self.layers[i-1]))*np.random.randn(self.layers[i], self.layers[i-1])
            self.b["b_" + str(i)] = np.zeros(self.layers[i]).reshape([self.layers[i], 1])        

    def activation_function(self, X):
        if self.activation == "sigmoid":
            return self.sigmoid(X)
        elif self.activation == "relu":
            return self.relu(X)
        elif self.activation == "leaky_relu":
            return self.leaky_relu(X)

    def forward_prop(self, X):
        net = {}
        output = {}
        out_layer_index = len(self.layers) - 1
        output["O_0"] = X.T # as here x vector is stored as row vector
        for i in range(1, len(self.layers)):
            w = self.W["W_" + str(i)]
            b = self.b["b_" + str(i)]
            ip = output["O_" + str(i - 1)]
            # print("forward prop: ")
            # print(w.shape, ip.shape, b.shape)
            net["net_" + str(i)] = np.matmul(w, ip) + b
            # print(net["net_" + str(i)].shape)
            if i &lt; out_layer_index:
                output["O_" + str(i)] = self.activation_function(net["net_" + str(i)])[0]            
            else:
                output["O_" + str(i)] = self.softmax(net["net_" + str(i)])
        
            # print(output["O_" + str(i)].shape)

        return output, net


    def back_prop(self, X, y, net, output):

        # print("Back prop: "

        # Output layer
        final = output["O_"+str(len(self.layers) - 1)]  # Output of last layer (softmax)
        prev = output["O_"+ str(len(self.layers) - 2)]  # Output of previous layer

        delta = final - y
        
        self.derivatives["dW_" + str(len(self.layers) - 1)] = np.dot(delta, prev.T) / X.shape[1]
        self.derivatives["db_" + str(len(self.layers) - 1)] = np.sum(delta, axis=1, keepdims=True) / X.shape[1]

 
        for i in range(len(self.layers) - 2, 0, -1):
            W_next = self.W["W_" + str(i+1)]
            delta = np.dot(W_next.T, delta)

            Z_curr = net["net_"+str(i)]
            dZ = self.activation_function(Z_curr)[1]

            delta = delta * dZ

            prev = output["O_" + str(i-1)]

            self.derivatives["dW_"+ str(i)] = np.dot(delta, prev.T) / X.shape[1]
            self.derivatives["db_"+ str(i)] = np.sum(delta, axis=1, keepdims=True) / X.shape[1]

        return self.derivatives

    def fit(self, X, y):
        print("Training...")
        examples = X.shape[0]
        self.initialize()
        print(X.shape, y.shape)
        Xy = np.concatenate([X, y], axis = 1)
        np.random.shuffle(Xy)
        batches = []
        for i in range(0, examples, self.mini_batch):
            batch = Xy[i:i + self.mini_batch]
            batches.append(batch)
        loss_str = 0
        loss_end = 0
        epoch = 0
        last = y.shape[1]
        while (abs(loss_end - loss_str) &gt; self.threshold and epoch &lt;= self.epochs) or epoch &lt; 2:
            epoch_loss = 0
            for batch in batches:
                yb = batch[:, -last:]
                xb = batch[:, :-last]
                # print("batch:")
                # print(xb.shape, yb.shape)
                # print(yb)
                # print(y[:32])
                output, net = self.forward_prop(xb)
                yp = output["O_" + str(len(self.layers)-1)].T
                batch_loss = self.cross_entropy(yb, yp)
                epoch_loss += batch_loss
                self.derivatives = self.back_prop(xb.T, yb.T, net, output)
                for i in range(1, len(self.layers), 1):
                    self.W["W_" + str(i)] -= self.learning_rate_function(epoch+1) * self.derivatives["dW_" + str(i)] 
                    self.b["b_" + str(i)] -= self.learning_rate_function(epoch+1) * self.derivatives["db_" + str(i)] 
            epoch += 1
            epoch_loss = epoch_loss/examples
            loss_str = loss_end
            loss_end = epoch_loss 
            print("Epoch: {0}, Loss: {1}".format(epoch, epoch_loss))
        
        return self.W, self.b
    
    def predict(self, X):
        '''
        Predicts the output for given input X
        Args : X : Input of shape (num_examples, n_features)
        Returns : pred : Predicted output of shape (num_examples, 1)
        '''
        output = self.forward_prop(X)[0]
        prediction_array = output["O_"+str(len(self.layers)-1)].T
        prediction = prediction_array.argmax(axis = 1)
        # output_df = pd.DataFrame({'Prediction': prediction })
        # output_df.to_csv("prediction.csv")
        return prediction
    
def main():
    
    path_train = sys.argv[1]
    path_test  = sys.argv[2]
    output_folder = sys.argv[3]
    q_part = sys.argv[4].lower()


    X_trn, y_trn = read_images_train(path_train)
    X_tst, y_tst = read_images_test(path_test)

    if q_part == "b":
        hidden_layers = [[1], [5], [10], [50], [100]]
        f1_avg_train = []
        f1_avg_test = []
        prediction_list = []
        l = [1, 5, 10, 50, 100]
        for hidden in hidden_layers:
            X_train = X_trn.copy()
            y_train = y_trn.copy()
            X_test = X_tst.copy()
            y_test = y_tst.copy()
            model = NeuralNetwork(hidden_layers=hidden, activation="sigmoid", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01)
            model.fit(X_train, y_train)

            print ("Hidden layer: ", hidden)
            print("Training report: ")
            y_train = y_train.argmax(axis = 1)
            y_train_predicted = model.predict(X_train)
            report = classification_report(y_train, y_train_predicted, output_dict=True)
            f1_avg_train.append(report['macro avg']['f1-score'])
            result = classification_report(y_train, y_train_predicted)
            print(result)

            print("Testing report: ")
            y_test = y_test.argmax(axis = 1)
            y_test_predicted = model.predict(X_test)
            report = classification_report(y_test, y_test_predicted, output_dict=True)
            f1_avg_test.append(report['macro avg']['f1-score'])
            result = classification_report(y_test, y_test_predicted)
            print(result)
            prediction_list.append(y_test_predicted)

        # F1-avg v/s neurons
        plt.figure()
        plt.plot(l, f1_avg_train, label="F1 averages for train set", marker='o')
        plt.plot(l, f1_avg_test, label="F1 averages for test set", marker='o')
        plt.xlabel("Neurons")
        plt.ylabel("F1-avg")
        plt.title("(Part b) Neural Network: F1-Avg vs Number of hidden units")
        plt.legend()
        plt.grid(True)
        plt.show()  

<A NAME="1"></A><FONT color = #00FF00><A HREF="match202-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        df_predicted = pd.DataFrame(prediction_list[-1], columns=['Predicted'])
        df_predicted.to_csv(f"{output_folder}/predicted_b.csv", index=False)


    elif q_part == "c":
        hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
</FONT>        f1_avg_train = []
        f1_avg_test = []
        l = [1, 2, 3, 4]
        for hidden_layer in hidden_layers:
            flag = False
            if hidden_layer == [512, 256, 128, 64]:
                flag = True
            X_train = X_trn.copy()
            y_train = y_trn.copy()
            X_test = X_tst.copy()
            y_test = y_tst.copy()

            t1 = time.time()
            model = NeuralNetwork(hidden_layers=hidden_layer, activation="sigmoid", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01)
            model.fit(X_train, y_train)
            t2 = time.time()

            print("Training time: ", (t2-t1))

            print ("Hidden layer: ", hidden_layer)
            print("Training report: ")
            y_train = y_train.argmax(axis = 1)
            y_train_predicted = model.predict(X_train)
            report = classification_report(y_train, y_train_predicted, output_dict=True)
            f1_avg_train.append(report['macro avg']['f1-score'])
            result = classification_report(y_train, y_train_predicted)
            print(result)

            print("Testing report: ")
            y_test = y_test.argmax(axis = 1)
            y_test_predicted = model.predict(X_test)
            report = classification_report(y_test, y_test_predicted, output_dict=True)
            f1_avg_test.append(report['macro avg']['f1-score'])
            result = classification_report(y_test, y_test_predicted)
            print(result)
            if flag:
                df_predicted = pd.DataFrame(y_test_predicted, columns=['Predicted'])
                df_predicted.to_csv(f"{output_folder}/predicted_c.csv", index=False)
        # F1-avg v/s network depth
        plt.figure()
        plt.plot(l, f1_avg_train, label="F1 averages for train set", marker='o')
        plt.plot(l, f1_avg_test, label="F1 averages for test set", marker='o')
        plt.xlabel("Network Depth")
        plt.ylabel("F1-avg")
        plt.title("(Part c) Neural Network: F1-Avg vs Network Depth")
        plt.legend()
        plt.grid(True)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match202-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.show() 

    elif q_part == "d":
        hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
</FONT>        f1_avg_train = []
        f1_avg_test = []
        l = [1, 2, 3, 4]

        for hidden_layer in hidden_layers:
            flag = False
            if hidden_layer == [512, 256, 128, 64]:
                flag = True
            X_train = X_trn.copy()
            y_train = y_trn.copy()
            X_test = X_tst.copy()
            y_test = y_tst.copy()
            t1 = time.time()
            model = NeuralNetwork(hidden_layers=hidden_layer, activation="sigmoid", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01, isAdaptive=True)
            # model = NeuralNetwork(threshold=1e-4, hidden_layers=hidden_layer, activation="sigmoid", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01, isAdaptive=True)
            model.fit(X_train, y_train)
            t2 = time.time()

            print("Training time: ", (t2-t1))
            
            print ("Hidden layer: ", hidden_layer)
            print("Training report: ")
            y_train = y_train.argmax(axis = 1)
            y_train_predicted = model.predict(X_train)
            report = classification_report(y_train, y_train_predicted, output_dict=True)
            f1_avg_train.append(report['macro avg']['f1-score'])
            result = classification_report(y_train, y_train_predicted)
            print(result)

            print("Testing report: ")
            y_test = y_test.argmax(axis = 1)
            y_test_predicted = model.predict(X_test)
            report = classification_report(y_test, y_test_predicted, output_dict=True)
            f1_avg_test.append(report['macro avg']['f1-score'])
            result = classification_report(y_test, y_test_predicted)
            print(result)
            if flag:
                df_predicted = pd.DataFrame(y_test_predicted, columns=['Predicted'])
                df_predicted.to_csv(f"{output_folder}/predicted_d.csv", index=False)
        # F1-avg v/s network depth
        plt.figure()
        plt.plot(l, f1_avg_train, label="F1 averages for train set", marker='o')
        plt.plot(l, f1_avg_test, label="F1 averages for test set", marker='o')
        plt.xlabel("Network Depth")
        plt.ylabel("F1-avg")
        plt.title("(Part d) Neural Network: F1-Avg vs Network Depth")
        plt.legend()
        plt.grid(True)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match202-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.show() 
    elif q_part == "e":
        hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
</FONT>        f1_avg_train = []
        f1_avg_test = []
        l = [1, 2, 3, 4]
        for hidden_layer in hidden_layers:
            flag = False
            if hidden_layer == [512, 256, 128, 64]:
                flag = True
            X_train = X_trn.copy()
            y_train = y_trn.copy()
            X_test = X_tst.copy()
            y_test = y_tst.copy()
            model = NeuralNetwork(hidden_layers=hidden_layer, activation="relu", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01, isAdaptive=True)
            # model = NeuralNetwork(threshold=1e-4, hidden_layers=hidden_layer, activation="relu", mini_batch=32, features=X_train.shape[1], classes= 43, learning_rate=0.01, isAdaptive=True)
            model.fit(X_train, y_train)

            print ("Hidden layer: ", hidden_layer)
            print("Training report: ")
            y_train = y_train.argmax(axis = 1)
            y_train_predicted = model.predict(X_train)
            report = classification_report(y_train, y_train_predicted, output_dict=True)
            f1_avg_train.append(report['macro avg']['f1-score'])
            result = classification_report(y_train, y_train_predicted)
            print(result)

            print("Testing report: ")
            y_test = y_test.argmax(axis = 1)
            y_test_predicted = model.predict(X_test)
            report = classification_report(y_test, y_test_predicted, output_dict=True)
            f1_avg_test.append(report['macro avg']['f1-score'])
            result = classification_report(y_test, y_test_predicted)
            print(result)
            if flag:
                df_predicted = pd.DataFrame(y_test_predicted, columns=['Predicted'])
                df_predicted.to_csv(f"{output_folder}/predicted_e.csv", index=False)
        # F1-avg v/s network depth
        plt.figure()
        plt.plot(l, f1_avg_train, label="F1 averages for train set", marker='o')
        plt.plot(l, f1_avg_test, label="F1 averages for test set", marker='o')
        plt.xlabel("Network Depth")
        plt.ylabel("F1-avg")
        plt.title("(Part e) Neural Network: F1-Avg vs Network Depth")
        plt.legend()
        plt.grid(True)
        plt.show()
    elif q_part == "f":
        hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
        f1_avg_train = []
        f1_avg_test = []
        l = [1, 2, 3, 4]
        for hidden_layer in hidden_layers:
            flag = False
            if hidden_layer == [512, 256, 128, 64]:
                flag = True
            X_train = X_trn.copy()
            y_train = y_trn.copy()
            X_test = X_tst.copy()
            y_test = y_tst.copy()
            
            # Convert y_train and y_test to integer labels if they are one-hot encoded.
            if y_train.ndim &gt; 1 and y_train.shape[1] &gt; 1:
                y_train_simple = np.argmax(y_train, axis=1)
            else:
                y_train_simple = y_train.ravel()
            if y_test.ndim &gt; 1 and y_test.shape[1] &gt; 1:
                y_test_simple = np.argmax(y_test, axis=1)
            else:
                y_test_simple = y_test.ravel()
            
            clf = MLPClassifier(hidden_layer_sizes=hidden_layer, activation='relu', solver='sgd', alpha=0, batch_size=32, learning_rate='invscaling', verbose = True, tol=1e-3, max_iter=300)
            clf.fit(X_train, y_train_simple)

            print ("Hidden layer: ", hidden_layer)
            print("Training report: ")
            y_train = y_train.argmax(axis = 1)
            y_train_predicted = clf.predict(X_train)
            report = classification_report(y_train, y_train_predicted, output_dict=True)
            f1_avg_train.append(report['macro avg']['f1-score'])
            result = classification_report(y_train, y_train_predicted)
            print(result)

            print("Testing report: ")
            y_test = y_test.argmax(axis = 1)
            y_test_predicted = clf.predict(X_test)
            report = classification_report(y_test_simple, y_test_predicted, output_dict=True)
            f1_avg_test.append(report['macro avg']['f1-score'])
            result = classification_report(y_test_simple, y_test_predicted)
            print(result)
            if flag:
                df_predicted = pd.DataFrame(y_test_predicted, columns=['Predicted'])
                df_predicted.to_csv(f"{output_folder}/predicted_f.csv", index=False)
        # F1-avg v/s network depth
        plt.figure()
        plt.plot(l, f1_avg_train, label="F1 averages for train set", marker='o')
        plt.plot(l, f1_avg_test, label="F1 averages for test set", marker='o')
        plt.xlabel("Network Depth")
        plt.ylabel("F1-avg")
        plt.title("(Part f) Neural Network: F1-Avg vs Network Depth")
        plt.legend()
        plt.grid(True)
        plt.show()        

if __name__ == "__main__":
    main()



</PRE>
</PRE>
</BODY>
</HTML>
