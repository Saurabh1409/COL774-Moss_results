<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_83PSN.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_83PSN.py<p><PRE>


import argparse
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import copy
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


class Node:
    def __init__(self, split_feature=None, threshold=None, is_leaf=False, prediction=None, depth=0, node_id=None):
        self.split_feature = split_feature
        self.threshold = threshold
        self.children = {}
        self.is_leaf = is_leaf
        self.prediction = prediction
        self.depth = depth
        self.subtree_size = 1 
        self.id = node_id

class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None
        self._next_id = 0  
        self.node_map = {}  

    def _get_next_id(self):
        result = self._next_id
        self._next_id += 1
        return result

    def fit(self, X: pd.DataFrame, y: pd.Series):
        self._next_id = 0
        self.node_map = {}
        self.root = self.build_tree(X, y, depth=0)
        self.compute_subtree_size(self.root)

    def build_tree(self, X: pd.DataFrame, y: pd.Series, depth: int) -&gt; Node:
        # Stopping conditions
        if len(y.unique()) == 1 or (self.max_depth is not None and depth &gt;= self.max_depth) or len(y) &lt; self.min_samples_split:
            leaf = Node(is_leaf=True, prediction=self.majority_class(y), depth=depth, node_id=self._get_next_id())
            leaf.subtree_size = 1
            self.node_map[leaf.id] = leaf
            return leaf

        best_feature, best_threshold, is_continuous = self.find_best_split(X, y)
        if best_feature is None:
            leaf = Node(is_leaf=True, prediction=self.majority_class(y), depth=depth, node_id=self._get_next_id())
            leaf.subtree_size = 1
            self.node_map[leaf.id] = leaf
            return leaf

        node = Node(split_feature=best_feature, threshold=best_threshold, prediction=self.majority_class(y), depth=depth, node_id=self._get_next_id())
        self.node_map[node.id] = node

        if is_continuous:
            left_mask = X[best_feature] &lt;= best_threshold
            right_mask = X[best_feature] &gt; best_threshold
            node.children['left'] = self.build_tree(X[left_mask], y[left_mask], depth + 1)
            node.children['right'] = self.build_tree(X[right_mask], y[right_mask], depth + 1)
        else:
            unique_vals = X[best_feature].unique()
            for val in unique_vals:
                mask = X[best_feature] == val
                if mask.sum() == 0:
                    child = Node(is_leaf=True, prediction=self.majority_class(y), depth=depth+1, node_id=self._get_next_id())
                    child.subtree_size = 1
                    node.children[val] = child
                    self.node_map[child.id] = child
                else:
                    child = self.build_tree(X[mask], y[mask], depth + 1)
                    node.children[val] = child
        node.subtree_size = 1 + sum(child.subtree_size for child in node.children.values())
        return node

    def compute_subtree_size(self, node: Node) -&gt; int:
        ### Recursively computes and caches the subtree size for a node.
        if node.is_leaf:
            node.subtree_size = 1
        else:
            node.subtree_size = 1 + sum(self.compute_subtree_size(child) for child in node.children.values())
        return node.subtree_size

    def count_nodes(self, node: Node = None) -&gt; int:
        if node is None:
            node = self.root
        return node.subtree_size

    def get_internal_nodes(self, node: Node = None, internal_nodes=None):
        if node is None:
            node = self.root
        if internal_nodes is None:
            internal_nodes = []
<A NAME="2"></A><FONT color = #0000FF><A HREF="match189-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        if not node.is_leaf:
            internal_nodes.append(node)
            for child in node.children.values():
                self.get_internal_nodes(child, internal_nodes)
        return internal_nodes

    def find_best_split(self, X: pd.DataFrame, y: pd.Series):
</FONT>        best_feature, best_threshold, best_gain = None, None, -np.inf
        is_continuous_best = True  
        n_samples = len(X)
        parent_entropy = self.entropy(y)
        for feature in X.columns:
            col = X[feature]
            if pd.api.types.is_numeric_dtype(col):
                threshold = col.median()
                left_mask = col &lt;= threshold
                right_mask = col &gt; threshold
                if left_mask.sum() == 0 or right_mask.sum() == 0:
                    continue
                gain = parent_entropy - (
                    (left_mask.sum() / n_samples) * self.entropy(y[left_mask]) +
                    (right_mask.sum() / n_samples) * self.entropy(y[right_mask])
                )
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
                    is_continuous_best = True
            else:
                unique_vals = col.unique()
                weighted_entropy = 0.0
                for val in unique_vals:
                    indices = col == val
                    weighted_entropy += (indices.sum() / n_samples) * self.entropy(y[indices])
                gain = parent_entropy - weighted_entropy
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = None
                    is_continuous_best = False
        return best_feature, best_threshold, is_continuous_best

    def entropy(self, y: pd.Series) -&gt; float:
        counts = y.value_counts()
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities))

    def majority_class(self, y: pd.Series):
        return y.value_counts().idxmax()

    def predict(self, X: pd.DataFrame):
        predictions = []
        for _, row in X.iterrows():
            predictions.append(self.traverse_tree(row, self.root))
        return predictions
    
    def compute_accuracy(self, X: pd.DataFrame, y: pd.Series) -&gt; float:
        predictions = self.predict(X)
        return np.mean(y == pd.Series(predictions))

    def traverse_tree(self, x: pd.Series, node: Node):
        if node.is_leaf:
            return node.prediction
        if node.threshold is not None:
            if x[node.split_feature] &lt;= node.threshold:
                return self.traverse_tree(x, node.children['left'])
            else:
                return self.traverse_tree(x, node.children['right'])
        else:
            val = x[node.split_feature]
            if val in node.children:
                return self.traverse_tree(x, node.children[val])
            else:
                return node.prediction

    
    def compute_decision_paths(self, X_val: pd.DataFrame, max_depth_estimate=55):
        n = X_val.shape[0]
        paths = -np.ones((n, max_depth_estimate), dtype=int)
        paths[:, 0] = self.root.id
        
        current_ids = np.full(n, self.root.id, dtype=int)
        
        for level in range(1, max_depth_estimate):
            current_nodes = [self.node_map[node_id] for node_id in current_ids]
            # For each sample, if current node is a leaf, we keep it.
            # Otherwise, evaluate the decision rule.
            new_ids = np.empty(n, dtype=int)
            for i, node in enumerate(current_nodes):
                if node.is_leaf:
                    new_ids[i] = node.id
                else:
                    # For continuous splits:
                    if node.threshold is not None:
                        val = X_val.iloc[i][node.split_feature]
                        if val &lt;= node.threshold:
                            new_ids[i] = node.children['left'].id
                        else:
                            new_ids[i] = node.children['right'].id
                    else:
                        # For categorical splits:
                        candidate_val = X_val.iloc[i][node.split_feature]
                        if candidate_val in node.children:
                            new_ids[i] = node.children[candidate_val].id
                        else:
                            new_ids[i] = node.id
            paths[:, level] = new_ids
            current_ids = new_ids
        return paths

    def vectorized_post_prune(self, X_train: pd.DataFrame, y_train: pd.Series,
                              X_val: pd.DataFrame, y_val: pd.Series,
                              X_test: pd.DataFrame, y_test: pd.Series,
                              threshold=0.0001):
        pruning_results = []
        current_val_acc = self.compute_accuracy(X_val, y_val)
        current_train_acc = self.compute_accuracy(X_train, y_train)
        current_test_acc = self.compute_accuracy(X_test, y_test)
        pruning_results.append((self.count_nodes(), current_train_acc, current_val_acc, current_test_acc))
        
        # Precompute decision paths for validation set.
        max_depth_estimate = 55 
        paths = self.compute_decision_paths(X_val, max_depth_estimate)
        iteration = 0

        improved = True
        while improved:
            candidates = self.get_internal_nodes()
            improvements = []
            counter = 0
            print(f"Iteration {iteration}: Evaluating {len(candidates)} internal nodes.")
            
            for candidate in candidates:
                counter += 1
                # Create a boolean mask for validation samples whose path contains candidate.id
                mask = np.any(paths == candidate.id, axis=1)
                # If no sample passes through candidate, improvement is 0.
                if np.sum(mask) == 0:
                    improvements.append(0.0)
                    continue
                
                current_preds = np.array(self.predict(X_val))
               
                modified_preds = current_preds.copy()
                modified_preds[mask] = candidate.prediction
                new_val_acc = np.mean(y_val.values == modified_preds)
                improvements.append(new_val_acc - current_val_acc)

                print(f"Candidate {counter}/{len(candidates)}: Improvement = {improvements[-1]:.6f}")

            improvements = np.array(improvements)
            best_improvement = np.max(improvements)
            if best_improvement &gt; threshold:
                
                ### find the nodes that have the best improvement, and select the node with the largest subtree size out of these
                best_candidates = [candidates[i] for i in range(len(candidates)) if improvements[i] == best_improvement]
                best_candidate = max(best_candidates, key=lambda c: c.subtree_size)

                best_candidate.children = {}
                best_candidate.is_leaf = True

                # Recompute accuracies and decision paths.
                current_val_acc = self.compute_accuracy(X_val, y_val)
                current_train_acc = self.compute_accuracy(X_train, y_train)
                current_test_acc = self.compute_accuracy(X_test, y_test)
                self.compute_subtree_size(self.root)
                pruning_results.append((self.count_nodes(), current_train_acc, current_val_acc, current_test_acc))
                paths = self.compute_decision_paths(X_val, max_depth_estimate)
                
                new_node_count = self.count_nodes()
                print(f"Iteration {iteration}: Permanently pruned node at depth {best_candidate.depth}.")

                print(f"Validation accuracy improved by {best_improvement:.6f}")
                print(f"New node count: {new_node_count}, New validation accuracy: {current_val_acc:.6f}\n")
                iteration += 1
            else:
                improved = False
        return pruning_results


# Helper Function to Save Predictions
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match189-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def save_predictions(predictions, output_folder, part):
    output_path = os.path.join(output_folder, f"prediction {part}.csv")
    pd.DataFrame({"prediction": predictions}).to_csv(output_path, index=False)
    print(f"Predictions saved to {output_path}")
</FONT>

def one_hot_encode_data(train_df, test_df, valid_df=None, target='income'):
    cat_cols = []
    for col in train_df.columns:
        if col == target:
            continue
        if train_df[col].dtype == 'object':
            cat_cols.append(col)
    
    X_train = train_df.drop(target, axis=1)
    y_train = train_df[target]
    X_train_encoded = pd.get_dummies(X_train, columns=cat_cols)
    
    # One-hot encode test data and align columns
    ## If there is no income column in the test_data, then we just set y_test to a dummy value.
    if target not in test_df.columns:
        y_test = pd.Series(["&lt;=50K"] * len(test_df))
        X_test = test_df
    else:
        y_test = test_df[target]
        X_test = test_df.drop(target, axis=1)

    X_test_encoded = pd.get_dummies(X_test, columns=cat_cols)
    X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=False)
    
    if valid_df is not None:
        X_val = valid_df.drop(target, axis=1)
        y_val = valid_df[target]
        X_val_encoded = pd.get_dummies(X_val, columns=cat_cols)
        X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=False)
    else:
        X_val_encoded, y_val = None, None
    
    return X_train_encoded, y_train, X_val_encoded, y_val, X_test_encoded, y_test


def run_part_a(train_data, test_data, max_depth):
    X_train = train_data.drop("income", axis=1)
    y_train = train_data["income"]
    
    if "income" not in test_data.columns:
        y_test = pd.Series(["&lt;=50K"] * len(test_data))
        X_test = test_data
    else:
        y_test = test_data["income"]
        X_test = test_data.drop("income", axis=1)

    tree = DecisionTree(max_depth=max_depth)
    tree.fit(X_train, y_train)

    y_train_pred = tree.predict(X_train)
    y_test_pred = tree.predict(X_test)
    train_acc = np.mean(y_train == pd.Series(y_train_pred))
    test_acc = np.mean(y_test == pd.Series(y_test_pred))
    return train_acc, test_acc, y_train_pred, y_test_pred

def run_part_b(train_data, test_data, max_depth):
    X_train_encoded, y_train, X_val_encoded, y_val, X_test_encoded, y_test = one_hot_encode_data(train_data, test_data, target='income')
    
    tree = DecisionTree(max_depth=max_depth)
    tree.fit(X_train_encoded, y_train)
    
    y_train_pred = tree.predict(X_train_encoded)
    y_test_pred = tree.predict(X_test_encoded)
    
    train_accuracy = np.mean(y_train == pd.Series(y_train_pred))
    test_accuracy = np.mean(y_test == pd.Series(y_test_pred))
  
    return train_accuracy, test_accuracy, y_train_pred, y_test_pred

def run_part_c(train_data, valid_data, test_data, max_depth):
    X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test = one_hot_encode_data(train_data, test_data, valid_data, target='income')
    
    tree = DecisionTree(max_depth=max_depth)
    tree.fit(X_train_enc, y_train)
    
    node_counts = []
    train_acc = []
    val_acc = []
    test_acc = []
    
    pruning_results = tree.vectorized_post_prune(X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test)
    
    for result in pruning_results:
        node_counts.append(result[0])
        train_acc.append(result[1])
        val_acc.append(result[2])
        test_acc.append(result[3])
    
    y_train_pred = tree.predict(X_train_enc)
    y_test_pred = tree.predict(X_test_enc)
    return node_counts, train_acc, val_acc, test_acc, y_train_pred, y_test_pred

def run_part_d(train_df, valid_df, test_df):
    X_train, y_train, X_val, y_val, X_test, y_test = one_hot_encode_data(train_df, test_df, valid_df, target='income')
    results = []
    
    # Experiment (i): vary max_depth.
    max_depths = [25, 35, 45, 55]
    print("Running Sub-part (i): Varying max_depth")
    for depth in max_depths:
        clf = DecisionTreeClassifier(criterion="entropy", max_depth=depth, random_state=42)
        clf.fit(X_train, y_train)
        y_train_pred = clf.predict(X_train)
        y_val_pred = clf.predict(X_val)
        y_test_pred = clf.predict(X_test)
        
        train_acc = accuracy_score(y_train, y_train_pred)
        val_acc = accuracy_score(y_val, y_val_pred)
        test_acc = accuracy_score(y_test, y_test_pred)
        
        results.append({
            'method': 'max_depth',
            'parameter_value': depth,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'test_acc': test_acc,
            'y_test_pred': y_test_pred,
        })
        print(f"Max Depth: {depth} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}")
    
    # Experiment (ii): vary ccp_alpha using a fully grown tree.
    ccp_alphas = [0.001, 0.01, 0.1, 0.2]
    print("\nRunning Sub-part (ii): Varying ccp_alpha")
    clf_full = DecisionTreeClassifier(criterion="entropy", random_state=42)
    clf_full.fit(X_train, y_train)
    for alpha in ccp_alphas:
        clf_pruned = DecisionTreeClassifier(criterion="entropy", random_state=42, ccp_alpha=alpha)
        clf_pruned.fit(X_train, y_train)
        y_train_pred = clf_pruned.predict(X_train)
        y_val_pred = clf_pruned.predict(X_val)
        y_test_pred = clf_pruned.predict(X_test)
        
        train_acc = accuracy_score(y_train, y_train_pred)
        val_acc = accuracy_score(y_val, y_val_pred)
        test_acc = accuracy_score(y_test, y_test_pred)
        
        results.append({
            'method': 'ccp_alpha',
            'parameter_value': alpha,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'test_acc': test_acc,
            'y_test_pred': y_test_pred,
        })
        print(f"ccp_alpha: {alpha} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}")
    
    return results

def run_part_e(train_df, valid_df, test_df):
    X_train, y_train, X_val, y_val, X_test, y_test = one_hot_encode_data(train_df, test_df, valid_df, target='income')
    results = []
    
    n_estimators_list = [50, 150, 250, 350]
    max_features_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
    min_samples_split_list = [2, 4, 6, 8, 10]
    
    print("Running grid search for Random Forest parameters")
    for n_est in n_estimators_list:
        for max_feat in max_features_list:
            for min_split in min_samples_split_list:
                clf = RandomForestClassifier(criterion="entropy",
                                             n_estimators=n_est,
                                             max_features=max_feat,
                                             min_samples_split=min_split,
                                             oob_score=True,
                                             random_state=42,
                                             n_jobs=-1)
                clf.fit(X_train, y_train)
                y_train_pred = clf.predict(X_train)
                y_val_pred = clf.predict(X_val)
                y_test_pred = clf.predict(X_test)
                
                train_acc = accuracy_score(y_train, y_train_pred)
                val_acc = accuracy_score(y_val, y_val_pred)
                test_acc = accuracy_score(y_test, y_test_pred)
                oob_acc = clf.oob_score_
                
                results.append({
                    'method': 'random_forest',
                    'n_estimators': n_est,
                    'max_features': max_feat,
                    'min_samples_split': min_split,
                    'train_acc': train_acc,
                    'oob_acc': oob_acc,
                    'val_acc': val_acc,
                    'test_acc': test_acc,
                    'y_test_pred': y_test_pred
                })
                print(f"n_estimators: {n_est}, max_features: {max_feat:.2f}, min_samples_split: {min_split} | "
                      f"Train: {train_acc:.4f}, OOB: {oob_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}")
    return results


<A NAME="4"></A><FONT color = #FF00FF><A HREF="match189-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def save_predictions(predictions, output_folder, part):
    output_path = os.path.join(output_folder, f"prediction_{part}.csv")
    pd.DataFrame({"prediction": predictions}).to_csv(output_path, index=False)
    print(f"Predictions saved to {output_path}")
</FONT>

def main():
    parser = argparse.ArgumentParser(description='Decision Tree Implementation for Assignment 3')
    parser.add_argument('train_data_path', type=str)
    parser.add_argument('validation_data_path', type=str)
    parser.add_argument('test_data_path', type=str)
    parser.add_argument('output_folder_path', type=str)
    parser.add_argument('question_part', type=str, choices=['a', 'b', 'c', 'd', 'e'])
    args = parser.parse_args()
    
    train_data = pd.read_csv(args.train_data_path)
    valid_data = pd.read_csv(args.validation_data_path)
    test_data = pd.read_csv(args.test_data_path)
    
    if args.question_part == 'a':
        _, _, _, predictions = run_part_a(train_data, test_data, max_depth=20)
        save_predictions(predictions, args.output_folder_path, 'a')
    elif args.question_part == 'b':
        _, _, _, predictions = run_part_b(train_data, test_data, max_depth=55)
        save_predictions(predictions, args.output_folder_path, 'b')
    elif args.question_part == 'c':
        _, _, _, _, _, predictions = run_part_c(train_data, valid_data, test_data, max_depth=55)
        save_predictions(predictions, args.output_folder_path, 'c')
    elif args.question_part == 'd':
        results = run_part_d(train_data, valid_data, test_data)
        best_model = max(results, key=lambda r: r['val_acc'])
        save_predictions(best_model['y_test_pred'], args.output_folder_path, 'd')
    elif args.question_part == 'e':
        results = run_part_e(train_data, valid_data, test_data)
        best_model = max(results, key=lambda r: r['val_acc'])
        save_predictions(best_model['y_test_pred'], args.output_folder_path, 'e')
    else:
        print("Invalid question part specified.")

if __name__ == "__main__":
    main()




#!/usr/bin/env python
# coding: utf-8

# # Part 1: Decision Trees

# ## Loading Data

# In[1]:


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys
sys.path.append(os.path.join(os.getcwd(), "Q1"))
import decision_tree


# In[2]:


current_dir = os.getcwd()
data_folder = os.path.join(current_dir, "data", "Q1")

train_file = os.path.join(data_folder, "train.csv")
test_file = os.path.join(data_folder, "test.csv")
valid_file = os.path.join(data_folder, "valid.csv") 


# In[3]:


train_data = pd.read_csv(train_file)
test_data = pd.read_csv(test_file)
valid_data = pd.read_csv(valid_file)


# ## 1.1: Decision Tree Construction

# ### 1.1.1: Training Model and Computing Accuracy

# In[23]:


depths = [5, 10, 15, 20]

train_accuracies = []
test_accuracies = []

for depth in depths:
    train_accuracy, test_accuracy, _, _ = decision_tree.run_part_a(train_data, test_data, depth)
    train_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)
    print(f"Max Depth: {depth} | Train Accuracy: {train_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}")


# ### 1.1.2: Plotting Accuracy vs Depth

# In[24]:


plt.figure(figsize=(8, 6))
plt.plot(depths, train_accuracies, marker="o", label="Train Accuracy")
plt.plot(depths, test_accuracies, marker="s", label="Test Accuracy")
plt.xlabel("Maximum Depth")
plt.ylabel("Accuracy")
plt.title("Decision Tree Accuracy vs. Maximum Depth")
plt.legend()
plt.grid(True)
plt.show()


# ## 1.2: One-Hot Encoding

# ### 1.2.1: Training Model and Computing Accuracy

# In[25]:


depths = [25, 35, 45, 55]

train_accuracies = []
test_accuracies = []

for depth in depths:
    train_accuracy, test_accuracy, _, _ = decision_tree.run_part_b(train_data, test_data, depth)
    train_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)
    print(f"Max Depth: {depth} | Train Accuracy: {train_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}")


# ### 1.2.2: Plotting Accuracy vs Depth

# In[26]:


plt.figure(figsize=(8, 6))
plt.plot(depths, train_accuracies, marker="o", label="Train Accuracy")
plt.plot(depths, test_accuracies, marker="s", label="Test Accuracy")
plt.xlabel("Maximum Depth")
plt.ylabel("Accuracy")
plt.title("Decision Tree Accuracy vs. Maximum Depth")
plt.legend()
plt.grid(True)
plt.show()


# ## 1.3: Post-Pruning

# ### 1.3.1: Training Model and Computing Accuracy

# In[ ]:


depths = [25, 35, 45, 55]

final_train_accuracies = []
final_test_accuracies = []
final_val_accuracies = []
final_node_counts = []

init_train_accuracies = []
init_test_accuracies = []
init_val_accuracies = []
init_node_counts = []

pruning_results = {}

for depth in depths:
    print(f"Running with max depth: {depth}")
    node_counts, train_acc, val_acc, test_acc, _, predictions = decision_tree.run_part_c(train_data, valid_data, test_data, max_depth=depth)
    pruning_results[depth] = (node_counts, train_acc, val_acc, test_acc)

    init_node_counts.append(node_counts[0])
    init_train_accuracies.append(train_acc[0])
    init_val_accuracies.append(val_acc[0])
    init_test_accuracies.append(test_acc[0])

    final_node_counts.append(node_counts[-1])
    final_train_accuracies.append(train_acc[-1])
    final_val_accuracies.append(val_acc[-1])
    final_test_accuracies.append(test_acc[-1])

    print(f"Initial Node Count: {node_counts[0]} | Initial Train Accuracy: {train_acc[0]:.4f} | Initial Validation Accuracy: {val_acc[0]:.4f} | Initial Test Accuracy: {test_acc[0]:.4f}")
    print(f"Final Node Count: {node_counts[-1]} | Final Train Accuracy: {train_acc[-1]:.4f} | Final Validation Accuracy: {val_acc[-1]:.4f} | Final Test Accuracy: {test_acc[-1]:.4f}\n")



    


# In[5]:


for i in range(len(depths)):
    depth = depths[i]
    print(f"Max Depth: {depth}")
    print(f"Initial Node Count: {init_node_counts[i]} | Initial Train Accuracy: {init_train_accuracies[i]:.4f} | Initial Validation Accuracy: {init_val_accuracies[i]:.4f} | Initial Test Accuracy: {init_test_accuracies[i]:.4f}")
    print(f"Final Node Count: {final_node_counts[i]} | Final Train Accuracy: {final_train_accuracies[i]:.4f} | Final Validation Accuracy: {final_val_accuracies[i]:.4f} | Final Test Accuracy: {final_test_accuracies[i]:.4f}\n")


# In[ ]:


import matplotlib.pyplot as plt

for i, d in enumerate(depths):
    node_counts, train_acc, val_acc, test_acc = pruning_results[d]
    
    plt.figure(figsize=(8, 6))
    plt.plot(node_counts, train_acc, marker="o", label="Train Accuracy")
    plt.plot(node_counts, val_acc, marker="s", label="Validation Accuracy")
    plt.plot(node_counts, test_acc, marker="^", label="Test Accuracy")
    plt.title(f"Initial Max Depth: {d}")
    plt.xlabel("Number of Nodes (decreasing)")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    plt.gca().invert_xaxis()  
    plt.show()


# In[ ]:


for depth, (node_counts, train_acc, val_acc, test_acc) in pruning_results.items():
    print(f"Max Depth: {depth}")
    print("Node Counts:", node_counts)
    print("Train Accuracies:", train_acc)
    print("Validation Accuracies:", val_acc)
    print("Test Accuracies:", test_acc)
    print()


# ## 1.4: Decision Tree Sci-Kit Learn

# ### 1.4.1: Max Depth

# In[ ]:


results = decision_tree.run_part_d(train_data, valid_data, test_data)


max_depth_results = [r for r in results if r['method'] == 'max_depth']
ccp_alpha_results = [r for r in results if r['method'] == 'ccp_alpha']


plt.figure(figsize=(8,6))
# Extract parameters and accuracies for max_depth models.
md_params = [r['parameter_value'] for r in max_depth_results]
train_acc_md = [r['train_acc'] for r in max_depth_results]
val_acc_md   = [r['val_acc'] for r in max_depth_results]
test_acc_md  = [r['test_acc'] for r in max_depth_results]

plt.plot(md_params, train_acc_md, marker='o', label='Train Accuracy')
plt.plot(md_params, val_acc_md, marker='s', label='Validation Accuracy')
plt.plot(md_params, test_acc_md, marker='^', label='Test Accuracy')
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.title("Model Accuracies vs. Max Depth (Sub-part i)")
plt.legend()
plt.grid(True)
plt.show()


# ### 1.4.2: ccp_alpha

# In[ ]:



plt.figure(figsize=(8,6))

# Extract parameters and accuracies for ccp_alpha models.
alpha_params = [r['parameter_value'] for r in ccp_alpha_results]
train_acc_alpha = [r['train_acc'] for r in ccp_alpha_results]
val_acc_alpha   = [r['val_acc'] for r in ccp_alpha_results]
test_acc_alpha  = [r['test_acc'] for r in ccp_alpha_results]

plt.plot(alpha_params, train_acc_alpha, marker='o', label='Train Accuracy')
plt.plot(alpha_params, val_acc_alpha, marker='s', label='Validation Accuracy')
plt.plot(alpha_params, test_acc_alpha, marker='^', label='Test Accuracy')
plt.xlabel("ccp_alpha")
plt.ylabel("Accuracy")
plt.title("Model Accuracies vs. ccp_alpha (Sub-part ii)")
plt.legend()
plt.grid(True)
plt.show()


# ### 1.4.3: Overall Best Model

# In[ ]:


## Identify and report the best model for each method. 4dp
best_md_model = max(max_depth_results, key=lambda x: x['val_acc'])
best_ccp_model = max(ccp_alpha_results, key=lambda x: x['val_acc'])
print("Best Max Depth Model:")
print(f"Max Depth: {best_md_model['parameter_value']}")
print(f"Train Accuracy: {best_md_model['train_acc']:.4f}")
print(f"Validation Accuracy: {best_md_model['val_acc']:.4f}")
print(f"Test Accuracy: {best_md_model['test_acc']:.4f}")
print("\nBest ccp_alpha Model:")
print(f"ccp_alpha: {best_ccp_model['parameter_value']}")
print(f"Train Accuracy: {best_ccp_model['train_acc']:.4f}")
print(f"Validation Accuracy: {best_ccp_model['val_acc']:.4f}")
print(f"Test Accuracy: {best_ccp_model['test_acc']:.4f}")



best_model = max(results, key=lambda r: r['val_acc'])
print("\nBest Overall Model:")
print(f"Method: {best_model['method']}")
print(f"Parameter Value: {best_model['parameter_value']}")
print(f"Train Accuracy: {best_model['train_acc']:.4f}")
print(f"Validation Accuracy: {best_model['val_acc']:.4f}")
print(f"Test Accuracy: {best_model['test_acc']:.4f}")


# ## 1.5: Random Forest

# ### 1.5.1: Training Model

# In[ ]:


print("Running run_part_e (Random Forest grid search)...")
rf_results = decision_tree.run_part_e(train_data, valid_data, test_data)

results_df = pd.DataFrame(rf_results)


# ### 1.5.2: Outputting Results

# In[ ]:


best_model = results_df.loc[results_df['oob_acc'].idxmax()]
print("==== Best Overall Model ====")
print(f"n_estimators: {best_model['n_estimators']}, max_features: {best_model['max_features']}, min_samples_split: {best_model['min_samples_split']}")
print(f"Train Accuracy: {best_model['train_acc']:.4f}")
print(f"OOB Accuracy: {best_model['oob_acc']:.4f}")
print(f"Validation Accuracy: {best_model['val_acc']:.4f}")
print(f"Test Accuracy: {best_model['test_acc']:.4f}")





import argparse
import os
import sys
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.neural_network import MLPClassifier

def sigmoid(z):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match189-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    return 1.0 / (1.0 + np.exp(-z))

def d_sigmoid(z):
    s = sigmoid(z)
    return s * (1 - s)

def relu(z):
    return np.maximum(0, z)

def d_relu(z):
    return (z &gt; 0).astype(float)
</FONT>
def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)


def to_one_hot(y, num_classes):
    m = y.shape[0]
    one_hot = np.zeros((m, num_classes))
    one_hot[np.arange(m), y.astype(int)] = 1
    return one_hot


def load_train_data(train_path, img_size=(28,28)):
    valid_extensions = ('.png', '.jpg', '.jpeg')
    images = []
    labels = []
    for folder in sorted(os.listdir(train_path)):
        folder_path = os.path.join(train_path, folder)
        if os.path.isdir(folder_path):
            try:
                # Convert folder name to integer and add 1.
                label = int(folder)
            except Exception:
                continue
            for filename in sorted(os.listdir(folder_path)):
                if filename.lower().endswith(valid_extensions):
                    file_path = os.path.join(folder_path, filename)
                    try:
                        with Image.open(file_path) as img:
                            img = img.convert("RGB")
                            img = img.resize(img_size)
                            img_array = np.array(img).astype(np.float32) / 255.0
                            images.append(img_array.flatten())
                            labels.append(label)
                    except Exception as e:
                        print(f"Error loading image {file_path}: {e}")
    X_train = np.stack(images, axis=0)
    y_train = np.array(labels)
    return X_train, y_train

def load_test_data(test_path, img_size=(28,28)):
    valid_extensions = ('.png', '.jpg', '.jpeg')
    images = []
    for filename in sorted(os.listdir(test_path)):
        if not filename.lower().endswith(valid_extensions):
            continue
        file_path = os.path.join(test_path, filename)
        try:
            with Image.open(file_path) as img:
                img = img.convert("RGB")
                img = img.resize(img_size)
                img_array = np.array(img).astype(np.float32) / 255.0
                images.append(img_array.flatten())
        except Exception as e:
            print(f"Error loading image {file_path}: {e}")
    X_test = np.stack(images, axis=0)
   
    return X_test


def load_data(train_path, test_path, img_size=(28,28)):
    X_train, y_train = load_train_data(train_path, img_size=img_size)
    X_test = load_test_data(test_path, img_size=img_size)
    return X_train, y_train, X_test


class NeuralNetwork:
    def __init__(self, input_dim, hidden_layers, output_dim,
                 learning_rate=0.01, activation='sigmoid',
                 use_adaptive_lr=False, epochs=500, batch_size=32, tol=1e-6):
        self.layers = [input_dim] + hidden_layers + [output_dim]
        self.learning_rate0 = learning_rate
        self.learning_rate = learning_rate
        self.use_adaptive_lr = use_adaptive_lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.activation = activation
        self.tol = tol
        self.weights = []
        self.biases = []
        for i in range(len(self.layers) - 1):
            ## Initialize wweights randomly with He initialization and biases to zero
            w = np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(2. / self.layers[i])
            b = np.zeros((1, self.layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
        for i in range(len(self.weights) - 1):
            z = activations[-1].dot(self.weights[i]) + self.biases[i]
            zs.append(z)
            a = sigmoid(z) if self.activation == 'sigmoid' else relu(z)
            activations.append(a)
        z = activations[-1].dot(self.weights[-1]) + self.biases[-1]
        zs.append(z)
        a = softmax(z)
        activations.append(a)
        return activations, zs

    def compute_loss(self, X, Y):
        m = X.shape[0]
        activations, _ = self.forward(X)
        output = activations[-1]
        epsilon = 1e-12
        ## Including epsilon to avoid log(0)
        loss = -np.sum(Y * np.log(output + epsilon)) / m
        return loss

    def backward(self, activations, zs, Y):
        grads_w = [None] * len(self.weights)
        grads_b = [None] * len(self.biases)
        m = Y.shape[0]
        delta = activations[-1] - Y
        grads_w[-1] = activations[-2].T.dot(delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m
        for l in range(2, len(self.layers)):
            z = zs[-l]
            d_act = d_sigmoid(z) if self.activation == 'sigmoid' else d_relu(z)
            delta = delta.dot(self.weights[-l+1].T) * d_act
            grads_w[-l] = activations[-l-1].T.dot(delta) / m
            grads_b[-l] = np.sum(delta, axis=0, keepdims=True) / m
        return grads_w, grads_b

    def update_params(self, grads_w, grads_b):
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * grads_w[i]
            self.biases[i] -= self.learning_rate * grads_b[i]

    def train(self, X, Y):
        m = X.shape[0]
        previous_loss = float('inf')
        for epoch in range(1, self.epochs + 1):
            if self.use_adaptive_lr:
                self.learning_rate = self.learning_rate0 / np.sqrt(epoch)
            indices = np.arange(m)
            np.random.shuffle(indices)
            X = X[indices]
            Y = Y[indices]
            for start in range(0, m, self.batch_size):
                end = start + self.batch_size
                X_batch = X[start:end]
                Y_batch = Y[start:end]
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(activations, zs, Y_batch)
                self.update_params(grads_w, grads_b)
            current_loss = self.compute_loss(X, Y)
            print(f"Epoch {epoch}/{self.epochs}, Loss: {current_loss:.6f}")
            if abs(previous_loss - current_loss) &lt; self.tol:
                print("Convergence reached. Stopping early.")
                break
            previous_loss = current_loss


    def predict(self, X):
        activations, _ = self.forward(X)
        predictions = np.argmax(activations[-1], axis=1)
        return predictions


def train_part_b(train_path, test_path, hidden_layers, epochs=500, batch_size=32, tol=1e-6):
    print("Loading data...")
    X_train, y_train, X_test = load_data(train_path, test_path, img_size=(28,28))
    input_dim = X_train.shape[1]
    output_dim = 43
    Y_train = to_one_hot(y_train, output_dim)
    print("Starting training...")
    nn = NeuralNetwork(input_dim=input_dim, hidden_layers=hidden_layers, output_dim=output_dim,
                       learning_rate=0.01, activation='sigmoid', use_adaptive_lr=False,
                       epochs=epochs, batch_size=batch_size, tol=tol)
    nn.train(X_train, Y_train)
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    return (pd.DataFrame({"train_prediction": train_preds}),
            pd.DataFrame({"prediction": test_preds}))

def train_part_c(train_path, test_path, hidden_layers, epochs=200, batch_size=32, tol=1e-6):
    X_train, y_train, X_test = load_data(train_path, test_path, img_size=(28,28))
    input_dim = X_train.shape[1]
    output_dim = 43
    Y_train = to_one_hot(y_train, output_dim)
    nn = NeuralNetwork(input_dim=input_dim, hidden_layers=hidden_layers, output_dim=output_dim,
                       learning_rate=0.01, activation='sigmoid', use_adaptive_lr=False,
                       epochs=epochs, batch_size=batch_size, tol=tol)
    nn.train(X_train, Y_train)
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    return (pd.DataFrame({"train_prediction": train_preds}),
            pd.DataFrame({"prediction": test_preds}))

def train_part_d(train_path, test_path, hidden_layers, epochs=200, batch_size=32, tol=1e-6):
    X_train, y_train, X_test = load_data(train_path, test_path, img_size=(28,28))
    input_dim = X_train.shape[1]
    output_dim = 43
    Y_train = to_one_hot(y_train, output_dim)
    nn = NeuralNetwork(input_dim=input_dim, hidden_layers=hidden_layers, output_dim=output_dim,
                       learning_rate=0.01, activation='sigmoid', use_adaptive_lr=True,
                       epochs=epochs, batch_size=batch_size, tol=tol)
    nn.train(X_train, Y_train)
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    return (pd.DataFrame({"train_prediction": train_preds}),
            pd.DataFrame({"prediction": test_preds}))

def train_part_e(train_path, test_path, hidden_layers, epochs=200, batch_size=32, tol=1e-6):
    X_train, y_train, X_test = load_data(train_path, test_path, img_size=(28,28))
    input_dim = X_train.shape[1]
    output_dim = 43
    Y_train = to_one_hot(y_train, output_dim)
    nn = NeuralNetwork(input_dim=input_dim, hidden_layers=hidden_layers, output_dim=output_dim,
                       learning_rate=0.01, activation='relu', use_adaptive_lr=True,
                       epochs=epochs, batch_size=batch_size, tol=tol)
    nn.train(X_train, Y_train)
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    return (pd.DataFrame({"train_prediction": train_preds}),
            pd.DataFrame({"prediction": test_preds}))

def train_part_f(train_path, test_path, hidden_layers):
    X_train, y_train, X_test = load_data(train_path, test_path, img_size=(28,28))
    mlp = MLPClassifier(hidden_layer_sizes=tuple(hidden_layers),
                        activation='relu',
                        solver='sgd',
                        alpha=0,
                        batch_size=32,
                        learning_rate='invscaling',
                        tol=1e-6,
                        verbose=True)
                        
    mlp.fit(X_train, y_train)
    train_preds = mlp.predict(X_train)
    test_preds = mlp.predict(X_test)
    return (pd.DataFrame({"train_prediction": train_preds}),
            pd.DataFrame({"prediction": test_preds}))


def main():
    # Expected command:
    # python neural_network.py &lt;train_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;
<A NAME="0"></A><FONT color = #FF0000><A HREF="match189-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    parser = argparse.ArgumentParser(
        description="Neural Network Assignment: Generate Predictions for Parts b to f"
    )
    parser.add_argument("train_data_path", type=str,
                        help="Path to the training images folder")
    parser.add_argument("test_data_path", type=str,
                        help="Path to the test images folder")
    parser.add_argument("output_folder_path", type=str,
                        help="Folder where the prediction CSV file will be saved")
    parser.add_argument("question_part", type=str, choices=['b', 'c', 'd', 'e', 'f'],
                        help="Question part to run: b, c, d, e, or f")
    args = parser.parse_args()
</FONT>
    if args.question_part == 'b':
        default_hidden_layers = [100]
    else:
        default_hidden_layers = [512, 256, 128, 64]
    hidden_layers = default_hidden_layers

    if args.question_part == "b":
        train_preds_df, test_preds_df = train_part_b(args.train_data_path, args.test_data_path,
                                                     hidden_layers=hidden_layers, epochs=500, batch_size=32, tol=1e-6)
    elif args.question_part == "c":
        train_preds_df, test_preds_df = train_part_c(args.train_data_path, args.test_data_path,
                                                     hidden_layers=hidden_layers, epochs=250, batch_size=32, tol=1e-6)
    elif args.question_part == "d":
        train_preds_df, test_preds_df = train_part_d(args.train_data_path, args.test_data_path,
                                                     hidden_layers=hidden_layers, epochs=250, batch_size=32, tol=1e-6)
    elif args.question_part == "e":
        train_preds_df, test_preds_df = train_part_e(args.train_data_path, args.test_data_path,
                                                     hidden_layers=hidden_layers, epochs=250, batch_size=32, tol=1e-6)
    elif args.question_part == "f":
        train_preds_df, test_preds_df = train_part_f(args.train_data_path, args.test_data_path,
                                                     hidden_layers=hidden_layers)
    else:
        print("Invalid question part. Choose from 'b', 'c', 'd', 'e', or 'f'.")
        sys.exit(1)

    if not os.path.exists(args.output_folder_path):
        os.makedirs(args.output_folder_path)
        
    output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
    try:
        test_preds_df.to_csv(output_file, index=False)
        print(f"Test predictions saved to {output_file}")
    except Exception as e:
        print(f"Error saving predictions: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()




#!/usr/bin/env python
# coding: utf-8

# # Part 2: Neural Networks

# In[2]:


import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import neural_network


# ## 2.1: Single Hidden Layer

# ### 2.1.1: Loading Data

# In[6]:


current_dir = os.getcwd()
train_data_path = os.path.join(current_dir, "data", "Q2", "train")
test_data_path = os.path.join(current_dir, "data", "Q2", "test")
data_path = os.path.join(current_dir, "data", "Q2")

print("Train data folder:", train_data_path)
print("Test data folder:", test_data_path)
print("Data folder:", data_path)


# ### 2.1.2: Loading Test Labels

# In[ ]:


test_labels_csv = os.path.join(data_path, "test_labels.csv")
if os.path.exists(test_labels_csv):
    df_test_labels = pd.read_csv(test_labels_csv)
    
    true_test_labels = df_test_labels.sort_values("image")["label"].values
    print("Test labels loaded for analysis.")
else:
    true_test_labels = None
    print("Test labels CSV not found; true test labels set to None.")

y_test = true_test_labels


# In[8]:


X_train, y_train, X_test = neural_network.load_data(train_data_path, test_data_path)


# ### 2.1.3: Training Models

# In[ ]:


hidden_units_options = [1, 5, 10, 50, 100]
epochs = 500      
batch_size = 32     
learning_rate = 0.01  
tol = 1e-6        


results = {}


for units in hidden_units_options:
    print(f"Training with a single hidden layer of {units} unit(s).")
  
    (train_preds_df, test_preds_df) = neural_network.train_part_b(
        train_data_path, test_data_path,
        hidden_layers=[units],
        epochs=epochs,
        batch_size=batch_size,
        tol=tol
    )
    
 
    train_preds = train_preds_df["train_prediction"].values
    test_preds = test_preds_df["test_prediction"].values
    
    train_report = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
    
    if y_test is not None:
        test_report = classification_report(y_test, test_preds, output_dict=True, zero_division=0)
    else:
        test_report = {"error": "True test labels not available"}
    
    results[units] = {"train_report": train_report, "test_report": test_report}
    
print("All models trained and reports stored.")


# ### 2.1.4: Outputting Classification Reports

# In[ ]:


print("Summary of Classification Reports:")
for units in hidden_units_options:
    print(f"\nHidden Units: {units}")
    print("Training Classification Report:")
   
    train_report_df = pd.DataFrame(results[units]["train_report"]).transpose()
    print(train_report_df)
    
    if "error" not in results[units]["test_report"]:
        print("Test Classification Report:")
        test_report_df = pd.DataFrame(results[units]["test_report"]).transpose()
        print(test_report_df)
    else:
        print("Test Classification Report: True test labels not available.")
    print("-"*60)


# ### 2.1.5: Summarizing Metrics

# In[ ]:


## Put the weighted average precision, recall, and f1-score and accuracy for each model into a table and print
train_metrics = []
test_metrics = []
for units in hidden_units_options:
    train_report = results[units]["train_report"]
    test_report = results[units]["test_report"]
    
    train_metrics.append({
        "Hidden Units": units,
        "Precision": train_report["weighted avg"]["precision"],
        "Recall": train_report["weighted avg"]["recall"],
        "F1-Score": train_report["weighted avg"]["f1-score"],
        "Accuracy": train_report["accuracy"]
    })
    
    if "error" not in test_report:
        test_metrics.append({
            "Hidden Units": units,
            "Precision": test_report["weighted avg"]["precision"],
            "Recall": test_report["weighted avg"]["recall"],
            "F1-Score": test_report["weighted avg"]["f1-score"],
            "Accuracy": test_report["accuracy"]
        })

train_metrics_df = pd.DataFrame(train_metrics)
test_metrics_df = pd.DataFrame(test_metrics)
print("\nTraining Metrics:")
print(train_metrics_df.to_string(index=False))
print("\nTest Metrics:")
print(test_metrics_df.to_string(index=False))


# ### 2.1.6: Plotting F1 score vs Hidden Units

# In[7]:


# Plot weighted average F1 scores for each hidden units configuration.
avg_f1_train = []
avg_f1_test = []
for units in hidden_units_options:
    train_f1 = results[units]["train_report"]["weighted avg"]["f1-score"]
    test_f1 = results[units]["test_report"].get("weighted avg", {}).get("f1-score", None)
    avg_f1_train.append(train_f1)
    avg_f1_test.append(test_f1)

plt.figure(figsize=(8, 6))
plt.plot(hidden_units_options, avg_f1_train, marker='o', label='Train Weighted Avg F1')
if all(f1 is not None for f1 in avg_f1_test):
    plt.plot(hidden_units_options, avg_f1_test, marker='o', label='Test Weighted Avg F1')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Weighted Average F1 Score')
plt.title('Effect of Hidden Units on F1 Score (Part b)')
plt.legend()
plt.grid(True)
plt.show()


# ## 2.2: Multiple Hidden Layers

# ### 2.2.1: Training Models

# In[ ]:


architectures = [
    [512],
    [512, 256],
    [512, 256, 128],
    [512, 256, 128, 64]
]

print("Architectures to experiment with:")
for arch in architectures:
    print(arch)


epochs = 100         
batch_size = 32    
learning_rate = 0.01  
tol = 1e-6        

results = {}

for arch in architectures:
    print(f"\nTraining with architecture: {arch}")
 
    (train_preds_df, test_preds_df) = neural_network.train_part_c(
        train_data_path, 
        test_data_path, 
        hidden_layers=arch, 
        epochs=epochs, 
        batch_size=batch_size, 
        tol=tol
    )
    
    train_preds = train_preds_df["train_prediction"].values
    test_preds = test_preds_df["test_prediction"].values
    
    train_report = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
    
    if y_test is not None:
        test_report = classification_report(y_test, test_preds, output_dict=True,zero_division=0)
    else:
        test_report = {"error": "True test labels not available"}
        print("Warning: True test labels not available for evaluation.")

    results[str(arch)] = {"train_report": train_report, "test_report": test_report}
    
    print(f"Finished training with architecture: {arch}")
    
print("All architectures have been trained and reports stored.")


# ### 2.2.2: Outputting Classification Reports

# In[13]:


print("Summary of Classification Reports for Different Architectures:")
for arch in architectures:
    arch_key = str(arch)
    print(f"\nArchitecture: {arch}")
    print("Training Classification Report:")
    train_report_df = pd.DataFrame(results[arch_key]["train_report"]).transpose()
    print(train_report_df)
    
    if "error" not in results[arch_key]["test_report"]:
        print("Test Classification Report:")
        test_report_df = pd.DataFrame(results[arch_key]["test_report"]).transpose()
        print(test_report_df)
    else:
        print("Test Classification Report: True test labels not available.")
    print("-" * 60)


# ### 2.2.3: Summarizing Metrics

# In[ ]:


## Summarize weighted average precision, recall, f1-score, and accuracy for each architecture in a table.
train_metrics = []
test_metrics = []
for arch in architectures:
    arch_key = str(arch)
    train_report = results[arch_key]["train_report"]
    test_report = results[arch_key]["test_report"]
    
    train_metrics.append({
        "Architecture": arch_key,
        "Precision": train_report["weighted avg"]["precision"],
        "Recall": train_report["weighted avg"]["recall"],
        "F1-Score": train_report["weighted avg"]["f1-score"],
        "Accuracy": train_report["accuracy"]
    })
    
    if "error" not in test_report:
        test_metrics.append({
            "Architecture": arch_key,
            "Precision": test_report["weighted avg"]["precision"],
            "Recall": test_report["weighted avg"]["recall"],
            "F1-Score": test_report["weighted avg"]["f1-score"],
            "Accuracy": test_report["accuracy"]
        })


train_metrics_df = pd.DataFrame(train_metrics)
test_metrics_df = pd.DataFrame(test_metrics)
print("\nTraining Metrics:")
print(train_metrics_df.to_string(index=False))
print("\nTest Metrics:")
print(test_metrics_df.to_string(index=False))


# ### 2.2.4: Plotting F1 score vs Depth

# In[15]:


avg_f1_train = []
avg_f1_test = []
arch_labels = []
for arch in architectures:
    arch_key = str(arch)
    arch_labels.append("-".join(map(str, arch)))
    train_f1 = results[arch_key]["train_report"]["weighted avg"]["f1-score"]
    test_f1 = results[arch_key]["test_report"].get("weighted avg", {}).get("f1-score", None)
    avg_f1_train.append(train_f1)
    avg_f1_test.append(test_f1)

plt.figure(figsize=(8, 6))
plt.plot(arch_labels, avg_f1_train, marker='o', label='Train Weighted Avg F1')
if all(f1 is not None for f1 in avg_f1_test):
    plt.plot(arch_labels, avg_f1_test, marker='o', label='Test Weighted Avg F1')
plt.xlabel('Architecture (Hidden Layers)')
plt.ylabel('Weighted Average F1 Score')
plt.title('Effect of Network Depth on F1 Score (Part c)')
plt.legend()
plt.grid(True)
plt.show()


# ## 2.3: Adaptive Learning Rate

# ### 2.3.1: Training Models

# In[ ]:


architectures = [
    [512],
    [512, 256],
    [512, 256, 128],
    [512, 256, 128, 64]
]

print("Architectures to experiment with:")
for arch in architectures:
    print(arch)


epochs = 100         
batch_size = 32      
learning_rate = 0.01 
tol = 1e-6         


results = {}

for arch in architectures:
    print(f"\nTraining with architecture: {arch}")
    
    (train_preds_df, test_preds_df) = neural_network.train_part_d(
        train_data_path, 
        test_data_path, 
        hidden_layers=arch, 
        epochs=epochs, 
        batch_size=batch_size, 
        tol=tol
    )
    
    train_preds = train_preds_df["train_prediction"].values
    test_preds = test_preds_df["test_prediction"].values
    
  
    train_report = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
    
    # For test predictions, if y_test is available (or true test labels loaded separately), compute the report.
    if y_test is not None:
        test_report = classification_report(y_test, test_preds, output_dict=True, zero_division=0)
    else:
        test_report = {"error": "True test labels not available"}
        print("Warning: True test labels not available for evaluation.")
    
  
    results[str(arch)] = {"train_report": train_report, "test_report": test_report}
    print(f"Finished training with architecture: {arch}")
    
print("All architectures have been trained and reports stored.")


# ### 2.3.2: Outputting Classification Reports

# In[17]:


print("Summary of Classification Reports for Different Architectures (Adaptive LR):")
for arch in architectures:
    arch_key = str(arch)
    print(f"\nArchitecture: {arch}")
    print("Training Classification Report:")
    train_report_df = pd.DataFrame(results[arch_key]["train_report"]).transpose()
    print(train_report_df)
    
    if "error" not in results[arch_key]["test_report"]:
        print("Test Classification Report:")
        test_report_df = pd.DataFrame(results[arch_key]["test_report"]).transpose()
        print(test_report_df)
    else:
        print("Test Classification Report: True test labels not available.")
    print("-" * 60)


# ### 2.3.3: Summarizing Metrics

# In[ ]:


## Summarize weighted average precision, recall, f1-score, and accuracy for each architecture in a table.
train_metrics = []
test_metrics = []
for arch in architectures:
    arch_key = str(arch)
    train_report = results[arch_key]["train_report"]
    test_report = results[arch_key]["test_report"]
    
    train_metrics.append({
        "Architecture": arch_key,
        "Precision": train_report["weighted avg"]["precision"],
        "Recall": train_report["weighted avg"]["recall"],
        "F1-Score": train_report["weighted avg"]["f1-score"],
        "Accuracy": train_report["accuracy"]
    })
    
    if "error" not in test_report:
        test_metrics.append({
            "Architecture": arch_key,
            "Precision": test_report["weighted avg"]["precision"],
            "Recall": test_report["weighted avg"]["recall"],
            "F1-Score": test_report["weighted avg"]["f1-score"],
            "Accuracy": test_report["accuracy"]
        })


train_metrics_df = pd.DataFrame(train_metrics)
test_metrics_df = pd.DataFrame(test_metrics)
print("\nTraining Metrics:")
print(train_metrics_df.to_string(index=False))
print("\nTest Metrics:")
print(test_metrics_df.to_string(index=False))


# ### 2.3.4: Plotting F1 Scores vs Depth

# In[19]:


avg_f1_train = []
avg_f1_test = []
arch_labels = []
for arch in architectures:
    arch_key = str(arch)
    arch_labels.append("-".join(map(str, arch)))
    train_f1 = results[arch_key]["train_report"]["weighted avg"]["f1-score"]
    test_f1 = results[arch_key]["test_report"].get("weighted avg", {}).get("f1-score", None)
    avg_f1_train.append(train_f1)
    avg_f1_test.append(test_f1)

plt.figure(figsize=(8, 6))
plt.plot(arch_labels, avg_f1_train, marker='o', label='Train Weighted Avg F1')
if all(f1 is not None for f1 in avg_f1_test):
    plt.plot(arch_labels, avg_f1_test, marker='o', label='Test Weighted Avg F1')
plt.xlabel('Architecture (Hidden Layers)')
plt.ylabel('Weighted Average F1 Score')
plt.title('Effect of Network Depth on F1 Score (Part d)')
plt.legend()
plt.grid(True)
plt.show()


# ## 2.4: ReLu Activation

# ### 2.4.1: Training Models

# In[ ]:


architectures = [
    [512],
    [512, 256],
    [512, 256, 128],
    [512, 256, 128, 64]
]

print("Architectures to experiment with:")
for arch in architectures:
    print(arch)


epochs = 100         
batch_size = 32      
learning_rate = 0.01 
tol = 1e-6          

results = {}

for arch in architectures:
    print(f"\nTraining with architecture: {arch}")
    
    (train_preds_df, test_preds_df) = neural_network.train_part_e(
        train_data_path, 
        test_data_path, 
        hidden_layers=arch, 
        epochs=epochs, 
        batch_size=batch_size, 
        tol=tol
    )
    
   
    train_preds = train_preds_df["train_prediction"].values
    test_preds = test_preds_df["test_prediction"].values
    
    
    train_report = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
    
    # For test predictions, if y_test is available (or if true test labels are loaded separately), compute the report.
    if y_test is not None:
        test_report = classification_report(y_test, test_preds, output_dict=True, zero_division=0)
    else:
        test_report = {"error": "True test labels not available"}
        print("Warning: True test labels not available for evaluation.")
   
    results[str(arch)] = {"train_report": train_report, "test_report": test_report}
    
    print(f"Finished training with architecture: {arch}")
    
print("All architectures have been trained and reports stored.")


# ### 2.4.2: Outputting Classification Reports

# In[22]:


print("Summary of Classification Reports for Different Architectures (Adaptive LR):")
for arch in architectures:
    arch_key = str(arch)
    print(f"\nArchitecture: {arch}")
    print("Training Classification Report:")
    train_report_df = pd.DataFrame(results[arch_key]["train_report"]).transpose()
    print(train_report_df)
    
    if "error" not in results[arch_key]["test_report"]:
        print("Test Classification Report:")
        test_report_df = pd.DataFrame(results[arch_key]["test_report"]).transpose()
        print(test_report_df)
    else:
        print("Test Classification Report: True test labels not available.")
    print("-" * 60)


# ### 2.4.3: Summarizing Metrics

# In[ ]:


## Summarize weighted average precision, recall, f1-score, and accuracy for each architecture in a table.
train_metrics = []
test_metrics = []
for arch in architectures:
    arch_key = str(arch)
    train_report = results[arch_key]["train_report"]
    test_report = results[arch_key]["test_report"]
    
    train_metrics.append({
        "Architecture": arch_key,
        "Precision": train_report["weighted avg"]["precision"],
        "Recall": train_report["weighted avg"]["recall"],
        "F1-Score": train_report["weighted avg"]["f1-score"],
        "Accuracy": train_report["accuracy"]
    })
    
    if "error" not in test_report:
        test_metrics.append({
            "Architecture": arch_key,
            "Precision": test_report["weighted avg"]["precision"],
            "Recall": test_report["weighted avg"]["recall"],
            "F1-Score": test_report["weighted avg"]["f1-score"],
            "Accuracy": test_report["accuracy"]
        })


train_metrics_df = pd.DataFrame(train_metrics)
test_metrics_df = pd.DataFrame(test_metrics)
print("\nTraining Metrics:")
print(train_metrics_df.to_string(index=False))
print("\nTest Metrics:")
print(test_metrics_df.to_string(index=False))


# ### 2.4.4: Plotting F1 Scores vs Depth

# In[24]:


avg_f1_train = []
avg_f1_test = []
arch_labels = []
for arch in architectures:
    arch_key = str(arch)
    arch_labels.append("-".join(map(str, arch)))
    train_f1 = results[arch_key]["train_report"]["weighted avg"]["f1-score"]
    test_f1 = results[arch_key]["test_report"].get("weighted avg", {}).get("f1-score", None)
    avg_f1_train.append(train_f1)
    avg_f1_test.append(test_f1)

plt.figure(figsize=(8, 6))
plt.plot(arch_labels, avg_f1_train, marker='o', label='Train Weighted Avg F1')
if all(f1 is not None for f1 in avg_f1_test):
    plt.plot(arch_labels, avg_f1_test, marker='o', label='Test Weighted Avg F1')
plt.xlabel('Architecture (Hidden Layers)')
plt.ylabel('Weighted Average F1 Score')
plt.title('Effect of Network Depth on F1 Score (Part e)')
plt.legend()
plt.grid(True)
plt.show()


# ## 2.5: Sci-Kit Learn

# ### 2.5.1: Training Models

# In[ ]:


architectures = [
    [512],
    [512, 256],
    [512, 256, 128],
    [512, 256, 128, 64]
]

print("Architectures to experiment with:")
for arch in architectures:
    print(arch)


results = {}

for arch in architectures:
    print(f"\nTraining with architecture: {arch}")
    
   
    (train_preds_df, test_preds_df) = neural_network.train_part_f(
        train_data_path, 
        test_data_path, 
        hidden_layers=arch
    )
    
   
    train_preds = train_preds_df["train_prediction"].values
    test_preds = test_preds_df["test_prediction"].values
    
   
    train_report = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
    
    # For test predictions, if y_test is available (or if true test labels are loaded separately), compute the report.
    if y_test is not None:
        test_report = classification_report(y_test, test_preds, output_dict=True, zero_division=0)
    else:
        test_report = {"error": "True test labels not available"}
        print("Warning: True test labels not available for evaluation.")
    
    
    results[str(arch)] = {"train_report": train_report, "test_report": test_report}
    
    print(f"Finished training with architecture: {arch}")
    
print("All architectures have been trained and reports stored.")


# ### 2.5.2: Outputting Classification Reports

# In[26]:


print("Summary of Classification Reports for Different Architectures (Adaptive LR):")
for arch in architectures:
    arch_key = str(arch)
    print(f"\nArchitecture: {arch}")
    print("Training Classification Report:")
    train_report_df = pd.DataFrame(results[arch_key]["train_report"]).transpose()
    print(train_report_df)
    
    if "error" not in results[arch_key]["test_report"]:
        print("Test Classification Report:")
        test_report_df = pd.DataFrame(results[arch_key]["test_report"]).transpose()
        print(test_report_df)
    else:
        print("Test Classification Report: True test labels not available.")
    print("-" * 60)


# ### 2.5.3: Summarizing Metrics

# In[ ]:


## Summarize weighted average precision, recall, f1-score, and accuracy for each architecture in a table.
train_metrics = []
test_metrics = []
for arch in architectures:
    arch_key = str(arch)
    train_report = results[arch_key]["train_report"]
    test_report = results[arch_key]["test_report"]
    
    train_metrics.append({
        "Architecture": arch_key,
        "Precision": train_report["weighted avg"]["precision"],
        "Recall": train_report["weighted avg"]["recall"],
        "F1-Score": train_report["weighted avg"]["f1-score"],
        "Accuracy": train_report["accuracy"]
    })
    
    if "error" not in test_report:
        test_metrics.append({
            "Architecture": arch_key,
            "Precision": test_report["weighted avg"]["precision"],
            "Recall": test_report["weighted avg"]["recall"],
            "F1-Score": test_report["weighted avg"]["f1-score"],
            "Accuracy": test_report["accuracy"]
        })


train_metrics_df = pd.DataFrame(train_metrics)
test_metrics_df = pd.DataFrame(test_metrics)
print("\nTraining Metrics:")
print(train_metrics_df.to_string(index=False))
print("\nTest Metrics:")
print(test_metrics_df.to_string(index=False))


# ### 2.5.4: Plotting F1 Scores vs Depth

# In[ ]:


avg_f1_train = []
avg_f1_test = []
arch_labels = []
for arch in architectures:
    arch_key = str(arch)
    arch_labels.append("-".join(map(str, arch)))
    train_f1 = results[arch_key]["train_report"]["weighted avg"]["f1-score"]
    test_f1 = results[arch_key]["test_report"].get("weighted avg", {}).get("f1-score", None)
    avg_f1_train.append(train_f1)
    avg_f1_test.append(test_f1)


plt.figure(figsize=(8, 6))
plt.plot(arch_labels, avg_f1_train, marker='o', label='Train Weighted Avg F1')
if all(f1 is not None for f1 in avg_f1_test):
    plt.plot(arch_labels, avg_f1_test, marker='o', label='Test Weighted Avg F1')
plt.xlabel('Architecture (Hidden Layers)')
plt.ylabel('Weighted Average F1 Score')
plt.title('Effect of Network Depth on F1 Score (Part f)')
plt.legend()
plt.grid(True)
plt.show()



</PRE>
</PRE>
</BODY>
</HTML>
