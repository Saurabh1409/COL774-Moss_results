<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_G9KZF.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_TWIXY.py<p><PRE>


#!/usr/bin/env python
"""
decision_tree.py
---------------
Usage:
    python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;

Arguments:
    train_data_path       Absolute path to train CSV file.
    validation_data_path  Absolute path to validation CSV file.
    test_data_path        Absolute path to test CSV file.
    output_folder_path    Absolute folder path to save the prediction CSV file.
    question_part         One of 'a', 'b', 'c', 'd', or 'e' which chooses:
                          a) Custom decision tree (raw features) with max depth 20.
                          b) Custom decision tree (one-hot encoded) with max depth 55.
                          c) (Pruning case) simply return 0.
                          d) Best decision tree (scikit-learn) based on validation accuracy.
                          e) Best Random Forest based on grid search (using OOB score).

The predicted labels are saved in a CSV file with a single column "prediction" preserving the order of test samples.
Predictions are converted into exactly "&gt;=50k" or "&lt;50k".
"""

import sys
import os
import pandas as pd
import numpy as np
import math
import copy
from collections import Counter

############################
# Custom Decision Tree Code
############################

def calculate_entropy(y):
    if len(y) == 0:
        return 0
    counter = Counter(y)
    probabilities = [count / len(y) for count in counter.values()]
    entropy = -sum(p * np.log2(p) for p in probabilities)
    return entropy

def calculate_mutual_information(X, y, attribute):
    total_entropy = calculate_entropy(y)
    attribute_values = X[attribute]
    if pd.api.types.is_numeric_dtype(attribute_values):
        median_value = attribute_values.median()
        left_indices = X[attribute] &lt;= median_value
        right_indices = ~left_indices
        left_entropy = calculate_entropy(y[left_indices])
        right_entropy = calculate_entropy(y[right_indices])
        n = len(y)
        weighted_entropy = (left_indices.sum() / n) * left_entropy + (right_indices.sum() / n) * right_entropy
        return total_entropy - weighted_entropy, median_value
    else:
        values = X[attribute].unique()
        weighted_entropy = 0
        n = len(y)
        for value in values:
            indices = X[attribute] == value
            subset_size = indices.sum()
            if subset_size &gt; 0:
                subset_entropy = calculate_entropy(y[indices])
                weighted_entropy += (subset_size / n) * subset_entropy
        return total_entropy - weighted_entropy, None

def find_best_split(X, y, attributes):
    best_attribute = None
    best_info_gain = -float('inf')
    best_split_value = None
    for attribute in attributes:
        info_gain, split_value = calculate_mutual_information(X, y, attribute)
        if info_gain &gt; best_info_gain:
            best_info_gain = info_gain
            best_attribute = attribute
            best_split_value = split_value
    return best_attribute, best_info_gain, best_split_value

class Node:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match236-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def __init__(self, attribute=None, split_value=None, children=None, label=None, is_leaf=False):
        self.attribute = attribute
        self.split_value = split_value
        self.children = children if children is not None else {}
</FONT>        self.label = label
        self.is_leaf = is_leaf

    def count_nodes(self):
        if self.is_leaf:
            return 1
        return 1 + sum(child.count_nodes() for child in self.children.values())

class DecisionTreeClassifier:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.root = None
        
    def fit(self, X, y):
        attributes = list(X.columns)
        self.root = self._build_tree(X, y, attributes, depth=0)
        return self
    
    def _build_tree(self, X, y, attributes, depth):
        if len(set(y)) == 1:
            return Node(label=y.iloc[0], is_leaf=True)
        if len(attributes) == 0 or (self.max_depth is not None and depth &gt;= self.max_depth):
            most_common_class = Counter(y).most_common(1)[0][0]
            return Node(label=most_common_class, is_leaf=True)
        best_attribute, info_gain, split_value = find_best_split(X, y, attributes)
        if info_gain &lt;= 0:
            most_common_class = Counter(y).most_common(1)[0][0]
            return Node(label=most_common_class, is_leaf=True)
        node = Node(attribute=best_attribute, split_value=split_value)
        if pd.api.types.is_numeric_dtype(X[best_attribute]):
            left_indices = X[best_attribute] &lt;= split_value
            right_indices = ~left_indices
            node.children = {
                'left': self._build_tree(X[left_indices], y[left_indices], attributes, depth + 1),
                'right': self._build_tree(X[right_indices], y[right_indices], attributes, depth + 1)
            }
        else:
            node.children = {}
            for value in X[best_attribute].unique():
                indices = X[best_attribute] == value
                if indices.sum() &gt; 0:
                    remaining_attributes = [attr for attr in attributes if attr != best_attribute]
                    node.children[value] = self._build_tree(X[indices], y[indices], remaining_attributes, depth + 1)
        return node
    
    def predict(self, X):
        return np.array([self._predict_sample(sample) for _, sample in X.iterrows()])
    
    def _predict_sample(self, sample):
        node = self.root
        while not node.is_leaf:
            attribute = node.attribute
            if pd.api.types.is_numeric_dtype(pd.Series([sample[attribute]])):
                if sample[attribute] &lt;= node.split_value:
                    node = node.children.get('left')
                else:
                    node = node.children.get('right')
            else:
                value = sample[attribute]
                if value in node.children:
                    node = node.children[value]
                else:
                    return node.label
        return node.label

def calculate_accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def prepare_data(df):
    X = df.drop('income', axis=1)
    y = df['income']
    return X, y

#####################################
# One-Hot Encoding Helper Functions
#####################################

def one_hot_encode(data):
    """
    Perform one-hot encoding on categorical columns that have more than 2 unique values.
    """
    categorical_cols = []
    for col in data.columns:
        if not pd.api.types.is_numeric_dtype(data[col]) and len(data[col].unique()) &gt; 2:
            categorical_cols.append(col)
    data_encoded = pd.get_dummies(data, columns=categorical_cols, prefix_sep='_')
    return data_encoded

def encode_all_categories(df, target_column='income'):
    """
    One-hot encode every column of type 'object' or 'category' (except the target).
    """
    cat_cols = df.select_dtypes(include=['object', 'category']).columns
    cat_cols = [col for col in cat_cols if col != target_column]
    df_encoded = pd.get_dummies(df, columns=cat_cols, prefix_sep='_')
    return df_encoded

#########################################
# Prediction Conversion Helper Function
#########################################

def convert_label(label):
    """
    Convert prediction label to one of the two required strings.
    Returns "&gt;=50k" if the label starts with "&gt;", otherwise "&lt;50k".
    """
    s = str(label).strip()
    if s.startswith("&gt;"):
        return "&gt;50k"
    else:
        return "&lt;=50k"

#####################################
# PART FUNCTIONS (a, b, c, d, and e)
#####################################

def part_a(train_df, test_df, output_folder):
    """
    Part (a): Custom decision tree (raw features) with max depth 20.
    """
    X_train, y_train = prepare_data(train_df)
    X_test, _ = prepare_data(test_df)
    model = DecisionTreeClassifier(max_depth=20)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    preds = [convert_label(pred) for pred in preds]
    output_file = os.path.join(output_folder, "prediction_a.csv")
    pd.DataFrame({"prediction": preds}).to_csv(output_file, index=False)
    print("Part (a) predictions saved to:", output_file)

def part_b(train_df, test_df, output_folder):
    """
    Part (b): One-hot encoding then custom decision tree with max depth 55.
    """
    train_encoded = one_hot_encode(train_df)
    test_encoded = one_hot_encode(test_df)
    train_cols = set(train_encoded.columns)
    test_cols = set(test_encoded.columns)
    missing_test = train_cols - test_cols
    for col in missing_test:
        test_encoded[col] = 0
    test_encoded = test_encoded[list(train_encoded.columns)]
    
    X_train, y_train = prepare_data(train_encoded)
    X_test, _ = prepare_data(test_encoded)
    model = DecisionTreeClassifier(max_depth=55)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    preds = [convert_label(pred) for pred in preds]
    output_file = os.path.join(output_folder, "prediction_b.csv")
    pd.DataFrame({"prediction": preds}).to_csv(output_file, index=False)
    print("Part (b) predictions saved to:", output_file)

def part_c(test_df, output_folder):
    """
    Part (c): Return 0 for every test example.
    """
    n = test_df.shape[0]
    preds = ["0"] * n
    output_file = os.path.join(output_folder, "prediction_c.csv")
    pd.DataFrame({"prediction": preds}).to_csv(output_file, index=False)
    print("Part (c) predictions (all 0) saved to:", output_file)

def part_d(train_df, valid_df, test_df, output_folder):
    """
    Part (d): Use scikit-learn's decision tree.
    First, one-hot encode all categorical features (except the target) using encode_all_categories.
    Then, perform two experiments:
      (i) Vary max_depth (among [25, 35, 45, 55]) with criterion='entropy'.
      (ii) Vary ccp_alpha (among [0.001, 0.01, 0.1, 0.2]) for a fully-grown tree.
    The best model (based on validation accuracy) among the two experiments is chosen to predict test data.
    """
    from sklearn.tree import DecisionTreeClassifier as SKDecisionTreeClassifier
    from sklearn.metrics import accuracy_score
    
    # One-hot encode data.
    train_enc = encode_all_categories(train_df, target_column='income')
    valid_enc = encode_all_categories(valid_df, target_column='income')
    test_enc  = encode_all_categories(test_df, target_column='income')
    
    # Reindex validation and test sets to match training.
    common_cols = train_enc.columns
    valid_enc = valid_enc.reindex(columns=common_cols, fill_value=0)
    test_enc  = test_enc.reindex(columns=common_cols, fill_value=0)
    
    def prepare(df):
        X = df.drop('income', axis=1)
        y = df['income']
        return X, y

    X_train, y_train = prepare(train_enc)
    X_valid, y_valid = prepare(valid_enc)
    X_test, _      = prepare(test_enc)

    # Experiment (i): Vary max_depth.
    max_depth_values = [25, 35, 45, 55]
    valid_acc_depth = []
    models_depth = {}

    print("---- Varying max_depth (criterion='entropy') ----")
    for d in max_depth_values:
        clf = SKDecisionTreeClassifier(criterion="entropy", max_depth=d, random_state=42)
        clf.fit(X_train, y_train)
        
        y_valid_pred = clf.predict(X_valid)
        valid_acc = accuracy_score(y_valid, y_valid_pred)
        valid_acc_depth.append(valid_acc)
        models_depth[d] = clf

        print(f"max_depth = {d}: Validation Acc = {valid_acc:.4f}")
    
    best_depth_index = np.argmax(valid_acc_depth)
    best_depth = max_depth_values[best_depth_index]
    best_depth_acc = valid_acc_depth[best_depth_index]
    best_model_depth = models_depth[best_depth]
    print(f"Best max_depth based on validation accuracy: {best_depth} (Acc = {best_depth_acc:.4f})")

    # Experiment (ii): Vary ccp_alpha.
    ccp_alpha_values = [0.001, 0.01, 0.1, 0.2]
    valid_acc_alpha = []
    models_alpha = {}
    
    print("---- Varying ccp_alpha (fully grown tree, criterion='entropy') ----")
    for alpha in ccp_alpha_values:
        clf = SKDecisionTreeClassifier(criterion="entropy", ccp_alpha=alpha, random_state=42)
        clf.fit(X_train, y_train)
        
        y_valid_pred = clf.predict(X_valid)
        valid_acc = accuracy_score(y_valid, y_valid_pred)
        valid_acc_alpha.append(valid_acc)
        models_alpha[alpha] = clf

        print(f"ccp_alpha = {alpha}: Validation Acc = {valid_acc:.4f}")
    
    best_alpha_index = np.argmax(valid_acc_alpha)
    best_alpha = ccp_alpha_values[best_alpha_index]
    best_alpha_acc = valid_acc_alpha[best_alpha_index]
    best_model_alpha = models_alpha[best_alpha]
    print(f"Best ccp_alpha based on validation accuracy: {best_alpha} (Acc = {best_alpha_acc:.4f})")
    
    # Choose the model with higher validation accuracy.
    if best_depth_acc &gt;= best_alpha_acc:
        chosen_model = best_model_depth
        print("Chosen model based on max_depth tuning.")
    else:
        chosen_model = best_model_alpha
        print("Chosen model based on ccp_alpha tuning.")
    
    # Predict test data using the chosen model.
    preds = chosen_model.predict(X_test)
    preds = [convert_label(pred) for pred in preds]
    output_file = os.path.join(output_folder, "prediction_d.csv")
    pd.DataFrame({"prediction": preds}).to_csv(output_file, index=False)
    print("Part (d) predictions saved to:", output_file)

def part_e(train_df, valid_df, test_df, output_folder):
    """
    Part (e): Use Random Forest.
    One-hot encode data (using encode_all_categories) and then perform grid search over the Random Forest hyperparameters.
    The grid search is done over:
      - n_estimators: [50, 150, 250, 350]
      - max_features: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
      - min_samples_split: [2, 4, 6, 8, 10]
    The best parameter combination is chosen based on the Out-Of-Bag (OOB) score.
    The chosen model is then used to predict the test set.
    """
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    # One-hot encode data.
    train_enc = encode_all_categories(train_df, target_column='income')
    valid_enc = encode_all_categories(valid_df, target_column='income')
    test_enc  = encode_all_categories(test_df, target_column='income')

    # Ensure that all datasets have the same columns and order.
    common_columns = train_enc.columns
    valid_enc = valid_enc.reindex(columns=common_columns, fill_value=0)
    test_enc = test_enc.reindex(columns=common_columns, fill_value=0)

    def prepare(df):
        X = df.drop('income', axis=1)
        y = df['income']
        return X, y
    
    X_train, y_train = prepare(train_enc)
    X_valid, y_valid = prepare(valid_enc)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match236-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X_test, y_test   = prepare(test_enc)

    # Grid search over hyperparameters.
    n_estimators_grid = [50, 150, 250, 350]
    max_features_grid = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
    min_samples_split_grid = [2, 4, 6, 8, 10]
</FONT>
    best_oob = -1
    best_params = {}
    grid_results = []

    print("---- Grid Search on Random Forest Hyperparameters ----")
    for n_est in n_estimators_grid:
        for max_feat in max_features_grid:
            for min_split in min_samples_split_grid:
                rf = RandomForestClassifier(criterion="entropy",
                                            n_estimators=n_est,
                                            max_features=max_feat,
                                            min_samples_split=min_split,
                                            oob_score=True,
                                            random_state=42,
                                            n_jobs=-1)
                rf.fit(X_train, y_train)
                oob_score = rf.oob_score_
                grid_results.append({
                    'n_estimators': n_est,
                    'max_features': max_feat,
                    'min_samples_split': min_split,
                    'oob_score': oob_score
                })
                print(f"n_estimators={n_est}, max_features={max_feat}, min_samples_split={min_split} -&gt; OOB Score: {oob_score:.4f}")
                if oob_score &gt; best_oob:
                    best_oob = oob_score
                    best_params = {'n_estimators': n_est,
                                   'max_features': max_feat,
                                   'min_samples_split': min_split}

    print("\nBest Parameter Combination (by OOB Score):")
    print(best_params)
    print(f"Best OOB Score: {best_oob:.4f}")

    # Evaluate the best Random Forest.
    best_rf = RandomForestClassifier(criterion="entropy",
                                     n_estimators=best_params['n_estimators'],
                                     max_features=best_params['max_features'],
                                     min_samples_split=best_params['min_samples_split'],
                                     oob_score=True,
                                     random_state=42,
                                     n_jobs=-1)
    best_rf.fit(X_train, y_train)
    
    # Optionally compute and print train/valid/test accuracy.
    train_acc = accuracy_score(y_train, best_rf.predict(X_train))
    valid_acc = accuracy_score(y_valid, best_rf.predict(X_valid))
    test_acc  = accuracy_score(y_test, best_rf.predict(X_test))
    oob_acc   = best_rf.oob_score_
    
    print("\n--- Optimal Random Forest Results ---")
    print(f"Parameters: {best_params}")
    print(f"Training Accuracy: {train_acc:.4f}")
    print(f"Out-of-Bag Accuracy: {oob_acc:.4f}")
    print(f"Validation Accuracy: {valid_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")

    preds = best_rf.predict(X_test)
    preds = [convert_label(pred) for pred in preds]
    output_file = os.path.join(output_folder, "prediction_e.csv")
    pd.DataFrame({"prediction": preds}).to_csv(output_file, index=False)
    print("Part (e) predictions saved to:", output_file)

############################
# Main Execution
############################

def main():
    if len(sys.argv) != 6:
        print("Usage: python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        sys.exit(1)
    
    train_path = sys.argv[1]
    valid_path = sys.argv[2]
    test_path = sys.argv[3]
    output_folder = sys.argv[4]
    question_part = sys.argv[5].lower()  # Should be one of 'a', 'b', 'c', 'd', 'e'
    
    os.makedirs(output_folder, exist_ok=True)
    
    # Load CSV files.
<A NAME="0"></A><FONT color = #FF0000><A HREF="match236-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    train_df = pd.read_csv(train_path)
    valid_df = pd.read_csv(valid_path)
    test_df = pd.read_csv(test_path)
    
    if question_part == "a":
        part_a(train_df, test_df, output_folder)
    elif question_part == "b":
        part_b(train_df, test_df, output_folder)
    elif question_part == "c":
        part_c(test_df, output_folder)
</FONT>    elif question_part == "d":
        part_d(train_df, valid_df, test_df, output_folder)
    elif question_part == "e":
        part_e(train_df, valid_df, test_df, output_folder)
    else:
        print("Invalid question part. Must be one of: a, b, c, d, e.")
        sys.exit(1)

if __name__ == "__main__":
    main()




#!/usr/bin/env python
"""
neural_network.py
-----------------
Usage:
    python neural_network.py &lt;train_folder_path&gt; &lt;test_folder_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;

Arguments:
    train_folder_path   
        Absolute path to the training images folder.
        (Each subfolder should have a numeric name representing its class.)
    test_folder_path    
        Absolute path to the test images folder.
    output_folder_path  
        Absolute path to the folder where the prediction CSV file will be saved.
    question_part       
        A single letter: 'b', 'c', 'd', 'e', or 'f', which selects the experiment to run:
            b: Experiment with single hidden layer sizes (custom NN using sigmoid).
            c: Experiment with varying network depth (fixed learning rate, sigmoid).
            d: Experiment with varying network depth (adaptive learning rate, sigmoid).
            e: Experiment with varying network depth (adaptive learning rate, ReLU).
            f: Experiment using scikit-learn’s MLPClassifier.
            
The script outputs a CSV file named "prediction_&lt;question_part&gt;.csv" containing one column "prediction".
Since no test labels are provided, evaluation on the test set is skipped.
"""

import sys
import os
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import train_test_split

##########################################
# Global Helper Functions
##########################################

def one_hot_encode(y, num_classes):
    one_hot = np.zeros((y.size, num_classes))
    one_hot[np.arange(y.size), y] = 1
    return one_hot

def load_train_data(train_folder):
    """Load training images from subfolders; each subfolder name (numeric) is the class label."""
    X_list = []
    y_list = []
    for folder in sorted(os.listdir(train_folder)):
        folder_path = os.path.join(train_folder, folder)
        if os.path.isdir(folder_path):
            try:
                label = int(folder)
            except ValueError:
                continue
            for filename in os.listdir(folder_path):
                file_path = os.path.join(folder_path, filename)
                try:
                    img = Image.open(file_path).convert("RGB")
                    img = img.resize((28, 28))
                    img_array = np.array(img, dtype=np.float32).flatten()
                    X_list.append(img_array)
                    y_list.append(label)
                except Exception as e:
                    print(f"Error loading {file_path}: {e}")
    X = np.array(X_list, dtype=np.float32) / 255.0
    y = np.array(y_list, dtype=np.int32)
    return X, y

def load_test_data_modified(test_folder):
    """
    Load test images by listing and sorting image files in test_folder.
    Returns:
        X: numpy array of test images.
        file_names: list of test image filenames.
        y_true: Always returns None as ground-truth is not provided.
    """
    file_names = sorted([f for f in os.listdir(test_folder) if f.lower().endswith((".png", ".jpg", ".jpeg"))])
    X_list = []
    for fname in file_names:
        file_path = os.path.join(test_folder, fname)
        try:
            img = Image.open(file_path).convert("RGB")
            img = img.resize((28, 28))
            img_array = np.array(img, dtype=np.float32).flatten()
            X_list.append(img_array)
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
    X = np.array(X_list, dtype=np.float32) / 255.0
    return X, file_names, None

##########################################
# Activation Functions
##########################################
def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

def sigmoid_derivative(a):
    return a * (1 - a)

def softmax(z):
    shift_z = z - np.max(z, axis=1, keepdims=True)
    exp_scores = np.exp(shift_z)
    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z &gt; 0).astype(np.float32)

##########################################
# Neural Network Classes
##########################################

# Custom NN with sigmoid hidden activations.
class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size, learning_rate=0.01, seed=None):
        if seed is not None:
            np.random.seed(seed)
        self.learning_rate = learning_rate
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes) - 1
        self.weights = []
        self.biases = []
        for i in range(self.num_layers):
            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1.0 / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        activations = [X]
        pre_activations = []
        A = X
        for i in range(self.num_layers - 1):
            Z = A.dot(self.weights[i]) + self.biases[i]
            pre_activations.append(Z)
            A = sigmoid(Z)
            activations.append(A)
        Z = A.dot(self.weights[-1]) + self.biases[-1]
        pre_activations.append(Z)
        A = softmax(Z)
        activations.append(A)
        return activations, pre_activations
    
    def compute_loss(self, y_pred, Y_true):
        m = Y_true.shape[0]
        eps = 1e-15
        correct_probs = y_pred[range(m), np.argmax(Y_true, axis=1)]
        loss = -np.sum(np.log(np.clip(correct_probs, eps, 1))) / m
        return loss
    
    def backward(self, X, Y_true, activations, pre_activations):
        grads_W = [None] * self.num_layers
        grads_b = [None] * self.num_layers
        m = X.shape[0]
        delta = activations[-1] - Y_true
        grads_W[-1] = activations[-2].T.dot(delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m
        for l in range(self.num_layers - 2, -1, -1):
            delta = delta.dot(self.weights[l+1].T) * sigmoid_derivative(activations[l+1])
            grads_W[l] = activations[l].T.dot(delta) / m
<A NAME="2"></A><FONT color = #0000FF><A HREF="match236-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m
        return grads_W, grads_b
    
    def update_parameters(self, grads_W, grads_b):
        for i in range(self.num_layers):
</FONT>            self.weights[i] -= self.learning_rate * grads_W[i]
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match236-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.biases[i] -= self.learning_rate * grads_b[i]
    
    def train(self, X_train, Y_train, num_epochs=50, batch_size=32, stopping_threshold=1e-5):
</FONT>        num_samples = X_train.shape[0]
        num_batches = int(np.ceil(num_samples / batch_size))
        prev_val_loss = np.inf
        for epoch in range(num_epochs):
            permutation = np.random.permutation(num_samples)
            X_shuffled = X_train[permutation]
            Y_shuffled = Y_train[permutation]
            epoch_loss = 0.0
            for b in range(num_batches):
                start = b * batch_size
                end = min(start + batch_size, num_samples)
                X_batch = X_shuffled[start:end]
                Y_batch = Y_shuffled[start:end]
                activations, pre_activations = self.forward(X_batch)
                batch_loss = self.compute_loss(activations[-1], Y_batch)
                epoch_loss += batch_loss
                grads_W, grads_b = self.backward(X_batch, Y_batch, activations, pre_activations)
                self.update_parameters(grads_W, grads_b)
            avg_loss = epoch_loss / num_batches
            yield epoch, avg_loss
    
    def predict(self, X):
        activations, _ = self.forward(X)
        return np.argmax(activations[-1], axis=1)

# Custom NN with ReLU activations in hidden layers.
class NeuralNetworkReLU:
    def __init__(self, input_size, hidden_layers, output_size, learning_rate=0.01, seed=None):
        if seed is not None:
            np.random.seed(seed)
        self.learning_rate = learning_rate
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes) - 1
        self.weights = []
        self.biases = []
        for i in range(self.num_layers):
            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1.0 / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        activations = [X]
        pre_activations = []
        A = X
        for i in range(self.num_layers - 1):
            Z = A.dot(self.weights[i]) + self.biases[i]
            pre_activations.append(Z)
            A = relu(Z)
            activations.append(A)
        Z = A.dot(self.weights[-1]) + self.biases[-1]
        pre_activations.append(Z)
        A = softmax(Z)
        activations.append(A)
        return activations, pre_activations
    
    def compute_loss(self, y_pred, Y_true):
        m = Y_true.shape[0]
        eps = 1e-15
        correct_probs = y_pred[range(m), np.argmax(Y_true, axis=1)]
        loss = -np.sum(np.log(np.clip(correct_probs, eps, 1))) / m
        return loss
    
    def backward(self, X, Y_true, activations, pre_activations):
        grads_W = [None] * self.num_layers
        grads_b = [None] * self.num_layers
        m = X.shape[0]
        delta = activations[-1] - Y_true
        grads_W[-1] = activations[-2].T.dot(delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m
        for l in range(self.num_layers - 2, -1, -1):
            dA = delta.dot(self.weights[l+1].T)
            delta = dA * relu_derivative(pre_activations[l])
            grads_W[l] = activations[l].T.dot(delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m
        return grads_W, grads_b
    
    def update_parameters(self, grads_W, grads_b):
        for i in range(self.num_layers):
            self.weights[i] -= self.learning_rate * grads_W[i]
            self.biases[i] -= self.learning_rate * grads_b[i]
    
    def train(self, X_train, Y_train, max_epochs=50, batch_size=32,
              stopping_threshold=1e-5, adaptive_lr=True, initial_lr=0.01):
        num_samples = X_train.shape[0]
        num_batches = int(np.ceil(num_samples / batch_size))
        prev_val_loss = np.inf
        for epoch in range(max_epochs):
            if adaptive_lr:
                self.learning_rate = initial_lr / np.sqrt(epoch + 1)
            permutation = np.random.permutation(num_samples)
            X_shuffled = X_train[permutation]
            Y_shuffled = Y_train[permutation]
            epoch_loss = 0.0
            for b in range(num_batches):
                start = b * batch_size
                end = min(start + batch_size, num_samples)
                X_batch = X_shuffled[start:end]
                Y_batch = Y_shuffled[start:end]
                activations, pre_activations = self.forward(X_batch)
                batch_loss = self.compute_loss(activations[-1], Y_batch)
                epoch_loss += batch_loss
                grads_W, grads_b = self.backward(X_batch, Y_batch, activations, pre_activations)
                self.update_parameters(grads_W, grads_b)
            avg_loss = epoch_loss / num_batches
            yield epoch, avg_loss
        
    def predict(self, X):
        activations, _ = self.forward(X)
        return np.argmax(activations[-1], axis=1)

##########################################
# PART FUNCTIONS: Experiments (Parts b, c, d, e, f)
##########################################

def part_b(train_folder, test_folder, output_folder):
    """Experiment with Single Hidden Layer Sizes (Part b)."""
    print("Loading training data...")
    X_train, y_train = load_train_data(train_folder)
    X_train_new, X_val, y_train_new, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    num_classes = 43
    Y_train_new_onehot = one_hot_encode(y_train_new, num_classes)
    Y_val_onehot = one_hot_encode(y_val, num_classes)
    
    print("Loading test data...")
    X_test, _, _ = load_test_data_modified(test_folder)
    
    input_size = X_train_new.shape[1]
    hidden_units_list = [1, 5, 10, 50, 100]
    learning_rate = 0.01
    batch_size = 32
    max_epochs = 50
    stopping_threshold = 1e-5
    
    best_val_f1 = -1.0
    best_model = None
    best_hidden_units = None
    
    for hu in hidden_units_list:
        print(f"\n=== Training NN with {hu} hidden unit(s) ===")
        nn_model = NeuralNetwork(input_size, hidden_layers=[hu], output_size=num_classes,
                                 learning_rate=learning_rate, seed=42)
        num_train = X_train_new.shape[0]
        num_batches = int(np.ceil(num_train / batch_size))
        prev_val_loss = np.inf
        
        for epoch in range(max_epochs):
            permutation = np.random.permutation(num_train)
            X_train_epoch = X_train_new[permutation]
            Y_train_epoch = one_hot_encode(y_train_new[permutation], num_classes)
            epoch_loss = 0.0
            for b in range(num_batches):
                start = b * batch_size
                end = min(start + batch_size, num_train)
                X_batch = X_train_epoch[start:end]
                Y_batch = one_hot_encode(y_train_new[start:end], num_classes)
                activations, pre_activations = nn_model.forward(X_batch)
                epoch_loss += nn_model.compute_loss(activations[-1], Y_batch)
                grads_W, grads_b = nn_model.backward(X_batch, Y_batch, activations, pre_activations)
                nn_model.update_parameters(grads_W, grads_b)
            activations_val, _ = nn_model.forward(X_val)
            val_loss = nn_model.compute_loss(activations_val[-1], Y_val_onehot)
            if abs(prev_val_loss - val_loss) &lt; stopping_threshold:
                print(f"Early stopping at epoch {epoch+1}: Δval_loss = {abs(prev_val_loss - val_loss):.6f} &lt; {stopping_threshold}")
                break
            prev_val_loss = val_loss
        
        # Since no ground truth for test data, we use validation f1 only.
        val_preds = nn_model.predict(X_val)
        val_f1 = f1_score(y_val, val_preds, average="weighted", zero_division=0)
        print(f"Hidden Units: {hu}, Validation Weighted F1: {val_f1:.4f}")
        if val_f1 &gt; best_val_f1:
            best_val_f1 = val_f1
            best_model = nn_model
            best_hidden_units = hu
    
    print(f"\nBest configuration: {best_hidden_units} hidden unit(s) with Validation Weighted F1 = {best_val_f1:.4f}")
    test_preds = best_model.predict(X_test)
    output_file = os.path.join(output_folder, "prediction_b.csv")
    pd.DataFrame({"prediction": test_preds}).to_csv(output_file, index=False)
    print(f"Part (b) predictions saved to: {output_file}")

def part_c(train_folder, test_folder, output_folder):
    """Experiment with Varying Network Depth (Part c) using fixed learning rate."""
    print("Loading training data...")
    X_train, y_train = load_train_data(train_folder)
    X_train_new, X_val, y_train_new, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    num_classes = 43
    Y_train_new_onehot = one_hot_encode(y_train_new, num_classes)
    Y_val_onehot = one_hot_encode(y_val, num_classes)
    
    print("Loading test data...")
    X_test, _, _ = load_test_data_modified(test_folder)
    
    input_size = X_train_new.shape[1]
    learning_rate = 0.01
    batch_size = 32
    max_epochs = 250
    stopping_threshold = 1e-6
    hidden_architectures = [
        [512],
        [512, 256],
        [512, 256, 128],
        [512, 256, 128, 64]
    ]
    
    best_architecture = None
    # Without ground truth for test, we select the architecture based on training/validation behavior.
    for arch in hidden_architectures:
        print(f"\n=== Training NN with architecture {arch} (Depth: {len(arch)}) ===")
        nn_model = NeuralNetwork(input_size, hidden_layers=arch, output_size=num_classes,
                                 learning_rate=learning_rate, seed=42)
        num_train = X_train_new.shape[0]
        num_batches = int(np.ceil(num_train / batch_size))
        prev_val_loss = np.inf
        for epoch in range(max_epochs):
            permutation = np.random.permutation(num_train)
            X_train_epoch = X_train_new[permutation]
            Y_train_epoch = one_hot_encode(y_train_new[permutation], num_classes)
            epoch_loss = 0.0
            for b in range(num_batches):
                start = b * batch_size
                end = min(start + batch_size, num_train)
                X_batch = X_train_epoch[start:end]
                Y_batch = one_hot_encode(y_train_new[start:end], num_classes)
                activations, pre_activations = nn_model.forward(X_batch)
                epoch_loss += nn_model.compute_loss(activations[-1], Y_batch)
                grads_W, grads_b = nn_model.backward(X_batch, Y_batch, activations, pre_activations)
                nn_model.update_parameters(grads_W, grads_b)
            activations_val, _ = nn_model.forward(X_val)
            val_loss = nn_model.compute_loss(activations_val[-1], Y_val_onehot)
            if abs(prev_val_loss - val_loss) &lt; stopping_threshold:
                print(f"Early stopping at epoch {epoch+1} for architecture {arch}")
                break
            prev_val_loss = val_loss
        best_architecture = arch  # Without test ground truth, we simply select the last trained architecture.
    print(f"\nSelected architecture: {best_architecture}")
    # Train final model with selected architecture.
    final_model = NeuralNetwork(input_size, hidden_layers=best_architecture, output_size=num_classes,
                                learning_rate=learning_rate, seed=42)
    num_train = X_train_new.shape[0]
    num_batches = int(np.ceil(num_train / batch_size))
    for epoch in range(max_epochs):
        permutation = np.random.permutation(num_train)
        X_train_epoch = X_train_new[permutation]
        Y_train_epoch = one_hot_encode(y_train_new[permutation], num_classes)
        for b in range(num_batches):
            start = b * batch_size
            end = min(start + batch_size, num_train)
            X_batch = X_train_epoch[start:end]
            Y_batch = one_hot_encode(y_train_new[start:end], num_classes)
            activations, pre_activations = final_model.forward(X_batch)
            grads_W, grads_b = final_model.backward(X_batch, Y_batch, activations, pre_activations)
            final_model.update_parameters(grads_W, grads_b)
    test_preds = final_model.predict(X_test)
    output_file = os.path.join(output_folder, "prediction_c.csv")
    pd.DataFrame({"prediction": test_preds}).to_csv(output_file, index=False)
    print(f"Part (c) predictions saved to: {output_file}")

def part_d(train_folder, test_folder, output_folder):
    """Experiment with Varying Network Depth using Adaptive Learning Rate (Part d, sigmoid)."""
    print("Loading training data...")
    X_train, y_train = load_train_data(train_folder)
    X_train_new, X_val, y_train_new, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    num_classes = 43
    Y_train_new_onehot = one_hot_encode(y_train_new, num_classes)
    Y_val_onehot = one_hot_encode(y_val, num_classes)
    
    print("Loading test data...")
    X_test, _, _ = load_test_data_modified(test_folder)
    
    input_size = X_train_new.shape[1]
    initial_lr = 0.01
    batch_size = 32
    max_epochs = 250
    stopping_threshold = 1e-6
    hidden_architectures = [
        [512],
        [512, 256],
        [512, 256, 128],
        [512, 256, 128, 64]
    ]
    
    best_architecture = None
    depths = []
    
    for arch in hidden_architectures:
        print(f"\n=== Training NN with Architecture: {arch} (Depth: {len(arch)}) ===")
        depths.append(len(arch))
        nn_model = NeuralNetwork(input_size, hidden_layers=arch, output_size=num_classes,
                                 learning_rate=initial_lr, seed=42)
        num_train = X_train_new.shape[0]
        num_batches = int(np.ceil(num_train / batch_size))
        prev_val_loss = np.inf
        for epoch in range(max_epochs):
            current_lr = initial_lr / np.sqrt(epoch + 1)
            nn_model.learning_rate = current_lr
            permutation = np.random.permutation(num_train)
            X_train_epoch = X_train_new[permutation]
            Y_train_epoch = one_hot_encode(y_train_new[permutation], num_classes)
            epoch_loss = 0.0
            for b in range(num_batches):
                start = b * batch_size
                end = min(start + batch_size, num_train)
                X_batch = X_train_epoch[start:end]
                Y_batch = one_hot_encode(y_train_new[start:end], num_classes)
                activations, pre_activations = nn_model.forward(X_batch)
                epoch_loss += nn_model.compute_loss(activations[-1], Y_batch)
                grads_W, grads_b = nn_model.backward(X_batch, Y_batch, activations, pre_activations)
                nn_model.update_parameters(grads_W, grads_b)
            activations_val, _ = nn_model.forward(X_val)
            val_loss = nn_model.compute_loss(activations_val[-1], Y_val_onehot)
            if (epoch+1) % 10 == 0:
                print(f"Epoch {epoch+1}/{max_epochs}, Training Loss: {val_loss:.6f} (lr = {current_lr:.6f})")
            if abs(prev_val_loss - val_loss) &lt; stopping_threshold:
                print(f"Early stopping at epoch {epoch+1} for architecture {arch}: Δval_loss = {abs(prev_val_loss - val_loss):.6f} &lt; {stopping_threshold}")
                break
            prev_val_loss = val_loss
        best_architecture = arch  # Without test ground truth, we select the last architecture trained.
    print(f"\nSelected architecture (adaptive LR): {best_architecture}")
    final_model = NeuralNetwork(input_size, hidden_layers=best_architecture, output_size=num_classes,
                                learning_rate=initial_lr, seed=42)
    num_train = X_train_new.shape[0]
    num_batches = int(np.ceil(num_train / batch_size))
    for epoch in range(max_epochs):
        current_lr = initial_lr / np.sqrt(epoch + 1)
        final_model.learning_rate = current_lr
        permutation = np.random.permutation(num_train)
        X_train_epoch = X_train_new[permutation]
        Y_train_epoch = one_hot_encode(y_train_new[permutation], num_classes)
        for b in range(num_batches):
            start = b * batch_size
            end = min(start + batch_size, num_train)
            X_batch = X_train_epoch[start:end]
            Y_batch = one_hot_encode(y_train_new[start:end], num_classes)
            activations, pre_activations = final_model.forward(X_batch)
            grads_W, grads_b = final_model.backward(X_batch, Y_batch, activations, pre_activations)
            final_model.update_parameters(grads_W, grads_b)
    test_preds = final_model.predict(X_test)
    output_file = os.path.join(output_folder, "prediction_d.csv")
    pd.DataFrame({"prediction": test_preds}).to_csv(output_file, index=False)
    print(f"Part (d) predictions saved to: {output_file}")

def part_e(train_folder, test_folder, output_folder):
    """Experiment with ReLU Activation using Adaptive Learning Rate (Part e)."""
    print("Loading training data...")
    X_train, y_train = load_train_data(train_folder)
    X_train_new, X_val, y_train_new, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    num_classes = 43
    Y_train_new_onehot = one_hot_encode(y_train_new, num_classes)
    Y_val_onehot = one_hot_encode(y_val, num_classes)
    
    print("Loading test data...")
    X_test, _, _ = load_test_data_modified(test_folder)
    
    input_size = X_train_new.shape[1]
    initial_lr = 0.01
    batch_size = 32
    max_epochs = 50
    stopping_threshold = 1e-5
    hidden_architectures = [
        [512],
        [512, 256],
        [512, 256, 128],
        [512, 256, 128, 64]
    ]
    
    train_f1_scores = []
    test_f1_scores = []
    depths = []
    best_architecture = None
    # Since no test ground truth, select based on last configuration.
    for arch in hidden_architectures:
        print(f"\n=== Training Neural Network (ReLU) with Architecture: {arch} (Depth: {len(arch)}) ===")
        depths.append(len(arch))
        nn_model = NeuralNetworkReLU(input_size, hidden_layers=arch, output_size=num_classes,
                                     learning_rate=initial_lr, seed=42)
        num_train = X_train_new.shape[0]
        num_batches = int(np.ceil(num_train / batch_size))
        prev_val_loss = np.inf
        finished_epoch = 0
        for epoch, train_loss in nn_model.train(X_train_new, Y_train_new_onehot,
                                                max_epochs=max_epochs, batch_size=batch_size,
                                                stopping_threshold=stopping_threshold,
                                                adaptive_lr=True, initial_lr=initial_lr):
            current_lr = initial_lr / np.sqrt(epoch + 1)
            nn_model.learning_rate = current_lr
            activations_val, _ = nn_model.forward(X_val)
            val_loss = nn_model.compute_loss(activations_val[-1], Y_val_onehot)
            if (epoch+1) % 10 == 0:
                print(f"Epoch {epoch+1}/{max_epochs}, Training Loss: {val_loss:.6f} (lr = {current_lr:.6f})")
            if abs(prev_val_loss - val_loss) &lt; stopping_threshold:
                print(f"Early stopping at epoch {epoch+1}: Δval_loss = {abs(prev_val_loss - val_loss):.6f} &lt; {stopping_threshold}")
                finished_epoch = epoch + 1
                break
            prev_val_loss = val_loss
            finished_epoch = epoch + 1
        print(f"Finished training at epoch {finished_epoch}")
        train_preds = nn_model.predict(X_train_new)
        print("\nClassification Report on Training Data:")
        print(classification_report(y_train_new, train_preds, zero_division=0))
        test_preds = nn_model.predict(X_test)
        print("\n(No ground truth available for test data, skipping test classification report.)")
        # Here we simply record the training performance.
        train_report = classification_report(y_train_new, train_preds, output_dict=True, zero_division=0)
        train_avg_f1 = train_report["weighted avg"]["f1-score"]
        train_f1_scores.append(train_avg_f1)
        best_architecture = arch
    print(f"\nSelected architecture (ReLU, adaptive LR): {best_architecture}")
    final_test_preds = nn_model.predict(X_test)
    output_file = os.path.join(output_folder, "prediction_e.csv")
    pd.DataFrame({"prediction": final_test_preds}).to_csv(output_file, index=False)
    print(f"Part (e) predictions saved to: {output_file}")

def part_f(train_folder, test_folder, output_folder):
    """Experiment with MLPClassifier using scikit-learn (Part f)."""
    print("Loading training data for MLPClassifier...")
    X_train, y_train = load_train_data(train_folder)
    print("Loading test data for MLPClassifier...")
    X_test, _, _ = load_test_data_modified(test_folder)
    
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import classification_report, f1_score
    
    hidden_architectures = [
        [512],
        [512, 256],
        [512, 256, 128],
        [512, 256, 128, 64]
    ]
    
    train_f1_scores_mlp = []
    test_f1_scores_mlp = []
    depths = []
    best_arch = None
    # Since no ground truth exists, select the architecture based on training behavior.
    for arch in hidden_architectures:
        print(f"\n=== Training MLPClassifier with architecture: {arch} (Depth: {len(arch)}) ===")
        depths.append(len(arch))
        clf = MLPClassifier(
            hidden_layer_sizes=tuple(arch),
            activation='relu',
            solver='sgd',
            alpha=0,
            batch_size=32,
            learning_rate='invscaling',
            learning_rate_init=0.01,
            max_iter=50,
            early_stopping=True,
            random_state=42
        )
        clf.fit(X_train, y_train)
        train_preds = clf.predict(X_train)
        train_report_dict = classification_report(y_train, train_preds, output_dict=True, zero_division=0)
        train_avg_f1 = train_report_dict["weighted avg"]["f1-score"]
        train_f1_scores_mlp.append(train_avg_f1)
        best_arch = arch  # Without test labels, we choose the last architecture.
        best_clf = clf
        print(f"Architecture {arch}: Train Weighted F1 = {train_avg_f1:.4f}")
    print(f"\nSelected architecture (MLPClassifier): {best_arch}")
    final_test_preds = best_clf.predict(X_test)
    output_file = os.path.join(output_folder, "prediction_f.csv")
    pd.DataFrame({"prediction": final_test_preds}).to_csv(output_file, index=False)
    print(f"Part (f) predictions saved to: {output_file}")

##########################################
# Main Execution
##########################################
def main():
    if len(sys.argv) != 5:
        print("Usage: python neural_network.py &lt;train_folder_path&gt; &lt;test_folder_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        sys.exit(1)
    
    train_folder = sys.argv[1]
    test_folder = sys.argv[2]
    output_folder = sys.argv[3]
    question_part = sys.argv[4].lower()  # Expect 'b', 'c', 'd', 'e', or 'f'
    
    os.makedirs(output_folder, exist_ok=True)
    
    if question_part == "b":
        part_b(train_folder, test_folder, output_folder)
    elif question_part == "c":
        part_c(train_folder, test_folder, output_folder)
    elif question_part == "d":
        part_d(train_folder, test_folder, output_folder)
    elif question_part == "e":
        part_e(train_folder, test_folder, output_folder)
    elif question_part == "f":
        part_f(train_folder, test_folder, output_folder)
    else:
        print("For now, only parts 'b', 'c', 'd', 'e', and 'f' are implemented in neural_network.py.")
        sys.exit(1)

if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
