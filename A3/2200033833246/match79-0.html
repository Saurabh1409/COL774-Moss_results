<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_R0S9Q.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_R0S9Q.py<p><PRE>


import sys
import os
import time

from dt.parta import parta
from dt.partb import partb
from dt.partc import partc
from dt.partd_i import partdi
from dt.parte import parte

def main():

    train_data_path = sys.argv[1]
    validation_data_path = sys.argv[2]
    test_data_path = sys.argv[3]
    output_folder_path = sys.argv[4]
    question_part = sys.argv[5].lower()

    if not os.path.exists(output_folder_path):
        os.makedirs(output_folder_path)


    if question_part == 'a':
        parta(train_data_path, validation_data_path, test_data_path, output_folder_path)
    elif question_part == 'b':
        partb(train_data_path, validation_data_path, test_data_path, output_folder_path)
    elif question_part == 'c':
        partc(train_data_path, validation_data_path, test_data_path, output_folder_path)
    elif question_part == 'd':
        partdi(train_data_path, validation_data_path, test_data_path, output_folder_path)
    elif question_part == 'e':
        parte(train_data_path, validation_data_path, test_data_path, output_folder_path)

if __name__ == "__main__":
    main()




import sys
import os
import time

from nn.partb import partb
from nn.partc import partc
from nn.partd import partd
from nn.parte import parte
from nn.partf import partf

def main():

    train_data_path = sys.argv[1]
    test_data_path = sys.argv[2]
    output_folder_path = sys.argv[3]
    question_part = sys.argv[4].lower()

    if not os.path.exists(output_folder_path):
        os.makedirs(output_folder_path)

    output_file=output_folder_path+f"/prediction_{question_part}.csv"
    if question_part == 'b':
        partb(train_data_path, test_data_path, output_file)
    elif question_part == 'c':
        partc(train_data_path, test_data_path, output_file)
    elif question_part == 'd':
        partd(train_data_path, test_data_path, output_file)
    elif question_part == 'e':
        parte(train_data_path, test_data_path, output_file)
    elif question_part == 'f':
        partf(train_data_path, test_data_path, output_file)

if __name__ == "__main__":
    main()




import matplotlib.pyplot as plt
def plot_acc(depths, train_accs, test_accs, val_accs=[]):
    plt.plot(depths, train_accs, label='Train F1-Score', marker='o')
    # plt.plot(depths, val_accs, label='Val Accuracy', marker='o')
    plt.plot(depths, test_accs, label='Test F1-Score', marker='o')
    plt.xlabel('Number of Hidden Layer')
    plt.ylabel('Average F1-Score')
    plt.legend()
    plt.grid()
    plt.show()
    
depths = ["[512]", "[512,256]", "[512,256,128]", "[512,256,128,64]"]
train_accs = [0.9060,0.9201,0.9098,0.8955]
test_accs = [0.7721,0.7930,0.7754,0.7644]
plot_acc(depths, train_accs,test_accs)




import numpy as np
import math
from collections import Counter, defaultdict
import csv
import matplotlib.pyplot as plt
np.random.seed(42)

class Node:
    def __init__(self, is_leaf, prediction=None, feature=None, threshold=None, children=None):
        self.is_leaf = is_leaf
        self.prediction = prediction
        self.feature = feature
        self.threshold = threshold
        self.children = children or {}

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def entropy(y):
    """Vectorized entropy using NumPy."""
    y = np.array(y)
    if len(y) == 0:
        return 0
    counts = np.bincount(y)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-9)) 

def information_gain_categorical(X, y, feature_idx):
    H = entropy(y)
    subsets = defaultdict(list)
    for i in range(len(X)):
        subsets[X[i][feature_idx]].append(y[i])
    weighted_entropy = sum((len(subset) / len(y)) * entropy(subset) for subset in subsets.values())
    return H - weighted_entropy

def information_gain_numeric(X, y, feature_idx):
    values = np.array([float(row[feature_idx]) for row in X])
    median = np.median(values)
    
    left_mask = values &lt;= median
    right_mask = values &gt; median
    
    left_y = np.array(y)[left_mask]
    right_y = np.array(y)[right_mask]

    if len(left_y) == 0 or len(right_y) == 0:
        return 0, median

    H = entropy(y)
    w_left, w_right = len(left_y) / len(y), len(right_y) / len(y)
    info_gain = H - (w_left * entropy(left_y) + w_right * entropy(right_y))
    return info_gain, median

def best_split(X, y, categorical_indices, numeric_indices):
    best_feature = None
    best_info_gain = -1
    best_threshold = None
    is_categorical = True

    for idx in categorical_indices:
        ig = information_gain_categorical(X, y, idx)
        if ig &gt; best_info_gain:
            best_info_gain = ig
            best_feature = idx
            best_threshold = None
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match79-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            is_categorical = True

    for idx in numeric_indices:
        ig, median = information_gain_numeric(X, y, idx)
        if ig &gt; best_info_gain:
            best_info_gain = ig
            best_feature = idx
            best_threshold = median
</FONT>            is_categorical = False

    return best_feature, best_threshold, is_categorical

def build_tree(X, y, depth, max_depth, categorical_indices, numeric_indices):
    y_array = np.array(y)
    if depth == max_depth or len(set(y_array)) == 1:
<A NAME="5"></A><FONT color = #FF0000><A HREF="match79-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        prediction = Counter(y_array).most_common(1)[0][0]
        return Node(is_leaf=True, prediction=prediction)

    feature, threshold, is_cat = best_split(X, y_array, categorical_indices, numeric_indices)
</FONT>    if feature is None:
        prediction = Counter(y_array).most_common(1)[0][0]
        return Node(is_leaf=True, prediction=prediction)

    if is_cat:
        children = {}
        subsets = defaultdict(lambda: ([], []))
        for i in range(len(X)):
            val = X[i][feature]
            subsets[val][0].append(X[i])
            subsets[val][1].append(y[i])
        for val, (sub_X, sub_y) in subsets.items():
            children[val] = build_tree(sub_X, sub_y, depth + 1, max_depth, categorical_indices, numeric_indices)
        return Node(is_leaf=False, feature=feature, children=children)
    else:
        left_X, left_y, right_X, right_y = [], [], [], []
        for i in range(len(X)):
            val = float(X[i][feature])
            if val &lt;= threshold:
                left_X.append(X[i])
                left_y.append(y[i])
            else:
                right_X.append(X[i])
                right_y.append(y[i])
        left_child = build_tree(left_X, left_y, depth + 1, max_depth, categorical_indices, numeric_indices)
        right_child = build_tree(right_X, right_y, depth + 1, max_depth, categorical_indices, numeric_indices)
        return Node(is_leaf=False, feature=feature, threshold=threshold, children={'&lt;=': left_child, '&gt;': right_child})

def predict_one(x, node):
    while not node.is_leaf:
        val = x[node.feature]
        if node.threshold is not None:
            try:
                val = float(val)
                node = node.children['&lt;='] if val &lt;= node.threshold else node.children['&gt;']
            except ValueError:
                break
        else:
            if val in node.children:
                node = node.children[val]
            else:
                break
    return node.prediction

def predict(X, tree):
    return [predict_one(x, tree) for x in X]

def accuracy(y_true, y_pred):
    return np.mean(np.array(y_true) == np.array(y_pred))

def process_labels(data):
    return [1 if row[-1].strip() == '&gt;50K' else 0 for row in data]

def get_feature_indices(header):
    categorical_features = {'workclass', 'education', 'marital.status', 'occupation',
                            'relationship', 'race', 'sex', 'native.country'}
    categorical_indices = [i for i, name in enumerate(header[:-1]) if name.strip() in categorical_features]
    numeric_indices = [i for i, name in enumerate(header[:-1]) if name.strip() not in categorical_features]
    return categorical_indices, numeric_indices

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def plot_acc(depths, train_accs, test_accs):
    plt.plot(depths, train_accs, label='Train Accuracy', marker='o')
    plt.plot(depths, test_accs, label='Test Accuracy', marker='o')
    plt.xlabel('Tree Depth')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()
def parta(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_a.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)

    categorical_indices, numeric_indices = get_feature_indices(header)

    train_X, train_y = [row[:-1] for row in train_data], process_labels(train_data)
    valid_X, valid_y = [row[:-1] for row in valid_data], process_labels(valid_data)
    test_X, test_y = [row[:-1] for row in test_data], process_labels(test_data)
    depths = [1, 5, 10, 15, 20]
    # test_accs = []
    # train_accs = []
    for depth in depths:
        tree = build_tree(train_X, train_y, 0, depth, categorical_indices, numeric_indices)
        train_acc = accuracy(train_y, predict(train_X, tree))
        test_preds = predict(test_X, tree)
        test_acc = accuracy(test_y, test_preds)
        
        print(f"{depth} {train_acc:.4f} {test_acc:.4f}")
        # test_accs.append(test_acc)
        # train_accs.append(train_acc)
        # plot_acc(depths, train_accs, test_accs)
        save_predictions(test_preds, output_path)

# if __name__ == '__main__':
#     main()




import numpy as np
import math
from collections import Counter
import csv
from sklearn.preprocessing import OneHotEncoder
from queue import Queue
import time
np.random.seed(42)

class Node:
    def __init__(self, is_leaf, prediction=None, feature=None, threshold=None, children=None):
        self.is_leaf = is_leaf
        self.prediction = prediction
        self.feature = feature
        self.threshold = threshold
        self.children = children or {}

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def entropy(y):
    counts = np.bincount(y)  
    probs = counts / len(y)  
    return -np.sum(probs * np.log2(probs + 1e-9))  

def information_gain(X, y, feature_idx):
    values = X[:, feature_idx]
    
    median = np.median(values)
    
    left_mask = values &lt;= median
    right_mask = values &gt; median

    left_y = y[left_mask]
    right_y = y[right_mask]

    if len(left_y) == 0 or len(right_y) == 0:
        return 0, median

    H = entropy(y)
    w_left = len(left_y) / len(y)
    w_right = len(right_y) / len(y)
    info_gain = H - (w_left * entropy(left_y) + w_right * entropy(right_y))
    return info_gain, median


def best_split(X, y):
    """Find the best feature and threshold to split on."""
    best_feature = None
    best_info_gain = -1
    best_threshold = None
    
    H = entropy(y) 
    
    for idx in range(X.shape[1]):
        values = X[:, idx]
        
        median = np.median(values)
        
        left_mask = values &lt;= median
        right_mask = values &gt; median
        
        left_y = y[left_mask]
        right_y = y[right_mask]

        if len(left_y) == 0 or len(right_y) == 0:
            continue

        w_left = len(left_y) / len(y)
        w_right = len(right_y) / len(y)
        
        info_gain = H - (w_left * entropy(left_y) + w_right * entropy(right_y))
        
        if info_gain &gt; best_info_gain:
            best_info_gain = info_gain
            best_feature = idx
            best_threshold = median
    
    return best_feature, best_threshold

def build_tree(X, y, max_depth):
    """Build the decision tree using an iterative breadth-first approach."""
    n_samples = X.shape[0]
    root = Node(is_leaf=False)
    
    queue = Queue()
    queue.put((root, np.arange(n_samples), 0))

    while not queue.empty():
        node, indices, depth = queue.get()
        y_sub = y[indices]

        if depth == max_depth or len(set(y_sub)) == 1:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        best_feature, best_threshold = best_split(X[indices], y_sub)

        if best_feature is None:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        
        values = X[indices, best_feature]
        
        left_indices = indices[values &lt;= best_threshold]
        right_indices = indices[values &gt; best_threshold]

        node.feature = best_feature
        node.threshold = best_threshold
        
        left_child = Node(is_leaf=False)
        right_child = Node(is_leaf=False)
        
        node.children['&lt;='] = left_child
        node.children['&gt;'] = right_child
        
        queue.put((left_child, left_indices, depth + 1))
        queue.put((right_child, right_indices, depth + 1))

    return root

def predict_one(x, node):
    """Predict the label for a single sample by traversing the tree."""
    while not node.is_leaf:
        val = x[node.feature]
        node = node.children['&lt;='] if val &lt;= node.threshold else node.children['&gt;']
    return node.prediction

def predict(X, tree):
    """Predict labels for all samples in X."""
    return np.array([predict_one(x, tree) for x in X])

def accuracy(y_true, y_pred):
    """Calculate accuracy of predictions."""
    return np.mean(y_true == y_pred)

def save_predictions(predictions, filename):
    """Save predictions to a CSV file."""
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def partb(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_b.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)

    X_train_raw = np.array([row[:-1] for row in train_data])
    y_train = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in train_data])

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_train = encoder.fit_transform(X_train_raw)

    X_valid_raw = np.array([row[:-1] for row in valid_data])
    y_valid = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in valid_data])
    X_valid = encoder.transform(X_valid_raw)

    X_test_raw = np.array([row[:-1] for row in test_data])
    y_test = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in test_data])
    X_test = encoder.transform(X_test_raw)

    for depth in [25,35,45,55]:
    
        tree = build_tree(X_train, y_train, depth)

        train_preds = predict(X_train, tree)
        # valid_preds = predict(X_valid, tree)
        test_preds = predict(X_test, tree)

        train_acc = accuracy(y_train, train_preds)
        # valid_acc = accuracy(y_valid, valid_preds)
        test_acc = accuracy(y_test, test_preds)
        print(f"{depth} {train_acc:.4f} {test_acc:.4f}")
        
        # print(f"{depth} {train_acc:.4f} {valid_acc:.4f} {test_acc:.4f}")
        
        save_predictions(test_preds, output_path)

# if __name__ == '__main__':
#     start_time = time.time()
#     main()
#     end_time = time.time()
#     print(f"Total runtime: {end_time - start_time:.2f} seconds")



import numpy as np
import math
from collections import Counter
import csv
from sklearn.preprocessing import OneHotEncoder
from queue import Queue

np.random.seed(42)

class Node:
    def __init__(self, is_leaf, prediction=None, feature=None, threshold=None, children=None):
        self.is_leaf = is_leaf
        self.prediction = prediction
        self.feature = feature
        self.threshold = threshold
        self.children = children or {}

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def entropy(y):
    counts = np.bincount(y)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-9))

def best_split(X, y):
    best_feature = None
    best_info_gain = -1
    best_threshold = None
    H = entropy(y)

    for idx in range(X.shape[1]):
        values = X[:, idx]
        median = np.median(values)

        left_mask = values &lt;= median
        right_mask = values &gt; median

        left_y = y[left_mask]
        right_y = y[right_mask]

        if len(left_y) == 0 or len(right_y) == 0:
            continue

        w_left = len(left_y) / len(y)
        w_right = len(right_y) / len(y)

        info_gain = H - (w_left * entropy(left_y) + w_right * entropy(right_y))

        if info_gain &gt; best_info_gain:
            best_info_gain = info_gain
            best_feature = idx
            best_threshold = median

    return best_feature, best_threshold

def build_tree(X, y, max_depth):
    n_samples = X.shape[0]
    root = Node(is_leaf=False)
    queue = Queue()
    queue.put((root, np.arange(n_samples), 0))

    while not queue.empty():
        node, indices, depth = queue.get()
        y_sub = y[indices]

        if depth == max_depth or len(set(y_sub)) == 1:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        best_feature, best_threshold = best_split(X[indices], y_sub)

        if best_feature is None:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        values = X[indices, best_feature]
        left_indices = indices[values &lt;= best_threshold]
        right_indices = indices[values &gt; best_threshold]

        node.feature = best_feature
        node.threshold = best_threshold

        left_child = Node(is_leaf=False)
        right_child = Node(is_leaf=False)

        node.children['&lt;='] = left_child
        node.children['&gt;'] = right_child

        queue.put((left_child, left_indices, depth + 1))
        queue.put((right_child, right_indices, depth + 1))

    return root

def predict_one(x, node):
    while not node.is_leaf:
        val = x[node.feature]
        node = node.children['&lt;='] if val &lt;= node.threshold else node.children['&gt;']
    return node.prediction

def predict(X, tree):
    return np.array([predict_one(x, tree) for x in X])

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def collect_prunable_nodes(node, path=()):
    prunable = []
    if not node.is_leaf and node.children:
        prunable.append((node, path))
        for direction, child in node.children.items():
            prunable.extend(collect_prunable_nodes(child, path + ((node, direction),)))
    return prunable

def prune_node(node):
    leaf_labels = []
    def collect_labels(n):
        if n.is_leaf:
            leaf_labels.append(n.prediction)
        else:
            for child in n.children.values():
                collect_labels(child)
    collect_labels(node)
    if leaf_labels:
        node.is_leaf = True
        node.prediction = Counter(leaf_labels).most_common(1)[0][0]
        node.children = {}

def post_prune(tree, X_valid, y_valid):
    best_acc = accuracy(y_valid, predict(X_valid, tree))

    while True:
        prunable_nodes = collect_prunable_nodes(tree)
        best_improvement = 0
        best_node_to_prune = None

        for node, _ in prunable_nodes:
            backup_children = node.children
            backup_leaf = node.is_leaf
            backup_prediction = node.prediction

            prune_node(node)
            new_acc = accuracy(y_valid, predict(X_valid, tree))

            improvement = new_acc - best_acc

            if improvement &gt; best_improvement:
                best_improvement = improvement
                best_node_to_prune = node
            else:
                node.is_leaf = backup_leaf
                node.children = backup_children
                node.prediction = backup_prediction

        if best_node_to_prune:
            prune_node(best_node_to_prune)
            best_acc += best_improvement
        else:
            break

    return tree

def partc(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_c.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)

    X_train_raw = np.array([row[:-1] for row in train_data])
    y_train = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in train_data])

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_train = encoder.fit_transform(X_train_raw)

    X_valid_raw = np.array([row[:-1] for row in valid_data])
    y_valid = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in valid_data])
    X_valid = encoder.transform(X_valid_raw)

    X_test_raw = np.array([row[:-1] for row in test_data])
    y_test = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in test_data])
    X_test = encoder.transform(X_test_raw)

    for depth in [25, 35, 45, 55]:
        tree = build_tree(X_train, y_train, depth)
        tree = post_prune(tree, X_valid, y_valid)

        train_preds = predict(X_train, tree)
        valid_preds = predict(X_valid, tree)
        test_preds = predict(X_test, tree)

        train_acc = accuracy(y_train, train_preds)
        valid_acc = accuracy(y_valid, valid_preds)
        test_acc = accuracy(y_test, test_preds)

        print(f"{depth} {train_acc:.4f} {valid_acc:.4f} {test_acc:.4f}")
        save_predictions(test_preds, output_path)

# if __name__ == '__main__':
#     main()




import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
import csv

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def process_labels(data):
    return np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in data])


def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def partdi(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_d.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)

    train_X_raw = np.array([row[:-1] for row in train_data])
    valid_X_raw = np.array([row[:-1] for row in valid_data])
    test_X_raw = np.array([row[:-1] for row in test_data])

    train_y = process_labels(train_data)
    valid_y = process_labels(valid_data)
    test_y = process_labels(test_data)

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_train = encoder.fit_transform(train_X_raw)
    X_valid = encoder.transform(valid_X_raw)
    X_test = encoder.transform(test_X_raw)

    depths = [25, 35, 45, 55]
    
    for depth in depths:
        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
        clf.fit(X_train, train_y)

        train_acc = accuracy_score(train_y, clf.predict(X_train))
        valid_acc = accuracy_score(valid_y, clf.predict(X_valid))
        test_acc = accuracy_score(test_y, clf.predict(X_test))

        print(f"{depth}\t{train_acc:.4f}\t\t{valid_acc:.4f}\t\t{test_acc:.4f}")
        save_predictions(clf.predict(X_test), output_path)


# if __name__ == '__main__':
#     main()




from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import csv

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def process_labels(data):
    return np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in data])


def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])
def partdii(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_d.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)
 
    X_train_raw = np.array([row[:-1] for row in train_data])
    y_train = process_labels(train_data)

    X_valid_raw = np.array([row[:-1] for row in valid_data])
    y_valid = process_labels(valid_data)

    X_test_raw = np.array([row[:-1] for row in test_data])
    y_test = process_labels(test_data)

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_train = encoder.fit_transform(X_train_raw)
    X_valid = encoder.transform(X_valid_raw)
    X_test = encoder.transform(X_test_raw)

    ccp_alphas = [0.001, 0.01, 0.1, 0.2]

    for alpha in ccp_alphas:
        clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha, random_state=42)
        clf.fit(X_train, y_train)

        train_acc = accuracy_score(y_train, clf.predict(X_train))
        valid_acc = accuracy_score(y_valid, clf.predict(X_valid))
        test_acc = accuracy_score(y_test, clf.predict(X_test))

        print(f"{alpha:.3f}\t\t{train_acc:.4f}\t\t{valid_acc:.4f}\t\t{test_acc:.4f}")
        save_predictions(clf.predict(X_test), output_path)





import numpy as np
import csv
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def prepare_data(data, encoder=None, fit_encoder=False):
    X_raw = np.array([row[:-1] for row in data])
    y = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in data])
    if fit_encoder:
        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        X = encoder.fit_transform(X_raw)
    else:
        X = encoder.transform(X_raw)
    return X, y, encoder

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def parte(train_data_path, validation_data_path, test_data_path, output_path):
    output_path = output_path + "/prediction_e.csv"
    header, train_data = read_data(train_data_path)
    _, valid_data = read_data(validation_data_path)
    _, test_data = read_data(test_data_path)

    X_train, y_train, encoder = prepare_data(train_data, fit_encoder=True)
    X_valid, y_valid, _ = prepare_data(valid_data, encoder)
    X_test, y_test, _ = prepare_data(test_data, encoder)

    param_grid = {
        'n_estimators': [50, 150, 250, 350],
        'max_features': [0.1, 0.3, 0.5, 0.7, 1.0],
        'min_samples_split': [2, 4, 6, 8, 10]
    }


    best_oob_accuracy = 0
    best_params = None

    for params in ParameterGrid(param_grid):
        print(f"Params: {params}")
        model = RandomForestClassifier( criterion='entropy', n_estimators=params['n_estimators'],
            max_features=params['max_features'], min_samples_split=params['min_samples_split'],
            oob_score=True, bootstrap=True,
            n_jobs=-1, random_state=42
        )

        model.fit(X_train, y_train)

        if hasattr(model, 'oob_score_'):
            oob_acc = model.oob_score_
            print(f"OOB Accuracy: {oob_acc:.4f}")

            if oob_acc &gt; best_oob_accuracy:
                best_oob_accuracy = oob_acc
                best_params = params

    print(f"\nBest Parameters: {best_params}")
    print(f"Best OOB Accuracy: {best_oob_accuracy:.4f}")

    final_model = RandomForestClassifier(
        criterion='entropy', n_estimators=best_params['n_estimators'], max_features=best_params['max_features'],
        min_samples_split=best_params['min_samples_split'], oob_score=False,
        bootstrap=True, n_jobs=-1, random_state=42
    )
    final_model.fit(X_train, y_train)

    train_preds = final_model.predict(X_train)
    valid_preds = final_model.predict(X_valid)
    test_preds = final_model.predict(X_test)

    train_acc = accuracy_score(y_train, train_preds)
    valid_acc = accuracy_score(y_valid, valid_preds)
    test_acc = accuracy_score(y_test, test_preds)

    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Validation Accuracy: {valid_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")

    save_predictions(test_preds, output_path)

# if __name__ == '__main__':
#     main()




import numpy as np
import math
from collections import Counter
import csv
from sklearn.preprocessing import OneHotEncoder
from queue import Queue

np.random.seed(42)

class Node:
    def __init__(self, is_leaf, prediction=None, feature=None, threshold=None, children=None):
        self.is_leaf = is_leaf
        self.prediction = prediction
        self.feature = feature
        self.threshold = threshold
        self.children = children or {}

def read_data(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    header = lines[0].strip().split(',')
    data = [line.strip().split(',') for line in lines[1:] if line.strip()]
    return header, data

def entropy(y):
    counts = np.bincount(y)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-9))

def best_split(X, y):
    best_feature = None
    best_info_gain = -1
    best_threshold = None
    H = entropy(y)

    for idx in range(X.shape[1]):
        values = X[:, idx]
        median = np.median(values)

        left_mask = values &lt;= median
        right_mask = values &gt; median

        left_y = y[left_mask]
        right_y = y[right_mask]

        if len(left_y) == 0 or len(right_y) == 0:
            continue

        w_left = len(left_y) / len(y)
        w_right = len(right_y) / len(y)

        info_gain = H - (w_left * entropy(left_y) + w_right * entropy(right_y))

        if info_gain &gt; best_info_gain:
            best_info_gain = info_gain
            best_feature = idx
            best_threshold = median

    return best_feature, best_threshold

def build_tree(X, y, max_depth):
    n_samples = X.shape[0]
    root = Node(is_leaf=False)
    queue = Queue()
    queue.put((root, np.arange(n_samples), 0))

    while not queue.empty():
        node, indices, depth = queue.get()
        y_sub = y[indices]

        if depth == max_depth or len(set(y_sub)) == 1:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        best_feature, best_threshold = best_split(X[indices], y_sub)

        if best_feature is None:
            node.is_leaf = True
            node.prediction = Counter(y_sub).most_common(1)[0][0]
            continue

        values = X[indices, best_feature]
        left_indices = indices[values &lt;= best_threshold]
        right_indices = indices[values &gt; best_threshold]

        node.feature = best_feature
        node.threshold = best_threshold

        left_child = Node(is_leaf=False)
        right_child = Node(is_leaf=False)

        node.children['&lt;='] = left_child
        node.children['&gt;'] = right_child

        queue.put((left_child, left_indices, depth + 1))
        queue.put((right_child, right_indices, depth + 1))

    return root

def predict_one(x, node):
    while not node.is_leaf:
        val = x[node.feature]
        node = node.children['&lt;='] if val &lt;= node.threshold else node.children['&gt;']
    return node.prediction

def predict(X, tree):
    return np.array([predict_one(x, tree) for x in X])

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])


def collect_prunable_nodes(node, path=()):
    prunable = []
    if not node.is_leaf and node.children:
        prunable.append((node, path))
        for direction, child in node.children.items():
            prunable.extend(collect_prunable_nodes(child, path + ((node, direction),)))
    return prunable

def prune_node(node):
    leaf_labels = []
    def collect_labels(n):
        if n.is_leaf:
            leaf_labels.append(n.prediction)
        else:
            for child in n.children.values():
                collect_labels(child)
    collect_labels(node)
    if leaf_labels:
        node.is_leaf = True
        node.prediction = Counter(leaf_labels).most_common(1)[0][0]
        node.children = {}

def count_nodes(node):
    if node.is_leaf:
        return 1
    return 1 + sum(count_nodes(child) for child in node.children.values())

def post_prune(tree, X_train, y_train, X_valid, y_valid, X_test, y_test):
    history = []

    def record_state():
        train_preds = predict(X_train, tree)
        valid_preds = predict(X_valid, tree)
        test_preds = predict(X_test, tree)
        train_acc = accuracy(y_train, train_preds)
        valid_acc = accuracy(y_valid, valid_preds)
        test_acc = accuracy(y_test, test_preds)
        node_count = count_nodes(tree)
        history.append((node_count, train_acc, valid_acc, test_acc))

    best_acc = accuracy(y_valid, predict(X_valid, tree))
    record_state()  # Initial state

    while True:
        prunable_nodes = collect_prunable_nodes(tree)
        best_improvement = 0
        best_node_to_prune = None

        for node, _ in prunable_nodes:
            backup_children = node.children
            backup_leaf = node.is_leaf
            backup_prediction = node.prediction

            prune_node(node)
            new_acc = accuracy(y_valid, predict(X_valid, tree))

            improvement = new_acc - best_acc

            if improvement &gt; best_improvement:
                best_improvement = improvement
                best_node_to_prune = node
            else:
                node.is_leaf = backup_leaf
                node.children = backup_children
                node.prediction = backup_prediction

        if best_node_to_prune:
            prune_node(best_node_to_prune)
            best_acc += best_improvement
            record_state()
        else:
            break

    return tree, history

def save_prune_history(history, depth):
    with open(f'prune_accuracy_depth_{depth}.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['NumNodes', 'TrainAccuracy', 'ValidAccuracy', 'TestAccuracy'])
        for row in history:
            writer.writerow(row)

def main():
    header, train_data = read_data('./data/train.csv')
    _, valid_data = read_data('./data/valid.csv')
    _, test_data = read_data('./data/test.csv')

    X_train_raw = np.array([row[:-1] for row in train_data])
    y_train = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in train_data])

    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_train = encoder.fit_transform(X_train_raw)

    X_valid_raw = np.array([row[:-1] for row in valid_data])
    y_valid = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in valid_data])
    X_valid = encoder.transform(X_valid_raw)

    X_test_raw = np.array([row[:-1] for row in test_data])
    y_test = np.array([1 if row[-1].strip() == '&gt;50K' else 0 for row in test_data])
    X_test = encoder.transform(X_test_raw)

    for depth in [25, 35, 45, 55]:
        print(f"Building tree of depth {depth}")
        tree = build_tree(X_train, y_train, depth)
        tree, history = post_prune(tree, X_train, y_train, X_valid, y_valid, X_test, y_test)
        save_prune_history(history, depth)

        final_train_acc = history[-1][1]
        final_valid_acc = history[-1][2]
        final_test_acc = history[-1][3]

        print(f"Final for depth {depth}: Train={final_train_acc:.4f}, Valid={final_valid_acc:.4f}, Test={final_test_acc:.4f}")

if __name__ == '__main__':
    main()




import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('prune_accuracy_depth_25.csv')

print(df)
plt.figure(figsize=(10, 6))
plt.plot(df['NumNodes'], df['TrainAccuracy'], label='Train Accuracy', marker='o')
plt.plot(df['NumNodes'], df['ValidAccuracy'], label='Validation Accuracy', marker='o')
plt.plot(df['NumNodes'], df['TestAccuracy'], label='Test Accuracy', marker='o')

plt.xlabel('Number of Nodes')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()




import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

results = [
    {'n_estimators': 50, 'max_features': 0.1, 'min_samples_split': 2, 'OOB Accuracy': 0.8506},
    {'n_estimators': 150, 'max_features': 0.1, 'min_samples_split': 2, 'OOB Accuracy': 0.8533},
    {'n_estimators': 250, 'max_features': 0.1, 'min_samples_split': 2, 'OOB Accuracy': 0.8540},
    {'n_estimators': 350, 'max_features': 0.1, 'min_samples_split': 2, 'OOB Accuracy': 0.8544},
    {'n_estimators': 50, 'max_features': 0.1, 'min_samples_split': 4, 'OOB Accuracy': 0.8520},
    {'n_estimators': 150, 'max_features': 0.1, 'min_samples_split': 4, 'OOB Accuracy': 0.8546},
    {'n_estimators': 250, 'max_features': 0.1, 'min_samples_split': 4, 'OOB Accuracy': 0.8545},
    {'n_estimators': 350, 'max_features': 0.1, 'min_samples_split': 4, 'OOB Accuracy': 0.8553},
    {'n_estimators': 50, 'max_features': 0.1, 'min_samples_split': 6, 'OOB Accuracy': 0.8526},
    {'n_estimators': 150, 'max_features': 0.1, 'min_samples_split': 6, 'OOB Accuracy': 0.8552},
    {'n_estimators': 250, 'max_features': 0.1, 'min_samples_split': 6, 'OOB Accuracy': 0.8544},
    {'n_estimators': 350, 'max_features': 0.1, 'min_samples_split': 6, 'OOB Accuracy': 0.8547},
    {'n_estimators': 50, 'max_features': 0.1, 'min_samples_split': 8, 'OOB Accuracy': 0.8532},
    {'n_estimators': 150, 'max_features': 0.1, 'min_samples_split': 8, 'OOB Accuracy': 0.8547},
    {'n_estimators': 250, 'max_features': 0.1, 'min_samples_split': 8, 'OOB Accuracy': 0.8544},
    {'n_estimators': 350, 'max_features': 0.1, 'min_samples_split': 8, 'OOB Accuracy': 0.8544},
    {'n_estimators': 50, 'max_features': 0.1, 'min_samples_split': 10, 'OOB Accuracy': 0.8542},
    {'n_estimators': 150, 'max_features': 0.1, 'min_samples_split': 10, 'OOB Accuracy': 0.8553},
    {'n_estimators': 250, 'max_features': 0.1, 'min_samples_split': 10, 'OOB Accuracy': 0.8557},
    {'n_estimators': 350, 'max_features': 0.1, 'min_samples_split': 10, 'OOB Accuracy': 0.8553},
    {'n_estimators': 50, 'max_features': 0.3, 'min_samples_split': 2, 'OOB Accuracy': 0.8542},
    {'n_estimators': 150, 'max_features': 0.3, 'min_samples_split': 2, 'OOB Accuracy': 0.8569},
    {'n_estimators': 250, 'max_features': 0.3, 'min_samples_split': 2, 'OOB Accuracy': 0.8581},
    {'n_estimators': 350, 'max_features': 0.3, 'min_samples_split': 2, 'OOB Accuracy': 0.8581},
    {'n_estimators': 50, 'max_features': 0.3, 'min_samples_split': 4, 'OOB Accuracy': 0.8551},
    {'n_estimators': 150, 'max_features': 0.3, 'min_samples_split': 4, 'OOB Accuracy': 0.8575},
    {'n_estimators': 250, 'max_features': 0.3, 'min_samples_split': 4, 'OOB Accuracy': 0.8575},
    {'n_estimators': 350, 'max_features': 0.3, 'min_samples_split': 4, 'OOB Accuracy': 0.8569},
    {'n_estimators': 50, 'max_features': 0.3, 'min_samples_split': 6, 'OOB Accuracy': 0.8559},
    {'n_estimators': 150, 'max_features': 0.3, 'min_samples_split': 6, 'OOB Accuracy': 0.8579},
    {'n_estimators': 250, 'max_features': 0.3, 'min_samples_split': 6, 'OOB Accuracy': 0.8585},
    {'n_estimators': 350, 'max_features': 0.3, 'min_samples_split': 6, 'OOB Accuracy': 0.8583},
    {'n_estimators': 50, 'max_features': 0.3, 'min_samples_split': 8, 'OOB Accuracy': 0.8549},
    {'n_estimators': 150, 'max_features': 0.3, 'min_samples_split': 8, 'OOB Accuracy': 0.8573},
    {'n_estimators': 250, 'max_features': 0.3, 'min_samples_split': 8, 'OOB Accuracy': 0.8579},
    {'n_estimators': 350, 'max_features': 0.3, 'min_samples_split': 8, 'OOB Accuracy': 0.8582},
    {'n_estimators': 50, 'max_features': 0.3, 'min_samples_split': 10, 'OOB Accuracy': 0.8561},
    {'n_estimators': 150, 'max_features': 0.3, 'min_samples_split': 10, 'OOB Accuracy': 0.8578},
    {'n_estimators': 250, 'max_features': 0.3, 'min_samples_split': 10, 'OOB Accuracy': 0.8586},
    {'n_estimators': 350, 'max_features': 0.3, 'min_samples_split': 10, 'OOB Accuracy': 0.8581},
    {'n_estimators': 50, 'max_features': 0.5, 'min_samples_split': 2, 'OOB Accuracy': 0.8557},
    {'n_estimators': 150, 'max_features': 0.5, 'min_samples_split': 2, 'OOB Accuracy': 0.8581},
    {'n_estimators': 250, 'max_features': 0.5, 'min_samples_split': 2, 'OOB Accuracy': 0.8581},
    {'n_estimators': 350, 'max_features': 0.5, 'min_samples_split': 2, 'OOB Accuracy': 0.8586},
    {'n_estimators': 50, 'max_features': 0.5, 'min_samples_split': 4, 'OOB Accuracy': 0.8579},
    {'n_estimators': 150, 'max_features': 0.5, 'min_samples_split': 4, 'OOB Accuracy': 0.8595},
    {'n_estimators': 250, 'max_features': 0.5, 'min_samples_split': 4, 'OOB Accuracy': 0.8588},
    {'n_estimators': 350, 'max_features': 0.5, 'min_samples_split': 4, 'OOB Accuracy': 0.8587},
    {'n_estimators': 50, 'max_features': 0.5, 'min_samples_split': 6, 'OOB Accuracy': 0.8568},
    {'n_estimators': 150, 'max_features': 0.5, 'min_samples_split': 6, 'OOB Accuracy': 0.8588},
    {'n_estimators': 250, 'max_features': 0.5, 'min_samples_split': 6, 'OOB Accuracy': 0.8586},
    {'n_estimators': 350, 'max_features': 0.5, 'min_samples_split': 6, 'OOB Accuracy': 0.8585},
    {'n_estimators': 50, 'max_features': 0.5, 'min_samples_split': 8, 'OOB Accuracy': 0.8569},
    {'n_estimators': 150, 'max_features': 0.5, 'min_samples_split': 8, 'OOB Accuracy': 0.8601},
    {'n_estimators': 250, 'max_features': 0.5, 'min_samples_split': 8, 'OOB Accuracy': 0.8594},
    {'n_estimators': 350, 'max_features': 0.5, 'min_samples_split': 8, 'OOB Accuracy': 0.8594},
    {'n_estimators': 50, 'max_features': 0.5, 'min_samples_split': 10, 'OOB Accuracy': 0.8574},
    {'n_estimators': 150, 'max_features': 0.5, 'min_samples_split': 10, 'OOB Accuracy': 0.8603},
    {'n_estimators': 250, 'max_features': 0.5, 'min_samples_split': 10, 'OOB Accuracy': 0.8601},
    {'n_estimators': 350, 'max_features': 0.5, 'min_samples_split': 10, 'OOB Accuracy': 0.8600}
]


# Convert the results to a DataFrame
df = pd.DataFrame(results)

# Set the style
sns.set(style="whitegrid")

# Create the FacetGrid
g = sns.FacetGrid(df, col="max_features", col_wrap=3, height=4)

# Draw heatmaps for each max_features value
def heatmap(data, **kwargs):
    pivot = data.pivot_table(index="min_samples_split", columns="n_estimators", values="OOB Accuracy")
    sns.heatmap(pivot, annot=True, fmt=".4f", cmap="viridis", **kwargs)

g.map_dataframe(heatmap)

# Adjust the plot
g.set_titles(col_template="max_features = {col_name}")
g.set_axis_labels("n_estimators", "min_samples_split")
plt.subplots_adjust(top=0.9)
g.fig.suptitle("Grid Search: OOB Accuracy Across All Hyperparameters", fontsize=16)

plt.show()



import numpy as np
import os
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score

def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def sigmoid_derivative(a):
    return a * (1 - a)


def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return e_z / np.sum(e_z, axis=1, keepdims=True)


def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    epsilon = 1e-12
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / m


def one_hot_encode(y, num_classes):
    return np.eye(num_classes)[y]


class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes)

        self.weights = []
        self.biases = []
        for i in range(self.num_layers - 1):
<A NAME="0"></A><FONT color = #FF0000><A HREF="match79-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1. / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
</FONT>        for i in range(self.num_layers - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            a = sigmoid(z)
            zs.append(z)
            activations.append(a)

        z_final = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        a_final = softmax(z_final)
        zs.append(z_final)
        activations.append(a_final)
        return activations, zs

    def backward(self, X, y_true, activations, zs):
        grads_w = [None] * (self.num_layers - 1)
        grads_b = [None] * (self.num_layers - 1)

        m = X.shape[0]
        delta = activations[-1] - y_true  # final layer softmax + cross-entropy

<A NAME="2"></A><FONT color = #0000FF><A HREF="match79-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        grads_w[-1] = np.dot(activations[-2].T, delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m

        for l in range(self.num_layers - 3, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * sigmoid_derivative(activations[l+1])
</FONT>            grads_w[l] = np.dot(activations[l].T, delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m

        return grads_w, grads_b

    def update_parameters(self, grads_w, grads_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * grads_w[i]
            self.biases[i] -= learning_rate * grads_b[i]

    def predict(self, X):
        a, _ = self.forward(X)
        return np.argmax(a[-1], axis=1)

    def fit(self, X, y, batch_size, epochs, learning_rate, verbose=True):
        num_samples = X.shape[0]
        for epoch in range(1, epochs + 1):
            perm = np.random.permutation(num_samples)
            X_shuffled = X[perm]
            y_shuffled = y[perm]

            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                y_encoded = one_hot_encode(y_batch, self.layer_sizes[-1])
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(X_batch, y_encoded, activations, zs)
                self.update_parameters(grads_w, grads_b, learning_rate)

            if verbose and epoch % 1 == 0:
                y_encoded = one_hot_encode(y, self.layer_sizes[-1])
                train_loss = cross_entropy_loss(y_encoded, self.forward(X)[0][-1])
                print(f"Epoch {epoch}, Loss: {train_loss:.4f}")

# --- Data loader assuming preprocessing is done ---

def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def parta(train_data_path, test_data_path, output_folder_path):
    np.random.seed(42)

    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)

    input_size = 28 * 28 * 3 
    hidden_layers = [50]
    output_size = 43         
    batch_size = 32
    epochs = 200
    learning_rate = 0.01

    nn = NeuralNetwork(input_size, hidden_layers, output_size)
    nn.fit(X_train, y_train, batch_size, epochs, learning_rate)
    
    y_pred = nn.predict(X_test)

    save_predictions(y_pred, output_folder_path)
    
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"Test Accuracy: {test_accuracy:.4f}")





import numpy as np
import os
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def sigmoid_derivative(a):
    return a * (1 - a)


def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return e_z / np.sum(e_z, axis=1, keepdims=True)


def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    epsilon = 1e-12
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / m


def one_hot_encode(y, num_classes):
    return np.eye(num_classes)[y]


class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes)

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(self.num_layers - 1):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match79-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1. / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
</FONT>        for i in range(self.num_layers - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            a = sigmoid(z)
            zs.append(z)
            activations.append(a)

        z_final = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        a_final = softmax(z_final)
        zs.append(z_final)
        activations.append(a_final)
        return activations, zs

    def backward(self, X, y_true, activations, zs):
        grads_w = [None] * (self.num_layers - 1)
        grads_b = [None] * (self.num_layers - 1)

        m = X.shape[0]
        delta = activations[-1] - y_true  # final layer softmax + cross-entropy

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match79-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        grads_w[-1] = np.dot(activations[-2].T, delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m

        for l in range(self.num_layers - 3, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * sigmoid_derivative(activations[l+1])
</FONT>            grads_w[l] = np.dot(activations[l].T, delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m

        return grads_w, grads_b

    def update_parameters(self, grads_w, grads_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * grads_w[i]
            self.biases[i] -= learning_rate * grads_b[i]

    def predict(self, X):
        a, _ = self.forward(X)
        return np.argmax(a[-1], axis=1)

    def fit(self, X, y, batch_size, epochs, learning_rate, early_stopping=True, patience=2):
        num_samples = X.shape[0]
        best_train_loss = float('inf')
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            perm = np.random.permutation(num_samples)
            X_shuffled = X[perm]
            y_shuffled = y[perm]

            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                y_encoded = one_hot_encode(y_batch, self.layer_sizes[-1])
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(X_batch, y_encoded, activations, zs)
                self.update_parameters(grads_w, grads_b, learning_rate)

            # Compute training loss at end of each epoch
            y_encoded_full = one_hot_encode(y, self.layer_sizes[-1])
            train_loss = cross_entropy_loss(y_encoded_full, self.forward(X)[0][-1])
            
            if early_stopping:
                if train_loss &lt; best_train_loss - 1e-5:  # small threshold to avoid floating point wiggle
                    best_train_loss = train_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter &gt;= patience:
                        # print(f"Early stopping at epoch {epoch} (no improvement in training loss for {patience} epochs).")
                        break


def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])


def save_classification_report(output_size, report_train, report_test, h):
    train_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_train[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_train[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_train[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    train_df = pd.DataFrame(train_metrics)
    train_df.to_csv(f"train_{h}.csv", index=False)

    test_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_test[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_test[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_test[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    test_df = pd.DataFrame(test_metrics)
    test_df.to_csv(f"test_{h}.csv", index=False)

def score(report_train, report_test, output_size):
    train_f1_scores = [report_train[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_train = np.mean(train_f1_scores)
    
    train_precision_scores = [report_train[str(i)]['precision'] for i in range(output_size)]
    avg_precision_train = np.mean(train_precision_scores)
    
    train_recall_scores = [report_train[str(i)]['recall'] for i in range(output_size)]
    avg_recall_train = np.mean(train_recall_scores)

    test_f1_scores = [report_test[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_test = np.mean(test_f1_scores)
    
    test_precision_scores = [report_test[str(i)]['precision'] for i in range(output_size)]
    avg_precision_test = np.mean(test_precision_scores)
    
    test_recall_scores = [report_test[str(i)]['recall'] for i in range(output_size)]
    avg_recall_test = np.mean(test_recall_scores)

    print(f"{avg_f1_train:.4f} | {avg_precision_train:.4f} | {avg_recall_train:.4f} | {avg_f1_test:.4f} | {avg_precision_test:.4f} | {avg_recall_test:.4f}")

def partb(train_data_path, test_data_path, output_folder_path):
 
    np.random.seed(42)

    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)
    hidden = [1, 5, 10, 50, 100]
    input_size = 28 * 28 * 3 
    output_size = 43         
    batch_size = 32
    epochs = 150
    learning_rate = 0.01


    for h in hidden:
        nn = NeuralNetwork(input_size, [h], output_size)
        nn.fit(X_train, y_train, batch_size, epochs, learning_rate)
        
        y_train_pred = nn.predict(X_train)
        y_test_pred = nn.predict(X_test)

        report_train = classification_report(y_train, y_train_pred, output_dict=True, zero_division=0)
        report_test = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)

        save_classification_report(output_size, report_train, report_test, h)
        score(report_train, report_test, output_size)

        save_predictions(y_test_pred, output_folder_path)



import numpy as np
import os
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def sigmoid_derivative(a):
    return a * (1 - a)


def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return e_z / np.sum(e_z, axis=1, keepdims=True)


def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    epsilon = 1e-12
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / m


def one_hot_encode(y, num_classes):
    return np.eye(num_classes)[y]


class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes)

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(self.num_layers - 1):
            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1. / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
        for i in range(self.num_layers - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            a = sigmoid(z)
            zs.append(z)
            activations.append(a)

        z_final = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        a_final = softmax(z_final)
        zs.append(z_final)
        activations.append(a_final)
        return activations, zs

    def backward(self, X, y_true, activations, zs):
        grads_w = [None] * (self.num_layers - 1)
        grads_b = [None] * (self.num_layers - 1)

        m = X.shape[0]
        delta = activations[-1] - y_true  # final layer softmax + cross-entropy

        grads_w[-1] = np.dot(activations[-2].T, delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m

        for l in range(self.num_layers - 3, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * sigmoid_derivative(activations[l+1])
            grads_w[l] = np.dot(activations[l].T, delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m

        return grads_w, grads_b

    def update_parameters(self, grads_w, grads_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * grads_w[i]
            self.biases[i] -= learning_rate * grads_b[i]

    def predict(self, X):
        a, _ = self.forward(X)
        return np.argmax(a[-1], axis=1)

    def fit(self, X, y, batch_size, epochs, learning_rate, early_stopping=True, patience=2):
        num_samples = X.shape[0]
        best_train_loss = float('inf')
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # print(epoch)
            perm = np.random.permutation(num_samples)
            X_shuffled = X[perm]
            y_shuffled = y[perm]

            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                y_encoded = one_hot_encode(y_batch, self.layer_sizes[-1])
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(X_batch, y_encoded, activations, zs)
                self.update_parameters(grads_w, grads_b, learning_rate)

            # Compute training loss at end of each epoch
            y_encoded_full = one_hot_encode(y, self.layer_sizes[-1])
            train_loss = cross_entropy_loss(y_encoded_full, self.forward(X)[0][-1])
            
            if early_stopping:
                if train_loss &lt; best_train_loss - 1e-5:  # small threshold to avoid floating point wiggle
                    best_train_loss = train_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter &gt;= patience:
                        # print(f"Early stopping at epoch {epoch} (no improvement in training loss for {patience} epochs).")
                        break

def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def save_classification_report(output_size, report_train, report_test, h):
    train_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_train[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_train[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_train[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    train_df = pd.DataFrame(train_metrics)
    train_df.to_csv(f"train_{h}.csv", index=False)

    test_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_test[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_test[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_test[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    test_df = pd.DataFrame(test_metrics)
    test_df.to_csv(f"test_{h}.csv", index=False)


def score(report_train, report_test,output_size):
    train_f1_scores = [report_train[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_train = np.mean(train_f1_scores)
    
    train_precision_scores = [report_train[str(i)]['precision'] for i in range(output_size)]
    avg_precision_train = np.mean(train_precision_scores)
    
    train_recall_scores = [report_train[str(i)]['recall'] for i in range(output_size)]
    avg_recall_train = np.mean(train_recall_scores)

    test_f1_scores = [report_test[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_test = np.mean(test_f1_scores)
    
    test_precision_scores = [report_test[str(i)]['precision'] for i in range(output_size)]
    avg_precision_test = np.mean(test_precision_scores)
    
    test_recall_scores = [report_test[str(i)]['recall'] for i in range(output_size)]
    avg_recall_test = np.mean(test_recall_scores)

    print(f"{avg_f1_train:.4f} | {avg_precision_train:.4f} | {avg_recall_train:.4f} | {avg_f1_test:.4f} | {avg_precision_test:.4f} | {avg_recall_test:.4f}")

def partc(train_data_path, test_data_path, output_folder_path):
    np.random.seed(42)

    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)

    hidden = [[512],[512,256],[512,256,128],[512,256,128,64]]
    input_size = 28 * 28 * 3 
    output_size = 43         
    batch_size = 32
    epochs = 100
    learning_rate = 0.01

    for h in hidden:
        nn = NeuralNetwork(input_size, h, output_size)
        print(f"Training with hidden layers: {h}")
        nn.fit(X_train, y_train, batch_size, epochs, learning_rate)
        
        y_train_pred = nn.predict(X_train)
        y_test_pred = nn.predict(X_test)

        report_train = classification_report(y_train, y_train_pred, output_dict=True, zero_division=0)
        report_test = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)

        save_classification_report(output_size, report_train, report_test, h)
        score(report_train, report_test,output_size)

        save_predictions(y_test_pred, output_folder_path)



import numpy as np
import os
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def sigmoid_derivative(a):
    return a * (1 - a)


def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return e_z / np.sum(e_z, axis=1, keepdims=True)


def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    epsilon = 1e-12
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / m


def one_hot_encode(y, num_classes):
    return np.eye(num_classes)[y]


class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes)

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(self.num_layers - 1):
            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1. / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
        for i in range(self.num_layers - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            a = sigmoid(z)
            zs.append(z)
            activations.append(a)

        z_final = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        a_final = softmax(z_final)
        zs.append(z_final)
        activations.append(a_final)
        return activations, zs

    def backward(self, X, y_true, activations, zs):
        grads_w = [None] * (self.num_layers - 1)
        grads_b = [None] * (self.num_layers - 1)

        m = X.shape[0]
        delta = activations[-1] - y_true  # final layer softmax + cross-entropy

        grads_w[-1] = np.dot(activations[-2].T, delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m

        for l in range(self.num_layers - 3, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * sigmoid_derivative(activations[l+1])
            grads_w[l] = np.dot(activations[l].T, delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m

        return grads_w, grads_b

    def update_parameters(self, grads_w, grads_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * grads_w[i]
            self.biases[i] -= learning_rate * grads_b[i]

    def predict(self, X):
        a, _ = self.forward(X)
        return np.argmax(a[-1], axis=1)

    def fit(self, X, y, batch_size, epochs, learning_rate, early_stopping=True, patience=2):
        num_samples = X.shape[0]
        best_train_loss = float('inf')
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            # print(epoch)
            lr = learning_rate/np.sqrt(epoch)
            perm = np.random.permutation(num_samples)
            X_shuffled = X[perm]
            y_shuffled = y[perm]

            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                y_encoded = one_hot_encode(y_batch, self.layer_sizes[-1])
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(X_batch, y_encoded, activations, zs)
                self.update_parameters(grads_w, grads_b, lr)

            # Compute training loss at end of each epoch
            y_encoded_full = one_hot_encode(y, self.layer_sizes[-1])
            train_loss = cross_entropy_loss(y_encoded_full, self.forward(X)[0][-1])
            
            if early_stopping:
                if train_loss &lt; best_train_loss - 1e-5:  # small threshold to avoid floating point wiggle
                    best_train_loss = train_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter &gt;= patience:
                        # print(f"Early stopping at epoch {epoch} (no improvement in training loss for {patience} epochs).")
                        break

def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def save_classification_report(output_size, report_train, report_test, h):
    train_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_train[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_train[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_train[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    train_df = pd.DataFrame(train_metrics)
    train_df.to_csv(f"train_{h}.csv", index=False)

    test_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_test[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_test[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_test[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    test_df = pd.DataFrame(test_metrics)
    test_df.to_csv(f"test_{h}.csv", index=False)


def score(report_train, report_test, output_size):
    train_f1_scores = [report_train[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_train = np.mean(train_f1_scores)
    
    train_precision_scores = [report_train[str(i)]['precision'] for i in range(output_size)]
    avg_precision_train = np.mean(train_precision_scores)
    
    train_recall_scores = [report_train[str(i)]['recall'] for i in range(output_size)]
    avg_recall_train = np.mean(train_recall_scores)

    test_f1_scores = [report_test[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_test = np.mean(test_f1_scores)
    
    test_precision_scores = [report_test[str(i)]['precision'] for i in range(output_size)]
    avg_precision_test = np.mean(test_precision_scores)
    
    test_recall_scores = [report_test[str(i)]['recall'] for i in range(output_size)]
    avg_recall_test = np.mean(test_recall_scores)

    print(f"{avg_f1_train:.4f} | {avg_precision_train:.4f} | {avg_recall_train:.4f} | {avg_f1_test:.4f} | {avg_precision_test:.4f} | {avg_recall_test:.4f}")

def partd(train_data_path, test_data_path, output_folder_path):
    np.random.seed(42)

    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)

    hidden = [[1],[512],[512,256],[512,256,128],[512,256,128,64]]
    input_size = 28 * 28 * 3 
    output_size = 43         
    batch_size = 32
    epochs = 100
    learning_rate = 0.01

    for h in hidden:
        nn = NeuralNetwork(input_size, h, output_size)
        nn.fit(X_train, y_train, batch_size, epochs, learning_rate)
        
        y_train_pred = nn.predict(X_train)
        y_test_pred = nn.predict(X_test)

        report_train = classification_report(y_train, y_train_pred, output_dict=True, zero_division=0)
        report_test = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)

        save_classification_report(output_size, report_train, report_test, h)
        score(report_train, report_test, output_size)

        save_predictions(y_test_pred, output_folder_path)



import numpy as np
import os
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

# ReLU Activation and its derivative
def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z &gt; 0).astype(float)

def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return e_z / np.sum(e_z, axis=1, keepdims=True)

def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    epsilon = 1e-12
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / m

def one_hot_encode(y, num_classes):
    return np.eye(num_classes)[y]

class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.num_layers = len(self.layer_sizes)

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(self.num_layers - 1):
            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1. / self.layer_sizes[i])
            b = np.zeros((1, self.layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        activations = [X]
        zs = []
        for i in range(self.num_layers - 2):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            a = relu(z)  # Use ReLU instead of sigmoid
            zs.append(z)
            activations.append(a)

        z_final = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        a_final = softmax(z_final)
        zs.append(z_final)
        activations.append(a_final)
        return activations, zs

    def backward(self, X, y_true, activations, zs):
        grads_w = [None] * (self.num_layers - 1)
        grads_b = [None] * (self.num_layers - 1)

        m = X.shape[0]
        delta = activations[-1] - y_true  # final layer softmax + cross-entropy

        grads_w[-1] = np.dot(activations[-2].T, delta) / m
        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / m

        for l in range(self.num_layers - 3, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * relu_derivative(activations[l+1])  # Use ReLU derivative
            grads_w[l] = np.dot(activations[l].T, delta) / m
            grads_b[l] = np.sum(delta, axis=0, keepdims=True) / m

        return grads_w, grads_b

    def update_parameters(self, grads_w, grads_b, learning_rate):
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * grads_w[i]
            self.biases[i] -= learning_rate * grads_b[i]

    def predict(self, X):
        a, _ = self.forward(X)
        return np.argmax(a[-1], axis=1)

    def fit(self, X, y, batch_size, epochs, learning_rate, early_stopping=True, patience=2):
        num_samples = X.shape[0]
        best_train_loss = float('inf')
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            lr = learning_rate/np.sqrt(epoch)
            perm = np.random.permutation(num_samples)
            X_shuffled = X[perm]
            y_shuffled = y[perm]

            for i in range(0, num_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]

                y_encoded = one_hot_encode(y_batch, self.layer_sizes[-1])
                activations, zs = self.forward(X_batch)
                grads_w, grads_b = self.backward(X_batch, y_encoded, activations, zs)
                self.update_parameters(grads_w, grads_b, lr)

            # Compute training loss at end of each epoch
            y_encoded_full = one_hot_encode(y, self.layer_sizes[-1])
            train_loss = cross_entropy_loss(y_encoded_full, self.forward(X)[0][-1])
            
            if early_stopping:
                if train_loss &lt; best_train_loss - 1e-6:  # small threshold to avoid floating point wiggle
                    best_train_loss = train_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter &gt;= patience:
                        # print(f"Early stopping at epoch {epoch} (no improvement in training loss for {patience} epochs).")
                        break

def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def save_classification_report(output_size, report_train, report_test, h):
    train_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_train[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_train[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_train[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    train_df = pd.DataFrame(train_metrics)
    train_df.to_csv(f"train_{h}.csv", index=False)

    test_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_test[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_test[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_test[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    test_df = pd.DataFrame(test_metrics)
    test_df.to_csv(f"test_{h}.csv", index=False)


def score(report_train, report_test, output_size):
    train_f1_scores = [report_train[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_train = np.mean(train_f1_scores)
    
    train_precision_scores = [report_train[str(i)]['precision'] for i in range(output_size)]
    avg_precision_train = np.mean(train_precision_scores)
    
    train_recall_scores = [report_train[str(i)]['recall'] for i in range(output_size)]
    avg_recall_train = np.mean(train_recall_scores)

    test_f1_scores = [report_test[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_test = np.mean(test_f1_scores)
    
    test_precision_scores = [report_test[str(i)]['precision'] for i in range(output_size)]
    avg_precision_test = np.mean(test_precision_scores)
    
    test_recall_scores = [report_test[str(i)]['recall'] for i in range(output_size)]
    avg_recall_test = np.mean(test_recall_scores)

    print(f"{avg_f1_train:.4f} | {avg_precision_train:.4f} | {avg_recall_train:.4f} | {avg_f1_test:.4f} | {avg_precision_test:.4f} | {avg_recall_test:.4f}")

def parte(train_data_path, test_data_path, output_folder_path):
 
    np.random.seed(42)

    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)

    hidden = [[512],[512,256],[512,256,128],[512,256,128,64]]
    input_size = 28 * 28 * 3 
    output_size = 43         
    batch_size = 32
    epochs = 100
    learning_rate = 0.01

    for h in hidden:
        nn = NeuralNetwork(input_size, h, output_size)
        nn.fit(X_train, y_train, batch_size, epochs, learning_rate)
        
        y_train_pred = nn.predict(X_train)
        y_test_pred = nn.predict(X_test)

        report_train = classification_report(y_train, y_train_pred, output_dict=True, zero_division=0)
        report_test = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)

        
        save_classification_report(output_size, report_train, report_test, h)
        score(report_train, report_test, output_size)

        save_predictions(y_test_pred, output_folder_path)




import numpy as np
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import os
import cv2

def load_preprocessed_gtsrb(train_data_path, test_data_path):
    X_train, y_train = [], []

    train_path = os.path.join(train_data_path, 'train')
    class_folders = sorted([d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))])


    for label, class_folder in enumerate(class_folders):
        folder_path = os.path.join(train_path, class_folder)
        for file in os.listdir(folder_path):
            img_path = os.path.join(folder_path, file)
            img = cv2.imread(img_path)
            if img is not None:
                X_train.append(img)
                y_train.append(label)

    X_train = np.array(X_train).astype('float32') / 255.0
    y_train = np.array(y_train)
    X_train = X_train.reshape(X_train.shape[0], -1)

    test_img_dir = os.path.join(test_data_path, 'test')
    labels_df = pd.read_csv(os.path.join(test_data_path, 'test_labels.csv'))

    X_test, y_test = [], []
    for _, row in labels_df.iterrows():
        img_path = os.path.join(test_img_dir, row['image'])
        img = cv2.imread(img_path)
        if img is not None:
            X_test.append(img)
            y_test.append(row['label'])

    X_test = np.array(X_test).astype('float32') / 255.0
    y_test = np.array(y_test)
    X_test = X_test.reshape(X_test.shape[0], -1)

    return X_train, y_train, X_test, y_test

import csv

def save_predictions(predictions, filename):
    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction'])
        for pred in predictions:
            writer.writerow([pred])

def save_classification_report(output_size, report_train, report_test, h):
    train_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_train[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_train[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_train[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    train_df = pd.DataFrame(train_metrics)
    train_df.to_csv(f"train_{h}.csv", index=False)

    test_metrics = {
        'Class': list(range(output_size)),
        'Precision': [round(report_test[str(i)]['precision'], 4) for i in range(output_size)],
        'Recall': [round(report_test[str(i)]['recall'], 4) for i in range(output_size)],
        'F1 Score': [round(report_test[str(i)]['f1-score'], 4) for i in range(output_size)]
    }
    test_df = pd.DataFrame(test_metrics)
    test_df.to_csv(f"test_{h}.csv", index=False)


def score(report_train, report_test, output_size):
    train_f1_scores = [report_train[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_train = np.mean(train_f1_scores)
    
    train_precision_scores = [report_train[str(i)]['precision'] for i in range(output_size)]
    avg_precision_train = np.mean(train_precision_scores)
    
    train_recall_scores = [report_train[str(i)]['recall'] for i in range(output_size)]
    avg_recall_train = np.mean(train_recall_scores)

    test_f1_scores = [report_test[str(i)]['f1-score'] for i in range(output_size)]
    avg_f1_test = np.mean(test_f1_scores)
    
    test_precision_scores = [report_test[str(i)]['precision'] for i in range(output_size)]
    avg_precision_test = np.mean(test_precision_scores)
    
    test_recall_scores = [report_test[str(i)]['recall'] for i in range(output_size)]
    avg_recall_test = np.mean(test_recall_scores)

    print(f"{avg_f1_train:.4f} | {avg_precision_train:.4f} | {avg_recall_train:.4f} | {avg_f1_test:.4f} | {avg_precision_test:.4f} | {avg_recall_test:.4f}")


def partf(train_data_path, test_data_path, output_folder_path):
    np.random.seed(42)
    X_train, y_train, X_test, y_test = load_preprocessed_gtsrb(train_data_path, test_data_path)


    hidden_layer = [[512],[512,256],[512,256,128],[512,256,128,64]]
    output_size = 43
    for h in hidden_layer:
        clf = MLPClassifier(hidden_layer_sizes=h, activation='relu', solver='sgd', alpha = 0.0, batch_size=32, learning_rate='invscaling', learning_rate_init=0.01, max_iter = 100, shuffle = True,
                            early_stopping=True, n_iter_no_change=3, random_state=42, verbose=False)
        clf.fit(X_train, y_train)
        y_train_pred = clf.predict(X_train)
        y_test_pred = clf.predict(X_test)
        report_train = classification_report(y_train, y_train_pred, output_dict=True, zero_division=0)
        report_test = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)

        save_classification_report(output_size, report_train, report_test, h)
        score(report_train, report_test, output_size)

        save_predictions(y_test_pred, output_folder_path)


</PRE>
</PRE>
</BODY>
</HTML>
