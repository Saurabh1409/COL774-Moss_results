<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_1HEUC.py<p><PRE>


import pandas as pd
import os
import sys
import numpy as np
from collections import Counter
import math

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import  GridSearchCV

import matplotlib.pyplot as plt

class Node:
    pass

class InternalNode(Node):
    def __init__(self, split_feature, is_continuous, split_threshold, split_categories, children, class_counts):
        self.split_feature = split_feature
        self.is_continuous = is_continuous
        self.split_threshold = split_threshold
        self.split_categories = split_categories
        self.children = children
        self.class_counts = class_counts

class LeafNode(Node):
    def __init__(self, class_counts):
        self.class_counts = class_counts
        self.prediction = max(class_counts, key=lambda k: class_counts[k])

def entropy(labels):
    counts = Counter(labels)
    total = len(labels)
    e = 0.0
    for count in counts.values():
        p = count / total
        if p &gt; 0:
            e -= p * math.log2(p)
    return e

def information_gain(parent_labels, children_labels):
    parent_e = entropy(parent_labels)
    total = len(parent_labels)
    if total == 0:
        return 0.0
    child_e_sum = 0.0
    for child in children_labels:
        child_total = len(child)
        if child_total == 0:
            continue
        child_e_sum += (child_total / total) * entropy(child)
    return parent_e - child_e_sum

class DT:
    def __init__(self, td, vd, testd, output_folder_path,q_p, max_depth=5):
        self.train_data = td
        self.valid_data = vd
        self.test_data = testd
        self.output_folder_path = output_folder_path
        self.numerical_features = ["age", "fnlwgt", "capital.gain", "capital.loss", "hours.per.week"]
        self.categorical_features = ["workclass", "education", "education.num", "marital.status", "occupation", "relationship", "race", "sex", "native.country"]
        self.target = "income"
        self.max_depth = max_depth
        self.root = None
        self.q_p = q_p
        self.computeXY()
    
    def data_preprocess(self):
        
        for feature in self.categorical_features:
            if feature in self.train_data.columns:
                self.train_data[feature] = pd.Categorical(self.train_data[feature]).codes
                self.valid_data[feature] = pd.Categorical(self.valid_data[feature]).codes
                self.test_data[feature] = pd.Categorical(self.test_data[feature]).codes
        return self.train_data, self.valid_data, self.test_data
    
    def get_enumrated_y(self):
        
        self.train_data[self.target] = pd.Categorical(self.train_data[self.target]).codes
        self.valid_data[self.target] = pd.Categorical(self.valid_data[self.target]).codes
        self.test_data[self.target] = pd.Categorical(self.test_data[self.target]).codes
        return self.train_data[self.target], self.valid_data[self.target], self.test_data[self.target]

    def computeXY(self):
        self.y = np.array(self.train_data[self.target])
        self.X = np.array(self.train_data.drop(columns=[self.target]))
        self.all_features = self.train_data.columns.tolist()
        if self.target in self.all_features:  
            self.all_features.remove(self.target)

    def fit(self):
        self.root = self._fit(self.X, self.y, depth=0)
    
    def calculate_accuracies(self, model, X_train, y_train, X_val, y_val, X_test, y_test):
        y_train_pred = model.predict(X_train)
        y_val_pred = model.predict(X_val)
        y_test_pred = model.predict(X_test)
        
        train_acc = accuracy_score(y_train, y_train_pred)
        val_acc = accuracy_score(y_val, y_val_pred)
        test_acc = accuracy_score(y_test, y_test_pred)
        
        return train_acc, val_acc, test_acc

    def use_sklearn_for_varing_max_depths(self):
        X_train, X_val, X_test = self.data_preprocess()

        
        y_train, y_val, y_test = self.get_enumrated_y()

        
        X_train = X_train.drop(columns=[self.target]).values
        X_val = X_val.drop(columns=[self.target]).values
        X_test = X_test.drop(columns=[self.target]).values

        max_depths = range(5,65, 5)  
        train_accuracies_depth = []
        val_accuracies_depth = []
        test_accuracies_depth = []

        for depth in max_depths:
            
            dt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
            
            
            dt_classifier.fit(X_train, y_train)
            
            
            train_acc, val_acc, test_acc = self.calculate_accuracies(dt_classifier, X_train, y_train, X_val, y_val, X_test, y_test)

            
            train_accuracies_depth.append(train_acc)
            val_accuracies_depth.append(val_acc)
            test_accuracies_depth.append(test_acc)
            
            print(f"Max Depth: {depth}")
            print(f"  Train Accuracy: {train_acc:.4f}")
            print(f"  Validation Accuracy: {val_acc:.4f}")
            print(f"  Test Accuracy: {test_acc:.4f}")

        
        best_depth_idx = np.argmax(val_accuracies_depth)
        best_depth = max_depths[best_depth_idx]
        print(f"\nBest max_depth: {best_depth} with validation accuracy: {val_accuracies_depth[best_depth_idx]:.4f}")

        
        self.save_results_to_csv(
            f'{self.output_folder_path}/decision_tree_max_depth_results.csv',
            zip(max_depths, train_accuracies_depth, val_accuracies_depth, test_accuracies_depth),
            ['max_depth', 'train_accuracy', 'validation_accuracy', 'test_accuracy']
        )

        
        self.plot_metrics(
            max_depths,
            {'Train Accuracy': train_accuracies_depth, 'Validation Accuracy': val_accuracies_depth, 'Test Accuracy': test_accuracies_depth},
            'Max Depth',
            'Accuracy',
            'Accuracy vs Max Depth',
            f"{self.output_folder_path}/max_depth_vs_accuracy"
        )

    def use_sklearn_for_varing_ccp_alphas(self):
        X_train, X_val, X_test = self.data_preprocess()

        
        y_train, y_val, y_test = self.get_enumrated_y()

        
        X_train = X_train.drop(columns=[self.target]).values
        X_val = X_val.drop(columns=[self.target]).values
        X_test = X_test.drop(columns=[self.target]).values

        ccp_alphas = [0.0001,0.0005,0.001,0.002 ,0.008, 0.01, 0.02, 0.05, 0.08, 0.1,0.15, 0.2]
        train_accuracies_ccp = []
        val_accuracies_ccp = []
        test_accuracies_ccp = []


        for alpha in ccp_alphas:
            
            dt_classifier = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha, random_state=42)
            
            
            dt_classifier.fit(X_train, y_train)
            
            
            train_acc, val_acc, test_acc = self.calculate_accuracies(dt_classifier, X_train, y_train, X_val, y_val, X_test, y_test)
            
            
            train_accuracies_ccp.append(train_acc)
            val_accuracies_ccp.append(val_acc)
            test_accuracies_ccp.append(test_acc)
            
            print(f"CCP Alpha: {alpha}")
            print(f"  Train Accuracy: {train_acc:.4f}")
            print(f"  Validation Accuracy: {val_acc:.4f}")
            print(f"  Test Accuracy: {test_acc:.4f}")

        
        best_alpha_idx = np.argmax(val_accuracies_ccp)
        best_alpha = ccp_alphas[best_alpha_idx]
        print(f"\nBest ccp_alpha: {best_alpha} with validation accuracy: {val_accuracies_ccp[best_alpha_idx]:.4f}")

        
        ccp_results = pd.DataFrame({
            'ccp_alpha': ccp_alphas,
            'train_accuracy': train_accuracies_ccp,
            'validation_accuracy': val_accuracies_ccp,
            'test_accuracy': test_accuracies_ccp
        })
        ccp_results.to_csv(f'{self.output_folder_path}/decision_tree_ccp_alpha_results.csv', index=False)
        print("CCP alpha results saved to 'decision_tree_ccp_alpha_results.csv'")

        
        final_model = DecisionTreeClassifier(
            criterion='entropy',
            max_depth=best_depth,
            ccp_alpha=best_alpha,
            random_state=42
        )
        final_model.fit(X_train, y_train)

        
        y_train_pred_final = final_model.predict(X_train)
        y_val_pred_final = final_model.predict(X_val)
        y_test_pred_final = final_model.predict(X_test)

        test_predictions = pd.DataFrame(y_test_pred_final, columns=['prediction'])
        test_predictions.to_csv(f'{args[4]}/prediction_{question_part}.csv', index=False)

        train_acc_final = accuracy_score(y_train, y_train_pred_final)
        val_acc_final = accuracy_score(y_val, y_val_pred_final)
        test_acc_final = accuracy_score(y_test, y_test_pred_final)

        print("\nFinal Model Performance:")
        print(f"  Parameters: max_depth={best_depth}, ccp_alpha={best_alpha}, criterion='entropy'")
        print(f"  Train Accuracy: {train_acc_final:.4f}")
        print(f"  Validation Accuracy: {val_acc_final:.4f}")
        print(f"  Test Accuracy: {test_acc_final:.4f}")

        
        final_results = pd.DataFrame({
            'parameter': ['max_depth', 'ccp_alpha', 'train_accuracy', 'validation_accuracy', 'test_accuracy'],
            'value': [best_depth, best_alpha, train_acc_final, val_acc_final, test_acc_final]
        })
        final_results.to_csv(f'{self.output_folder_path}/decision_tree_final_model_results.csv', index=False)
        print("Final model results saved to 'decision_tree_final_model_results.csv'")

    def fit_with_one_hot_encoding(self):
        
        original_numerical_features = self.numerical_features.copy()
        
        
        new_feature_names = []
        
        
        for feature in self.categorical_features:
            
            if feature not in self.train_data.columns:
                continue
                
            unique_values = np.unique(self.train_data[feature])
            
            if unique_values.shape[0] == 2:
                continue
                
            
            for value in unique_values:
                new_f = ""
                if type(value) is str:
                    new_f = f"{feature}_{value.replace(' ', '')}"
                else:
                    new_f = f"{feature}_{value}"
                    
                self.train_data[new_f] = (self.train_data[feature] == value).astype(int)
                self.valid_data[new_f] = (self.valid_data[feature] == value).astype(int)
                self.test_data[new_f] = (self.test_data[feature] == value).astype(int)
                
                
                new_feature_names.append(new_f)
                
                self.categorical_features.append(new_f)
        
        
        features_to_drop = [f for f in self.categorical_features 
                           if f in self.train_data.columns and np.unique(self.train_data[f]).shape[0] &gt; 2]
        if features_to_drop:
            self.train_data.drop(columns=features_to_drop, inplace=True)
            self.valid_data.drop(columns=features_to_drop, inplace=True)
            self.test_data.drop(columns=features_to_drop, inplace=True)
        
        
        self.categorical_features = [f for f in self.categorical_features 
                                   if f in self.train_data.columns and f not in features_to_drop]
        
        
        self.computeXY()  
        
        
        self.root = self._fit(self.X, self.y, depth=0)

    def _fit(self, X, y, depth):
        
        all_features = self.all_features
        
        
        class_counts = Counter(y)
        
        
        
        
        
        
        if depth &gt;= self.max_depth or len(set(y)) == 1 or len(all_features) == 0 or len(y) == 0:
            return LeafNode(class_counts)
        
        
        best_feature = None
        best_gain = -float('inf')
        best_is_continuous = False
        best_threshold = None
        best_children = None
        best_split_categories = None
        best_children_labels = None
        
        
        for feature_idx, feature in enumerate(all_features):
            
            if feature_idx &gt;= X.shape[1]:
                continue
                
            is_continuous = feature in self.numerical_features
            feature_values = X[:, feature_idx]
            
            if is_continuous:
                
                threshold = np.median(feature_values)
                
                
                left_mask = feature_values &lt;= threshold
                right_mask = ~left_mask
                
                left_indices = np.where(left_mask)[0]
                right_indices = np.where(right_mask)[0]
                
                
                if len(left_indices) == 0 or len(right_indices) == 0:
                    continue
                
                
                children_labels = [y[left_indices], y[right_indices]]
                gain = information_gain(y, children_labels)
                
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_is_continuous = True
                    best_threshold = threshold
                    best_children_labels = children_labels
                    best_split_categories = None
            else:
                
                unique_values = np.unique(feature_values)
                
                
                if len(unique_values) == 1:
                    continue
                
                
                children_indices = {value: np.where(feature_values == value)[0] for value in unique_values}
                children_labels = [y[indices] for indices in children_indices.values()]
                
                
                if any(len(child) == 0 for child in children_labels):
                    continue
                
                
                gain = information_gain(y, children_labels)
                
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_is_continuous = False
                    best_threshold = None
                    best_children_labels = children_labels
                    best_split_categories = {value: i for i, value in enumerate(unique_values)}
        
        
        if best_feature is None:
            return LeafNode(class_counts)
        
        
        children = {}
        
        if best_is_continuous:
            
            feature_values = X[:, best_feature]
            left_mask = feature_values &lt;= best_threshold
            right_mask = ~left_mask
            
            
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match108-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            left_child = self._fit(X[left_mask], y[left_mask], depth + 1)
            right_child = self._fit(X[right_mask], y[right_mask], depth + 1)
</FONT>            
            children = {0: left_child, 1: right_child}
        else:
            
            feature_values = X[:, best_feature]
            unique_values = np.unique(feature_values)
            
            for value in unique_values:
                mask = feature_values == value
                if np.any(mask):
                    children[value] = self._fit(X[mask], y[mask], depth + 1)
        
        
        return InternalNode(
            split_feature=best_feature,
            is_continuous=best_is_continuous,
            split_threshold=best_threshold,
            split_categories=best_split_categories,
            children=children,
            class_counts=class_counts
        )

    def save_accuracies(self,save_pred=False):
        train_accuracy = self.print_train_accuracy()
        valid_accuracy = self.print_valid_accuracy()
        test_accuracy = self.print_test_accuracy(save=save_pred)
        
        if save_pred:
            if not os.path.exists(self.output_folder_path):
                os.makedirs(self.output_folder_path)
            
            
            accuracies_file_path = os.path.join(self.output_folder_path, f"accuracies_{self.q_p}.csv")
            file_exists = os.path.isfile(accuracies_file_path)
            
            with open(accuracies_file_path, "a") as f:
                if not file_exists:
                    f.write("Max Depth,train_accuracy,valid_accuracy,test_accuracy\n")
                f.write(f"{self.max_depth},{train_accuracy},{valid_accuracy},{test_accuracy}\n")   
        return train_accuracy, valid_accuracy, test_accuracy
    
    def predict(self):
        return self._predict(self.X)
    
    def predict_test(self,save_pred=False):
        test_features = self.test_data.drop(columns=[self.target]).values
        pred_y = self._predict(test_features)
        
        if save_pred:
            if not os.path.exists(self.output_folder_path):
                os.makedirs(self.output_folder_path)
            prediction_file_path = os.path.join(self.output_folder_path, f"prediction_{self.q_p}.csv")
            if os.path.exists(prediction_file_path):
                os.remove(prediction_file_path)
            with open(prediction_file_path, "w") as f:
                f.write("prediction\n")
                for pred in pred_y:
                    f.write(f"{pred}\n")
        return pred_y

    def print_test_accuracy(self,save=False):
        predictions = self.predict_test(save)
        accuracy = np.mean(predictions == self.test_data[self.target].values)
        print(f"Test Accuracy: {accuracy:.4f}")
        return accuracy
    
    def print_train_accuracy(self):
        predictions = self.predict()
        accuracy = np.mean(predictions == self.y)
        print(f"Train Accuracy: {accuracy:.4f}")
        return accuracy
    
    def print_valid_accuracy(self):
        valid_features = self.valid_data.drop(columns=[self.target]).values
        predictions = self._predict(valid_features)
        accuracy = np.mean(predictions == self.valid_data[self.target].values)
        print(f"Validation Accuracy: {accuracy:.4f}")
        return accuracy
    
    def print_tree(self):
        
        if self.root is None:
            print("Tree is not built yet. Call fit() first.")
            return
            
        print("Decision Tree Structure:")
        print("Root Node:")
        
        if isinstance(self.root, LeafNode):
            print("Leaf Node with prediction:", self.root.prediction)
            print("Class counts:", self.root.class_counts)
        else:
            print("Internal Node")
            if self.root.is_continuous:
                feature_name = self.all_features[self.root.split_feature]
                print(f"Split on feature: {feature_name} (continuous)")
                print(f"Split threshold: {self.root.split_threshold}")
            else:
                feature_name = self.all_features[self.root.split_feature]
                print(f"Split on feature: {feature_name} (categorical)")
            
            print(f"Number of children: {len(self.root.children)}")
            
            count = 0
            for child_key, child in self.root.children.items():
                child_type = "Leaf Node" if isinstance(child, LeafNode) else "Internal Node"
                print(f"Child {child_key}: {child_type}")
                count += 1
            
            if count != len(self.root.children):
                print("Warning: Child count mismatch!")

    def prune_tree(self):
        if not self.root:
            print("Tree not built yet. Call fit() first.")
            return

        
        best_accuracy = self.print_valid_accuracy()
        print(f"Initial validation accuracy before pruning: {best_accuracy:.4f}")
        
        
        pruning_history = []
        
        
        initial_node_count = self.count_nodes(self.root)
        pruning_history.append({
            'node_count': initial_node_count,
            'train_acc': self.print_train_accuracy(),
            'valid_acc': best_accuracy,
            'test_acc': self.print_test_accuracy()
        })
        
        print(f"Initial tree has {initial_node_count} nodes.")
        
        
        improvement_made = True
        while improvement_made:
            improvement_made = False
            
            
            best_node_parent = None
            best_node_key = None
            best_pruned_accuracy = best_accuracy
            
            
            all_prunables = self.find_prunable_nodes(self.root)
            print(f"Found {len(all_prunables)} nodes that can be pruned.")
            
            
            for parent, key in all_prunables:
                
                original_subtree = parent.children[key]
                
                
                leaf_node = LeafNode(original_subtree.class_counts)
                
                
                parent.children[key] = leaf_node
                
                
                valid_acc = self.print_valid_accuracy()
                
                
                parent.children[key] = original_subtree
                
                
                if valid_acc &gt; best_pruned_accuracy:
                    best_pruned_accuracy = valid_acc
                    best_node_parent = parent
                    best_node_key = key
            
            
            if best_node_parent is not None:
                
                leaf_node = LeafNode(best_node_parent.children[best_node_key].class_counts)
                
                
                best_node_parent.children[best_node_key] = leaf_node
                
                
                best_accuracy = best_pruned_accuracy
                improvement_made = True
                
                
                node_count = self.count_nodes(self.root)
                
                
                pruning_history.append({
                    'node_count': node_count,
                    'train_acc': self.print_train_accuracy(),
                    'valid_acc': best_accuracy,
                    'test_acc': self.print_test_accuracy()
                })
                
                print(f"Pruned a node. New validation accuracy: {best_accuracy:.4f}, Nodes: {node_count}")
        
        print(f"Pruning complete. Final validation accuracy: {best_accuracy:.4f}")
        return pruning_history

    def find_prunable_nodes(self, node, parent=None, key=None, result=None):
        if result is None:
            result = []
        
        if isinstance(node, InternalNode):        
            if parent is not None:
                result.append((parent, key))
    
            for child_key, child in node.children.items():
                self.find_prunable_nodes(child, node, child_key, result)
        
        return result

    def count_nodes(self, node):
        if isinstance(node, LeafNode):
            return 1
        
        count = 1  
        for child in node.children.values():
            count += self.count_nodes(child)
        
        return count

    def plot_pruning_metrics(self, pruning_history):
        try:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match108-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            import matplotlib.pyplot as plt
            
            
            node_counts = [entry['node_count'] for entry in pruning_history]
            train_accs = [entry['train_acc'] for entry in pruning_history]
</FONT>            valid_accs = [entry['valid_acc'] for entry in pruning_history]
            test_accs = [entry['test_acc'] for entry in pruning_history]
            
            
            sorted_indices = sorted(range(len(node_counts)), key=lambda i: node_counts[i], reverse=True)
            node_counts = [node_counts[i] for i in sorted_indices]
            train_accs = [train_accs[i] for i in sorted_indices]
            valid_accs = [valid_accs[i] for i in sorted_indices]
            test_accs = [test_accs[i] for i in sorted_indices]
            
            plt.figure(figsize=(10, 6))
            plt.plot(node_counts, train_accs, 'o-', label='Training Accuracy')
            plt.plot(node_counts, valid_accs, 's-', label='Validation Accuracy')
            plt.plot(node_counts, test_accs, '^-', label='Test Accuracy')
            
            plt.xlabel('Number of Nodes')
            plt.ylabel('Accuracy')
            plt.title(f'Accuracy vs. Tree Size (Max Depth: {self.max_depth})')
            plt.legend()
            plt.grid(True)
            
            
            plot_path = os.path.join(self.output_folder_path, f'pruning_plot_depth_{self.max_depth}.png')
            plt.savefig(plot_path)
            plt.close()
            
            print(f"Pruning plot saved to: {plot_path}")
            
        except ImportError:
            print("Matplotlib not available. Skipping plot generation.")
            
            print("Pruning Metrics:")
            print("Nodes\tTrain Acc\tValid Acc\tTest Acc")
            for entry in sorted(pruning_history, key=lambda x: x['node_count'], reverse=True):
                print(f"{entry['node_count']}\t{entry['train_acc']:.4f}\t{entry['valid_acc']:.4f}\t{entry['test_acc']:.4f}")

    def save_pruning_metrics(self, pruning_history):
        metrics_path = os.path.join(self.output_folder_path, f'pruning_metrics_depth_{self.max_depth}.csv')
        
        
        sorted_history = sorted(pruning_history, key=lambda x: x['node_count'], reverse=True)
        
        with open(metrics_path, 'w') as f:
            f.write("node_count,train_accuracy,valid_accuracy,test_accuracy\n")
            for entry in sorted_history:
                f.write(f"{entry['node_count']},{entry['train_acc']},{entry['valid_acc']},{entry['test_acc']}\n")
        
        print(f"Pruning metrics saved to: {metrics_path}")

    def run_with_pruning(self):
        print(f"Fitting tree with max_depth={self.max_depth}...")
        self.fit()
        print("Starting pruning process...")
        pruning_history = self.prune_tree()
        self.plot_pruning_metrics(pruning_history)
        self.save_pruning_metrics(pruning_history)
        return pruning_history

    def _predict(self, X):
        if self.root is None:
            raise ValueError("Model not fitted. Call fit() first.")
            
        predictions = []
        for sample in X:
            node = self.root
            while True:
                if isinstance(node, LeafNode):
                    predictions.append(node.prediction)
                    break
                else:
                    
                    if node.split_feature &gt;= len(sample):
                        
                        predictions.append(max(node.class_counts, key=lambda k: node.class_counts[k]))
                        break
                        
                    feat = node.split_feature
                    if node.is_continuous:
                        if sample[feat] &lt;= node.split_threshold:
                            node = node.children[0]
                        else:
                            node = node.children[1]
                    else:
                        value = sample[feat]
                        if value in node.children:
                            node = node.children[value]
                        else:
                            
                            predictions.append(max(node.class_counts, key=lambda k: node.class_counts[k]))
                            break
        
        return np.array(predictions)

    def random_forest(self):
        
        X_train, X_val, X_test = self.data_preprocess()

        
        y_train, y_val, y_test = self.get_enumrated_y()

        
        X_train = X_train.drop(columns=[self.target]).values
        X_val = X_val.drop(columns=[self.target]).values
        X_test = X_test.drop(columns=[self.target]).values

<A NAME="5"></A><FONT color = #FF0000><A HREF="match108-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        param_grid = {
        'n_estimators': [50, 150, 250, 350],
        'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
</FONT>        'min_samples_split': [2, 4, 6, 8, 10],
        'criterion': ['entropy']  
        }

        
        rf = RandomForestClassifier(oob_score=True, random_state=42)

        
        grid_search = GridSearchCV(estimator=rf, 
                                param_grid=param_grid,
                                cv=5,  
                                n_jobs=-1,  
                                verbose=1, 
                                scoring='accuracy')

        
        grid_search.fit(X_train, y_train)

        
        best_params = grid_search.best_params_
        best_rf = grid_search.best_estimator_

        print(f"Best Parameters: {best_params}")

        
        
        oob_accuracy = best_rf.oob_score_

        pred_y = best_rf.predict(X_test)
        test_predictions = pd.DataFrame(pred_y, columns=['prediction'])
        test_predictions.to_csv(f'{args[4]}/prediction_{question_part}.csv', index=False)


        
<A NAME="2"></A><FONT color = #0000FF><A HREF="match108-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        train_accuracy = accuracy_score(y_train, best_rf.predict(X_train))
        validation_accuracy = accuracy_score(y_val, best_rf.predict(X_val))
        test_accuracy = accuracy_score(y_test, best_rf.predict(X_test))

        print("\nAccuracy Metrics for Optimal Parameters:")
        print(f"Training Accuracy: {train_accuracy:.4f}")
        print(f"Out-of-Bag Accuracy: {oob_accuracy:.4f}")
</FONT>        print(f"Validation Accuracy: {validation_accuracy:.4f}")
        print(f"Test Accuracy: {test_accuracy:.4f}")

        
        
        n_estimators_range = [50, 150, 250, 350]
        oob_scores = []

        for n_est in n_estimators_range:
            rf = RandomForestClassifier(
                n_estimators=n_est,
                max_features=best_params['max_features'],
                min_samples_split=best_params['min_samples_split'],
                criterion=best_params['criterion'],
                oob_score=True,
                random_state=42
            )
            rf.fit(X_train, y_train)
            oob_scores.append(rf.oob_score_)

        plt.figure(figsize=(12, 6))
        plt.subplot(1, 3, 1)
        plt.plot(n_estimators_range, oob_scores, marker='o')
        plt.xlabel('n_estimators')
        plt.ylabel('OOB Accuracy')
        plt.title('OOB Accuracy vs n_estimators')
        plt.grid(True)

        
        max_features_range = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
        oob_scores = []

        for max_feat in max_features_range:
            rf = RandomForestClassifier(
                n_estimators=best_params['n_estimators'],
                max_features=max_feat,
                min_samples_split=best_params['min_samples_split'],
                criterion=best_params['criterion'],
                oob_score=True,
                random_state=42
            )
            rf.fit(X_train, y_train)
            oob_scores.append(rf.oob_score_)

        plt.subplot(1, 3, 2)
        plt.plot(max_features_range, oob_scores, marker='o')
        plt.xlabel('max_features')
        plt.ylabel('OOB Accuracy')
        plt.title('OOB Accuracy vs max_features')
        plt.grid(True)

        
        min_samples_split_range = [2, 4, 6, 8, 10]
        oob_scores = []

        for min_samples in min_samples_split_range:
            rf = RandomForestClassifier(
                n_estimators=best_params['n_estimators'],
                max_features=best_params['max_features'],
                min_samples_split=min_samples,
                criterion=best_params['criterion'],
                oob_score=True,
                random_state=42
            )
            rf.fit(X_train, y_train)
            oob_scores.append(rf.oob_score_)

        plt.subplot(1, 3, 3)
        plt.plot(min_samples_split_range, oob_scores, marker='o')
        plt.xlabel('min_samples_split')
        plt.ylabel('OOB Accuracy')
        plt.title('OOB Accuracy vs min_samples_split')
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('random_forest_parameter_analysis.png')
        plt.show()

        
        feature_importance = best_rf.feature_importances_
        features = dt.all_features

        plt.figure(figsize=(10, 6))
        plt.bar(features, feature_importance)
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.title('Feature Importance from Random Forest')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        plt.show()

        print("\nFeature Importance:")
        for i, feature in enumerate(features):
            print(f"{feature}: {feature_importance[i]:.4f}")
        
    def plot_metrics(self, x_values, y_values_dict, xlabel, ylabel, title, filename):
        plt.figure(figsize=(10, 6))
        for label, y_values in y_values_dict.items():
            plt.plot(x_values, y_values, label=label, marker='o')
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.title(title)
        plt.legend()
        plt.grid()
        plt.savefig(filename)
        plt.close()
        
    def save_results_to_csv(self, filename, data, columns):
        df = pd.DataFrame(data, columns=columns)
        df.to_csv(filename, index=False)

if __name__ == "__main__":
    args = sys.argv
    if(args.__len__() != 6):
        print("Please provide cmd in format python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        exit(1)

    train_data_file_path = os.path.join(os.path.dirname(__file__), args[1])
    valid_data_file_path = os.path.join(os.path.dirname(__file__), args[2])
    test_data_file_path = os.path.join(os.path.dirname(__file__), args[3])
        
    train_data = pd.read_csv(train_data_file_path)
    test_data = pd.read_csv(test_data_file_path)
    valid_data = pd.read_csv(valid_data_file_path)

    if not os.path.exists(args[4]):
        os.makedirs(args[4])
    
    dt = DT(train_data, valid_data, test_data, args[4], args[5])

    question_part = args[5]
    if question_part == "a":
        
        print("Question Part A")
        best_depth = 5
        best_val_acc = -float('inf')

        acc_for_diff_max_depth = {}
        for s in range(5,55,5):
            dt.max_depth = s
            print(f"Max Depth: {s}")
            dt.fit()
            train_acc,val_acc,test_acc = dt.save_accuracies()
            acc_for_diff_max_depth[s] = [train_acc,val_acc,test_acc]
            if val_acc &gt; best_val_acc:
                best_val_acc =  val_acc
                best_depth = s
        
        dt.plot_metrics(
            list(acc_for_diff_max_depth.keys()),
            {'Train Accuracy': [accuracy[0] for accuracy in acc_for_diff_max_depth.values()],
             'Validation Accuracy': [accuracy[1] for accuracy in acc_for_diff_max_depth.values()],
             'Test Accuracy': [accuracy[2] for accuracy in acc_for_diff_max_depth.values()]},
            'Max Depth',
            'Accuracy',
            'Accuracy vs Max Depth',
            f"{args[4]}/max_depth_vs_accuracy"
        )
        dt.max_depth = best_depth
        dt.fit()
        dt.save_accuracies(True)

    elif question_part == "b":
        
        print("Question Part B")
        best_depth = 5
        best_val_acc = -float('inf')
        acc_for_diff_max_depth = {}
        for s in range(5,65,5):
            dt.max_depth = s
            print(f"Max Depth: {s}")
            dt.fit_with_one_hot_encoding()
            train_acc,val_acc,test_acc = dt.save_accuracies()
            acc_for_diff_max_depth[s] = [train_acc,val_acc,test_acc]
            if val_acc &gt; best_val_acc:
                best_val_acc =  val_acc
                best_depth = s
        
        dt.plot_metrics(
            list(acc_for_diff_max_depth.keys()),
            {'Train Accuracy': [accuracy[0] for accuracy in acc_for_diff_max_depth.values()],
             'Validation Accuracy': [accuracy[1] for accuracy in acc_for_diff_max_depth.values()],
             'Test Accuracy': [accuracy[2] for accuracy in acc_for_diff_max_depth.values()]},
            'Max Depth',
            'Accuracy',
            'Accuracy vs Max Depth (One Hot Encoding)',
            f"{args[4]}/max_depth_vs_accuracy_one_hot"
        )
        dt.max_depth = best_depth
        print(best_depth)
        dt.fit()
        dt.save_accuracies(True)

    elif question_part == "c":
        
        print("Question Part C")
        # for s in range(5,65,5):
        #     dt.max_depth = s
        #     print(f"Max Depth: {s}")
        #     dt.run_with_pruning()
        dt.max_depth = 15
        dt.run_with_pruning()
        dt.save_accuracies(True)

    elif question_part == "d":
        print("Question Part D")
        dt.use_sklearn_for_varing_max_depths()
        dt.use_sklearn_for_varing_ccp_alphas()
        
    elif question_part == "e":
        print("Question Part E")
        dt.random_forest()
    
    else:
        print("Invalid question part. Please provide a valid question part (a, b, c, d, e).")
        exit(1)



import os 
import sys
import numpy as np
from PIL import Image
import pandas as pd
import time
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score
from sklearn.neural_network import MLPClassifier
import math
import argparse

class NeuralNetworks:
    def __init__(self, input_size, hidden_sizes, output_size, lr=0.01):
        # Define the architecture: input, hidden layers, output
        self.sizes = [input_size] + hidden_sizes + [output_size]
        self.layers = len(self.sizes) - 1
        self.lr = lr

        np.random.seed(42)
        
        self.weights = []
        self.biases = []
        for i in range(self.layers):
            weight = np.random.randn(self.sizes[i+1], self.sizes[i]) * np.sqrt(2. / self.sizes[i])
            bias = np.zeros((self.sizes[i+1], 1))
            print(f"Layer {i}: weights shape: {weight.shape}, biases shape: {bias.shape}")
            self.weights.append(weight)
            self.biases.append(bias)

    def sgd_shuffle(self, X, y):
        m = X.shape[1]
        indices = np.random.permutation(m)
        X_shuffled = X[:, indices]
        y_shuffled = y[:, indices]
        return X_shuffled, y_shuffled
    
    def fit(self, X, y, epochs=50, batch_size=1, convergence_threshold=0):
        
        print(f"Sizes of layers: {self.sizes}")
        print(f"Number of layers: {self.layers}")
        
        # X shape: (input_size, num_samples)
        # Convert labels to one-hot encoding
        y_encoded = self.one_hot_encode(y)
        _, m = X.shape

        previous_loss = float('inf')
        for epoch in range(1, epochs + 1):
            # Shuffle indices for mini-batch sampling
            X_shuffled, y_shuffled = self.sgd_shuffle(X, y_encoded)
            
            for i in range(0, m, batch_size):
                start_index = i
                end_index = min(i + batch_size, m)
                X_batch = X_shuffled[:, start_index:end_index]
                y_batch = y_shuffled[:, start_index:end_index]
                
                # Forward propagation
                zs, acts = self.for_prop(X_batch)
                # Backward propagation and update parameters
                self.back_propagation(acts, X_batch, y_batch, self.lr)

            loss = self.cross_entropy_loss(y_batch, acts[-1])\
            
            if epoch % 10 == 0:
                # Compute loss on the last batch (for simplicity)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

            abs_loss = abs(previous_loss - loss)
            # Check for convergence
            if abs_loss &lt; convergence_threshold:
                print(f"Convergence reached at epoch {epoch}, loss: {loss:.4f}")
                break
            previous_loss = loss
        
        return self.weights, self.biases

    def predict(self, X):
        _, acts = self.for_prop(X)

        # Get the predicted class labels
        predictions = np.argmax(acts[-1], axis=0)
        return predictions

    def one_hot_encode(self, y):
        num_classes = np.max(y) + 1
        one_hot_y = np.zeros((num_classes, y.shape[0]))
        one_hot_y[y, np.arange(y.shape[0])] = 1
        return one_hot_y

    def sigmoid(self, z):
        return 1. / (1. + np.exp(-z))
    
    def sigmoid_derivative(self, a):
        return a * (1 - a)
    
    def softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
        return exp_z / np.sum(exp_z, axis=0, keepdims=True)
    
    def cross_entropy_loss(self, y, y_hat):
        """
        y: one-hot encoded true labels, shape (num_classes, m)
        y_hat: predicted probabilities, shape (num_classes, m)
        """
        m = y.shape[1]
        # Clip predictions for numerical stability
        eps = 1e-12
        y_hat_clipped = np.clip(y_hat, eps, 1. - eps)
        # Compute cross-entropy loss using matrix operations
        loss = -np.sum(y * np.log(y_hat_clipped)) / m
        return loss

    def for_prop(self, X):
        """
        Performs forward propagation through the network.
        
        Returns:
            zs: list of weighted sums (z values) for each layer.
            acts: list of activations for each layer.
        """
        A = X
        zs = []
        acts = [X]
        
        for layer in range(self.layers):
            z = self.biases[layer] + np.dot(self.weights[layer], A)
            zs.append(z)

            # Use sigmoid for hidden layers and softmax for output layer
            if layer == self.layers - 1:
                A = self.softmax(z)
            else:
                A = self.sigmoid(z)

            acts.append(A)
        
        return zs, acts

    def back_propagation(self, activations, inputs, labels, learning_rate):
        """
        Executes the backpropagation algorithm to compute gradients and update parameters.

        Parameters:
            activations: List of activations from forward pass.
            weighted_inputs: List of weighted sums (z values) from forward pass.
            inputs: Input data for the current batch.
            labels: One-hot encoded true labels for the current batch.
            learning_rate: Learning rate for parameter updates.
            activation_function: Activation function used in the network ('relu' or 'sigmoid').
        """
        batch_size = inputs.shape[1]

        output_error = activations[-1] - labels

        weight_gradients = [0] * self.layers
        weight_gradients[-1] = np.dot(output_error, activations[-2].T) / batch_size

        bias_gradients = [0] * self.layers
        bias_gradients[-1] = np.sum(output_error, axis=1, keepdims=True) / batch_size

        self.weights[-1] -= learning_rate * weight_gradients[-1]
        self.biases[-1] -= learning_rate * bias_gradients[-1]

        # Propagate error backward through hidden layers
<A NAME="1"></A><FONT color = #00FF00><A HREF="match108-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for layer in range(2, self.layers + 1):

            activation_derivative = self.sigmoid_derivative(activations[-layer])

            output_error = np.dot(self.weights[-layer + 1].T, output_error) * activation_derivative

            bias_gradients[-layer] = np.sum(output_error, axis=1, keepdims=True) / batch_size
</FONT>            self.biases[-layer] -= learning_rate * bias_gradients[-layer]

            weight_gradients[-layer] = np.dot(output_error, activations[-layer - 1].T) / batch_size
            self.weights[-layer] -= learning_rate * weight_gradients[-layer]
    
    def eval(y_train, train_pred, y_test, test_pred):
        report = classification_report(y_test, test_pred, output_dict=True, zero_division=0)
        f1_train = f1_score(y_train, train_pred, average='macro')
        f1_test = report['macro avg']['f1-score']

        return f1_train, f1_test, report
            
            

# ------------------------------
# Data Loading Functions
# ----------------------------p--

def pre_process(img):
    img = img.convert('RGB')
    img = img.resize((28, 28))
    img = np.array(img, dtype=np.float32)
    img = img.flatten()
    return img / 255.0

def load_data(data_path, isTrain=True):
    X = []
    y = []
    if isTrain:
        for sub_dir in os.listdir(data_path):
            sub_dir_path = os.path.join(data_path, sub_dir)
            if not sub_dir.isdigit():  # Skip non-numeric directories
                continue
            label = int(sub_dir)

            for file in os.listdir(sub_dir_path):
                file_path = os.path.join(sub_dir_path, file)
                if file.startswith('.') or not file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Skip hidden and non-image files
                    continue
                y.append(label)
                X.append(pre_process(Image.open(file_path)))
    else:
        label_csv_path = os.path.join(data_path, "test_labels.csv")

        labels = pd.read_csv(label_csv_path)

        files = sorted(os.listdir(data_path))

        for file in files:
            file_path = os.path.join(data_path, file)
            if file.startswith('.') or not file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Skip hidden and non-image files
                continue
            X.append(pre_process(Image.open(file_path)))
            label = labels[labels['image'] == file]['label'].values[0]
            y.append(label)

    X = np.array(X).T  # shape: (input_size, num_samples)
    y = np.array(y)
    return X, y

def handle_question_part_a():
    hidden_sizes = [100]
    classifier = NeuralNetworks(n, hidden_sizes, num_classes)
    classifier.fit(X_train, y_train, epochs=100, batch_size=32)
    predictions = classifier.predict(X_test)
    # Compute accuracy using integer labels (not one-hot)
    accuracy = np.mean(predictions == y_test)
    print(f"Accuracy: {accuracy * 100:.2f}%")

    return predictions


def handle_question_part_b():
    hidden_layer_units = [1, 5, 10, 50, 100]
    train_scores = []
    test_scores = []
    reports = {}
    for units in hidden_layer_units:
        print(f"\n\nTraining with {units} hidden layer units...")
        classifier = NeuralNetworks(n, [units], num_classes)
        classifier.fit(X_train, y_train, epochs=100, batch_size=32, lr=0.01, convergence_threshold=1e-4)
        predictions = classifier.predict(X_test)
        accuracy = np.mean(predictions == y_test)
        print(f"Accuracy: {accuracy * 100:.2f}%")

        train_pred = classifier.predict(X_train)

        train_score, test_score, report = classifier.eval(y_train, train_pred, y_test, predictions)

        train_scores.append(train_score)
        test_scores.append(test_score)

        reports[units] = report
        print(f"Following are the metrics for {[100]} hidden layers")
        print(f"Accuracy: {report['accuracy']:.4f}, F1 Score: {test_score:.4f}, Recall Score: {report['macro avg']['recall']:.4f}, Precision Score: {report['macro avg']['precision']:.4f}")

    plt.figure(figsize=(10, 5))
    plt.plot(train_scores, label='Train F1 Score')
    plt.plot(test_scores, label='Test F1 Score')
    plt.title('F1 Score vs Hidden Layer Units')
    plt.xlabel('Hidden Layer Units')
    plt.ylabel('F1 Score')
    plt.xticks(range(len(hidden_layer_units)), hidden_layer_units)
    plt.legend()
    filename = os.path.join(output_folder_path, f'plot_b.png')
    if os.path.exists(filename):
        os.remove(filename)

    plt.savefig(os.path.join(output_folder_path, f'plot_b.png'))
    plt.show()
    # Save the reports to CSV
    report_df = pd.DataFrame.from_dict(reports, orient='index')
    report_df.to_csv(os.path.join(output_folder_path, f'reports_b.csv'))

    return predictions


def handle_question_part_c():
    varried_hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    train_scores = []
    test_scores = []
    reports = {}
    for hidden_layers in varried_hidden_layers:
        print(f"\n\nTraining with hidden layers: {hidden_layers}...")
        classifier = NeuralNetworks(n, hidden_layers, num_classes)
        classifier.fit(X_train, y_train, epochs=100, batch_size=32, convergence_threshold=1e-2)
        predictions = classifier.predict(X_test)
        accuracy = np.mean(predictions == y_test)
        print(f"Accuracy: {accuracy * 100:.2f}%")

        train_pred = classifier.predict(X_train)

        train_score, test_score, report = classifier.eval(y_train, train_pred, y_test, predictions)

        train_scores.append(train_score)
        test_scores.append(test_score)

        reports[str(hidden_layers)] = report
        print(f"Following are the metrics for {hidden_layers} hidden layers")
        print(f"Accuracy: {report['accuracy']:.4f}, F1 Score: {test_score:.4f}, Recall Score: {report['macro avg']['recall']:.4f}, Precision Score: {report['macro avg']['precision']:.4f}")

    plt.figure(figsize=(10, 5))
    plt.plot(train_scores, label='Train F1 Score')
    plt.plot(test_scores, label='Test F1 Score')
    plt.title('F1 Score vs Network Depth')
    plt.xlabel('Network Depth')
    plt.ylabel('F1 Score')
    plt.xticks(range(len(varried_hidden_layers)), [str(layer) for layer in varried_hidden_layers])
    plt.legend()
    plt.savefig(os.path.join(output_folder_path, f'plot_c.png'))
    plt.show()

    return predictions


def handle_question_part_f():
    varried_hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    train_scores = []
    test_scores = []
    reports = {}
    for hidden_layers in varried_hidden_layers:
        print(f"\n\nTraining with hidden layers: {hidden_layers}...")
        mlp = MLPClassifier(hidden_layer_sizes=hidden_layers, max_iter=100, learning_rate_init=0.01, random_state=42, activation='relu', solver='sgd', alpha=0, batch_size=32, learning_rate='invscaling', tol=1e-4)
        mlp.fit(X_train.T, y_train)
        predictions = mlp.predict(X_test.T)
        train_pred = mlp.predict(X_train.T)
        classifier = NeuralNetworks(n, hidden_layers, num_classes)
        train_score, test_score, report = classifier.eval(y_train, train_pred, y_test, predictions)
        train_scores.append(train_score)
        test_scores.append(test_score)
        reports[str(hidden_layers)] = report
        print(f"Following are the metrics for {hidden_layers} hidden layers")
        print(f"Accuracy: {report['accuracy']:.4f}, F1 Score: {test_score:.4f}, Recall Score: {report['macro avg']['recall']:.4f}, Precision Score: {report['macro avg']['precision']:.4f}")

    plt.figure(figsize=(10, 5))
    plt.plot(train_scores, label='Train F1 Score')
    plt.plot(test_scores, label='Test F1 Score')
    plt.title('F1 Score vs Network Depth')
    plt.xlabel('Network Depth')
    plt.ylabel('F1 Score')
    plt.xticks(range(len(varried_hidden_layers)), [str(layer) for layer in varried_hidden_layers])
    plt.legend()
    plt.savefig(os.path.join(output_folder_path, f'plot_f.png'))
    plt.show()
    # Save the reports to CSV
    report_df = pd.DataFrame.from_dict(reports, orient='index')
    report_df.to_csv(os.path.join(output_folder_path, f'reports_f.csv'))

    return predictions


# Simulating switch-case using a dictionary
question_part_handlers = {
    'a': handle_question_part_a,
    'b': handle_question_part_b,
    'c': handle_question_part_c,
    'f': handle_question_part_f
}

<A NAME="0"></A><FONT color = #FF0000><A HREF="match108-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Neural Network Training and Testing")
    parser.add_argument("train_data_path", type=str, help="Path to the training data folder")
    parser.add_argument("test_data_path", type=str, help="Path to the testing data folder")
    parser.add_argument("output_folder_path", type=str, help="Path to the output folder")
    parser.add_argument("question_part", type=str, help="Part of the question to run (a, b, c, d, e, f)")
    args = parser.parse_args()
</FONT>    train_data_path = args.train_data_path
    test_data_path = args.test_data_path
    output_folder_path = args.output_folder_path
    question_part = args.question_part

    X_train, y_train = load_data(train_data_path)
    X_test, y_test = load_data(test_data_path, isTrain=False)

    n = X_train.shape[0]
    num_classes = 43

    if not os.path.exists(output_folder_path):
        os.makedirs(output_folder_path)

    # Call the appropriate handler based on the question part
    if question_part in question_part_handlers:
        predictions = question_part_handlers[question_part]()
    else:
        print("Invalid question part. Please specify 'a', 'b', 'c', 'd', 'e', or 'f'.")
        sys.exit(1)

    os.makedirs(output_folder_path, exist_ok=True)
    output_csv = os.path.join(output_folder_path, f"prediction_{question_part}.csv")
    df = pd.DataFrame({"prediction": predictions})
    df.to_csv(output_csv, index=False)
    print(f"Predictions saved to {output_csv}")

    output_csv = os.path.join(output_folder_path, f"test_{question_part}.csv")
    df = pd.DataFrame({"label": y_test})
    df.to_csv(output_csv, index=False)
    print(f"Predictions saved to {output_csv}")

</PRE>
</PRE>
</BODY>
</HTML>
