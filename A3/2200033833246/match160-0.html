<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_DBJMZ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_DBJMZ.py<p><PRE>


import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder

class treeNode():
    def __init__(self):
        self.j = None 
        self.is_num = False 
        self.criterion = None 
        self.is_leaf = False 
        self.num_children = 0
        self.children_dict = {} 
        self.parent = None 
        self.prediction = None 
        self.ht = 0
        self.info = [0,0]   

class decisionTree():
    def __init__(self, depth = 0):
        self.root = treeNode()
        self.height = depth
        self.root.ht = depth
        self.leaves = []

    def fit(self, Xc, Xn, y, node = None):
        if (node == None):
            node = self.root
        
        if self.done(Xc, Xn, y, node):
            self.leaves.append(node)
            return 
        
        node.prediction = pd.Series(y).mode()[0]
        list_Xc, list_Xn, list_y, split_values = self.split(Xc, Xn, y, node)

        if node.is_num:
            for i, (Xci, Xni, yi) in enumerate(zip(list_Xc, list_Xn, list_y)):
                child_node = treeNode()
                child_node.parent = node 
                child_node.ht = node.ht - 1
                self.fit(Xci, Xni, yi, child_node)
                node.children_dict[i] = child_node
                node.num_children+=1
        else:
            for v, Xci, Xni, yi in zip(split_values, list_Xc, list_Xn, list_y):
                child_node = treeNode()
                child_node.parent = node 
                child_node.ht = node.ht - 1
                self.fit(Xci, Xni, yi, child_node)
                node.children_dict[v] = child_node
                node.num_children+=1
        
        if node.num_children == 0:
            node.is_leaf = True
            self.leaves.append(node)
        
        return

    def done(self, Xc, Xn, y, node):    
        if node.ht &lt;= 0:
            node.prediction = pd.Series(y).mode()[0]
            node.is_leaf = True
            return True

        if len(np.unique(y)) == 1:
            node.prediction = pd.Series(y).mode()[0]
            node.is_leaf = True
            return True

        if Xc.shape[1] == 0 and Xn.shape[1] == 0:
            node.prediction = pd.Series(y).mode()[0]
            node.is_leaf = True
            return True
        
        return False

    def split(self, Xc, Xn, y, node):
        mi_c = {}
<A NAME="2"></A><FONT color = #0000FF><A HREF="match160-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for col in Xc.columns:
            values = Xc[col].unique()
            weighted_entropy = 0
            for v in values:
                idx = (Xc[col] == v)
</FONT>                weighted_entropy += (np.sum(idx) / len(y)) * self.entropy(y[idx])
            mi_c[col] = weighted_entropy

        mi_n = {}
        for col in Xn.columns:
            threshold = Xn[col].median()
            left_idx = (Xn[col] &lt;= threshold)
            right_idx = (Xn[col] &gt; threshold)
            weighted_entropy = (
                np.sum(left_idx) / len(y) * self.entropy(y[left_idx]) +
                np.sum(right_idx) / len(y) * self.entropy(y[right_idx])
            )
            mi_n[col] = weighted_entropy

        best_c_attr = min(mi_c, key=mi_c.get) if mi_c else None
        best_n_attr = min(mi_n, key=mi_n.get) if mi_n else None

        min_c = mi_c[best_c_attr] if best_c_attr else float("inf")
        min_n = mi_n[best_n_attr] if best_n_attr else float("inf")

        list_Xc, list_Xn, list_y, split_values = [], [], [], []

        H_before = self.entropy(y)
        H_after = min(min_c, min_n)
        if H_after &gt;= H_before:
            return list_Xc, list_Xn, list_y, split_values

        if min_c &lt; min_n:
            node.is_num = False
            node.j = best_c_attr
            values = Xc[best_c_attr].unique()
            for v in values:
                idx = (Xc[best_c_attr] == v)
                Xc_new = Xc[idx].drop(columns=[best_c_attr])
                list_Xc.append(Xc_new)
                list_Xn.append(Xn[idx])
                list_y.append(y[idx])
                split_values.append(v)
        else:
            node.is_num = True
            node.j = best_n_attr
            threshold = Xn[best_n_attr].median()
            node.criterion = threshold
            left_idx = (Xn[best_n_attr] &lt;= threshold)
            right_idx = (Xn[best_n_attr] &gt; threshold)
            for idx in [left_idx, right_idx]:
                list_Xc.append(Xc[idx])
                list_Xn.append(Xn[idx])
                list_y.append(y[idx])

        return list_Xc, list_Xn, list_y, split_values

    def entropy(self, y):
        values, counts = np.unique(y, return_counts=True)
        probabilities = counts / counts.sum()
        return -np.sum(probabilities * np.log2(probabilities + 1e-9))

    def predict(self, Xc, Xn):
        y = []
        for i in range(len(Xc)):
            node = self.root
            while not node.is_leaf:
                if node.is_num:
                    if node.j not in Xn.columns:
                        break
                    value = Xn.iloc[i][node.j]
                    if value &lt;= node.criterion:
                        node = node.children_dict[0]
                    else:
                        node = node.children_dict[1]
                else:
                    if node.j not in Xc.columns:
                        break
                    value = Xc.iloc[i][node.j]
                    if value in node.children_dict:
                        node = node.children_dict[value]
                    else:
                        break
            y.append(node.prediction)
        return y

class pruneTree():
    def __init__(self, tree, Xc, Xn, y):
        self.tree = tree    # tree to prune
        self.Xc = Xc        # validation X (categorical)
        self.Xn = Xn        # validation X (numerical)
        self.y = y          # validation y
      
    def fit(self,track_accuracies=False,train_data=None,test_data=None,val_data=None):
        self.init_info()
        num_nodes = self.count_nodes()
        accuracies_data = [[],[],[],[]]
        current_accuracy = 0
        while True:
            if (track_accuracies==True):
                num_nodes = self.count_nodes()
                accuracies_data[0].append(num_nodes)
                y_train_pred = self.tree.predict(train_data[0],train_data[1])
                train_acc = np.mean(y_train_pred == train_data[2])
                accuracies_data[1].append(train_acc)

                y_test_pred = self.tree.predict(test_data[0],test_data[1])
                test_acc = np.mean(y_test_pred == test_data[2])
                accuracies_data[2].append(test_acc)

                y_val_pred = self.tree.predict(val_data[0],val_data[1])
                val_acc = np.mean(y_val_pred == val_data[2])
                accuracies_data[3].append(val_acc)
            new_accuracy, node = self.best_node_to_prune()
            
            if (new_accuracy &lt;= current_accuracy) or (node is None):
                break
            num_nodes-=1
            current_accuracy = new_accuracy
            self.update_tree(node)

        return accuracies_data

    def init_info(self):
        pred = self.tree.predict(self.Xc, self.Xn)
        for i in range(len(self.Xc)):
            node = self.tree.root
            while not node.is_leaf:
                if (pred[i]!=node.prediction): node.info[0]+=1
                if (pred[i]!=node.prediction and self.y[i]!=node.prediction): node.info[1]+=1

                if node.is_num:
                    if node.j not in self.Xn.columns:
                        break
                    value = self.Xn.iloc[i][node.j]
                    if value &lt;= node.criterion:
                        node = node.children_dict[0]
                    else:
                        node = node.children_dict[1]
                else:
                    if node.j not in self.Xc.columns:
                        break
                    value = self.Xc.iloc[i][node.j]
                    if value in node.children_dict:
                        node = node.children_dict[value]
                    else:
                        break

            if (node.is_leaf):
                if (pred[i]!=node.prediction): node.info[0]+=1
                if (pred[i]!=node.prediction and self.y[i]!=node.prediction): node.info[1]+=1

        return

    def best_node_to_prune(self):
        q = deque()
        q.append(self.tree.root)
        max_accuracy = 0
        best_node = None

        while (len(q)&gt;0):
            node = q.popleft()

            if node.is_leaf:
                continue

            tmp_accuracy = (node.info[0] - node.info[1]) - (node.info[1])

            if tmp_accuracy &gt; max_accuracy:
                max_accuracy = tmp_accuracy
                best_node = node

            for child in node.children_dict.values():
                q.append(child)

        return max_accuracy, best_node

    def count_nodes(self):
        nodes_num = 0
        q = deque()
        q.append(self.tree.root)
        while (len(q)&gt;0):
            node = q.popleft()
            nodes_num+=1
            if node.is_leaf:
                continue

            for child in node.children_dict.values():
                q.append(child)

        return nodes_num

    def update_tree(self, node):
        # Update predictions
        updates = [node.prediction, node.info[0], node.info[1], node.info[0]-node.info[1]]
        node.is_leaf = True

        node = node.parent

        while (node != None):
            if (node.prediction == updates[0]):
                node.info[0] -= updates[1] 
                node.info[1] -= updates[2]
            else:
                node.info[0] += updates[0]
                node.info[1] += updates[3]

            node = node.parent
        return

def preprocess(df, question_part, evaluate = False):
    y = []
    if (evaluate == False):
        y = df["income"].to_numpy()
        X = df.drop(columns=["income"])
    else:
        X = df
        if ("income" in df.columns):
            X = df.drop(columns=["income"])

    if question_part == 'a':
        Xc = X.select_dtypes(include=["object", "category"])
        Xn = X.select_dtypes(include=["number"])
        return Xc, Xn, y

    elif question_part == 'd' or question_part == 'e':
        Xc = X.select_dtypes(include=["object", "category"])
        Xn = X.select_dtypes(include=["number"])
        return Xc, Xn, y

    else:
        Xc = X.select_dtypes(include=["object", "category"])
        Xn = X.select_dtypes(include=["number"])
        Xc_encoded = pd.get_dummies(Xc, prefix=Xc.columns)
        return Xc_encoded, Xn, y

def analysis_a(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val):
    depths = [5, 10, 15, 20]
    accuracies = [[],[],[]]

    for depth in depths:
        print("Training for depth:", depth)
        tree = decisionTree(depth)
        tree.fit(Xc_train, Xn_train, y_train)

        y_train_pred = tree.predict(Xc_train, Xn_train)
        train_acc = np.mean(y_train_pred == y_train)
        accuracies[0].append(train_acc)
        print("Train Accuracy: ",train_acc)

        y_val_pred = tree.predict(Xc_val, Xn_val)
        val_acc = np.mean(y_val_pred == y_val)
        accuracies[1].append(val_acc)
        print("Validation Accuracy: ",val_acc)

        y_test_pred = tree.predict(Xc_test, Xn_test)
        test_acc = np.mean(y_test_pred == y_test)
        accuracies[2].append(test_acc)
        print("Test Accuracy: ", test_acc)

    plt.figure()
    plt.plot(depths, accuracies[0], marker='o', label='Train Accuracy')
    plt.plot(depths, accuracies[1], marker='s', label='Test Accuracy')
    plt.plot(depths, accuracies[2], marker='x', label='Validation Accuracy')
    plt.title("Accuracies vs Maximum Depth")
    plt.ylabel("Accuracies")
    plt.xlabel("Maximum Depth")
    plt.legend()
    plt.show()

def analysis_b(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val):
    depths = [25, 35, 45, 55]
    accuracies = [[],[],[]]

    for depth in depths:
        print("Training for depth:", depth)
        tree = decisionTree(depth)
        tree.fit(Xc_train, Xn_train, y_train)

        y_train_pred = tree.predict(Xc_train, Xn_train)
        train_acc = np.mean(y_train_pred == y_train)
        accuracies[0].append(train_acc)
        print("Train Accuracy: ",train_acc)

        y_val_pred = tree.predict(Xc_val, Xn_val)
        val_acc = np.mean(y_val_pred == y_val)
        accuracies[1].append(val_acc)
        print("Validation Accuracy: ",val_acc)

        y_test_pred = tree.predict(Xc_test, Xn_test)
        test_acc = np.mean(y_test_pred == y_test)
        accuracies[2].append(test_acc)
        print("Test Accuracy: ", test_acc)

    plt.figure()
    plt.plot(depths, accuracies[0], marker='o', label='Train Accuracy')
    plt.plot(depths, accuracies[1], marker='s', label='Test Accuracy')
    plt.plot(depths, accuracies[2], marker='x', label='Validation Accuracy')
    plt.title("Accuracies vs Maximum Depth")
    plt.ylabel("Accuracies")
    plt.xlabel("Maximum Depth")
    plt.legend()
    plt.show()

def analysis_c(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val):
    # code for analysis
    depths = [25,35,45,55]
    accuracies = []

    for depth in depths:
        print("Training for depth: ", depth)
        tree = decisionTree(depth)
        tree.fit(Xc_train, Xn_train, y_train)
        print("Pruning for depth: ",depth)
        pruner = pruneTree(tree,Xc_val,Xn_val,y_val)
        acc = pruner.fit(True,[Xc_train,Xn_train,y_train],[Xc_test,Xn_test,y_test],[Xc_val,Xn_val,y_val])
        accuracies.append(acc)
    
    for i in range(4):
        plt.figure(figsize=(6, 4))
        plt.plot(accuracies[i][0], accuracies[i][1], marker='o', label='Train Accuracy')
        plt.plot(accuracies[i][0], accuracies[i][2], marker='s', label='Test Accuracy')
        plt.plot(accuracies[i][0], accuracies[i][3], marker='x', label='Validation Accuracy')
        plt.title(f"Accuracies vs Num Nodes for depth {depths[i]}")
        plt.ylabel("Accuracies")
        plt.xlabel("Num Nodes")
        plt.legend()
        plt.show()

def analysis_d(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val):
    # code for analysis, comment out before submission

    # for finding best depth
    print("Finding Best Depth...")
    depths = [25, 35, 45, 55]
    accuracies = [[],[],[]]
    X_train = pd.concat([Xc_train, Xn_train], axis=1)
    X_test = pd.concat([Xc_test, Xn_test], axis=1)
    X_val = pd.concat([Xc_val, Xn_val], axis=1)

    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoder.fit(X_train)
    X_train = encoder.transform(X_train)
    X_test = encoder.transform(X_test)
    X_val = encoder.transform(X_val)

    for depth in depths:
        print("Training for depth:", depth)
        tree = DecisionTreeClassifier(criterion='entropy', max_depth=depth)
        tree.fit(X_train, y_train)

        y_train_pred = tree.predict(X_train)
        train_acc = np.mean(y_train_pred == y_train)
        accuracies[0].append(train_acc)
        print("Train Accuracy: ",train_acc)

        y_val_pred = tree.predict(X_val)
        val_acc = np.mean(y_val_pred == y_val)
        accuracies[1].append(val_acc)
        print("Validation Accuracy: ",val_acc)

        y_test_pred = tree.predict(X_test)
        test_acc = np.mean(y_test_pred == y_test)
        accuracies[2].append(test_acc)
        print("Test Accuracy: ", test_acc)

    plt.figure()
    plt.plot(depths, accuracies[0], marker='o', label='Train Accuracy')
    plt.plot(depths, accuracies[1], marker='x', label='Validation Accuracy')
    plt.plot(depths, accuracies[2], marker='s', label='Test Accuracy')
    plt.xlabel("Maximum Depth")
    plt.ylabel("Accuracies")
    plt.title("Accuracy vs Maximum Depth")
    plt.legend()
    plt.grid(True)
    plt.show()

    # best depth
    best_index = accuracies[1].index(max(accuracies[1]))
    best_depth = depths[best_index]
    print("Best Depth: ",best_depth)

    # for finding best pruning parameter
    print("For finding best ccp_alpha...")
    ccp_alphas = [0.001, 0.01, 0.1, 0.2]
    accuracies = [[],[],[]]

    for ccp_alpha in ccp_alphas:
        print("Training for ccp_alpha:", ccp_alpha)
        tree = DecisionTreeClassifier(criterion='entropy', ccp_alpha = ccp_alpha)
        tree.fit(X_train, y_train)

        y_train_pred = tree.predict(X_train)
        train_acc = np.mean(y_train_pred == y_train)
        accuracies[0].append(train_acc)
        print("Train Accuracy: ",train_acc)

        y_val_pred = tree.predict(X_val)
        val_acc = np.mean(y_val_pred == y_val)
        accuracies[1].append(val_acc)
        print("Validation Accuracy: ",val_acc)

        y_test_pred = tree.predict(X_test)
        test_acc = np.mean(y_test_pred == y_test)
        accuracies[2].append(test_acc)
        print("Test Accuracy: ", test_acc)

    plt.figure()
    plt.plot(ccp_alphas, accuracies[0], marker='o', label='Train Accuracy')
    plt.plot(ccp_alphas, accuracies[1], marker='x', label='Validation Accuracy')
    plt.plot(ccp_alphas, accuracies[2], marker='s', label='Test Accuracy')
    plt.xlabel("ccp_alpha")
    plt.ylabel("Accuracies")
    plt.title("Accuracy vs ccp_alpha")
    plt.legend()
    plt.grid(True)
    plt.show()

    # best ccp_alpha
    best_index = accuracies[1].index(max(accuracies[1]))
    best_ccp_alpha = ccp_alphas[best_index]
    print("Best ccp_alpha: ",best_ccp_alpha)
    
    print("using best scikit model")
    tree = DecisionTreeClassifier(criterion='entropy', max_depth=best_depth, ccp_alpha = best_ccp_alpha)
    tree.fit(X_train, y_train)

    y_train_pred = tree.predict(X_train)
    train_acc = np.mean(y_train_pred == y_train)
    print("Best Train Accuracy: ",train_acc)

    y_val_pred = tree.predict(X_val)
    val_acc = np.mean(y_val_pred == y_val)
    print("Best Validation Accuracy: ",val_acc)

    y_test_pred = tree.predict(X_test)
    test_acc = np.mean(y_test_pred == y_test)
    print("Best Test Accuracy: ", test_acc)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("train_data_path", type=str)
    parser.add_argument("validation_data_path", type=str)
    parser.add_argument("test_data_path", type=str)
    parser.add_argument("output_folder_path", type=str)
    parser.add_argument("question_part", type=str, choices=["a", "b", "c", "d", "e"])
    args = parser.parse_args()

    train_df = pd.read_csv(args.train_data_path)
    val_df = pd.read_csv(args.validation_data_path)
    test_df = pd.read_csv(args.test_data_path)

    Xc_train, Xn_train, y_train = preprocess(train_df,args.question_part)
    Xc_val, Xn_val, y_val = preprocess(val_df,args.question_part)
    Xc_test, Xn_test, y_test = preprocess(test_df,args.question_part, evaluate=True)

    if args.question_part == 'a':
        # analysis_a(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val)
        # sharing the prediction for a tree of max depth 20 in csv file in output folder
        tree = decisionTree(depth=20)
        tree.fit(Xc_train,Xn_train,y_train)
        predictions = tree.predict(Xc_test,Xn_test)
        pd.DataFrame({"prediction": predictions}).to_csv(f"{args.output_folder_path}/prediction_a.csv", index=False)
    
    if args.question_part == 'b':
        # analysis_b(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val)
        # sharing the prediction for a tree of max depth 55 in csv file in output folder
        tree = decisionTree(depth=55)
        tree.fit(Xc_train,Xn_train,y_train)
        predictions = tree.predict(Xc_test,Xn_test)
        pd.DataFrame({"prediction": predictions}).to_csv(f"{args.output_folder_path}/prediction_b.csv", index=False)

    if args.question_part == 'c':
        # analysis_c(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val)
        tree = decisionTree(5)
        tree.fit(Xc_train, Xn_train, y_train)
        pruner = pruneTree(tree,Xc_val,Xn_val,y_val)
        pruner.fit()
        tree = pruner.tree 
        predictions = tree.predict(Xc_test,Xn_test)
        pd.DataFrame({"prediction": predictions}).to_csv(f"{args.output_folder_path}/prediction_c.csv", index=False)

    if args.question_part == 'd':
        # analysis_d(Xc_train, Xn_train, y_train, Xc_test, Xn_test, y_test, Xc_val, Xn_val, y_val)
        X_train = pd.concat([Xc_train, Xn_train], axis=1)
        X_test = pd.concat([Xc_test, Xn_test], axis=1)
        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        encoder.fit(X_train)
        X_train = encoder.transform(X_train)
        X_test = encoder.transform(X_test)
        tree = DecisionTreeClassifier(criterion='entropy', max_depth=25, ccp_alpha=0.001)
        tree.fit(X_train,y_train)
        predictions = tree.predict(X_test)
        pd.DataFrame({"prediction": predictions}).to_csv(f"{args.output_folder_path}/prediction_d.csv", index=False)

    if args.question_part == 'e':
        # param_grid = {
        #     'n_estimators': [50, 150, 250, 350],
        #     'max_features': [0.1, 0.3, 0.5, 0.7, 1.0],
        #     'min_samples_split': [2, 4, 6, 8, 10],
        # }

        X_train = pd.concat([Xc_train, Xn_train], axis=1)
        X_test = pd.concat([Xc_test, Xn_test], axis=1)
        X_val = pd.concat([Xc_val, Xn_val], axis=1)
        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
        encoder.fit(X_train)
        X_train = encoder.transform(X_train)
        X_test = encoder.transform(X_test)
        X_val = encoder.transform(X_val)

        # def scoring_func(estimator, X, y):
        #     return estimator.oob_score_

        # forest = RandomForestClassifier(criterion='entropy', oob_score=True, bootstrap=True, random_state=42, n_jobs=-1)
        # grid = GridSearchCV(estimator=forest, param_grid=param_grid, cv=2, scoring=scoring_func, n_jobs=-1, verbose=2)
        # grid.fit(X_val, y_val) 
        # best_forest = grid.best_estimator_
        # oob_acc = best_forest.oob_score_
        # val_acc = accuracy_score(y_val, best_forest.predict(X_val))
        # test_acc = accuracy_score(y_test, best_forest.predict(X_test))
        # train_acc = accuracy_score(y_train, best_forest.predict(X_train))

        # print("Train Accuracy:", train_acc)
        # print("Validation Accuracy:", val_acc)
        # print("Test Accuracy:", test_acc)
        # print("Out-of-Bag Accuracy:", oob_acc)
        # print("Best Parameters:", grid.best_params_)

        forest = RandomForestClassifier(
            criterion='entropy',
            oob_score=True,
            bootstrap=True,
            random_state=42,
            n_jobs=-1,
            max_features=0.7,
            min_samples_split=10,
            n_estimators=350
        )
        forest.fit(X_train, y_train)
        predictions = forest.predict(X_test)
        pd.DataFrame({"prediction": predictions}).to_csv(f"{args.output_folder_path}/prediction_e.csv", index=False)

if __name__ == "__main__":
    main()



import os
import argparse
import pandas as pd
import numpy as np
from PIL import Image
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.neural_network import MLPClassifier

def softmax(z):
    z = z - np.max(z, axis=0, keepdims=True)  
    return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)

class NeuralNet:
    def __init__(self, M, n, hidden_layers, r, g=0, decay = 0):
        self.batch_size = M
        self.num_features = n
        self.hidden_layers = hidden_layers
        self.num_classes = r
        self.all_layers = [n] + hidden_layers + [r]
        self.parameters_n1 = []
        self.parameter_0 = []
        for i in range(len(self.all_layers)-1):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match160-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.parameters_n1.append(np.random.randn(self.all_layers[i+1], self.all_layers[i]) * np.sqrt(2./self.all_layers[i]))
</FONT>            self.parameter_0.append(np.zeros((self.all_layers[i+1], 1)))
        self.g = g
        self.decay = decay
        
    def apply_activation(self,inp):
        if (self.g == 0): return 1/(1+np.exp(inp))
        else: return np.maximum(0,inp)

    def act_grad(self, inp):
        if (self.g == 0): 
            return 1/(1+np.exp(inp))
        else: 
            tt = inp &gt; 0
            return tt.astype(float)

    def forward_pass(self, X):
        g_net_j = [X.T]
        pre_os = []
        
        # Hidden layers with sigmoid
        for i in range(len(self.parameters_n1) - 1):
            W = self.parameters_n1[i]
            b = self.parameter_0[i]
            pre_o = np.dot(W,g_net_j[-1]) + b
            pre_os.append(pre_o)
            g_net_j.append(self.apply_activation(pre_o))
        
        # Output layer with softmax
        pre_o = np.dot(self.parameters_n1[-1], g_net_j[-1]) + self.parameter_0[-1]
        pre_os.append(pre_o)
        g_net_j.append(softmax(pre_o))
        
        return g_net_j, pre_os

    def compute_loss(self, y_pred, y_true):
        m = y_true.shape[1]
        log_probs = -np.log(y_pred[y_true.argmax(axis=0), np.arange(m)])
        return np.sum(log_probs) / m

    def backpropagation(self, g_net_j, pre_os, y_true):
        del_W = [np.zeros_like(W) for W in self.parameters_n1]
        del_b = [np.zeros_like(b) for b in self.parameter_0]
        m = y_true.shape[1]
        
        del_net_j = g_net_j[-1] - y_true
<A NAME="0"></A><FONT color = #FF0000><A HREF="match160-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        del_W[-1] = np.dot(del_net_j,g_net_j[-2].T) / m
        del_b[-1] = np.sum(del_net_j, axis=1, keepdims=True) / m
        
        # Hidden layers
        for l in range(len(self.parameters_n1)-2, -1, -1):
</FONT>            if(self.g==0):
                del_net_j = (np.dot(self.parameters_n1[l+1].T , del_net_j)) * self.act_grad(pre_os[l]) * (1 - self.act_grad(pre_os[l]))
            else:
                del_net_j = (np.dot(self.parameters_n1[l+1].T , del_net_j)) * self.act_grad(pre_os[l])
            del_W[l] = np.dot(del_net_j,g_net_j[l].T) / m
            del_b[l] = np.sum(del_net_j, axis=1, keepdims=True) / m
        
        return del_W, del_b

    def train_network(self, X_train, y_train, epochs=10, lr=0.01):
        y_train_onehot = np.eye(self.num_classes)[y_train].T
        num_samples = X_train.shape[0]
        permutation = np.random.permutation(num_samples)
        X_shuffled = X_train[permutation]
        y_shuffled = y_train_onehot[:, permutation]
        
        for epoch in range(epochs):
            if (self.decay==1): lr = 0.01 * (1/np.sqrt(epoch+1)) 
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match160-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            for i in range(0, num_samples, self.batch_size):
                low = i 
                high = i+self.batch_size
                X_batch = X_shuffled[low:high]
</FONT>                y_batch = y_shuffled[:, low:high]
                g_net_j, zs = self.forward_pass(X_batch)
                del_W, del_b = self.backpropagation(g_net_j, zs, y_batch)
                for l in range(len(self.parameters_n1)):
                    self.parameters_n1[l] -= lr * del_W[l]
                    self.parameter_0[l] -= lr * del_b[l]
                
            g_net_j, _ = self.forward_pass(X_train)
            # loss = self.compute_loss(g_net_j[-1], y_train_onehot)
            # print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")

    def predict(self, X_test):
        g_net_j, _ = self.forward_pass(X_test)
        predictions = np.argmax(g_net_j[-1], axis=0)
        return predictions
        
def dataloader(train_path, test_path):
    datas = [[],[],[]]
    class_folders = []
    for cat in os.listdir(train_path):
        tmp_path = os.path.join(train_path, cat)
        if os.path.isdir(tmp_path):
            class_folders.append(cat)
    class_folders = sorted(class_folders)

    files_te = sorted(os.listdir(test_path))
    
    for label in class_folders:
        label_path = os.path.join(train_path, label)
        for ite in sorted(os.listdir(label_path)):
            img = Image.open(os.path.join(label_path, ite)).convert('RGB')  
            img = np.array(img).astype(np.float32) / 255.0
            datas[0].append(img.flatten())
            datas[1].append(int(label))

    for fte in files_te:
        tmp_path = os.path.join(test_path, fte)
        img = Image.open(tmp_path).convert('RGB')
        img = np.array(img).astype(np.float32) / 255.0 
        datas[2].append(img.flatten())
    
    return np.array(datas[0]), np.array(datas[1]), np.array(datas[2])

def load_test_labels(data_folder):
    labels_path = os.path.join(data_folder, "test_labels.csv")
    df = pd.read_csv(labels_path)
    return df["label"].to_numpy()

def analysis_b(X_train, y_train, X_test, y_test):
    hidden_layers = [1,5,10,50,100]
    f1_scores = []
    for layer in hidden_layers:
        print("Training for features: ",layer)
        model = NeuralNet(32,2352,[layer],43)
        model.train_network(X_train, y_train, epochs=100)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        print(f"\nClassification Report on Train Data (num features {layer}):")
        print(classification_report(y_train, y_train_pred, digits=4))
        print(f"\nClassification Report on Test Data (num features {layer}):")
        print(classification_report(y_test, y_test_pred, digits=4))

        avg_f1 = f1_score(y_test, y_test_pred, average='macro') 
        f1_scores.append(avg_f1)
    
    plt.figure()
    plt.plot(hidden_layers,f1_scores, marker='o')
    plt.xlabel("Number of hidden units")
    plt.ylabel("F1 scores")
    plt.title("F1 scores v/s number of hidden units")
    plt.legend()
    plt.grid(True)
    plt.show()

def analysis_c(X_train, y_train, X_test, y_test):
    hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    f1_scores = []
    for layer in hidden_layers:
        print("Training for arch: ",layer)
        model = NeuralNet(32,2352,layer,43)
        model.train_network(X_train, y_train, epochs=100)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        print(f"\nClassification Report on Train Data (arch {layer}):")
        print(classification_report(y_train, y_train_pred, digits=4))
        print(f"\nClassification Report on Test Data (arch {layer}):")
        print(classification_report(y_test, y_test_pred, digits=4))

        avg_f1 = f1_score(y_test, y_test_pred, average='macro') 
        f1_scores.append(avg_f1)
    
    plt.figure()
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match160-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot([1,2,3,4],f1_scores, marker='o')
    plt.xlabel("Depth")
    plt.ylabel("F1 scores")
    plt.title("F1 scores v/s Depth")
    plt.legend()
</FONT>    plt.grid(True)
    plt.show()

def analysis_d(X_train, y_train, X_test, y_test):
    hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    f1_scores = []
    for layer in hidden_layers:
        print("Training for arch: ",layer)
        model = NeuralNet(32,2352,layer,43,decay=1)
        model.train_network(X_train, y_train, epochs=100)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        print(f"\nClassification Report on Train Data (arch {layer}):")
        print(classification_report(y_train, y_train_pred, digits=4))
        print(f"\nClassification Report on Test Data (arch {layer}):")
        print(classification_report(y_test, y_test_pred, digits=4))

        avg_f1 = f1_score(y_test, y_test_pred, average='macro') 
        f1_scores.append(avg_f1)
    
    plt.figure()
<A NAME="5"></A><FONT color = #FF0000><A HREF="match160-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot([1,2,3,4],f1_scores, marker='o')
    plt.xlabel("Depth")
    plt.ylabel("F1 scores")
    plt.title("F1 scores v/s Depth")
    plt.legend()
</FONT>    plt.grid(True)
    plt.show()

def analysis_e(X_train, y_train, X_test, y_test):
    hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    f1_scores = []
    for layer in hidden_layers:
        print("Training for arch: ",layer)
        model = NeuralNet(32,2352,layer,43,g=1,decay=1)
        model.train_network(X_train, y_train, epochs=100)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        print(f"\nClassification Report on Train Data (arch {layer}):")
        print(classification_report(y_train, y_train_pred, digits=4))
        print(f"\nClassification Report on Test Data (arch {layer}):")
        print(classification_report(y_test, y_test_pred, digits=4))

        avg_f1 = f1_score(y_test, y_test_pred, average='macro') 
        f1_scores.append(avg_f1)
    
    plt.figure()
<A NAME="6"></A><FONT color = #00FF00><A HREF="match160-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot([1,2,3,4],f1_scores, marker='o')
    plt.xlabel("Depth")
    plt.ylabel("F1 scores")
    plt.title("F1 scores v/s Depth")
    plt.legend()
</FONT>    plt.grid(True)
    plt.show()

def analysis_f(X_train, y_train, X_test, y_test):
    hidden_layers = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
    f1_scores = []
    for layer in hidden_layers:
        print("Training for arch: ",layer)
        model = MLPClassifier(hidden_layer_sizes=layer,activation='relu',solver='sgd',alpha=0,batch_size=32,learning_rate='invscaling',max_iter=100)
        model.fit(X_train, y_train)
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        print(f"\nClassification Report on Train Data (arch {layer}):")
        print(classification_report(y_train, y_train_pred, digits=4))
        print(f"\nClassification Report on Test Data (arch {layer}):")
        print(classification_report(y_test, y_test_pred, digits=4))

        avg_f1 = f1_score(y_test, y_test_pred, average='macro') 
        f1_scores.append(avg_f1)
    
    plt.figure()
    plt.plot([1,2,3,4],f1_scores, marker='o')
    plt.xlabel("Depth")
    plt.ylabel("F1 scores")
    plt.title("F1 scores v/s Depth")
    plt.legend()
    plt.grid(True)
    plt.show()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("train_data_path", type=str)
    parser.add_argument("test_data_path", type=str)
    parser.add_argument("output_folder_path", type=str)
    parser.add_argument("question_part", type=str, choices=["b", "c", "d", "e", "f"])
    args = parser.parse_args()

    X_train, y_train, X_test = dataloader(args.train_data_path, args.test_data_path)
    # y_test = load_test_labels(os.path.dirname(args.test_data_path))

    if args.question_part == 'b':
        # analysis_b(X_train,y_train,X_test,y_test)
        model = NeuralNet(32,2352,[100],43)
        model.train_network(X_train, y_train, epochs=100)  
        y_test_pred = model.predict(X_test)
        output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
        pd.DataFrame({"prediction": y_test_pred}).to_csv(output_file, index=False)

    if args.question_part == 'c':
        # analysis_c(X_train,y_train,X_test,y_test)
        model = NeuralNet(32,2352,[512, 256, 128, 64],43)
        model.train_network(X_train, y_train, epochs=100)  
        y_test_pred = model.predict(X_test)
        output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
        pd.DataFrame({"prediction": y_test_pred}).to_csv(output_file, index=False)

    if args.question_part == 'd':
        # analysis_d(X_train,y_train,X_test,y_test)
        model = NeuralNet(32,2352,[512, 256, 128, 64],43,decay=1)
        model.train_network(X_train, y_train, epochs=100)  
        y_test_pred = model.predict(X_test)
        output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
        pd.DataFrame({"prediction": y_test_pred}).to_csv(output_file, index=False)

    if args.question_part == 'e':
        # analysis_e(X_train,y_train,X_test,y_test)
        model = NeuralNet(32,2352,[512, 256, 128, 64],43,g=1,decay=1)
        model.train_network(X_train, y_train, epochs=100)  
        y_test_pred = model.predict(X_test)
        output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
        pd.DataFrame({"prediction": y_test_pred}).to_csv(output_file, index=False)

    if args.question_part == 'f':
        # analysis_f(X_train,y_train,X_test,y_test)
        model = MLPClassifier(hidden_layer_sizes=[512, 256, 128, 64],activation='relu',solver='sgd',alpha=0,batch_size=32,learning_rate='invscaling',max_iter=100)
        model.fit(X_train, y_train)
        y_test_pred = model.predict(X_test)
        output_file = os.path.join(args.output_folder_path, f"prediction_{args.question_part}.csv")
        pd.DataFrame({"prediction": y_test_pred}).to_csv(output_file, index=False)

if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
