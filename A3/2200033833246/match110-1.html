<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_0A0UU.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_YHRMJ.py<p><PRE>


import pandas as pd
import numpy as np
import math
import os
import sys
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import GridSearchCV
from copy import deepcopy
import warnings
import pickle
warnings.filterwarnings('ignore', category=FutureWarning) # Ignore numpy future warnings
warnings.filterwarnings('ignore', category=UserWarning) 
# --- Helper Functions ---

def calculate_entropy(y):
    """Calculates the entropy of a target variable Series."""
    counts = y.value_counts()
    probabilities = counts / len(y)
    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9)) # Added epsilon for log(0)
    return entropy

def  calculate_mutual_information(data, feature, target_col):
    """Calculates the mutual information (information gain) for a feature."""
    total_entropy = calculate_entropy(data[target_col])
    
    # Identify feature type
    if pd.api.types.is_numeric_dtype(data[feature]):
        # We have got continuous data...
        median_val = data[feature].median()
        left_split = data[data[feature] &lt;= median_val]
        right_split = data[data[feature] &gt; median_val]

        if len(left_split) == 0 or len(right_split) == 0:
            return 0, None # No split possible i.e no gain, because now total_entropy = conditional_entropy

        p_left = len(left_split) / len(data)
        p_right = len(right_split) / len(data)

        conditional_entropy = (p_left * calculate_entropy(left_split[target_col]) +
                               p_right * calculate_entropy(right_split[target_col]))
        
        info_gain = total_entropy - conditional_entropy
        return info_gain, median_val # Return gain and threshold
        
    else:
        # We have got categorical data...
        values = data[feature].unique() #getting all the unique categories
        weighted_entropy = 0
        for value in values:
            subset = data[data[feature] == value]
            if len(subset) &gt; 0:
                prob = len(subset) / len(data)
                weighted_entropy += prob * calculate_entropy(subset[target_col])
        
        info_gain = total_entropy - weighted_entropy
        return info_gain, None # Return gain, no threshold needed for multi-way split

# --- Node Class for Custom Tree ---
class Node:
    def __init__(self, feature=None, threshold=None, value=None, children=None, is_leaf=False, prediction=None, data_indices=None):
        self.feature = feature           # Feature used for splitting
        self.threshold = threshold       # Threshold for continuous feature split
        self.value = value             # Specific value for categorical split branch (for multi-way)
        self.children = children if children is not None else {} # Dictionary mapping split outcome to child node
        self.is_leaf = is_leaf           # Is this a leaf node?
        self.prediction = prediction       # Prediction if it's a leaf node
        self.data_indices = data_indices # Indices of data reaching this node (used for pruning later) - Optional but helpful

# --- Custom Decision Tree Implementation ---

class CustomDecisionTree:
    def __init__(self, max_depth=None, use_one_hot=False):
        self.max_depth = max_depth
        self.use_one_hot = use_one_hot # Flag to know if one-hot encoding logic applies internally
        self.root = None
        self.features = None
        self.target_col = None
        self.categorical_features = None # Needed for prediction logic in part 'a'
        self.feature_splits = {} # Stores splits for unseen value handling in part 'a'

    def _find_best_split(self, data):
        best_feature = None
        best_threshold = None
        max_info_gain = -1
        best_split_type = None # 'continuous' or 'categorical'

        for feature in self.features:
            if feature == self.target_col:
                continue

            # Handle continuous features (or one-hot encoded)
            if pd.api.types.is_numeric_dtype(data[feature]):
                if data[feature].nunique() &lt;= 1: # Skip if only one value
                    continue
                info_gain, threshold = calculate_mutual_information(data, feature, self.target_col)
                if info_gain &gt; max_info_gain:
                    max_info_gain = info_gain
                    best_feature = feature
                    best_threshold = threshold
                    best_split_type = 'continuous' 
            # Handle original categorical features (only if not use_one_hot)
            elif not self.use_one_hot:
                 if data[feature].nunique() &lt;= 1: # Skip if only one value
                     continue
                 info_gain, _ = calculate_mutual_information(data, feature, self.target_col)
                 if info_gain &gt; max_info_gain:
                    max_info_gain = info_gain
                    best_feature = feature
                    best_threshold = None # No threshold for multi-way split
                    best_split_type = 'categorical'
            elif not pd.api.types.is_numeric_dtype(data[feature]):
                 continue # Skip original categoricals if using one-hot

        return best_feature, best_threshold, max_info_gain, best_split_type


    def _build_tree_recursive(self, data, current_depth):
        
        # --- Stopping Criteria ---
        # 1. Max depth reached
        if self.max_depth is not None and current_depth &gt;= self.max_depth:
            prediction = data[self.target_col].mode()[0]
            return Node(is_leaf=True, prediction=prediction, data_indices=data.index.tolist())

        # 2. All samples belong to the same class (pure node)
        if data[self.target_col].nunique() == 1:
            prediction = data[self.target_col].iloc[0]
            return Node(is_leaf=True, prediction=prediction, data_indices=data.index.tolist())

        # 3. No data left (shouldn't happen with proper splits, but good practice)
        if len(data) == 0:
            pass


        # 4. No features left (or no feature provides information gain)
        best_feature, best_threshold, max_info_gain, best_split_type = self._find_best_split(data)
        if max_info_gain &lt;= 1e-9 or best_feature is None: # Use tolerance for float comparison
            prediction = data[self.target_col].mode()[0]
            return Node(is_leaf=True, prediction=prediction, data_indices=data.index.tolist())

        # --- Recursive Step ---
        node = Node(feature=best_feature, threshold=best_threshold, data_indices=data.index.tolist())
        children = {}

        if best_split_type == 'continuous':
            # Split continuous (or one-hot)
            left_indices = data[data[best_feature] &lt;= best_threshold].index
            right_indices = data[data[best_feature] &gt; best_threshold].index
            
            if len(left_indices) &gt; 0:
                children['&lt;='] = self._build_tree_recursive(data.loc[left_indices], current_depth + 1)
            else: # Handle empty branch -&gt; make leaf with current majority
                 children['&lt;='] = Node(is_leaf=True, prediction=data[self.target_col].mode()[0], data_indices=[])


            if len(right_indices) &gt; 0:
                children['&gt;'] = self._build_tree_recursive(data.loc[right_indices], current_depth + 1)
            else: # Handle empty branch
                 children['&gt;'] = Node(is_leaf=True, prediction=data[self.target_col].mode()[0], data_indices=[])
                 
            node.children = children

        elif best_split_type == 'categorical' and not self.use_one_hot:
            values = data[best_feature].unique()
            self.feature_splits[best_feature] = values 

            for value in values:
                subset_indices = data[data[best_feature] == value].index
                if len(subset_indices) &gt; 0:
                     children[value] = self._build_tree_recursive(data.loc[subset_indices], current_depth + 1)
                else:
                    # Predict majority of current node data if a category has no samples
                    children[value] = Node(is_leaf=True, prediction=data[self.target_col].mode()[0], data_indices=[])
            node.children = children
            node.value = None 
        return node

    def fit(self, data, target_col):
        self.target_col = target_col
        self.features = data.columns.tolist()
        if not self.use_one_hot:
            self.categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
        else:
             self.categorical_features = [] # All features are numeric after OHE
        self.root = self._build_tree_recursive(data, current_depth=0)

    def _predict_instance(self, instance, node):
        if node.is_leaf:
            return node.prediction

        feature = node.feature
        
        # Check if feature exists in instance (might happen with one-hot encoding differences)
        if feature not in instance:
            return 0

        instance_value = instance[feature]

        if node.threshold is not None: # Continuous split (or one-hot binary)
            if instance_value &lt;= node.threshold:
                child_node = node.children.get('&lt;=')
            else:
                child_node = node.children.get('&gt;')
            
            if child_node:
                 return self._predict_instance(instance, child_node)
            else:
                 return node.prediction if node.prediction is not None else 0 # Requires storing prediction at internal nodes

        else: # Categorical split (Part 'a')
            if instance_value not in node.children:
                return node.prediction 
            else:
                child_node = node.children[instance_value]
                return self._predict_instance(instance, child_node)


    def predict(self, data):
        predictions = []    
        for index, instance in data.iterrows():
            try:
                predictions.append(self._predict_instance(instance, self.root))
            except Exception as e:
                 # print(f"Error predicting instance {index}: {e}")
                 predictions.append(0) # Default prediction on error
        return np.array(predictions)
        
    def _update_predictions_recursive(self, node, data, target_col):
        """Helper to update node predictions after build (needed for unseen cat handling)."""
        if len(node.data_indices) &gt; 0:
             node_data = data.loc[node.data_indices]
             node.prediction = node_data[target_col].mode()[0]
        else:
             node.prediction = 0 # Default for empty nodes

        if not node.is_leaf:
            for child in node.children.values():
                self._update_predictions_recursive(child, data, target_col)

    def update_node_predictions(self, train_data):
         """ Call this after fit to ensure all nodes have predictions stored. """
         if self.root and self.target_col:
             self._update_predictions_recursive(self.root, train_data, self.target_col)


# --- Post-Pruning Functions ---

def get_non_leaf_nodes(node):
    """Returns a list of all non-leaf nodes in the tree."""
    nodes = []
    if node and not node.is_leaf:
        nodes.append(node)
        for child in node.children.values():
            nodes.extend(get_non_leaf_nodes(child))
    return nodes

def predict_tree(tree, data):
    """Predicts using a tree instance."""
    return tree.predict(data) # Use the tree's predict method

def calculate_accuracy(y_true, y_pred):
    """Calculates accuracy."""
    return accuracy_score(y_true, y_pred)

def count_nodes(node):
    """Counts the total number of nodes in the tree."""
    if not node:
        return 0
    count = 1
    if not node.is_leaf:
        for child in node.children.values():
            count += count_nodes(child)
    return count


# --- Modified post_prune_tree (Added comments on complexity) ---
def post_prune_tree(tree, X_train, y_train, X_valid, y_valid):
    """
    Performs post-pruning based on validation set accuracy.
    NOTE: This greedy approach can be computationally intensive due to repeated
          tree copying and predictions on the validation set in each iteration.
    """

    best_tree = deepcopy(tree) # Start with the original tree

    validation_data = X_valid.copy()
    validation_data[tree.target_col] = y_valid 

    best_accuracy = calculate_accuracy(y_valid, best_tree.predict(X_valid))

    history = []

    pruning_iteration = 0
    while True:
        pruning_iteration += 1
        # print(f"Pruning Iteration: {pruning_iteration}") # Optional debug print

        current_tree = deepcopy(best_tree) # Base for this iteration's candidates
        non_leaf_nodes = get_non_leaf_nodes(current_tree.root)

        if not non_leaf_nodes:
            # print("No non-leaf nodes left to prune.")
            break

        candidate_trees = []
        candidate_accuracies = []
        nodes_pruned_indices = [] 

        node_index = -1
        nodes_to_evaluate = []
        for node_obj in non_leaf_nodes:
             node_index += 1
             if node_obj.prediction is None:
                  print(f"Warning: Node being considered for pruning has no prediction!")
                  continue
             nodes_to_evaluate.append({'node': node_obj, 'original_index': node_index})


        # print(f"  Evaluating {len(nodes_to_evaluate)} potential prunes...") # Optional debug print

        for node_info in nodes_to_evaluate:
            node_to_prune = node_info['node']
            original_is_leaf = node_to_prune.is_leaf
            original_children = node_to_prune.children

            node_to_prune.is_leaf = True
            node_to_prune.children = {}

            current_accuracy = calculate_accuracy(y_valid, current_tree.predict(X_valid))
            nodes_pruned_indices.append(node_info['original_index'])
            candidate_accuracies.append(current_accuracy)

            node_to_prune.is_leaf = original_is_leaf
            node_to_prune.children = original_children


        if not candidate_accuracies: # No valid candidates were evaluated
             # print("No candidate accuracies generated.")
             break

        best_candidate_relative_idx = np.argmax(candidate_accuracies)
        best_candidate_accuracy = candidate_accuracies[best_candidate_relative_idx]
        best_node_original_idx = nodes_pruned_indices[best_candidate_relative_idx]

        # print(f"  Best candidate accuracy: {best_candidate_accuracy:.4f}, Current best: {best_accuracy:.4f}") # Optional debug print

        if best_candidate_accuracy &gt;= best_accuracy:
            node_to_permanently_prune = get_non_leaf_nodes(best_tree.root)[best_node_original_idx]
            node_to_permanently_prune.is_leaf = True
            if node_to_permanently_prune.prediction is None:
                print("ERROR: Node selected for final prune has no prediction!")
            node_to_permanently_prune.children = {}
            best_accuracy = best_candidate_accuracy 
            num_nodes = count_nodes(best_tree.root)
            # print(f"  Pruned. New best accuracy: {best_accuracy:.4f}, Nodes: {num_nodes}") # Optional debug print
            history.append({'nodes': num_nodes, 'val_acc': best_accuracy, 'tree': deepcopy(best_tree)})

        else:
            # print("No pruning improved validation accuracy. Stopping.")
            break 

    initial_nodes = count_nodes(tree.root)
    initial_val_acc = calculate_accuracy(y_valid, tree.predict(X_valid))
    if not history or history[0]['nodes'] != initial_nodes:
        history.insert(0, {'nodes': initial_nodes, 'val_acc': initial_val_acc, 'tree': deepcopy(tree)})

    final_best_tree = None
    if history:
       best_record = max(history, key=lambda x: x['val_acc'])
       max_val_acc = best_record['val_acc']
       best_records = [h for h in history if abs(h['val_acc'] - max_val_acc) &lt; 1e-9] # Check for floating point equality
       best_record_final = min(best_records, key=lambda x: x['nodes'])

       final_best_tree = best_record_final['tree']
       # print(f"Best accuracy achieved during pruning: {best_record_final['val_acc']:.4f} with {best_record_final['nodes']} nodes.") # Optional debug print
    else:
        final_best_tree = best_tree

    return final_best_tree, history
def get_part_b_models(depths, cache_folder, output_folder, train_path, valid_path, test_path):
    """Loads models from cache if available, otherwise generates them by running Part B."""
    trained_models = {}
    all_loaded = True
    print(f"\nAttempting to load Part B models from cache: {cache_folder}")
    os.makedirs(cache_folder, exist_ok=True) # Ensure cache folder exists

    for depth in depths:
        cache_path = os.path.join(cache_folder, f'model_b_depth_{depth}.pkl')
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    model = pickle.load(f)
                trained_models[depth] = model
                print(f"  Loaded model for depth {depth} from cache.")
            except Exception as e:
                print(f"  Error loading model for depth {depth} from cache: {e}. Will regenerate.")
                all_loaded = False
                break 
        else:
            print(f"  Cached model for depth {depth} not found. Will regenerate.")
            all_loaded = False
            break 

    if all_loaded and len(trained_models) == len(depths):
        print("Successfully loaded all required models from cache.")
        X_train, y_train, X_valid, y_valid, X_test, y_test, _, _, _, _ = \
            load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=True)
        return trained_models, X_train, y_train, X_valid, y_valid, X_test, y_test
    else:
        print("Could not load all models from cache. Running Part B to generate models...")
        trained_models, X_train, y_train, X_valid, y_valid, X_test, y_test = \
            run_part_b(train_path, valid_path, test_path, output_folder, cache_folder)
        return trained_models, X_train, y_train, X_valid, y_valid, X_test, y_test


# --- Data Loading and Preprocessing ---

def load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=False):
    """Loads data and performs necessary preprocessing."""
    train_df = pd.read_csv(train_path)
    valid_df = pd.read_csv(valid_path)
    test_df = pd.read_csv(test_path)

    target_col = 'income'
    train_df[target_col] = train_df[target_col].apply(lambda x: 1 if x.strip() == '&gt;50K' else 0)
    valid_df[target_col] = valid_df[target_col].apply(lambda x: 1 if x.strip() == '&gt;50K' else 0)
    if target_col in test_df.columns:
         test_df[target_col] = test_df[target_col].apply(lambda x: 1 if x.strip() == '&gt;50K' else 0)
         y_test = test_df[target_col]
    else:
         y_test = None

    X_train = train_df.drop(columns=[target_col])
    y_train = train_df[target_col]
    X_valid = valid_df.drop(columns=[target_col])
    y_valid = valid_df[target_col]
    X_test = test_df.drop(columns=[target_col], errors='ignore') 


    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()

    if use_one_hot:
<A NAME="0"></A><FONT color = #FF0000><A HREF="match110-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X_all = pd.concat([X_train, X_valid, X_test], keys=['train', 'valid', 'test'])
        X_all_encoded = pd.get_dummies(X_all, columns=categorical_features, dummy_na=False)

        X_train = X_all_encoded.loc['train']
        X_valid = X_all_encoded.loc['valid']
        X_test = X_all_encoded.loc['test']

        train_cols = X_train.columns
        X_valid = X_valid.reindex(columns=train_cols, fill_value=0)
</FONT>        X_test = X_test.reindex(columns=train_cols, fill_value=0)

    if not use_one_hot:
        train_data_full = train_df
        valid_data_full = valid_df
        test_data_full = test_df 
    else:
        train_data_full = X_train.copy()
        train_data_full[target_col] = y_train
        valid_data_full = X_valid.copy()
        valid_data_full[target_col] = y_valid
        test_data_full = X_test.copy()
        if y_test is not None:
             test_data_full[target_col] = y_test


    return X_train, y_train, X_valid, y_valid, X_test, y_test, train_data_full, valid_data_full, test_data_full, target_col


# --- Plotting Function ---
def plot_accuracies(depths, train_accs, test_accs, title, xlabel, output_folder, filename):
    plt.figure(figsize=(10, 6))
    plt.plot(depths, train_accs, marker='o', label='Train Accuracy')
    plt.plot(depths, test_accs, marker='x', label='Test Accuracy')
    plt.xlabel(xlabel)
    plt.ylabel('Accuracy')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.xticks(depths) 
    filepath = os.path.join(output_folder, filename)
    plt.savefig(filepath)
    print(f"Plot saved to {filepath}")
    plt.close()

def plot_pruning_accuracies(pruning_history, y_train, y_valid, y_test, X_train, X_valid, X_test, title, output_folder, filename):
    nodes_list = [h['nodes'] for h in pruning_history]
    val_accs = [h['val_acc'] for h in pruning_history]
    train_accs = []
    test_accs = []

    for h in pruning_history:
        tree = h['tree']
        train_pred = tree.predict(X_train)
        test_pred = tree.predict(X_test)
        train_accs.append(accuracy_score(y_train, train_pred))
        if y_test is not None:
             test_accs.append(accuracy_score(y_test, test_pred))
        else:
             test_accs.append(None) 

    sorted_indices = np.argsort(nodes_list)
    nodes_list = np.array(nodes_list)[sorted_indices]
    train_accs = np.array(train_accs)[sorted_indices]
    val_accs = np.array(val_accs)[sorted_indices]
    if y_test is not None:
        test_accs = np.array(test_accs)[sorted_indices]


    plt.figure(figsize=(12, 7))
    plt.plot(nodes_list, train_accs, marker='o', label='Train Accuracy')
    plt.plot(nodes_list, val_accs, marker='s', label='Validation Accuracy')
    if y_test is not None:
        plt.plot(nodes_list, test_accs, marker='x', label='Test Accuracy')

    plt.xlabel('Number of Nodes')
    plt.ylabel('Accuracy')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    if len(nodes_list) &gt; 15:
         plt.xticks(rotation=45, ha='right')
         
    else:
         plt.xticks(nodes_list)


    filepath = os.path.join(output_folder, filename)
    plt.savefig(filepath)
    print(f"Pruning plot saved to {filepath}")
    plt.close()


# --- Main Execution Logic ---

def run_part_a(train_path, valid_path, test_path, output_folder):
    print("\n--- Running Part (a): Custom Decision Tree (Mixed Attributes) ---")
    X_train, y_train, X_valid, y_valid, X_test, y_test, \
    train_data_full, valid_data_full, test_data_full, target_col = \
        load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=False)

<A NAME="5"></A><FONT color = #FF0000><A HREF="match110-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    max_depths = [5, 10, 15, 20]
    train_accuracies = []
    test_accuracies = []
    best_test_acc = -1
    best_depth = -1
    best_model_preds = None

    print("Training and evaluating models for different depths...")
</FONT>    for depth in max_depths:
        print(f"Training model with max_depth = {depth}")
        tree = CustomDecisionTree(max_depth=depth, use_one_hot=False)
        tree.fit(train_data_full, target_col)
        tree.update_node_predictions(train_data_full) 

        # Evaluate on Training Set
        train_pred = tree.predict(X_train)
        train_acc = accuracy_score(y_train, train_pred)
        train_accuracies.append(train_acc)
        print(f"  Train Accuracy: {train_acc:.4f}")

        # Evaluate on Test Set
        test_pred = tree.predict(X_test)
        if y_test is not None:
            test_acc = accuracy_score(y_test, test_pred)
            test_accuracies.append(test_acc)
            print(f"  Test Accuracy: {test_acc:.4f}")
            if test_acc &gt; best_test_acc:
                 best_test_acc = test_acc
                 best_depth = depth
                 best_model_preds = test_pred
        else:
            test_accuracies.append(None)
            print("  Test Accuracy: Not available (no target column)")
            best_model_preds = test_pred 
    # Plotting
    if y_test is not None:
        plot_accuracies(max_depths, train_accuracies, test_accuracies,
                        'Part (a): Accuracy vs. Max Depth (Custom Tree)',
                        'Maximum Depth', output_folder, 'part_a_accuracy_plot.png')
    else:
         print("Skipping plot as test accuracies are not available.")

    # Save predictions
    pred_df = pd.DataFrame({'prediction': best_model_preds if best_model_preds is not None else test_pred}) # Save last preds if no best
    output_path = os.path.join(output_folder, 'prediction_a.csv')
    pred_df.to_csv(output_path, index=False)
    print(f"Test predictions saved to {output_path}")


def run_part_b(train_path, valid_path, test_path, output_folder, cache_folder = 'cache'):
    print("\n--- Running Part (b): Custom Decision Tree (One-Hot Encoding) ---")
    X_train, y_train, X_valid, y_valid, X_test, y_test, \
    train_data_full, valid_data_full, test_data_full, target_col = \
        load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=True)

    max_depths = [25, 35, 45, 55] 
    train_accuracies = []
    test_accuracies = []
    best_test_acc = -1
    best_depth = -1
    best_model_preds = None
    trained_models = {} 
    print("Training and evaluating models for different depths (with One-Hot Encoding)...")
    for depth in max_depths:
        print(f"Training model with max_depth = {depth}")
        tree = CustomDecisionTree(max_depth=depth, use_one_hot=True)
        train_data_for_fit = X_train.copy()
        train_data_for_fit[target_col] = y_train
        
        tree.fit(train_data_for_fit, target_col)
        tree.update_node_predictions(train_data_for_fit)
        trained_models[depth] = tree 


        # Evaluate on Training Set
        train_pred = tree.predict(X_train) # Predict using OHE features
        train_acc = accuracy_score(y_train, train_pred)
        train_accuracies.append(train_acc)
        print(f"  Train Accuracy: {train_acc:.4f}")

        # Evaluate on Test Set
        test_pred = tree.predict(X_test) # Predict using OHE features
        if y_test is not None:
            test_acc = accuracy_score(y_test, test_pred)
            test_accuracies.append(test_acc)
            print(f"  Test Accuracy: {test_acc:.4f}")
            if test_acc &gt; best_test_acc:
                 best_test_acc = test_acc
                 best_depth = depth
                 best_model_preds = test_pred
        else:
            test_accuracies.append(None)
            print("  Test Accuracy: Not available (no target column)")
            best_model_preds = test_pred # Save predictions from last model

    # Plotting
    if y_test is not None:
        plot_accuracies(max_depths, train_accuracies, test_accuracies,
                        'Part (b): Accuracy vs. Max Depth (OHE Custom Tree)',
                        'Maximum Depth', output_folder, 'part_b_accuracy_plot.png')
    else:
         print("Skipping plot as test accuracies are not available.")

    # Save predictions
    pred_df = pd.DataFrame({'prediction': best_model_preds if best_model_preds is not None else test_pred}) # Save last preds if no best
    output_path = os.path.join(output_folder, 'prediction_b.csv')
    pred_df.to_csv(output_path, index=False)
    print(f"Test predictions saved to {output_path}")

    print(f"Saving trained models to cache folder: {cache_folder}")
    os.makedirs(cache_folder, exist_ok=True) # Ensure cache folder exists
    for depth, model in trained_models.items():
        cache_path = os.path.join(cache_folder, f'model_b_depth_{depth}.pkl')
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(model, f)
            print(f"  Saved model for depth {depth} to {cache_path}")
        except Exception as e:
            print(f"  Error saving model for depth {depth}: {e}")

    # Return models for part c
    return trained_models, X_train, y_train, X_valid, y_valid, X_test, y_test


# --- Modified post_prune_tree (Added comments on complexity) ---
def post_prune_tree(tree, X_train, y_train, X_valid, y_valid):
    """
    Performs post-pruning based on validation set accuracy.
    NOTE: This greedy approach can be computationally intensive due to repeated
          tree copying and predictions on the validation set in each iteration.
    """

    best_tree = deepcopy(tree) 

    validation_data = X_valid.copy()
    validation_data[tree.target_col] = y_valid 

    best_accuracy = calculate_accuracy(y_valid, best_tree.predict(X_valid))

    history = [] 

    pruning_iteration = 0
    while True:
        pruning_iteration += 1
        print(f"Pruning Iteration: {pruning_iteration}", end="\r")

        current_tree = deepcopy(best_tree)
        non_leaf_nodes = get_non_leaf_nodes(current_tree.root)

        if not non_leaf_nodes:
            # print("No non-leaf nodes left to prune.")
            break

        candidate_trees = []
        candidate_accuracies = []
        nodes_pruned_indices = [] 

        node_index = -1
        nodes_to_evaluate = []
        for node_obj in non_leaf_nodes:
             node_index += 1
             if node_obj.prediction is None:
                  print(f"Warning: Node being considered for pruning has no prediction!")
                  continue
             nodes_to_evaluate.append({'node': node_obj, 'original_index': node_index})


        # print(f"  Evaluating {len(nodes_to_evaluate)} potential prunes...") # Optional debug print

        for node_info in nodes_to_evaluate:
            node_to_prune = node_info['node']
            original_is_leaf = node_to_prune.is_leaf
            original_children = node_to_prune.children

            node_to_prune.is_leaf = True
            node_to_prune.children = {}

            current_accuracy = calculate_accuracy(y_valid, current_tree.predict(X_valid))
            nodes_pruned_indices.append(node_info['original_index'])
            candidate_accuracies.append(current_accuracy)

            node_to_prune.is_leaf = original_is_leaf
            node_to_prune.children = original_children


        if not candidate_accuracies: # No valid candidates were evaluated
             # print("No candidate accuracies generated.")
             break

        best_candidate_relative_idx = np.argmax(candidate_accuracies)
        best_candidate_accuracy = candidate_accuracies[best_candidate_relative_idx]
        best_node_original_idx = nodes_pruned_indices[best_candidate_relative_idx]

        # print(f"  Best candidate accuracy: {best_candidate_accuracy:.4f}, Current best: {best_accuracy:.4f}") # Optional debug print

        if best_candidate_accuracy &gt;= best_accuracy:
            
            # --- Re-evaluate the best prune action and create the new best_tree ---
            node_to_permanently_prune = get_non_leaf_nodes(best_tree.root)[best_node_original_idx]
            node_to_permanently_prune.is_leaf = True
            # Ensure prediction is set (should be from build/update)
            if node_to_permanently_prune.prediction is None:
                print("ERROR: Node selected for final prune has no prediction!")
            node_to_permanently_prune.children = {}
            # Now 'best_tree' *is* the result of the best prune action for this iteration.
            best_accuracy = best_candidate_accuracy # Update the best accuracy
            num_nodes = count_nodes(best_tree.root)
            # print(f"  Pruned. New best accuracy: {best_accuracy:.4f}, Nodes: {num_nodes}") # Optional debug print

            history.append({'nodes': num_nodes, 'val_acc': best_accuracy, 'tree': deepcopy(best_tree)})

        else:
            # print("No pruning improved validation accuracy. Stopping.")
            break # Stop pruning

    initial_nodes = count_nodes(tree.root)
    initial_val_acc = calculate_accuracy(y_valid, tree.predict(X_valid))
    if not history or history[0]['nodes'] != initial_nodes:
        history.insert(0, {'nodes': initial_nodes, 'val_acc': initial_val_acc, 'tree': deepcopy(tree)})

    final_best_tree = None
    if history:
       best_record = max(history, key=lambda x: x['val_acc'])
       max_val_acc = best_record['val_acc']
       best_records = [h for h in history if abs(h['val_acc'] - max_val_acc) &lt; 1e-9] # Check for floating point equality
       best_record_final = min(best_records, key=lambda x: x['nodes'])

       final_best_tree = best_record_final['tree']
       # print(f"Best accuracy achieved during pruning: {best_record_final['val_acc']:.4f} with {best_record_final['nodes']} nodes.") # Optional debug print
    else:
        final_best_tree = best_tree 


    return final_best_tree, history 


def run_part_c(train_path, valid_path, test_path, output_folder, cache_folder="cache"): 
    print("\n--- Running Part (c): Custom Decision Tree Post-Pruning ---")

    part_b_depths = [25, 35, 45, 55]
    trained_models, X_train, y_train, X_valid, y_valid, X_test, y_test = \
        get_part_b_models(part_b_depths, cache_folder, output_folder, train_path, valid_path, test_path)

    if not trained_models:
         print("Error: Failed to load or generate models from Part (b). Cannot run Part (c).")
         output_path = os.path.join(output_folder, 'prediction_c.csv')
         pd.DataFrame({'prediction': []}).to_csv(output_path, index=False)
         return


    all_pruning_histories = {}
    best_pruned_tree = None
    best_pruned_val_acc = -1
    best_pruned_test_acc = -1
    initial_depth_of_best = -1

    test_accuracy_available = y_test is not None

    print("\nPost-pruning trees from Part (b)...")
    for depth, initial_tree in trained_models.items():
        print(f"Pruning tree initially trained with max_depth = {depth}")
        if not hasattr(initial_tree, 'target_col') or initial_tree.target_col is None:
             print(f"Warning: Tree for depth {depth} seems incomplete (missing target_col). Trying to set it.")
             if y_train is not None:
                 initial_tree.target_col = y_train.name
             else:
                  print("Error: Cannot determine target column name. Skipping pruning for this tree.")
                  continue

        pruned_tree, history = post_prune_tree(initial_tree, X_train, y_train, X_valid, y_valid)
        all_pruning_histories[depth] = history

        if pruned_tree is None:
             print(f"  Pruning failed for depth {depth}.")
             continue

        final_val_pred = pruned_tree.predict(X_valid)
        final_val_acc = accuracy_score(y_valid, final_val_pred)

        final_test_pred = pruned_tree.predict(X_test)
        current_test_acc = -1 
        if test_accuracy_available:
            current_test_acc = accuracy_score(y_test, final_test_pred)
            print(f"  Final Pruned Tree (from depth {depth}): Validation Acc = {final_val_acc:.4f}, Test Acc = {current_test_acc:.4f}, Nodes = {count_nodes(pruned_tree.root)}")

            if final_val_acc &gt; best_pruned_val_acc:
                 best_pruned_val_acc = final_val_acc
                 best_pruned_test_acc = current_test_acc
                 best_pruned_tree = pruned_tree
                 initial_depth_of_best = depth
            elif abs(final_val_acc - best_pruned_val_acc) &lt; 1e-9:
                 if best_pruned_tree is None: # Should not happen if accuracy &gt;= -1
                      best_pruned_val_acc = final_val_acc
                      best_pruned_test_acc = current_test_acc
                      best_pruned_tree = pruned_tree
                      initial_depth_of_best = depth
                 # Prefer higher test accuracy if val acc is tied
                 elif test_accuracy_available and current_test_acc &gt; best_pruned_test_acc:
                       best_pruned_test_acc = current_test_acc
                       best_pruned_tree = pruned_tree
                       initial_depth_of_best = depth
                 # If test acc also tied or unavailable, prefer smaller tree
                 elif (not test_accuracy_available or abs(current_test_acc - best_pruned_test_acc) &lt; 1e-9) and \
                      count_nodes(pruned_tree.root) &lt; count_nodes(best_pruned_tree.root):
                       best_pruned_tree = pruned_tree
                       initial_depth_of_best = depth

        else:
             print(f"  Final Pruned Tree (from depth {depth}): Validation Acc = {final_val_acc:.4f}, Test Acc = N/A, Nodes = {count_nodes(pruned_tree.root)}")
             # Track best based on validation acc anyway, tie-break with nodes
             if final_val_acc &gt; best_pruned_val_acc:
                 best_pruned_val_acc = final_val_acc
                 best_pruned_test_acc = -1 # Mark as unavailable
                 best_pruned_tree = pruned_tree
                 initial_depth_of_best = depth
             elif abs(final_val_acc - best_pruned_val_acc) &lt; 1e-9:
                  if best_pruned_tree is None or count_nodes(pruned_tree.root) &lt; count_nodes(best_pruned_tree.root):
                       best_pruned_tree = pruned_tree
                       initial_depth_of_best = depth


        # Plot pruning history for this tree
        plot_title = f'Part (c): Pruning Accuracies vs. Nodes (Initial Depth {depth})'
        plot_filename = f'part_c_pruning_plot_depth_{depth}.png'
        plot_pruning_accuracies(history, y_train, y_valid, y_test, X_train, X_valid, X_test,
                                plot_title, output_folder, plot_filename)


    print("\nPart (c) Results Summary:")
    if best_pruned_tree:
        print(f"Best overall pruned tree originated from initial max_depth = {initial_depth_of_best}")
        print(f"Best Validation Accuracy (Pruned): {best_pruned_val_acc:.4f}")
        if test_accuracy_available:
             print(f"Corresponding Test Accuracy (Pruned): {best_pruned_test_acc:.4f}")
             print(f"Number of nodes in best pruned tree: {count_nodes(best_pruned_tree.root)}")
        else:
             print("Corresponding Test Accuracy (Pruned): Not calculated (no labels)")
             print(f"Number of nodes in best pruned tree: {count_nodes(best_pruned_tree.root)}")


        # Save predictions from the best pruned tree
        best_preds = best_pruned_tree.predict(X_test)
        pred_df = pd.DataFrame({'prediction': best_preds})
        output_path = os.path.join(output_folder, 'prediction_c.csv')
        pred_df.to_csv(output_path, index=False)
        print(f"Test predictions from best pruned tree saved to {output_path}")
    else:
        print("No pruned tree was selected as best.")
        output_path = os.path.join(output_folder, 'prediction_c.csv')
        pd.DataFrame({'prediction': []}).to_csv(output_path, index=False) # Empty file
        print(f"No best pruned tree found. Empty prediction file saved to {output_path}")


def run_part_d(train_path, valid_path, test_path, output_folder):
    print("\n--- Running Part (d): Scikit-learn Decision Tree ---")
    X_train, y_train, X_valid, y_valid, X_test, y_test, _, _, _, _ = \
        load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=True) # Use OHE for sklearn

    test_accuracy_available = y_test is not None
    
    # --- Part (d) (i): Varying Max Depth ---
<A NAME="1"></A><FONT color = #00FF00><A HREF="match110-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    print("\nPart (d)(i): Varying max_depth...")
    depths = [25, 35, 45, 55]
    train_accs_depth = []
    valid_accs_depth = []
    test_accs_depth = []
    best_val_acc_depth = -1
    best_depth_sklearn = -1
    best_model_depth = None
</FONT>
    for depth in depths:
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match110-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
        clf.fit(X_train, y_train)

        train_pred = clf.predict(X_train)
        valid_pred = clf.predict(X_valid)
        
        train_acc = accuracy_score(y_train, train_pred)
        valid_acc = accuracy_score(y_valid, valid_pred)
</FONT>        train_accs_depth.append(train_acc)
        valid_accs_depth.append(valid_acc)
        
        print(f"Depth={depth}: Train Acc={train_acc:.4f}, Valid Acc={valid_acc:.4f}")

        if valid_acc &gt; best_val_acc_depth:
             best_val_acc_depth = valid_acc
             best_depth_sklearn = depth
             best_model_depth = clf # Store the best model based on validation

        if test_accuracy_available:
            test_pred = clf.predict(X_test)
            test_acc = accuracy_score(y_test, test_pred)
            test_accs_depth.append(test_acc)
            print(f"         Test Acc={test_acc:.4f}")
        else:
            test_accs_depth.append(None)


    print(f"\nBest max_depth based on validation accuracy: {best_depth_sklearn} (Validation Acc: {best_val_acc_depth:.4f})")
    
    # Plotting for max_depth
    plt.figure(figsize=(10, 6))
    plt.plot(depths, train_accs_depth, marker='o', label='Train Accuracy')
    plt.plot(depths, valid_accs_depth, marker='s', label='Validation Accuracy')
    if test_accuracy_available:
        plt.plot(depths, test_accs_depth, marker='x', label='Test Accuracy')
    plt.xlabel('Maximum Depth')
    plt.ylabel('Accuracy')
    plt.title('Part (d)(i): Sklearn Accuracy vs. Max Depth')
    plt.legend()
    plt.grid(True)
    plt.xticks(depths)
    filepath = os.path.join(output_folder, 'part_d_i_accuracy_plot.png')
    plt.savefig(filepath)
    print(f"Plot saved to {filepath}")
    plt.close()


    # --- Part (d) (ii): Varying ccp_alpha (Pruning) ---
    print("\nPart (d)(ii): Varying ccp_alpha...")
<A NAME="2"></A><FONT color = #0000FF><A HREF="match110-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ccp_alphas = [0.001, 0.01, 0.1, 0.2]
    train_accs_ccp = []
    valid_accs_ccp = []
    test_accs_ccp = []
    best_val_acc_ccp = -1
    best_ccp_alpha = -1
    best_model_ccp = None
</FONT>
    for alpha in ccp_alphas:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match110-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha, random_state=42)
        clf.fit(X_train, y_train)

        train_pred = clf.predict(X_train)
        valid_pred = clf.predict(X_valid)

        train_acc = accuracy_score(y_train, train_pred)
        valid_acc = accuracy_score(y_valid, valid_pred)
</FONT>        train_accs_ccp.append(train_acc)
        valid_accs_ccp.append(valid_acc)

        print(f"ccp_alpha={alpha}: Train Acc={train_acc:.4f}, Valid Acc={valid_acc:.4f}, Nodes={clf.tree_.node_count}")

        if valid_acc &gt; best_val_acc_ccp:
            best_val_acc_ccp = valid_acc
            best_ccp_alpha = alpha
            best_model_ccp = clf # Store best model

        if test_accuracy_available:
            test_pred = clf.predict(X_test)
            test_acc = accuracy_score(y_test, test_pred)
            test_accs_ccp.append(test_acc)
            print(f"           Test Acc={test_acc:.4f}")
        else:
            test_accs_ccp.append(None)

    print(f"\nBest ccp_alpha based on validation accuracy: {best_ccp_alpha} (Validation Acc: {best_val_acc_ccp:.4f})")

    # Plotting for ccp_alpha
    plt.figure(figsize=(10, 6))
    x_axis_labels = [str(a) for a in ccp_alphas] 
    x_axis_pos = range(len(ccp_alphas))

    plt.plot(x_axis_pos, train_accs_ccp, marker='o', label='Train Accuracy')
    plt.plot(x_axis_pos, valid_accs_ccp, marker='s', label='Validation Accuracy')
    if test_accuracy_available:
        plt.plot(x_axis_pos, test_accs_ccp, marker='x', label='Test Accuracy')
    
    plt.xlabel('ccp_alpha')
    plt.ylabel('Accuracy')
    plt.title('Part (d)(ii): Sklearn Accuracy vs. ccp_alpha')
    plt.xticks(x_axis_pos, x_axis_labels) # Set labels
    plt.legend()
    plt.grid(True)
    filepath = os.path.join(output_folder, 'part_d_ii_accuracy_plot.png')
    plt.savefig(filepath)
    print(f"Plot saved to {filepath}")
    plt.close()


    # --- Comparison and Reporting ---
    print("\nPart (d) Comparison & Observations:")
    print(f"- Best model from varying max_depth (depth={best_depth_sklearn}): Valid Acc={best_val_acc_depth:.4f}")
    print(f"- Best model from varying ccp_alpha (alpha={best_ccp_alpha}): Valid Acc={best_val_acc_ccp:.4f}")

    # Choosing the overall best sklearn model based on validation accuracy
    if best_val_acc_depth &gt;= best_val_acc_ccp:
        print("- Model with tuned max_depth performed slightly better or equal on validation set.")
        final_sklearn_model = best_model_depth
        best_sklearn_val_acc = best_val_acc_depth
    else:
        print("- Model with tuned ccp_alpha performed better on validation set.")
        final_sklearn_model = best_model_ccp
        best_sklearn_val_acc = best_val_acc_ccp
        
    if final_sklearn_model:
        best_sklearn_preds = final_sklearn_model.predict(X_test)
        pred_df = pd.DataFrame({'prediction': best_sklearn_preds})
        output_path = os.path.join(output_folder, 'prediction_d.csv')
        pred_df.to_csv(output_path, index=False)
        print(f"Test predictions from best sklearn tree saved to {output_path}")
    else:
        print("No best sklearn model found.")
        output_path = os.path.join(output_folder, 'prediction_d.csv')
        pd.DataFrame({'prediction': []}).to_csv(output_path, index=False) # Empty file
        print(f"No best model found. Empty prediction file saved to {output_path}")

    return best_sklearn_val_acc 


def run_part_e(train_path, valid_path, test_path, output_folder):
    print("\n--- Running Part (e): Scikit-learn Random Forest ---")
    X_train, y_train, X_valid, y_valid, X_test, y_test, _, _, _, _ = \
        load_and_preprocess_data(train_path, valid_path, test_path, use_one_hot=True) # Use OHE

    test_accuracy_available = y_test is not None

    # Define parameter grid for Grid Search
    param_grid = {
        'n_estimators': [50, 150, 250, 350],
        'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0], # Range 0.1 to 1.0 step 0.2 + 1.0
        'min_samples_split': [2, 4, 6, 8, 10]
    }

    
    rf = RandomForestClassifier(criterion='entropy', random_state=42, oob_score=True, n_jobs=-1) # Use all cores

    print("Performing Grid Search with OOB score...")
     
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, 
                               scoring='accuracy', verbose=1, n_jobs=-1) 
                               
    grid_search.fit(X_train, y_train) 

    print("\nGrid Search Completed.")
    print(f"Best Parameters found: {grid_search.best_params_}")
    
    # Get the best model from grid search
    best_rf = grid_search.best_estimator_
    
    # --- Evaluate the Best Random Forest Model ---
    print("\nEvaluating the best Random Forest model:")
    
    # Train Accuracy
    train_pred_rf = best_rf.predict(X_train)
    train_acc_rf = accuracy_score(y_train, train_pred_rf)
    print(f"  Train Accuracy: {train_acc_rf:.4f}")
    
    oob_acc_rf = best_rf.oob_score_ # Access the OOB score computed during fit
    print(f"  Out-of-Bag (OOB) Accuracy: {oob_acc_rf:.4f}")

    # Validation Accuracy
    valid_pred_rf = best_rf.predict(X_valid)
    valid_acc_rf = accuracy_score(y_valid, valid_pred_rf)
    print(f"  Validation Accuracy: {valid_acc_rf:.4f}")

    # Test Accuracy
    test_pred_rf = best_rf.predict(X_test)
    if test_accuracy_available:
        test_acc_rf = accuracy_score(y_test, test_pred_rf)
        print(f"  Test Accuracy: {test_acc_rf:.4f}")
    else:
        test_acc_rf = None
        print("  Test Accuracy: Not available (no target column)")

       # Save predictions
    pred_df = pd.DataFrame({'prediction': test_pred_rf})
    output_path = os.path.join(output_folder, 'prediction_e.csv')
    pred_df.to_csv(output_path, index=False)
    print(f"Test predictions from best Random Forest model saved to {output_path}")



if __name__ == "__main__":
    if len(sys.argv) != 6:
        print("Usage: python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        sys.exit(1)

    train_path = sys.argv[1]
    valid_path = sys.argv[2]
    test_path = sys.argv[3]
    output_folder = sys.argv[4]
    question_part = sys.argv[5].lower()

    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    if question_part == 'a':
        run_part_a(train_path, valid_path, test_path, output_folder)
    elif question_part == 'b':
        run_part_b(train_path, valid_path, test_path, output_folder)
    elif question_part == 'c':
        run_part_c(train_path, valid_path, test_path, output_folder)
    elif question_part == 'd':
        run_part_d(train_path, valid_path, test_path, output_folder)
    elif question_part == 'e':
        run_part_e(train_path, valid_path, test_path, output_folder)
    else:
        print(f"Error: Invalid question part '{question_part}'. Choose from 'a', 'b', 'c', 'd', 'e'.")
        sys.exit(1)

    print("\nScript finished.")



import numpy as np
import pandas as pd
import time
import os
import sys
import argparse
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, classification_report, f1_score, accuracy_score
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
from PIL import Image
import glob


def sigmoid(z):
    z_clipped = np.clip(z, -500, 500)
    return 1.0 / (1.0 + np.exp(-z_clipped))

def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return np.where(z &gt; 0, 1, 0)

def softmax(z):
    z_shifted = z - np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z_shifted)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

class NeuralNetwork:
    def __init__(self, n_features, hidden_layer_sizes, n_classes, activation='sigmoid'):
        self.n_features = n_features
        self.hidden_layer_sizes = hidden_layer_sizes
        self.n_classes = n_classes
        self.n_layers = len(hidden_layer_sizes) + 1
        self.activation_name = activation.lower()

        if self.activation_name == 'sigmoid':
            self.activation = sigmoid
            self.activation_derivative = sigmoid_derivative
        elif self.activation_name == 'relu':
            self.activation = relu
            self.activation_derivative = relu_derivative
        else:
            raise ValueError("Unsupported activation function. Choose 'sigmoid' or 'relu'.")

        self.weights = []
        self.biases = []
        layer_input_size = n_features
        for layer_size in hidden_layer_sizes:
            limit = np.sqrt(6 / (layer_input_size + layer_size))
            W = np.random.uniform(-limit, limit, (layer_input_size, layer_size))
            b = np.zeros((1, layer_size))
            self.weights.append(W)
            self.biases.append(b)
            layer_input_size = layer_size
        limit = np.sqrt(6 / (layer_input_size + n_classes))
        W_out = np.random.uniform(-limit, limit, (layer_input_size, n_classes))
        b_out = np.zeros((1, n_classes))
        self.weights.append(W_out)
        self.biases.append(b_out)

    def _forward(self, X):
        activations = [X]
        net_inputs = []
        A = X
        for i in range(self.n_layers - 1):
            W = self.weights[i]
            b = self.biases[i]
            Z = np.dot(A, W) + b
            net_inputs.append(Z)
            A = self.activation(Z)
            activations.append(A)
        W_out = self.weights[-1]
        b_out = self.biases[-1]
        Z_out = np.dot(A, W_out) + b_out
        net_inputs.append(Z_out)
        output_probs = softmax(Z_out)
        activations.append(output_probs)
        return output_probs, activations, net_inputs

    def _one_hot(self, y, num_classes):
        return np.eye(num_classes)[y]

    def _compute_loss(self, y_true_one_hot, y_pred_probs):
        m = y_true_one_hot.shape[0]
        epsilon = 1e-9
        log_likelihood = -np.sum(y_true_one_hot * np.log(y_pred_probs + epsilon), axis=1) 
        loss = np.sum(log_likelihood) / m
        return loss

    def _backward(self, y_batch_one_hot, activations, net_inputs):
        m = y_batch_one_hot.shape[0]
        grad_weights = [None] * self.n_layers
        grad_biases = [None] * self.n_layers
        delta_L = activations[-1] - y_batch_one_hot
        grad_weights[-1] = np.dot(activations[-2].T, delta_L) / m
        grad_biases[-1] = np.sum(delta_L, axis=0, keepdims=True) / m
        delta_prev = delta_L
        for l in range(self.n_layers - 2, -1, -1):
            W_next = self.weights[l+1]
            net_input_l = net_inputs[l]
            activation_deriv_l = self.activation_derivative(net_input_l)
            delta_l = np.dot(delta_prev, W_next.T) * activation_deriv_l
            grad_weights[l] = np.dot(activations[l].T, delta_l) / m
            grad_biases[l] = np.sum(delta_l, axis=0, keepdims=True) / m
            delta_prev = delta_l
        return grad_weights, grad_biases

    def fit(self, X_train, y_train, batch_size, epochs, learning_rate, adaptive_lr=False, eta0=0.01, validation_split=0.1, patience=10):
        n_samples = X_train.shape[0]
        n_classes_actual = len(np.unique(y_train)) 
        if n_classes_actual != self.n_classes:
            print(f"Warning: Number of classes in data ({n_classes_actual}) doesn't match network output units ({self.n_classes}). Adjusting network or check data.")
            
        y_train_one_hot = self._one_hot(y_train, self.n_classes) 

        if validation_split &gt; 0 and n_samples &gt; 1/validation_split : 
            try:
                X_train_, X_val, y_train_, y_val = train_test_split(
                    X_train, y_train_one_hot, test_size=validation_split, random_state=42, stratify=y_train
                )
                print(f"Using {X_val.shape[0]} samples for validation.")
            except ValueError as e: 
                 print(f"Warning: Could not stratify validation split ({e}). Splitting without stratification.")
                 X_train_, X_val, y_train_, y_val = train_test_split(
                     X_train, y_train_one_hot, test_size=validation_split, random_state=42
                 )
                 print(f"Using {X_val.shape[0]} samples for validation.")

        else:
            X_train_, y_train_ = X_train, y_train_one_hot
            X_val, y_val = None, None
            print("Validation split not used or not possible.")


        history = {'loss': [], 'val_loss': []}
        best_val_loss = float('inf')
        epochs_no_improve = 0

        print(f"Starting training with: Batch Size={batch_size}, Max Epochs={epochs}, Initial LR={learning_rate}, Adaptive LR={adaptive_lr}, Activation={self.activation_name}")

        start_time = time.time()
        for epoch in range(1, epochs + 1):
            epoch_start_time = time.time()
            epoch_loss = 0.0
            current_lr = learning_rate
            if adaptive_lr:
                current_lr = eta0 / np.sqrt(epoch)

            permutation = np.random.permutation(X_train_.shape[0])
            X_shuffled = X_train_[permutation]
            y_shuffled = y_train_[permutation]

            for i in range(0, X_train_.shape[0], batch_size):
                X_batch = X_shuffled[i : i + batch_size]
                y_batch = y_shuffled[i : i + batch_size]
                output_probs, activations, net_inputs = self._forward(X_batch)
                batch_loss = self._compute_loss(y_batch, output_probs)
                epoch_loss += batch_loss * X_batch.shape[0]
                grad_weights, grad_biases = self._backward(y_batch, activations, net_inputs)
                for l in range(self.n_layers):
                    self.weights[l] -= current_lr * grad_weights[l]
                    self.biases[l] -= current_lr * grad_biases[l]

            avg_epoch_loss = epoch_loss / X_train_.shape[0]
            history['loss'].append(avg_epoch_loss)

            if X_val is not None:
                val_probs, _, _ = self._forward(X_val)
                val_loss = self._compute_loss(y_val, val_probs)
                history['val_loss'].append(val_loss)
                epoch_duration = time.time() - epoch_start_time
                print(f"Epoch {epoch}/{epochs} - Loss: {avg_epoch_loss:.4f} - Val Loss: {val_loss:.4f} - LR: {current_lr:.6f} - Time: {epoch_duration:.2f}s", end="")

                if val_loss &lt; best_val_loss:
                    best_val_loss = val_loss
                    epochs_no_improve = 0
                    print(" (New best validation loss)")
                else:
                    epochs_no_improve += 1
                    print(f" (No improvement for {epochs_no_improve} epochs)")
                    if epochs_no_improve &gt;= patience:
                        print(f"\nEarly stopping triggered after {epoch} epochs.")
                        break
            else:
                 epoch_duration = time.time() - epoch_start_time
                 print(f"Epoch {epoch}/{epochs} - Loss: {avg_epoch_loss:.4f} - LR: {current_lr:.6f} - Time: {epoch_duration:.2f}s")

        total_time = time.time() - start_time
        print(f"\nTraining finished in {total_time:.2f} seconds.")
        return history

    def predict_proba(self, X):
        output_probs, _, _ = self._forward(X)
        return output_probs

    def predict(self, X):
        probabilities = self.predict_proba(X)
        return np.argmax(probabilities, axis=1)


# --- Data Loading and Preprocessing ---

def load_image(image_path, target_size=(28, 28)):
    """Loads, resizes, and preprocesses a single image."""
    try:
        img = Image.open(image_path).convert('RGB')
        img = img.resize(target_size)
        img_array = np.array(img, dtype=np.float32) / 255.0 # Normalize here
        return img_array.flatten() # Flatten to 1D array (28*28*3 = 2352)
    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return None

def load_and_preprocess_data(train_folder_path, test_folder_path):
    """Loads and preprocesses data from image folders."""
    # --- Load Training Data ---
    print(f"Loading training data from: {train_folder_path}")
    X_train_list = []
    y_train_list = []
    train_classes = sorted([d for d in os.listdir(train_folder_path) if os.path.isdir(os.path.join(train_folder_path, d))])
    n_classes = len(train_classes)
    print(f"Found {n_classes} training classes: {train_classes}")

    for class_label in train_classes:
        class_path = os.path.join(train_folder_path, class_label)
        # Use glob to find common image formats
        image_files = glob.glob(os.path.join(class_path, '*.png')) + \
                      glob.glob(os.path.join(class_path, '*.jpg')) + \
                      glob.glob(os.path.join(class_path, '*.ppm')) # Add other formats if needed

        print(f"  Loading class {class_label} ({len(image_files)} images)...")
        for img_path in image_files:
            img_data = load_image(img_path)
            if img_data is not None:
                X_train_list.append(img_data)
                y_train_list.append(int(class_label)) # Folder name is the label

    X_train = np.array(X_train_list)
    y_train = np.array(y_train_list, dtype=np.int32)
    print(f"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}")

    # --- Load Test Data ---
    print(f"\nLoading test data from: {test_folder_path}")
    X_test_list = []
    test_filenames = []
    # Use glob to find common image formats, sort for consistent order
    test_image_files = sorted(glob.glob(os.path.join(test_folder_path, '*.png')) + \
                              glob.glob(os.path.join(test_folder_path, '*.jpg')) + \
                              glob.glob(os.path.join(test_folder_path, '*.ppm')))
    print(f"Found {len(test_image_files)} test images.")

    for img_path in test_image_files:
        img_data = load_image(img_path)
        if img_data is not None:
            X_test_list.append(img_data)
            test_filenames.append(os.path.basename(img_path)) # Store filename

    X_test = np.array(X_test_list)
    print(f"Test data shape: {X_test.shape}")
    y_test = None
    # --- Load Test Labels (Conditionally) ---
    expected_csv_filename = "test_labels.csv"

    try:
        parent_dir = os.path.dirname(os.path.abspath(test_folder_path))
        derived_csv_path = os.path.join(parent_dir, expected_csv_filename)
    except Exception as e:
        print(f"Could not determine parent directory for {test_folder_path}: {e}")
        derived_csv_path = None 
    if derived_csv_path:
        TEST_LABELS_CSV_PATH = derived_csv_path
        print(f"\nChecking for test labels CSV at: {TEST_LABELS_CSV_PATH}")
        if os.path.exists(TEST_LABELS_CSV_PATH):
            print("Test labels CSV found. Loading labels...")
            try:
                label_df = pd.read_csv(TEST_LABELS_CSV_PATH, delimiter=',') 

                label_map = pd.Series(label_df.label.values, index=label_df.image).to_dict()

                y_test_list = []
                missing_labels = 0
                for fname in test_filenames: 
                    if fname in label_map:
                        y_test_list.append(label_map[fname])
                    else:
                        print(f"Warning: Label for test image '{fname}' not found in CSV.")
                        missing_labels += 1
                        y_test_list.append(-1) 

                if missing_labels &gt; 0:
                    print(f"Warning: {missing_labels} test images did not have corresponding labels in the CSV.")
                    print("Evaluation will proceed only on images with labels.")
                    y_test = np.array(y_test_list, dtype=np.int32)
                    print(f"Test labels shape (including placeholders): {y_test.shape}")
                else:
                    y_test = np.array(y_test_list, dtype=np.int32)
                    print(f"Test labels shape: {y_test.shape}")

            except KeyError as e:
                print(f"Error: Column name {e} not found in the CSV file '{TEST_LABELS_CSV_PATH}'.")
                print("Please ensure the CSV has columns named 'image' and 'label'.")
                print("Proceeding without test labels.")
                y_test = None
            except Exception as e:
                print(f"Error reading or processing test labels CSV: {e}")
                print("Proceeding without test labels.")
                y_test = None
        else:
            print("Test labels CSV not found. Proceeding without test labels.")
            y_test = None

    return X_train, y_train, X_test, y_test, n_classes, test_filenames


# --- Evaluation ---

def evaluate_model(model, X, y_true, dataset_name="Test", class_labels=None):
    """Evaluates the model and returns metrics. Handles y_true=None."""
    print(f"\n--- Evaluating on {dataset_name} Data ---")

    if y_true is None:
        print(f"Ground truth labels for {dataset_name} data are not available. Skipping evaluation.")
        y_pred = model.predict(X)
        return {'predictions': y_pred} # Return only predictions

    y_pred = model.predict(X)

    valid_indices = (y_true != -1) 
    if not np.all(valid_indices):
         print(f"Warning: Evaluating only on {np.sum(valid_indices)} examples with valid labels.")
         y_true_filtered = y_true[valid_indices]
         y_pred_filtered = y_pred[valid_indices]
    else:
         y_true_filtered = y_true
         y_pred_filtered = y_pred

    if len(y_true_filtered) == 0:
        print("No valid ground truth labels found for evaluation.")
        return {'predictions': y_pred}

    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)
    f1_w = f1_score(y_true_filtered, y_pred_filtered, average='weighted', zero_division=0)
    f1_m = f1_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0)

    print(f"{dataset_name} Accuracy: {accuracy:.4f}")
    print(f"{dataset_name} F1 Score (Weighted): {f1_w:.4f}")
    print(f"{dataset_name} F1 Score (Macro): {f1_m:.4f}")
    print(f"\nClassification Report ({dataset_name}):")

    target_names = None
    if class_labels:
        target_names = [str(lbl) for lbl in class_labels] 

    report = classification_report(y_true_filtered, y_pred_filtered, target_names=target_names, zero_division=0)
    print(report)

    precision, recall, f1, _ = precision_recall_fscore_support(y_true_filtered, y_pred_filtered, average=None, zero_division=0)

    metrics = {
        'accuracy': accuracy,
        'f1_weighted': f1_w,
        'f1_macro': f1_m,
        'report': report,
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'predictions': y_pred 
    }
    return metrics

def plot_f1_vs_param(param_values, f1_scores_train, f1_scores_test, param_name, title, filename):
    plt.figure(figsize=(10, 6))
    plt.plot(param_values, f1_scores_train, marker='o', linestyle='-', label='Train F1 (Weighted)')
    if f1_scores_test:
        valid_param_values = [p for p, score in zip(param_values, f1_scores_test) if score is not None]
        valid_f1_test = [score for score in f1_scores_test if score is not None]
        if valid_f1_test:
             plt.plot(valid_param_values, valid_f1_test, marker='s', linestyle='--', label='Test F1 (Weighted)')
    plt.xlabel(param_name)
    plt.ylabel('Average F1 Score (Weighted)')
    plt.title(title)
    if isinstance(param_values[0], (int, float)): 
         plt.xticks(param_values)
    else: 
         plt.xticks(range(len(param_values)), param_values) 
    plt.grid(True)
    plt.legend()
    plt.savefig(filename)
    plt.close()
    print(f"Plot saved to {filename}")


# --- Main Execution Logic ---

def run_part_b(X_train, y_train, X_test, y_test, n_features, n_classes, output_folder, test_filenames):
    print("\n\n===== Running Part (b): Varying Hidden Units (Single Layer) =====")
    hidden_unit_options = [1, 5, 10, 50, 100]
    learning_rate = 0.01
    batch_size = 32
    epochs = 2000
    activation = 'sigmoid'
    train_f1_scores = []
    test_f1_scores = []
    results_b = {}
    best_test_f1 = -1.0 
    best_predictions = None

    print(f"Stopping Criterion: Max {epochs} epochs with early stopping (patience=5) based on validation loss.")
    print(f"Using Sigmoid activation, LR={learning_rate}, Batch Size={batch_size}")

    for units in hidden_unit_options:
        print(f"\n--- Training with Hidden Layer Size: [{units}] ---")
        hidden_layers = [units]

        model = NeuralNetwork(n_features=n_features,
                              hidden_layer_sizes=hidden_layers,
                              n_classes=n_classes, 
                              activation=activation)

        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,
                           learning_rate=learning_rate, adaptive_lr=False, validation_split=0.1, patience=5)

        train_metrics = evaluate_model(model, X_train, y_train, dataset_name="Train", class_labels=list(range(n_classes)))
        test_metrics = evaluate_model(model, X_test, y_test, dataset_name="Test", class_labels=list(range(n_classes))) 

        train_f1_scores.append(train_metrics.get('f1_weighted', 0)) 
        current_test_f1 = None
        if test_metrics and 'f1_weighted' in test_metrics: 
            current_test_f1 = test_metrics['f1_weighted']
            if current_test_f1 &gt; best_test_f1:
                best_test_f1 = current_test_f1
                best_predictions = test_metrics['predictions']
        elif 'predictions' in test_metrics: 
             if best_predictions is None:  
                  best_predictions = test_metrics['predictions']

        test_f1_scores.append(current_test_f1) 
        results_b[f'hidden_{units}'] = {'train': train_metrics, 'test': test_metrics}

        if 'predictions' in test_metrics:
            pred_filename_run = os.path.join(output_folder, f"prediction_b_hidden_{units}.csv")
            pd.DataFrame({'prediction': test_metrics['predictions']}).to_csv(pred_filename_run, index=False)

    if best_predictions is not None:
        best_pred_filename_b = os.path.join(output_folder, f"prediction_b.csv")
        pd.DataFrame({'prediction': best_predictions}).to_csv(best_pred_filename_b, index=False)
        print(f"\nSaving final predictions for part (b) to {best_pred_filename_b}")
    else:
        print("\nNo test predictions available to save for part (b).")

    
    plot_filename = os.path.join(output_folder, "part_b_f1_vs_hidden_units.png")
    plot_f1_vs_param(hidden_unit_options, train_f1_scores, test_f1_scores,
                       'Number of Hidden Units',
                       'Part (b): Avg F1 Score vs. Hidden Units (Sigmoid, LR=0.01)',
                       plot_filename)


def run_part_c(X_train, y_train, X_test, y_test, n_features, n_classes, output_folder, test_filenames):
    print("\n\n===== Running Part (c): Varying Network Depth =====")
    architectures = { 'Depth 1': [512], 'Depth 2': [512, 256], 'Depth 3': [512, 256, 128], 'Depth 4': [512, 256, 128, 64] }
    learning_rate = 0.01
    batch_size = 32
    epochs = 2000
    activation = 'sigmoid'
    train_f1_scores = []
    test_f1_scores = []
    depths = list(range(1, len(architectures) + 1))
    results_c = {}
    best_test_f1 = -1.0
    best_predictions = None

    print(f"Stopping Criterion: Max {epochs} epochs with early stopping (patience=5) based on validation loss.")
    print(f"Using Sigmoid activation, LR={learning_rate}, Batch Size={batch_size}")

    for i, (name, hidden_layers) in enumerate(architectures.items()):
        depth = i + 1
        print(f"\n--- Training with Network Depth: {depth} ({hidden_layers}) ---")
        model = NeuralNetwork(n_features=n_features, hidden_layer_sizes=hidden_layers, n_classes=n_classes, activation=activation)
        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, learning_rate=learning_rate, adaptive_lr=False, validation_split=0.1, patience=5)

        train_metrics = evaluate_model(model, X_train, y_train, dataset_name="Train", class_labels=list(range(n_classes)))
        test_metrics = evaluate_model(model, X_test, y_test, dataset_name="Test", class_labels=list(range(n_classes)))

        train_f1_scores.append(train_metrics.get('f1_weighted', 0))
        current_test_f1 = None
        if test_metrics and 'f1_weighted' in test_metrics:
            current_test_f1 = test_metrics['f1_weighted']
            if current_test_f1 &gt; best_test_f1:
                best_test_f1 = current_test_f1
                best_predictions = test_metrics['predictions']
        elif 'predictions' in test_metrics:
             if best_predictions is None:
                  best_predictions = test_metrics['predictions']

        test_f1_scores.append(current_test_f1)
        results_c[f'depth_{depth}'] = {'train': train_metrics, 'test': test_metrics}

    if best_predictions is not None:
        best_pred_filename_c = os.path.join(output_folder, f"prediction_c.csv")
        pd.DataFrame({'prediction': best_predictions}).to_csv(best_pred_filename_c, index=False)
        print(f"\nSaving final predictions for part (c) to {best_pred_filename_c}")
    else:
        print("\nNo test predictions available to save for part (c).")

    plot_filename = os.path.join(output_folder, "part_c_f1_vs_depth.png")
    plot_f1_vs_param(depths, train_f1_scores, test_f1_scores, 'Network Depth', 'Part (c): Avg F1 Score vs. Network Depth (Sigmoid, LR=0.01)', plot_filename)


def run_part_d(X_train, y_train, X_test, y_test, n_features, n_classes, output_folder, test_filenames):
    print("\n\n===== Running Part (d): Varying Network Depth with Adaptive Learning Rate =====")
    architectures = { 'Depth 1': [512], 'Depth 2': [512, 256], 'Depth 3': [512, 256, 128], 'Depth 4': [512, 256, 128, 64] }
    eta0 = 0.01
    batch_size = 32
    epochs = 2000
    activation = 'sigmoid'
    patience = 10
    train_f1_scores = []
    test_f1_scores = []
    depths = list(range(1, len(architectures) + 1))
    results_d = {}
    best_test_f1 = -1.0
    best_predictions = None

    print(f"Stopping Criterion: Max {epochs} epochs with early stopping (patience={patience}) based on validation loss.")
    print(f"Using Sigmoid activation, Adaptive LR (eta0={eta0}), Batch Size={batch_size}")

    for i, (name, hidden_layers) in enumerate(architectures.items()):
        depth = i + 1
        print(f"\n--- Training with Network Depth: {depth} ({hidden_layers}), Adaptive LR ---")
        model = NeuralNetwork(n_features=n_features, hidden_layer_sizes=hidden_layers, n_classes=n_classes, activation=activation)
        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, learning_rate=eta0, adaptive_lr=True, eta0=eta0, validation_split=0.1, patience=patience)

        train_metrics = evaluate_model(model, X_train, y_train, dataset_name="Train", class_labels=list(range(n_classes)))
        test_metrics = evaluate_model(model, X_test, y_test, dataset_name="Test", class_labels=list(range(n_classes)))

        train_f1_scores.append(train_metrics.get('f1_weighted', 0))
        current_test_f1 = None
        if test_metrics and 'f1_weighted' in test_metrics:
            current_test_f1 = test_metrics['f1_weighted']
            if current_test_f1 &gt; best_test_f1:
                best_test_f1 = current_test_f1
                best_predictions = test_metrics['predictions']
        elif 'predictions' in test_metrics:
             if best_predictions is None:
                  best_predictions = test_metrics['predictions']

        test_f1_scores.append(current_test_f1)
        results_d[f'depth_{depth}'] = {'train': train_metrics, 'test': test_metrics}

    if best_predictions is not None:
        best_pred_filename_d = os.path.join(output_folder, f"prediction_d.csv")
        pd.DataFrame({'prediction': best_predictions}).to_csv(best_pred_filename_d, index=False)
        print(f"\nSaving final predictions for part (d) to {best_pred_filename_d}")
    else:
        print("\nNo test predictions available to save for part (d).")

    plot_filename = os.path.join(output_folder, "part_d_f1_vs_depth_adaptive_lr.png")
    plot_f1_vs_param(depths, train_f1_scores, test_f1_scores, 'Network Depth', 'Part (d): Avg F1 Score vs. Depth (Sigmoid, Adaptive LR)', plot_filename)


def run_part_e(X_train, y_train, X_test, y_test, n_features, n_classes, output_folder, test_filenames):
    print("\n\n===== Running Part (e): Varying Network Depth with ReLU and Adaptive LR =====")
    architectures = { 'Depth 1': [512], 'Depth 2': [512, 256], 'Depth 3': [512, 256, 128], 'Depth 4': [512, 256, 128, 64] }
    eta0 = 0.01
    batch_size = 32
    epochs = 2000
    activation = 'relu' 
    patience = 10
    train_f1_scores = []
    test_f1_scores = []
    depths = list(range(1, len(architectures) + 1))
    results_e = {}
    best_test_f1 = -1.0
    best_predictions = None

    print(f"Stopping Criterion: Max {epochs} epochs with early stopping (patience={patience}) based on validation loss.")
    print(f"Using ReLU activation, Adaptive LR (eta0={eta0}), Batch Size={batch_size}")

    for i, (name, hidden_layers) in enumerate(architectures.items()):
        depth = i + 1
        print(f"\n--- Training with Network Depth: {depth} ({hidden_layers}), ReLU, Adaptive LR ---")
        model = NeuralNetwork(n_features=n_features, hidden_layer_sizes=hidden_layers, n_classes=n_classes, activation=activation)
        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, learning_rate=eta0, adaptive_lr=True, eta0=eta0, validation_split=0.1, patience=patience)

        train_metrics = evaluate_model(model, X_train, y_train, dataset_name="Train", class_labels=list(range(n_classes)))
        test_metrics = evaluate_model(model, X_test, y_test, dataset_name="Test", class_labels=list(range(n_classes)))

        train_f1_scores.append(train_metrics.get('f1_weighted', 0))
        current_test_f1 = None
        if test_metrics and 'f1_weighted' in test_metrics:
            current_test_f1 = test_metrics['f1_weighted']
            if current_test_f1 &gt; best_test_f1:
                best_test_f1 = current_test_f1
                best_predictions = test_metrics['predictions']
        elif 'predictions' in test_metrics:
             if best_predictions is None:
                  best_predictions = test_metrics['predictions']

        test_f1_scores.append(current_test_f1)
        results_e[f'depth_{depth}'] = {'train': train_metrics, 'test': test_metrics}

    if best_predictions is not None:
        best_pred_filename_e = os.path.join(output_folder, f"prediction_e.csv")
        pd.DataFrame({'prediction': best_predictions}).to_csv(best_pred_filename_e, index=False)
        print(f"\nSaving final predictions for part (e) to {best_pred_filename_e}")
    else:
        print("\nNo test predictions available to save for part (e).")

    plot_filename = os.path.join(output_folder, "part_e_f1_vs_depth_relu_adaptive_lr.png")
    plot_f1_vs_param(depths, train_f1_scores, test_f1_scores, 'Network Depth', 'Part (e): Avg F1 Score vs. Depth (ReLU, Adaptive LR)', plot_filename)
    


def run_part_f(X_train, y_train, X_test, y_test, n_features, n_classes, output_folder, test_filenames):
    print("\n\n===== Running Part (f): Scikit-learn MLPClassifier Comparison =====")
    architectures_sklearn = { 'Depth 1': (512,), 'Depth 2': (512, 256), 'Depth 3': (512, 256, 128), 'Depth 4': (512, 256, 128, 64) }
    eta0 = 0.01
    batch_size = 32
    max_iter = 2000
    train_f1_scores = []
    test_f1_scores = []
    depths = list(range(1, len(architectures_sklearn) + 1))
    results_f = {}
    best_test_f1 = -1.0
    best_predictions_sk = None

    print(f"Stopping Criterion: Scikit-learn default (max_iter={max_iter}, early_stopping=True)")
    print(f"Using ReLU activation, 'invscaling' LR (eta0={eta0}), Batch Size={batch_size}, Solver='sgd', Alpha=0")

    for i, (name, hidden_layers_tuple) in enumerate(architectures_sklearn.items()):
        depth = i + 1
        print(f"\n--- Training with Scikit-learn MLP: Depth {depth} ({hidden_layers_tuple}) ---")
        mlp = MLPClassifier(hidden_layer_sizes=hidden_layers_tuple, activation='relu', solver='sgd', alpha=0,
                            batch_size=batch_size, learning_rate='invscaling', learning_rate_init=eta0,
                            max_iter=max_iter, shuffle=True, random_state=42, verbose=False, # Quieter output
                            early_stopping=True, n_iter_no_change=10, validation_fraction=0.1)

        start_time_sk = time.time()
        mlp.fit(X_train, y_train)
        end_time_sk = time.time()
        print(f"Scikit-learn training time: {end_time_sk - start_time_sk:.2f}s")

        print(f"\n--- Evaluating Scikit-learn MLP (Depth {depth}) ---")
        train_metrics_sk = evaluate_model(mlp, X_train, y_train, dataset_name="Train (sklearn)", class_labels=list(range(n_classes)))
        test_metrics_sk = evaluate_model(mlp, X_test, y_test, dataset_name="Test (sklearn)", class_labels=list(range(n_classes))) 

        train_f1_scores.append(train_metrics_sk.get('f1_weighted', 0))
        current_test_f1 = None
        if test_metrics_sk and 'f1_weighted' in test_metrics_sk:
            current_test_f1 = test_metrics_sk['f1_weighted']
            if current_test_f1 &gt; best_test_f1:
                best_test_f1 = current_test_f1
                best_predictions_sk = test_metrics_sk['predictions'] # Get predictions from metrics dict
        elif 'predictions' in test_metrics_sk:
             if best_predictions_sk is None:
                  best_predictions_sk = test_metrics_sk['predictions']

        test_f1_scores.append(current_test_f1)
        results_f[f'depth_{depth}'] = {'train': train_metrics_sk, 'test': test_metrics_sk}

    if best_predictions_sk is not None:
        best_pred_filename_f = os.path.join(output_folder, f"prediction_f.csv")
        pd.DataFrame({'prediction': best_predictions_sk}).to_csv(best_pred_filename_f, index=False)
        print(f"\nSaving final predictions for part (f) to {best_pred_filename_f}")
    else:
        print("\nNo test predictions available to save for part (f).")

    plot_filename = os.path.join(output_folder, "part_f_f1_vs_depth_sklearn.png")
    plot_f1_vs_param(depths, train_f1_scores, test_f1_scores, 'Network Depth', 'Part (f): Avg F1 Score vs. Depth (Scikit-learn MLP, ReLU)', plot_filename)
    # Comments...

# --- Command Line Argument Parsing ---
def main():
    parser = argparse.ArgumentParser(description="Train and evaluate a Neural Network on GTSRB data from image folders.")
    # &lt;&lt;&lt; CHANGE: Updated help text for paths &gt;&gt;&gt;
    parser.add_argument("train_data_path", type=str, help="Absolute path to the training data ROOT FOLDER (containing class subdirectories).")
    parser.add_argument("test_data_path", type=str, help="Absolute path to the test data FOLDER (containing test images).")
    parser.add_argument("output_folder_path", type=str, help="Absolute path to the folder where predictions and plots will be saved.")
    parser.add_argument("question_part", type=str, choices=['b', 'c', 'd', 'e', 'f'], help="Which part of the question to run ('b', 'c', 'd', 'e', 'f').")

    args = parser.parse_args()

    os.makedirs(args.output_folder_path, exist_ok=True)

    # Load and preprocess data using the new function
    X_train, y_train, X_test, y_test, n_classes, test_filenames = load_and_preprocess_data(
        args.train_data_path, args.test_data_path
    )

    # &lt;&lt;&lt; CHANGE: Check if data loading was successful &gt;&gt;&gt;
    if X_train is None or X_test is None:
         print("Error during data loading. Exiting.")
         sys.exit(1)
    if X_train.size == 0 or X_test.size == 0:
         print("No data loaded. Check input paths and image files. Exiting.")
         sys.exit(1)


    n_features = X_train.shape[1] 

    common_args = {
        "X_train": X_train, "y_train": y_train, "X_test": X_test, "y_test": y_test,
        "n_features": n_features, "n_classes": n_classes,
        "output_folder": args.output_folder_path, "test_filenames": test_filenames
    }

    if args.question_part == 'b':
        run_part_b(**common_args)
    elif args.question_part == 'c':
        run_part_c(**common_args)
    elif args.question_part == 'd':
        run_part_d(**common_args)
    elif args.question_part == 'e':
        run_part_e(**common_args)
    elif args.question_part == 'f':
        run_part_f(**common_args)

if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
