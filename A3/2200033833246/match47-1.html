<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_0Y80D.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_Q0XYE.py<p><PRE>


import sys
import os
import pandas as pd
import numpy as np
from math import log2
import matplotlib.pyplot as plt
<A NAME="2"></A><FONT color = #0000FF><A HREF="match47-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score

class Node:
    def __init__(self, attribute=None):
</FONT>        self.attribute = attribute      # Splitting attribute
        self.split_value = None         # For continuous splits
        self.children = {}              # For categorical splits
        self.prediction = None
        self.isLeaf = False

def load_and_encode_data(train_data_path, test_data_path, validation_data_path):

    train_data = pd.read_csv(train_data_path)
    test_data = pd.read_csv(test_data_path)
    val_data = pd.read_csv(validation_data_path)
    
    # Identify categorical columns with &gt;2 categories
    categorical_cols = train_data.select_dtypes(include=['object']).columns
    cols_to_encode = [col for col in categorical_cols if col not in ['income']]
    
    # Apply one-hot encoding
    train_encoded = pd.get_dummies(train_data, columns=cols_to_encode)
    test_encoded = pd.get_dummies(test_data, columns=cols_to_encode)
    val_encoded = pd.get_dummies(val_data, columns=cols_to_encode)
    
    # Align columns between train and test
    train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)
    train_encoded, val_encoded = train_encoded.align(val_encoded, join='left', axis=1, fill_value=0)
    
    return train_encoded, test_encoded, val_encoded    

def entropy(data):
    counts = data['income'].value_counts()
    total = len(data)
    return -sum((count/total) * log2(count/total) for count in counts)

def mutual_information_a(data, attribute):
    if data[attribute].dtype == object:
        groups = data.groupby(attribute)
        weighted_entropy = sum((len(g)/len(data)) * entropy(g) for _,g in groups)
    else:
        median = data[attribute].median()
        left = data[data[attribute] &lt;= median]
        right = data[data[attribute] &gt; median]
        weighted_entropy = (len(left)/len(data))*entropy(left) + (len(right)/len(data))*entropy(right)
    return entropy(data) - weighted_entropy 

def build_tree_a(data, current_depth, max_depth):
    node = Node()
    node.prediction = data['income'].mode()[0] 
    # Base case
    if current_depth &gt;= max_depth or entropy(data) == 0:
        node.isLeaf = True
        return node
    
    # Find best split
    best_attr = max(data.columns[:-1], key=lambda a: mutual_information_a(data, a))
    node.attribute = best_attr
    
    # Handle splits
    if data[best_attr].dtype == object:
        # Categorical split
        for category in data[best_attr].unique():
            subset = data[data[best_attr] == category]
            node.children[category] = build_tree_a(subset, current_depth+1, max_depth)
    else:
        # Continuous split
        median = data[best_attr].median()
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match47-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        node.split_value = median
        split1 = data[data[best_attr] &lt;= median]
        split2 = data[data[best_attr] &gt; median]
</FONT>        if(not split1.empty):
            node.children['left'] = build_tree_a(split1, current_depth+1, max_depth)
        if(not split2.empty):
            node.children['right'] = build_tree_a(split2, current_depth+1, max_depth)
    
    return node

# Prediction Function
def predict_a(node, sample):
    if node.isLeaf:
        return node.prediction
    
    attr = node.attribute
    value = sample[attr]
    
    if node.split_value is not None:  # Continuous split
        if value &lt;= node.split_value and 'left' in node.children:
            return predict_a(node.children['left'], sample)
        elif 'right' in node.children:
            return predict_a(node.children['right'], sample)
        return node.prediction
    else:  # Categorical split
        if value in node.children:
            return predict_a(node.children[value], sample)
        # Handle unseen categories
        return node.prediction  
    

def mutual_information_b(data, attribute):

    median = data[attribute].median()
    left = data[data[attribute] &lt;= median]
    right = data[data[attribute] &gt; median]
    weighted_entropy = (len(left)/len(data))*entropy(left) + (len(right)/len(data))*entropy(right)
    return entropy(data) - weighted_entropy

# Modified tree construction to handle encoded features
def build_tree_encoded(data, current_depth, max_depth):
    # if(data.empty):
    #     return None
    node = Node()
    node.prediction = data['income'].mode()[0]
    
    if current_depth &gt;= max_depth or entropy(data) == 0:
        node.isLeaf = True
        return node
    
    # Exclude non-feature columns
    features = [col for col in data.columns if col not in ['income']]
    
    best_attr = max(features, key=lambda a: mutual_information_b(data, a))
    node.attribute = best_attr
    
    # Handle continuous/binary splits
    median = data[best_attr].median()
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match47-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    node.split_value = median
    
    left_split = data[data[best_attr] &lt;= median]
    right_split = data[data[best_attr] &gt; median]
</FONT>    
    if not left_split.empty:
        node.children['left'] = build_tree_encoded(left_split, current_depth+1, max_depth)
    if not right_split.empty:
        node.children['right'] = build_tree_encoded(right_split, current_depth+1, max_depth)
    
    return node

# Prediction function remains the same
def predict_b(node, sample):
    if node.isLeaf:
        return node.prediction
    
    value = sample[node.attribute]
    
    if value &lt;= node.split_value and 'left' in node.children:
        return predict_b(node.children['left'], sample)
    elif 'right' in node.children:
        return predict_b(node.children['right'], sample)
    return node.prediction

def calculate_accuracy(tree, data):
    correct = 0
    for _, row in data.iterrows():
        correct += (predict_b(tree, row) == row['income'])
    return correct / len(data)

def post_prune(tree, val_data):
    
    current_tree = tree
    while True:
        
        # Find best node to prune
        best_delta = -np.inf
        best_node = None

        curr_val_acc=calculate_accuracy(current_tree, val_data)
        
        # BFS traversal to find prunable nodes
        queue = [current_tree]  
        while queue:
            node = queue.pop(0)
            if node.isLeaf:
                continue
                
            # Temporary prune this node
            original_children = node.children.copy()
            node.isLeaf = True
            node.children = {}
            new_acc = calculate_accuracy(current_tree, val_data)
            delta = new_acc - curr_val_acc  
            
            if delta &gt; best_delta:
                best_delta = delta
                best_node = node
                
            # Restore node
            node.isLeaf = False
            node.children = original_children
            
            # Add children to queue
            for k, child in node.children.items():
                queue.append( child)
        
        # Stop if no improvement possible
        if best_delta &lt;= 0:
            break
            
        # Perform the best prune
        best_node.isLeaf = True
        best_node.children = {}
    
def main():
    # Ensure correct number of arguments are provided
    if len(sys.argv) != 6:
        print("Usage: python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        sys.exit(1)
    
    # Parse command-line arguments
    train_data_path = sys.argv[1]
    validation_data_path = sys.argv[2]
    test_data_path = sys.argv[3]
    output_folder_path = sys.argv[4]
    question_part = sys.argv[5]

    # Validate question part
    if question_part not in ['a', 'b', 'c', 'd', 'e']:
        print("Error: question_part must be one of 'a', 'b', 'c', 'd', or 'e'.")
        sys.exit(1)

    # Create output directory if it doesn't exist
    os.makedirs(output_folder_path, exist_ok=True)
    out_file_path = os.path.join(output_folder_path, f'prediction_{question_part}.csv')

    if question_part == 'a':
        train_data = pd.read_csv(train_data_path)
        test_data = pd.read_csv(test_data_path)
        
        dt = build_tree_a(train_data, 0, 20)
        predictions_a = [None]*len(test_data)
        i=0
        for _,row in test_data.iterrows():
            predictions_a[i]=predict_a(dt,row)
            i+=1
        pd.DataFrame({'prediction':predictions_a}).to_csv(out_file_path, index=False)

    elif question_part == 'b':
        train_data, test_data, _ = load_and_encode_data(train_data_path, test_data_path, validation_data_path)
        dt = build_tree_encoded(train_data, 0, 55)
        predictions_b = [None]*len(test_data)
        i=0
        for _,row in test_data.iterrows():
            predictions_b[i]=predict_b(dt,row)
            i+=1
        pd.DataFrame({'prediction':predictions_b}).to_csv(out_file_path, index=False)

    elif question_part == 'c':
        train_data, test_data, val_data = load_and_encode_data(train_data_path, test_data_path, validation_data_path)
        dt = build_tree_encoded(train_data, 0, 55)
        post_prune(dt,val_data)
        predictions_c = [None]*len(test_data)
        i=0
        for _,row in test_data.iterrows():
            predictions_c[i]=predict_b(dt,row)
            i+=1
        pd.DataFrame({'prediction':predictions_c}).to_csv(out_file_path, index=False)

    elif question_part == 'd':
        train_data, test_data, val_data = load_and_encode_data(train_data_path, test_data_path, validation_data_path)
    
        depths = [25, 35, 45, 55]
        results = {'depth': [], 'valid_acc': []}
        X_train = train_data.drop(columns=['income'])
        y_train = train_data['income']
        X_val = val_data.drop(columns=['income'])
        y_val = val_data['income']

        X_test = test_data.drop(columns=['income']) if 'income' in test_data.columns else test_data

        for depth in depths:
            clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth)
            clf.fit(X_train, y_train)
            
            results['depth'].append(depth)
            pred_depth = clf.predict(X_val)
            results['valid_acc'].append(np.sum(pred_depth==y_val))
    
        best_depth = depths[np.argmax(results['valid_acc'])]

        ccp_alphas = [0.001, 0.01, 0.1, 0.2]
        results = {'alpha': [], 'valid_acc': []}
    
        for alpha in ccp_alphas:
            clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha)
            clf.fit(X_train, y_train)
            
            results['alpha'].append(alpha)
            pred_alpha = clf.predict(X_val)
            results['valid_acc'].append(np.sum(pred_alpha==y_val))
        
        best_alpha = ccp_alphas[np.argmax(results['valid_acc'])]

        clf = DecisionTreeClassifier(criterion='entropy', max_depth=best_depth, ccp_alpha=best_alpha)
        clf.fit(X_train, y_train)

        predictions_d = clf.predict(X_test)
        pd.DataFrame({'prediction':predictions_d}).to_csv(out_file_path, index=False)

    else:
        train_data, test_data, val_data = load_and_encode_data(train_data_path, test_data_path, validation_data_path)
        X_train = train_data.drop(columns=['income'])
        y_train = train_data['income']
        X_val = val_data.drop(columns=['income'])
        y_val = val_data['income']
        X_test = test_data.drop(columns=['income']) if 'income' in test_data.columns else test_data

        param_grid = {
            'n_estimators': [50, 150, 250, 350],        # 50 to 350 in steps of 100
<A NAME="7"></A><FONT color = #0000FF><A HREF="match47-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],  # 0.1 to 1.0 in steps of 0.2
            'min_samples_split': [2, 4, 6, 8, 10]       # 2 to 10 in steps of 2
        }

        # Track best model
        best_acc = -1
        best_model = None
</FONT>
        # Grid search using OOB accuracy
        for params in ParameterGrid(param_grid):
            rf = RandomForestClassifier(
                criterion='entropy',
                oob_score=True,
                bootstrap=True,
                random_state=42,
                **params
            )
            rf.fit(X_train, y_train)
            
            y_val_pred = rf.predict(X_val)
            val_acc = accuracy_score(y_val, y_val_pred)
            # Update best model if OOB improves
            if val_acc &gt; best_acc:
                best_acc = best_acc
                best_model = rf
        
        predictions_e = best_model.predict(X_test)
        pd.DataFrame({'prediction':predictions_e}).to_csv(out_file_path, index=False)



if __name__ == "__main__":
    main()




import pandas as pd
import numpy as np
from math import log2
import matplotlib.pyplot as plt

# Node and Tree Class Definitions
class Node:
    def __init__(self, attribute=None):
        self.attribute = attribute      # Splitting attribute
        self.split_value = None         # For continuous splits
        self.children = {}              # For categorical splits
        self.prediction = None
        self.isLeaf = False
    

# Helper Functions
def entropy(data):
    counts = data['income'].value_counts()
    total = len(data)
    return -sum((count/total) * log2(count/total) for count in counts)

def mutual_information(data, attribute):
    if data[attribute].dtype == object:
        groups = data.groupby(attribute)
        weighted_entropy = sum((len(g)/len(data)) * entropy(g) for _,g in groups)
    else:
        median = data[attribute].median()
        left = data[data[attribute] &lt;= median]
        right = data[data[attribute] &gt; median]
        weighted_entropy = (len(left)/len(data))*entropy(left) + (len(right)/len(data))*entropy(right)
    return entropy(data) - weighted_entropy

# Tree Construction
def build_tree(data, current_depth, max_depth):
    node = Node()
    node.prediction = data['income'].mode()[0] 
    # Base case
    if current_depth &gt;= max_depth or entropy(data) == 0:
        node.isLeaf = True
        return node
    
    # Find best split
    best_attr = max(data.columns[:-1], key=lambda a: mutual_information(data, a))
    node.attribute = best_attr
    
    # Handle splits
    if data[best_attr].dtype == object:
        # Categorical split
        for category in data[best_attr].unique():
            subset = data[data[best_attr] == category]
            node.children[category] = build_tree(subset, current_depth+1, max_depth)
    else:
        # Continuous split
        median = data[best_attr].median()
        node.split_value = median
        split1 = data[data[best_attr] &lt;= median]
        split2 = data[data[best_attr] &gt; median]
        if(not split1.empty):
            node.children['left'] = build_tree(split1, current_depth+1, max_depth)
        if(not split2.empty):
            node.children['right'] = build_tree(split2, current_depth+1, max_depth)
    
    return node

# Prediction Function
def predict(node, sample):
    if node.isLeaf:
        return node.prediction
    
    attr = node.attribute
    value = sample[attr]
    
    if node.split_value is not None:  # Continuous split
        if value &lt;= node.split_value and 'left' in node.children:
            return predict(node.children['left'], sample)
        elif 'right' in node.children:
            return predict(node.children['right'], sample)
        return node.prediction
    else:  # Categorical split
        if value in node.children:
            return predict(node.children[value], sample)
        # Handle unseen categories
        return node.prediction  

try:
    train_data = pd.read_csv('data/Q1/train.csv')
    test_data = pd.read_csv('data/Q1/test.csv')
    # validation_data = pd.read_csv('validation.csv')  
except FileNotFoundError as e:
    print(f"File loading error: {e}")
    exit()

# Calculate Accuracy
def calculate_accuracy(tree, data):

    return sum(predict(tree,row)==row['income'] for _,row in data.iterrows())/len(data)

# Experiment with depths
max_depths = [5,10,15,25]
results = []

for depth in max_depths:
    dt = build_tree(train_data, 0, depth)
    
    train_acc = calculate_accuracy(dt, train_data)
    test_acc = calculate_accuracy(dt, test_data)
    
    results.append({
        'max_depth': depth,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc
    })

# Print results
print("Depth | Train Acc | Test Acc")
train_accuracies = []
test_accuracies = []
for res in results:
    print(f"{res['max_depth']:5} | {res['train_accuracy']:.3f}    | {res['test_accuracy']:.3f}")
    train_accuracies.append(res['train_accuracy'])
    test_accuracies.append(res['test_accuracy'])

plt.figure(figsize=(10, 6))
plt.plot(max_depths, train_accuracies, label='Train Accuracy', marker='o')
plt.plot(max_depths, test_accuracies, label='Test Accuracy', marker='o')
plt.xlabel('Maximum Depth')
plt.ylabel('Accuracy')
plt.title('Train and Test Accuracies vs Maximum Depth')
plt.legend()
plt.grid()
plt.show()



import pandas as pd
import numpy as np
from math import log2
import matplotlib.pyplot as plt

# Node class remains the same
class Node:
    def __init__(self, attribute=None):
        self.attribute = attribute
        self.split_value = None
        self.children = {}
        self.prediction = None
        self.isLeaf = False

# Modified data loading with one-hot encoding
def load_and_encode_data():
    try:
        train_data = pd.read_csv('data/Q1/train.csv')
        test_data = pd.read_csv('data/Q1/test.csv')
        
        # Identify categorical columns with &gt;2 categories
        categorical_cols = train_data.select_dtypes(include=['object']).columns
        cols_to_encode = [col for col in categorical_cols if col not in ['income']]
        
        # Apply one-hot encoding
        train_encoded = pd.get_dummies(train_data, columns=cols_to_encode)
        test_encoded = pd.get_dummies(test_data, columns=cols_to_encode)
        
        # Align columns between train and test
        train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)
        
        return train_encoded, test_encoded
    
    except FileNotFoundError as e:
        print(f"File error: {e}")
        exit()

# Helper functions remain the same
def entropy(data):
    counts = data['income'].value_counts()
    total = len(data)
    return -sum((count/total) * log2(count/total) for count in counts)

def mutual_information(data, attribute):

    median = data[attribute].median()
    left = data[data[attribute] &lt;= median]
    right = data[data[attribute] &gt; median]
    weighted_entropy = (len(left)/len(data))*entropy(left) + (len(right)/len(data))*entropy(right)
    return entropy(data) - weighted_entropy

# Modified tree construction to handle encoded features
def build_tree_encoded(data, current_depth, max_depth):
    # if(data.empty):
    #     return None
    node = Node()
    node.prediction = data['income'].mode()[0]
    
    if current_depth &gt;= max_depth or entropy(data) == 0:
        node.isLeaf = True
        return node
    
    # Exclude non-feature columns
    features = [col for col in data.columns if col not in ['income']]
    
    best_attr = max(features, key=lambda a: mutual_information(data, a))
    node.attribute = best_attr
    
    # Handle continuous/binary splits
    median = data[best_attr].median()
    node.split_value = median
    
    left_split = data[data[best_attr] &lt;= median]
    right_split = data[data[best_attr] &gt; median]
    
    if not left_split.empty:
        node.children['left'] = build_tree_encoded(left_split, current_depth+1, max_depth)
    if not right_split.empty:
        node.children['right'] = build_tree_encoded(right_split, current_depth+1, max_depth)
    
    return node

# Prediction function remains the same
def predict(node, sample):
    if node.isLeaf:
        return node.prediction
    
    value = sample[node.attribute]
    
    if value &lt;= node.split_value and 'left' in node.children:
        return predict(node.children['left'], sample)
    elif 'right' in node.children:
        return predict(node.children['right'], sample)
    return node.prediction

# Accuracy calculation
def calculate_accuracy(tree, data):
    correct = 0
    for _, row in data.iterrows():
        correct += (predict(tree, row) == row['income'])
    return correct / len(data)

# Main experiment
train_data, test_data = load_and_encode_data()

max_depths = [5]
results = []

for depth in max_depths:
    tree = build_tree_encoded(train_data, 0, depth)
    train_acc = calculate_accuracy(tree, train_data)
    test_acc = calculate_accuracy(tree, test_data)
    
    results.append({
        'max_depth': depth,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc
    })

print("Depth | Train Acc | Test Acc")
train_accuracies = []
test_accuracies = []
for res in results:
    print(f"{res['max_depth']:5} | {res['train_accuracy']:.3f}    | {res['test_accuracy']:.3f}")
    train_accuracies.append(res['train_accuracy'])
    test_accuracies.append(res['test_accuracy'])

# Plotting results
plt.figure(figsize=(10, 6))
plt.plot(max_depths, train_accuracies, 'b-o', label='Train Accuracy')
plt.plot(max_depths, test_accuracies, 'r-o', label='Test Accuracy')
plt.xlabel('Maximum Depth')
plt.ylabel('Accuracy')
plt.title('Encoded Data: Accuracy vs Max Depth')
plt.legend()
plt.grid()
plt.show()




import pandas as pd
import numpy as np
from math import log2
import matplotlib.pyplot as plt

# Node class remains the same
class Node:
    def __init__(self, attribute=None):
        self.attribute = attribute
        self.split_value = None
        self.children = {}
        self.prediction = None
        self.isLeaf = False

# Modified data loading with one-hot encoding
def load_and_encode_data():
    try:
        train_data = pd.read_csv('data/Q1/train.csv')
        test_data = pd.read_csv('data/Q1/test.csv')
        val_data = pd.read_csv('data/Q1/valid.csv')
        
        # Identify categorical columns with &gt;2 categories
        categorical_cols = train_data.select_dtypes(include=['object']).columns
        cols_to_encode = [col for col in categorical_cols if col not in ['income']]
        
        # Apply one-hot encoding
        train_encoded = pd.get_dummies(train_data, columns=cols_to_encode)
        test_encoded = pd.get_dummies(test_data, columns=cols_to_encode)
        val_encoded = pd.get_dummies(val_data, columns=cols_to_encode)
        
        # Align columns between train and test
        train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)
        train_encoded, val_encoded = train_encoded.align(val_encoded, join='left', axis=1, fill_value=0)
        
        return train_encoded, test_encoded, val_encoded
    
    except FileNotFoundError as e:
        print(f"File error: {e}")
        exit()

# Helper functions remain the same
def entropy(data):
    counts = data['income'].value_counts()
    total = len(data)
    return -sum((count/total) * log2(count/total) for count in counts)

def mutual_information(data, attribute):

    median = data[attribute].median()
    left = data[data[attribute] &lt;= median]
    right = data[data[attribute] &gt; median]
    weighted_entropy = (len(left)/len(data))*entropy(left) + (len(right)/len(data))*entropy(right)
    return entropy(data) - weighted_entropy

# Modified tree construction to handle encoded features
def build_tree_encoded(data, current_depth, max_depth):
    # if(data.empty):
    #     return None
    node = Node()
    node.prediction = data['income'].mode()[0]
    
    if current_depth &gt;= max_depth or entropy(data) == 0:
        node.isLeaf = True
        return node
    
    # Exclude non-feature columns
    features = [col for col in data.columns if col not in ['income']]
    
    best_attr = max(features, key=lambda a: mutual_information(data, a))
    node.attribute = best_attr
    
    # Handle continuous/binary splits
    median = data[best_attr].median()
    node.split_value = median
    
    left_split = data[data[best_attr] &lt;= median]
    right_split = data[data[best_attr] &gt; median]
    
    if not left_split.empty:
        node.children['left'] = build_tree_encoded(left_split, current_depth+1, max_depth)
    if not right_split.empty:
        node.children['right'] = build_tree_encoded(right_split, current_depth+1, max_depth)
    
    return node

# Prediction function remains the same
def predict(node, sample):
    if node.isLeaf:
        return node.prediction
    
    value = sample[node.attribute]
    
    if value &lt;= node.split_value and 'left' in node.children:
        return predict(node.children['left'], sample)
    elif 'right' in node.children:
        return predict(node.children['right'], sample)
    return node.prediction

# Accuracy calculation
def calculate_accuracy(tree, data):
    correct = 0
    for _, row in data.iterrows():
        correct += (predict(tree, row) == row['income'])
    return correct / len(data)

def count_nodes(tree):
    """Count number of nodes in the tree"""
    if tree.isLeaf:
        return 1
    count = 1
    for child in tree.children.values():
        count += count_nodes(child)
    return count

def post_prune_with_metrics(tree, val_data, test_data):
    """Prune tree while tracking metrics during pruning"""
    metrics = {
        'nodes': [],
        'train_acc': [],
        'val_acc': [],
        'test_acc': []
    }
    
    current_tree = tree
    while True:
        print("pruning next node:")
        # Calculate current metrics
        metrics['nodes'].append(count_nodes(current_tree))
        metrics['train_acc'].append(calculate_accuracy(current_tree, train_data))
        metrics['val_acc'].append(calculate_accuracy(current_tree, val_data))
        metrics['test_acc'].append(calculate_accuracy(current_tree, test_data))
        
        # Find best node to prune
        best_delta = -np.inf
        best_node = None
        
        # BFS traversal to find prunable nodes
        queue = [current_tree]  # (parent, node, key)
        while queue:
            node = queue.pop(0)
            if node.isLeaf:
                continue
                
            # Temporary prune this node
            original_children = node.children.copy()
            node.isLeaf = True
            node.children = {}
            new_acc = calculate_accuracy(current_tree, val_data)
            delta = new_acc - metrics['val_acc'][-1]
            
            if delta &gt; best_delta:
                best_delta = delta
                best_node = node
                
            # Restore node
            node.isLeaf = False
            node.children = original_children
            
            # Add children to queue
            for k, child in node.children.items():
                queue.append( child)
        
        # Stop if no improvement possible
        if best_delta &lt;= 0:
            break
            
        # Perform the best prune
        best_node.isLeaf = True
        best_node.children = {}
    
    return metrics

# Main experiment
train_data, test_data, val_data = load_and_encode_data()

max_depths = [45,55]
results = []
pruning_results=[]
for depth in max_depths:
    print(f"Trainig for max depth {depth}")
    tree = build_tree_encoded(train_data, 0, depth)
    train_acc = calculate_accuracy(tree, train_data)
    test_acc = calculate_accuracy(tree, test_data)
    val_acc = calculate_accuracy(tree, val_data)
    print(f"pruning with val accuracy: {val_acc}")
    metrics = post_prune_with_metrics(tree, val_data, test_data)
    pruning_results.append(metrics)
    
    results.append({
        'max_depth': depth,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc
    })

    plt.figure(figsize=(10, 6))
    plt.plot(metrics['nodes'], metrics['train_acc'], 'b-o', label='Train')
    plt.plot(metrics['nodes'], metrics['val_acc'], 'g-o', label='Validation')
    plt.plot(metrics['nodes'], metrics['test_acc'], 'r-o', label='Test')
    plt.xlabel('Number of Nodes')
    plt.ylabel('Accuracy')
    plt.title(f'Accuracy vs Tree Size (Max Depth {depth})')
    plt.legend()
    plt.gca().invert_xaxis()  # Show pruning progression from right to left
    plt.grid()
<A NAME="6"></A><FONT color = #00FF00><A HREF="match47-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.savefig(f"dt_postprune_{depth}.png")





import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score
</FONT>
DATA_PATH = "data/Q1/"
train_file = f"{DATA_PATH}train.csv"
test_file = f"{DATA_PATH}test.csv"
valid_file = f"{DATA_PATH}valid.csv"
train = pd.read_csv(train_file)
test = pd.read_csv(test_file)
valid = pd.read_csv(valid_file)

# One-hot encode categorical features
categorical_cols = train.select_dtypes(include=['object']).columns.drop('income', errors='ignore')
X_train = pd.get_dummies(train.drop('income', axis=1), columns=categorical_cols)
X_test = pd.get_dummies(test.drop('income', axis=1), columns=categorical_cols)
X_val = pd.get_dummies(valid.drop('income', axis=1), columns=categorical_cols)

# Align columns
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)
X_train, X_val = X_train.align(X_val, join='left', axis=1, fill_value=0)

y_train = train['income']
y_test = test['income']
y_val = valid['income']

# Define parameter grid
param_grid = {
    'n_estimators': [50, 150, 250, 350],        # 50 to 350 in steps of 100
    'max_features': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],  # 0.1 to 1.0 in steps of 0.2
    'min_samples_split': [2, 4, 6, 8, 10]       # 2 to 10 in steps of 2
}

# Track best model
best_oob = -1
best_model = None
best_params = {}

# Grid search using OOB accuracy
for params in ParameterGrid(param_grid):
    rf = RandomForestClassifier(
        criterion='entropy',
        oob_score=True,
        bootstrap=True,
        random_state=42,
        **params
    )
    rf.fit(X_train, y_train)
    
    # Update best model if OOB improves
    if rf.oob_score_ &gt; best_oob:
        best_oob = rf.oob_score_
        best_model = rf
        best_params = params

# Calculate final metrics
results = {
    'Training Accuracy': accuracy_score(y_train, best_model.predict(X_train)),
    'OOB Accuracy': best_model.oob_score_,
    'Validation Accuracy': accuracy_score(y_val, best_model.predict(X_val)),
    'Test Accuracy': accuracy_score(y_test, best_model.predict(X_test))
}

# Display results
print("Optimal Parameters:")
print(best_params)
print("\nPerformance Metrics:")
for metric, value in results.items():
    print(f"{metric}: {value:.4f}")




import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# File paths - ADJUST THESE TO MATCH YOUR ACTUAL DATA LOCATION
DATA_PATH = "data/Q1/"
train_file = f"{DATA_PATH}train.csv"
test_file = f"{DATA_PATH}test.csv"
valid_file = f"{DATA_PATH}valid.csv"

def load_and_preprocess():
    try:
        # Load datasets
        train = pd.read_csv(train_file)
        test = pd.read_csv(test_file)
        valid = pd.read_csv(valid_file)

        # One-hot encode categorical features
        categorical_cols = train.select_dtypes(include=['object']).columns.drop('income', errors='ignore')
        X_train = pd.get_dummies(train.drop('income', axis=1), columns=categorical_cols)
        X_test = pd.get_dummies(test.drop('income', axis=1), columns=categorical_cols)
        X_valid = pd.get_dummies(valid.drop('income', axis=1), columns=categorical_cols)

        # Align columns
        X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)
        X_train, X_valid = X_train.align(X_valid, join='left', axis=1, fill_value=0)

        y_train = train['income']
        y_test = test['income']
        y_valid = valid['income']

        return X_train, X_test, X_valid, y_train, y_test, y_valid

    except FileNotFoundError as e:
        print(f"Error: {e}\nPlease verify file paths and data directory structure")
        exit()

# Part (i): Vary max_depth
def experiment_max_depth(X_train, X_test, X_valid, y_train, y_test, y_valid):
    
    depths = [25, 35, 45, 55]
    results = {'depth': [], 'train_acc': [], 'test_acc': [], 'valid_acc': []}
    
    for depth in depths:
        clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth)
        clf.fit(X_train, y_train)
        
        results['depth'].append(depth)
        results['train_acc'].append(accuracy_score(y_train, clf.predict(X_train)))
        results['test_acc'].append(accuracy_score(y_test, clf.predict(X_test)))
        results['valid_acc'].append(accuracy_score(y_valid, clf.predict(X_valid)))
    
    # Plot results
    plt.figure(figsize=(10,6))
    plt.plot(results['depth'], results['train_acc'], 'b-o', label='Train')
    plt.plot(results['depth'], results['test_acc'], 'r-o', label='Test')
<A NAME="5"></A><FONT color = #FF0000><A HREF="match47-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot(results['depth'], results['valid_acc'], 'g-o', label='Validation')
    plt.xlabel('Max Depth')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Max Depth')
    plt.legend()
    plt.grid()
    plt.savefig("dt_scikit_depth.png")
    
    # Find best depth
    best_depth_idx = np.argmax(results['valid_acc'])
</FONT>    print(f"Best depth: {depths[best_depth_idx]} with validation accuracy: {results['valid_acc'][best_depth_idx]:.3f}")
    return depths[best_depth_idx]

# Part (ii): Cost-complexity pruning
def experiment_ccp_alpha(X_train, X_test, X_valid, y_train, y_test, y_valid):
    
    
    # Experiment with ccp_alpha
    ccp_alphas = [0.001, 0.01, 0.1, 0.2]
    results = {'alpha': [], 'train_acc': [], 'test_acc': [], 'valid_acc': []}
    
    for alpha in ccp_alphas:
        clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha)
        clf.fit(X_train, y_train)
        
        results['alpha'].append(alpha)
        results['train_acc'].append(accuracy_score(y_train, clf.predict(X_train)))
        results['test_acc'].append(accuracy_score(y_test, clf.predict(X_test)))
        results['valid_acc'].append(accuracy_score(y_valid, clf.predict(X_valid)))
    
    # Plot results
    plt.figure(figsize=(10,6))
    plt.plot(results['alpha'], results['train_acc'], 'b-o', label='Train')
    plt.plot(results['alpha'], results['test_acc'], 'r-o', label='Test')
    plt.plot(results['alpha'], results['valid_acc'], 'g-o', label='Validation')
    plt.xlabel('CCP Alpha')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Pruning Strength')
    plt.xscale('log')
    plt.legend()
    plt.grid()
    plt.savefig("dt_scikit_alpha.png")
    
    # Find best alpha
    best_alpha_idx = np.argmax(results['valid_acc'])
    print(f"Best alpha: {ccp_alphas[best_alpha_idx]} with validation accuracy: {results['valid_acc'][best_alpha_idx]:.3f}")
    return ccp_alphas[best_alpha_idx]

X_train, X_test, X_valid, y_train, y_test, y_valid = load_and_preprocess()
# Run experiments
best_depth=experiment_max_depth(X_train, X_test, X_valid, y_train, y_test, y_valid)
best_alpha=experiment_ccp_alpha(X_train, X_test, X_valid, y_train, y_test, y_valid)


clf = DecisionTreeClassifier(criterion='entropy', max_depth=best_depth, ccp_alpha=best_alpha)
clf.fit(X_train, y_train)

print(f"Best Model Results: \nTrain Acc: {accuracy_score(y_train, clf.predict(X_train))} \nTest Acc: {accuracy_score(y_test, clf.predict(X_test))} \nValidation Acc: {accuracy_score(y_valid, clf.predict(X_valid))}")



import sys
import os
import pandas as pd
import numpy as np
import cv2
from sklearn.neural_network import MLPClassifier



def load_dataset(train_data_path, test_data_path):

    required_dirs = {
        'train': train_data_path,
        'test': test_data_path
    }
    
    # Check if all required paths exist
    for name, path in required_dirs.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"Required path not found: {path}")

    # Load training data
    train_data = []
    train_labels = []
    train_dir = required_dirs['train']
    
    for label in os.listdir(train_dir):
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                try:
                    img= cv2.imread(img_path)
                    train_data.append(np.array(img))
                    train_labels.append(int(label))
                except Exception as e:
                    print(f"Skipping corrupt image {img_path}: {e}")

    # Load test data with labels
    test_data = []
    test_dir = required_dirs['test']
    
    for img_file in os.listdir(test_dir):
        img_path = os.path.join(test_dir, img_file)
        try:
            img= cv2.imread(img_path)
            test_data.append(np.array(img))
        except Exception as e:
            print(f"Skipping corrupt image {img_path}: {e}")

    return (
        np.array(train_data), 
        np.array(train_labels),
        np.array(test_data)
    )


def split_train_validation(data, labels, validation_ratio=0.2, random_seed=42):
    np.random.seed(random_seed)
    indices = np.random.permutation(len(data))
    split_idx = int(len(data) * (1 - validation_ratio))
    train_indices, val_indices = indices[:split_idx], indices[split_idx:]
    return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]

class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.architecture = [input_size] + hidden_layers + [output_size]
        self.weights = []
        self.biases = []
        
        for i in range(len(self.architecture)-1):
            limit = np.sqrt(6 / (self.architecture[i] + self.architecture[i+1]))
            self.weights.append(np.random.uniform(-limit, limit, 
                                (self.architecture[i], self.architecture[i+1])))
            self.biases.append(np.random.uniform(-limit, limit, self.architecture[i+1]))

    def sigmoid(self, x):
        pos_mask = x &gt;= 0
        neg_mask = ~pos_mask
        z = np.zeros_like(x)
        z[pos_mask] = 1 / (1 + np.exp(-x[pos_mask]))
        z[neg_mask] = np.exp(x[neg_mask]) / (1 + np.exp(x[neg_mask]))
        return z

    def softmax(self, x):
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, X):
        activations = [None] * (len(self.weights) + 1)
        activations[0] = X
        for i in range(len(self.weights)-1):
            z = np.dot(activations[i], self.weights[i]) + self.biases[i]
            activations[i+1] = self.sigmoid(z)
        
        # Output layer
        z = np.dot(activations[-2], self.weights[-1]) + self.biases[-1]
        activations[-1] = self.softmax(z)
        return activations

    def cross_entropy_loss(self, y_true, y_pred):
        epsilon = 1e-12
        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))

    def backpropagation(self, activations, y_true):
        m = y_true.shape[0]
        n = len(self.weights)
        gradients_w = [None]*n
        gradients_b = [None]*n
        
        # Output layer gradient
        delta = (activations[-1] - y_true) / m
        gradients_w[-1]=(np.dot(activations[-2].T, delta))
        gradients_b[-1]=(np.sum(delta, axis=0))
        
        # Hidden layers
        for l in range(n-2, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * activations[l+1] * (1 - activations[l+1])
            gradients_w[l] = np.dot(activations[l].T, delta)
            gradients_b[l] = np.sum(delta, axis=0)
            
        return gradients_w, gradients_b

    def train(self, X_train, y_train, X_val, y_val, batch_size=32, learning_rate=0.01, patience=8, delta=1e-4, max_epochs=1000, method='const'):

        # Convert labels to one-hot
        num_classes = self.architecture[-1]
        y_train = y_train.astype(int)
        y_train_onehot = np.eye(num_classes)[y_train]
        y_val_onehot = np.eye(num_classes)[y_val]

        best_loss = float('inf')
        no_improvement = 0
        best_weights = [w.copy() for w in self.weights]
        best_biases = [b.copy() for b in self.biases]
        loss_history = []

        for epoch in range(max_epochs):
            # Shuffle and train
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train_onehot[permutation]

            if(method=='const'):
                lr=learning_rate
            else:
                lr=learning_rate/np.sqrt(epoch)


            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                activations = self.forward_pass(X_batch)
                grad_w, grad_b = self.backpropagation(activations, y_batch)
                
                for l in range(len(self.weights)):
                    self.weights[l] -= lr * grad_w[l]
                    self.biases[l] -= lr * grad_b[l]

            # Calculate validation loss
            val_activations = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(y_val_onehot, val_activations[-1])
            loss_history.append(val_loss)

            # Early stopping check
            if val_loss &lt; best_loss - delta:
                best_loss = val_loss
<A NAME="0"></A><FONT color = #FF0000><A HREF="match47-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                no_improvement = 0
            else:
                no_improvement += 1

            if no_improvement &gt;= patience:
                print(f"Early stopping at epoch {epoch}")
                break

        # Restore best weights
        self.weights = best_weights
        self.biases = best_biases
</FONT>        return loss_history

    def predict(self, X):
        return np.argmax(self.forward_pass(X)[-1], axis=1)
    

class NeuralNetworkRelu:
    def __init__(self, input_size, hidden_layers, output_size):
        self.architecture = [input_size] + hidden_layers + [output_size]
        self.weights = []
        self.biases = []
        
        for i in range(len(self.architecture)-1):
            limit = np.sqrt(2 / self.architecture[i]) 
            self.weights.append(np.random.normal(0, limit, 
                                (self.architecture[i], self.architecture[i+1])))
            self.biases.append(np.zeros(self.architecture[i+1]))

    def relu(self, x):
        return np.maximum(0, x)

    def softmax(self, x):
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, X):
        activations = [None] * (len(self.weights) + 1)
        activations[0] = X
        for i in range(len(self.weights)-1):
            z = np.dot(activations[i], self.weights[i]) + self.biases[i]
            activations[i+1] = self.relu(z)
        
        # Output layer
        z = np.dot(activations[-2], self.weights[-1]) + self.biases[-1]
        activations[-1] = self.softmax(z)
        return activations

    def cross_entropy_loss(self, y_true, y_pred):
        epsilon = 1e-12
        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))

    def backpropagation(self, activations, y_true):
        m = y_true.shape[0]
        n = len(self.weights)
        gradients_w = [None]*n
        gradients_b = [None]*n
        
        # Output layer gradient
        delta = (activations[-1] - y_true) / m
        gradients_w[-1]=(np.dot(activations[-2].T, delta))
        gradients_b[-1]=(np.sum(delta, axis=0))
        
        # Hidden layers
        for l in range(n-2, -1, -1):
            relu_deriv = (activations[l+1] &gt; 0).astype(float)
            delta = np.dot(delta, self.weights[l+1].T) * relu_deriv
            gradients_w[l] = np.dot(activations[l].T, delta)
            gradients_b[l] = np.sum(delta, axis=0)
            
        return gradients_w, gradients_b

    def train(self, X_train, y_train, X_val, y_val, batch_size=32, learning_rate=0.01, patience=8, delta=1e-4, max_epochs=1000, method='const'):

        # Convert labels to one-hot
        num_classes = self.architecture[-1]
        y_train = y_train.astype(int)
        y_train_onehot = np.eye(num_classes)[y_train]
        y_val_onehot = np.eye(num_classes)[y_val]

        best_loss = float('inf')
        no_improvement = 0
        best_weights = [w.copy() for w in self.weights]
        best_biases = [b.copy() for b in self.biases]
        loss_history = []

        for epoch in range(1,max_epochs):
            # Shuffle and train
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train_onehot[permutation]
            if(method=='const'):
                lr=learning_rate
            else:
                lr=learning_rate/np.sqrt(epoch)

            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                activations = self.forward_pass(X_batch)
                grad_w, grad_b = self.backpropagation(activations, y_batch)
                
                for l in range(len(self.weights)):
                    self.weights[l] -= lr * grad_w[l]
                    self.biases[l] -= lr * grad_b[l]

            # Calculate validation loss
            val_activations = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(y_val_onehot, val_activations[-1])
            loss_history.append(val_loss)

            # Early stopping check
            if val_loss &lt; best_loss - delta:
                best_loss = val_loss
<A NAME="1"></A><FONT color = #00FF00><A HREF="match47-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                no_improvement = 0
            else:
                no_improvement += 1

            if no_improvement &gt;= patience:
                print(f"Early stopping at epoch {epoch}\n")
                break

        # Restore best weights
        self.weights = best_weights
        self.biases = best_biases
</FONT>        return loss_history

    def predict(self, X):
        return np.argmax(self.forward_pass(X)[-1], axis=1)
    

def main():
    # Ensure correct number of arguments are provided
    if len(sys.argv) != 5:
        print("Usage: neural_network.py &lt;train_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;")
        sys.exit(1)
    
    # Parse command-line arguments
    train_data_path = sys.argv[1]
    test_data_path = sys.argv[2]
    output_folder_path = sys.argv[3]
    question_part = sys.argv[4]

    # Validate question part
    if question_part not in ['b', 'c', 'd', 'e', 'f']:
        print("Error: question_part must be one of 'b', 'c', 'd', 'e', or 'f'.")
        sys.exit(1)

    # Create output directory if it doesn't exist
    os.makedirs(output_folder_path, exist_ok=True)
    out_file_path = os.path.join(output_folder_path, f'prediction_{question_part}.csv')

    X_train, y_train, X_test = load_dataset(train_data_path, test_data_path)
    input_size = 28 * 28 * 3  
    num_classes = 43

    X_train = X_train.reshape(-1, input_size).astype(np.float32) / 255.0
    X_test = X_test.reshape(-1, input_size).astype(np.float32) / 255.0
    y_train = y_train.astype(int)

    X_split_train, y_split_train, X_split_val, y_split_val = split_train_validation(X_train,y_train,validation_ratio=0.2)
 

    if question_part == 'b':
               
        nn = NeuralNetwork(input_size=X_train.shape[1],hidden_layers=[5],output_size=num_classes)
        nn.train( X_split_train, y_split_train, X_split_val, y_split_val, batch_size=32, learning_rate=0.01, patience=10, delta=1e-3, max_epochs=200)
    
        test_preds = nn.predict(X_test)
        pd.DataFrame({'prediction':test_preds}).to_csv(out_file_path, index=False)

    elif question_part == 'c':
        
        nn = NeuralNetwork(input_size=X_train.shape[1],hidden_layers=[5],output_size=num_classes)
        nn.train( X_split_train, y_split_train, X_split_val, y_split_val, batch_size=32, learning_rate=0.01, patience=10, delta=1e-4, max_epochs=200)
    
        test_preds = nn.predict(X_test)
        pd.DataFrame({'prediction':test_preds}).to_csv(out_file_path, index=False)

    elif question_part == 'd':

        nn = NeuralNetwork(input_size=X_train.shape[1],hidden_layers=[5],output_size=num_classes)
        nn.train( X_split_train, y_split_train, X_split_val, y_split_val, batch_size=32, learning_rate=0.01, patience=15, delta=1e-3, max_epochs=200, method='adaptive')
    
        test_preds = nn.predict(X_test)
        pd.DataFrame({'prediction':test_preds}).to_csv(out_file_path, index=False)

    elif question_part == 'e':
        
        nn = NeuralNetworkRelu(input_size=X_train.shape[1],hidden_layers=[5],output_size=num_classes)
        nn.train( X_split_train, y_split_train, X_split_val, y_split_val, batch_size=32, learning_rate=0.01, patience=15, delta=1e-3, max_epochs=200, method='adaptive')
    
        test_preds = nn.predict(X_test)
        pd.DataFrame({'prediction':test_preds}).to_csv(out_file_path, index=False)

    else:

        mlp = MLPClassifier(hidden_layer_sizes=[5], activation='relu', solver='sgd', alpha=0, batch_size=32, learning_rate='invscaling', learning_rate_init=0.01, early_stopping=True, validation_fraction=0.2, max_iter=200, random_state=42)
        mlp.fit(X_train, y_train)

        test_preds = mlp.predict(X_test)
        pd.DataFrame({'prediction':test_preds}).to_csv(out_file_path, index=False)


if __name__ == "__main__":
    main()




import os
import pandas as pd
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

# Define base path (adjust if needed)
BASE_PATH = 'data/Q2/'



def load_dataset():
    """Load training and test data with labels"""
    # Validate directory structure
    required_dirs = {
        'train': os.path.join(BASE_PATH, 'train'),
        'test': os.path.join(BASE_PATH, 'test'),
        'test_labels': os.path.join(BASE_PATH, 'test_labels.csv')
    }
    
    # Check if all required paths exist
    for name, path in required_dirs.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"Required path not found: {path}")

    # Load training data
    train_data = []
    train_labels = []
    train_dir = required_dirs['train']
    
    for label in os.listdir(train_dir):
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                try:
                    img= cv2.imread(img_path)
                    train_data.append(np.array(img))
                    train_labels.append(int(label))
                except Exception as e:
                    print(f"Skipping corrupt image {img_path}: {e}")

    # Load test data with labels
    test_data = []
    test_labels = []
    test_dir = required_dirs['test']
    
    # Read test labels from CSV
    labels_df = pd.read_csv(required_dirs['test_labels'])
    label_map = dict(zip(labels_df['image'], labels_df['label']))
    
    for img_file in os.listdir(test_dir):
        img_path = os.path.join(test_dir, img_file)
        try:
            img= cv2.imread(img_path)
            test_data.append(np.array(img))
            test_labels.append(label_map.get(img_file, -1))  # -1 for missing labels
        except Exception as e:
            print(f"Skipping corrupt image {img_path}: {e}")

    return (
        np.array(train_data), 
        np.array(train_labels),
        np.array(test_data),
        np.array(test_labels)
    )


def split_train_validation(data, labels, validation_ratio=0.2, random_seed=42):
    np.random.seed(random_seed)
    indices = np.random.permutation(len(data))
    split_idx = int(len(data) * (1 - validation_ratio))
    train_indices, val_indices = indices[:split_idx], indices[split_idx:]
    return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]

class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.architecture = [input_size] + hidden_layers + [output_size]
        self.weights = []
        self.biases = []
        
        # Xavier/Glorot initialization
        for i in range(len(self.architecture)-1):
            limit = np.sqrt(6 / (self.architecture[i] + self.architecture[i+1]))
            self.weights.append(np.random.uniform(-limit, limit, 
                                (self.architecture[i], self.architecture[i+1])))
            self.biases.append(np.random.uniform(-limit, limit, self.architecture[i+1]))

    def sigmoid(self, x):
        """Numerically stable sigmoid"""
        pos_mask = x &gt;= 0
        neg_mask = ~pos_mask
        z = np.zeros_like(x)
        z[pos_mask] = 1 / (1 + np.exp(-x[pos_mask]))
        z[neg_mask] = np.exp(x[neg_mask]) / (1 + np.exp(x[neg_mask]))
        return z

    def softmax(self, x):
        """Numerically stable softmax"""
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, X):
        activations = [None] * (len(self.weights) + 1)
        activations[0] = X
        for i in range(len(self.weights)-1):
            z = np.dot(activations[i], self.weights[i]) + self.biases[i]
            activations[i+1] = self.sigmoid(z)
        
        # Output layer
        z = np.dot(activations[-2], self.weights[-1]) + self.biases[-1]
        activations[-1] = self.softmax(z)
        return activations

    def cross_entropy_loss(self, y_true, y_pred):
        epsilon = 1e-12
        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))

    def backpropagation(self, activations, y_true):
        m = y_true.shape[0]
        n = len(self.weights)
        gradients_w = [None]*n
        gradients_b = [None]*n
        
        # Output layer gradient
        delta = (activations[-1] - y_true) / m
        gradients_w[-1]=(np.dot(activations[-2].T, delta))
        gradients_b[-1]=(np.sum(delta, axis=0))
        
        # Hidden layers
        for l in range(n-2, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * activations[l+1] * (1 - activations[l+1])
            gradients_w[l] = np.dot(activations[l].T, delta)
            gradients_b[l] = np.sum(delta, axis=0)
            
        return gradients_w, gradients_b

    def train(self, X_train, y_train, X_val, y_val, f,
            batch_size=32, learning_rate=0.01, 
            patience=8, delta=1e-4, max_epochs=1000):
        """
        Modified training with early stopping
        """
        # Convert labels to one-hot
        num_classes = self.architecture[-1]
        y_train = y_train.astype(int)
        y_train_onehot = np.eye(num_classes)[y_train]
        y_val_onehot = np.eye(num_classes)[y_val]

        best_loss = float('inf')
        no_improvement = 0
        best_weights = [w.copy() for w in self.weights]
        best_biases = [b.copy() for b in self.biases]
        loss_history = []

        for epoch in range(max_epochs):
            # Shuffle and train
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train_onehot[permutation]

            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                activations = self.forward_pass(X_batch)
                grad_w, grad_b = self.backpropagation(activations, y_batch)
                
                for l in range(len(self.weights)):
                    self.weights[l] -= learning_rate * grad_w[l]
                    self.biases[l] -= learning_rate * grad_b[l]

            # Calculate validation loss
            val_activations = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(y_val_onehot, val_activations[-1])
            loss_history.append(val_loss)

            # Early stopping check
            if val_loss &lt; best_loss - delta:
                best_loss = val_loss
                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                no_improvement = 0
            else:
                no_improvement += 1

            # Print training progress
            if epoch % 10 == 0:
                y_pred = self.forward_pass(X_train)[-1]
                train_loss = self.cross_entropy_loss(y_train_onehot, y_pred)
                train_acc = np.mean(np.argmax(y_pred, axis=1) == y_train)
                f.write(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc= {train_acc}, Val Loss={val_loss:.4f}\n")
                f.flush()

            if no_improvement &gt;= patience:
                f.write(f"\nEarly stopping at epoch {epoch}\n")
                f.flush()
                break

        # Restore best weights
        self.weights = best_weights
        self.biases = best_biases
        return loss_history

    def predict(self, X):
        return np.argmax(self.forward_pass(X)[-1], axis=1)
    

f=open('nn_basic_results_short2.txt', 'w')
f.write("Loading the Dataset\n")
f.flush()
    
X_train, y_train, X_test, y_test =load_dataset()
input_size = 28 * 28 * 3  # Flattened image dimensions (28x28x3)
num_classes = 43  # Should match your dataset's class count

# Normalize and flatten the input data
X_train = X_train.reshape(-1, input_size).astype(np.float32) / 255.0
X_test = X_test.reshape(-1, input_size).astype(np.float32) / 255.0

# Verify label dimensions
y_train = y_train.astype(int)
y_test = y_test.astype(int)

X_train_full, y_train_full, X_val_full, y_val_full = split_train_validation(X_train,y_train,validation_ratio=0.2)

# Results storage
hidden_layer_sizes = [[512],[512,256],[512,256,128],[512,256,128,64]]
results = {
    'hidden_units': [],
    'train_f1': [],
    'test_f1': [],
    'class_metrics': []
}

# Run experiments
for hidden_units in hidden_layer_sizes:
    f.write(f"\nTraining with {hidden_units} hidden units...\n")
    
    # Initialize and train network
    nn = NeuralNetwork(
        input_size=X_train.shape[1],
        hidden_layers=hidden_units,
        output_size=len(np.unique(y_train))
    )
    nn.train( X_train_full, y_train_full, X_val_full, y_val_full, 
            batch_size=32, learning_rate=0.01, 
            patience=15, delta=1e-3, max_epochs=200, f=f)
    
    # Predictions
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    
    # Calculate metrics
    train_metrics = {
        'precision': precision_score(y_train, train_preds, average=None, zero_division=0),
        'recall': recall_score(y_train, train_preds, average=None, zero_division=0),
        'f1': f1_score(y_train, train_preds, average=None, zero_division=0)
    }
    
    test_metrics = {
        'precision': precision_score(y_test, test_preds, average=None, zero_division=0),
        'recall': recall_score(y_test, test_preds, average=None, zero_division=0),
        'f1': f1_score(y_test, test_preds, average=None, zero_division=0)
    }
    
    # Store results
    hu_str=''
    for x in hidden_units:
        hu_str+=str(x)+","
    results['hidden_units'].append(f'[{hu_str }]')
    results['train_f1'].append(np.mean(train_metrics['f1']))
    results['test_f1'].append(np.mean(test_metrics['f1']))
    results['class_metrics'].append({
        'train': train_metrics,
        'test': test_metrics
    })
    
    # Print per-class metrics
    f.write(f"\nHidden Units: {hidden_units}\n")
    f.write("Train Data Metrics:\n")
    for cls in range(len(train_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {train_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {train_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {train_metrics['f1'][cls]:.3f}\n")
    
    f.write("\nTest Data Metrics:")
    for cls in range(len(test_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {test_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {test_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {test_metrics['f1'][cls]:.3f}\n")
    f.flush()

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(results['hidden_units'], results['train_f1'], 'b-o', label='Train')
plt.plot(results['hidden_units'], results['test_f1'], 'r-o', label='Test')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Average F1 Score')
plt.title('Model Performance vs Hidden Layer Size')
plt.legend()
plt.grid()
plt.show()



import os
import pandas as pd
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

# Define base path (adjust if needed)
BASE_PATH = 'data/Q2/'



def load_dataset():
    """Load training and test data with labels"""
    # Validate directory structure
    required_dirs = {
        'train': os.path.join(BASE_PATH, 'train'),
        'test': os.path.join(BASE_PATH, 'test'),
        'test_labels': os.path.join(BASE_PATH, 'test_labels.csv')
    }
    
    # Check if all required paths exist
    for name, path in required_dirs.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"Required path not found: {path}")

    # Load training data
    train_data = []
    train_labels = []
    train_dir = required_dirs['train']
    
    for label in os.listdir(train_dir):
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                try:
                    img= cv2.imread(img_path)
                    train_data.append(np.array(img))
                    train_labels.append(int(label))
                except Exception as e:
                    print(f"Skipping corrupt image {img_path}: {e}")

    # Load test data with labels
    test_data = []
    test_labels = []
    test_dir = required_dirs['test']
    
    # Read test labels from CSV
    labels_df = pd.read_csv(required_dirs['test_labels'])
    label_map = dict(zip(labels_df['image'], labels_df['label']))
    
    for img_file in os.listdir(test_dir):
        img_path = os.path.join(test_dir, img_file)
        try:
            img= cv2.imread(img_path)
            test_data.append(np.array(img))
            test_labels.append(label_map.get(img_file, -1))  # -1 for missing labels
        except Exception as e:
            print(f"Skipping corrupt image {img_path}: {e}")

    return (
        np.array(train_data), 
        np.array(train_labels),
        np.array(test_data),
        np.array(test_labels)
    )


def split_train_validation(data, labels, validation_ratio=0.2, random_seed=42):
    np.random.seed(random_seed)
    indices = np.random.permutation(len(data))
    split_idx = int(len(data) * (1 - validation_ratio))
    train_indices, val_indices = indices[:split_idx], indices[split_idx:]
    return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]
class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.architecture = [input_size] + hidden_layers + [output_size]
        self.weights = []
        self.biases = []
        
        # Xavier/Glorot initialization
        for i in range(len(self.architecture)-1):
            limit = np.sqrt(6 / (self.architecture[i] + self.architecture[i+1]))
            self.weights.append(np.random.uniform(-limit, limit, 
                                (self.architecture[i], self.architecture[i+1])))
            self.biases.append(np.random.uniform(-limit, limit, self.architecture[i+1]))

    def sigmoid(self, x):
        """Numerically stable sigmoid"""
        pos_mask = x &gt;= 0
        neg_mask = ~pos_mask
        z = np.zeros_like(x)
        z[pos_mask] = 1 / (1 + np.exp(-x[pos_mask]))
        z[neg_mask] = np.exp(x[neg_mask]) / (1 + np.exp(x[neg_mask]))
        return z

    def softmax(self, x):
        """Numerically stable softmax"""
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, X):
        activations = [None] * (len(self.weights) + 1)
        activations[0] = X
        for i in range(len(self.weights)-1):
            z = np.dot(activations[i], self.weights[i]) + self.biases[i]
            activations[i+1] = self.sigmoid(z)
        
        # Output layer
        z = np.dot(activations[-2], self.weights[-1]) + self.biases[-1]
        activations[-1] = self.softmax(z)
        return activations

    def cross_entropy_loss(self, y_true, y_pred):
        epsilon = 1e-12
        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))

    def backpropagation(self, activations, y_true):
        m = y_true.shape[0]
        n = len(self.weights)
        gradients_w = [None]*n
        gradients_b = [None]*n
        
        # Output layer gradient
        delta = (activations[-1] - y_true) / m
        gradients_w[-1]=(np.dot(activations[-2].T, delta))
        gradients_b[-1]=(np.sum(delta, axis=0))
        
        # Hidden layers
        for l in range(n-2, -1, -1):
            delta = np.dot(delta, self.weights[l+1].T) * activations[l+1] * (1 - activations[l+1])
            gradients_w[l] = np.dot(activations[l].T, delta)
            gradients_b[l] = np.sum(delta, axis=0)
            
        return gradients_w, gradients_b

    def train(self, X_train, y_train, X_val, y_val, f,batch_size=32, learning_rate=0.01, patience=8, delta=1e-4, max_epochs=1000):
        """
        Modified training with early stopping
        """
        # Convert labels to one-hot
        num_classes = self.architecture[-1]
        y_train = y_train.astype(int)
        y_train_onehot = np.eye(num_classes)[y_train]
        y_val_onehot = np.eye(num_classes)[y_val]

        best_loss = float('inf')
        no_improvement = 0
        best_weights = [w.copy() for w in self.weights]
        best_biases = [b.copy() for b in self.biases]
        loss_history = []

        for epoch in range(1, max_epochs):
            # Shuffle and train
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train_onehot[permutation]
            lr=learning_rate/np.sqrt(epoch)

            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                activations = self.forward_pass(X_batch)
                grad_w, grad_b = self.backpropagation(activations, y_batch)
                for l in range(len(self.weights)):
                    self.weights[l] -= lr * grad_w[l]
                    self.biases[l] -= lr * grad_b[l]

            # Calculate validation loss
            val_activations = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(y_val_onehot, val_activations[-1])
            loss_history.append(val_loss)

            # Early stopping check
            if val_loss &lt; best_loss - delta:
                best_loss = val_loss
                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                no_improvement = 0
            else:
                no_improvement += 1

            # Print training progress
            if epoch % 10 == 0:
                y_pred = self.forward_pass(X_train)[-1]
                train_loss = self.cross_entropy_loss(y_train_onehot, y_pred)
                train_acc = np.mean(np.argmax(y_pred, axis=1) == y_train)
                f.write(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc= {train_acc}, Val Loss={val_loss:.4f}\n")
                f.flush()

            if no_improvement &gt;= patience:
                f.write(f"\nEarly stopping at epoch {epoch}\n")
                f.flush()
                break

        # Restore best weights
        self.weights = best_weights
        self.biases = best_biases
        return loss_history

    def predict(self, X):
        return np.argmax(self.forward_pass(X)[-1], axis=1)
    

f=open('nn_lr_results2.txt', 'w')
f.write("Loading the Dataset\n")
f.flush()
    
X_train, y_train, X_test, y_test =load_dataset()
input_size = 28 * 28 * 3  # Flattened image dimensions (28x28x3)
num_classes = 43  # Should match your dataset's class count

# Normalize and flatten the input data
X_train = X_train.reshape(-1, input_size).astype(np.float32) /255.0
X_test = X_test.reshape(-1, input_size).astype(np.float32) /255.0

# Verify label dimensions
y_train = y_train.astype(int)
y_test = y_test.astype(int)

X_train_full, y_train_full, X_val_full, y_val_full = split_train_validation(X_train,y_train,validation_ratio=0.2)

# Example Usage
input_size = 2352  # Flattened image size (28x28x3)
num_classes = 43

# Results storage
hidden_layer_sizes = [[512],[512,256],[512,256,128],[512,256,128,64]]
results = {
    'hidden_units': [],
    'train_f1': [],
    'test_f1': [],
    'class_metrics': []
}

# Run experiments
for hidden_units in hidden_layer_sizes:
    f.write(f"\nTraining with {hidden_units} hidden units...\n")
    
    # Initialize and train network
    nn = NeuralNetwork(
        input_size=X_train.shape[1],
        hidden_layers=hidden_units,
        output_size=len(np.unique(y_train))
    )
    nn.train( X_train_full, y_train_full, X_val_full, y_val_full, 
            batch_size=32, learning_rate=0.01, 
            patience=15, delta=1e-3, max_epochs=200, f=f)
    
    # Predictions
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    
    # Calculate metrics
    train_metrics = {
        'precision': precision_score(y_train, train_preds, average=None, zero_division=0),
        'recall': recall_score(y_train, train_preds, average=None, zero_division=0),
        'f1': f1_score(y_train, train_preds, average=None, zero_division=0)
    }
    
    test_metrics = {
        'precision': precision_score(y_test, test_preds, average=None, zero_division=0),
        'recall': recall_score(y_test, test_preds, average=None, zero_division=0),
        'f1': f1_score(y_test, test_preds, average=None, zero_division=0)
    }
    
    # Store results
    hu_str=''
    for x in hidden_units:
        hu_str+=str(x)+","
    results['hidden_units'].append(f'[{hu_str }]')
    results['train_f1'].append(np.mean(train_metrics['f1']))
    results['test_f1'].append(np.mean(test_metrics['f1']))
    results['class_metrics'].append({
        'train': train_metrics,
        'test': test_metrics
    })
    
    # Print per-class metrics
    f.write(f"\nHidden Units: {hidden_units}\n")
    f.write("Train Data Metrics:\n")
    for cls in range(len(train_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {train_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {train_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {train_metrics['f1'][cls]:.3f}\n")
    
    f.write("\nTest Data Metrics:")
    for cls in range(len(test_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {test_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {test_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {test_metrics['f1'][cls]:.3f}\n")
    f.flush()

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(results['hidden_units'], results['train_f1'], 'b-o', label='Train')
plt.plot(results['hidden_units'], results['test_f1'], 'r-o', label='Test')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Average F1 Score')
plt.title('Model Performance vs Hidden Layer Size')
plt.legend()
plt.grid()
plt.show()



import os
import pandas as pd
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

# Define base path (adjust if needed)
BASE_PATH = 'data/Q2/'



def load_dataset():
    """Load training and test data with labels"""
    # Validate directory structure
    required_dirs = {
        'train': os.path.join(BASE_PATH, 'train'),
        'test': os.path.join(BASE_PATH, 'test'),
        'test_labels': os.path.join(BASE_PATH, 'test_labels.csv')
    }
    
    # Check if all required paths exist
    for name, path in required_dirs.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"Required path not found: {path}")

    # Load training data
    train_data = []
    train_labels = []
    train_dir = required_dirs['train']
    
    for label in os.listdir(train_dir):
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                try:
                    img= cv2.imread(img_path)
                    train_data.append(np.array(img))
                    train_labels.append(int(label))
                except Exception as e:
                    print(f"Skipping corrupt image {img_path}: {e}")

    # Load test data with labels
    test_data = []
    test_labels = []
    test_dir = required_dirs['test']
    
    # Read test labels from CSV
    labels_df = pd.read_csv(required_dirs['test_labels'])
    label_map = dict(zip(labels_df['image'], labels_df['label']))
    
    for img_file in os.listdir(test_dir):
        img_path = os.path.join(test_dir, img_file)
        try:
            img= cv2.imread(img_path)
            test_data.append(np.array(img))
            test_labels.append(label_map.get(img_file, -1))  # -1 for missing labels
        except Exception as e:
            print(f"Skipping corrupt image {img_path}: {e}")

    return (
        np.array(train_data), 
        np.array(train_labels),
        np.array(test_data),
        np.array(test_labels)
    )


def split_train_validation(data, labels, validation_ratio=0.2, random_seed=42):
    np.random.seed(random_seed)
    indices = np.random.permutation(len(data))
    split_idx = int(len(data) * (1 - validation_ratio))
    train_indices, val_indices = indices[:split_idx], indices[split_idx:]
    return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]

class NeuralNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.architecture = [input_size] + hidden_layers + [output_size]
        self.weights = []
        self.biases = []
        
        for i in range(len(self.architecture)-1):
            limit = np.sqrt(2 / self.architecture[i])  # He initialization
            self.weights.append(np.random.normal(0, limit, 
                                (self.architecture[i], self.architecture[i+1])))
            self.biases.append(np.zeros(self.architecture[i+1]))

    def relu(self, x):
        """Rectified Linear Unit activation"""
        return np.maximum(0, x)

    def softmax(self, x):
        """Numerically stable softmax"""
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_pass(self, X):
        activations = [None] * (len(self.weights) + 1)
        activations[0] = X
        for i in range(len(self.weights)-1):
            z = np.dot(activations[i], self.weights[i]) + self.biases[i]
            activations[i+1] = self.relu(z)
        
        # Output layer
        z = np.dot(activations[-2], self.weights[-1]) + self.biases[-1]
        activations[-1] = self.softmax(z)
        return activations

    def cross_entropy_loss(self, y_true, y_pred):
        epsilon = 1e-12
        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))

    def backpropagation(self, activations, y_true):
        m = y_true.shape[0]
        n = len(self.weights)
        gradients_w = [None]*n
        gradients_b = [None]*n
        
        # Output layer gradient
        delta = (activations[-1] - y_true) / m
        gradients_w[-1]=(np.dot(activations[-2].T, delta))
        gradients_b[-1]=(np.sum(delta, axis=0))
        
        # Hidden layers
        for l in range(n-2, -1, -1):
            relu_deriv = (activations[l+1] &gt; 0).astype(float)
            delta = np.dot(delta, self.weights[l+1].T) * relu_deriv
            gradients_w[l] = np.dot(activations[l].T, delta)
            gradients_b[l] = np.sum(delta, axis=0)
            
        return gradients_w, gradients_b

    def train(self, X_train, y_train, X_val, y_val, f,
            batch_size=32, learning_rate=0.01, 
            patience=8, delta=1e-4, max_epochs=1000):
        """
        Modified training with early stopping
        """
        # Convert labels to one-hot
        num_classes = self.architecture[-1]
        y_train = y_train.astype(int)
        y_train_onehot = np.eye(num_classes)[y_train]
        y_val_onehot = np.eye(num_classes)[y_val]

        best_loss = float('inf')
        no_improvement = 0
        best_weights = [w.copy() for w in self.weights]
        best_biases = [b.copy() for b in self.biases]
        loss_history = []

        for epoch in range(1,max_epochs):
            # Shuffle and train
            permutation = np.random.permutation(X_train.shape[0])
            X_shuffled = X_train[permutation]
            y_shuffled = y_train_onehot[permutation]
            lr=learning_rate/np.sqrt(epoch)

            for i in range(0, X_train.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                activations = self.forward_pass(X_batch)
                grad_w, grad_b = self.backpropagation(activations, y_batch)
                
                for l in range(len(self.weights)):
                    self.weights[l] -= lr * grad_w[l]
                    self.biases[l] -= lr * grad_b[l]

            # Calculate validation loss
            val_activations = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(y_val_onehot, val_activations[-1])
            loss_history.append(val_loss)

            # Early stopping check
            if val_loss &lt; best_loss - delta:
                best_loss = val_loss
                best_weights = [w.copy() for w in self.weights]
                best_biases = [b.copy() for b in self.biases]
                no_improvement = 0
            else:
                no_improvement += 1

            # Print training progress
            if epoch % 10 == 0:
                y_pred = self.forward_pass(X_train)[-1]
                train_loss = self.cross_entropy_loss(y_train_onehot, y_pred)
                train_acc = np.mean(np.argmax(y_pred, axis=1) == y_train)
                f.write(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc= {train_acc}, Val Loss={val_loss:.4f}\n")
                f.flush()

            if no_improvement &gt;= patience:
                f.write(f"\nEarly stopping at epoch {epoch}\n")
                f.flush()
                break

        # Restore best weights
        self.weights = best_weights
        self.biases = best_biases
        return loss_history

    def predict(self, X):
        return np.argmax(self.forward_pass(X)[-1], axis=1)
    

f=open('nn_relu_results_short.txt', 'w')
f.write("Loading the Dataset\n")
f.flush()
    
X_train, y_train, X_test, y_test =load_dataset()
input_size = 28 * 28 * 3  # Flattened image dimensions (28x28x3)
num_classes = 43  # Should match your dataset's class count

# Normalize and flatten the input data
X_train = X_train.reshape(-1, input_size).astype(np.float32) / 255.0
X_test = X_test.reshape(-1, input_size).astype(np.float32) / 255.0

# Verify label dimensions
y_train = y_train.astype(int)
y_test = y_test.astype(int)

X_train_full, y_train_full, X_val_full, y_val_full = split_train_validation(X_train,y_train,validation_ratio=0.2)

# Example Usage
input_size = 2352  # Flattened image size (28x28x3)
num_classes = 43

# Results storage
hidden_layer_sizes = [[512],[512,256],[512,256,128],[512,256,128,64]]
results = {
    'hidden_units': [],
    'train_f1': [],
    'test_f1': [],
    'class_metrics': []
}

# Run experiments
for hidden_units in hidden_layer_sizes:
    f.write(f"\nTraining with {hidden_units} hidden units...\n")
    
    # Initialize and train network
    nn = NeuralNetwork(
        input_size=X_train.shape[1],
        hidden_layers=hidden_units,
        output_size=len(np.unique(y_train))
    )
    nn.train( X_train_full, y_train_full, X_val_full, y_val_full, 
            batch_size=32, learning_rate=0.01, 
            patience=15, delta=1e-3, max_epochs=200, f=f)
    
    # Predictions
    train_preds = nn.predict(X_train)
    test_preds = nn.predict(X_test)
    
    # Calculate metrics
    train_metrics = {
        'precision': precision_score(y_train, train_preds, average=None, zero_division=0),
        'recall': recall_score(y_train, train_preds, average=None, zero_division=0),
        'f1': f1_score(y_train, train_preds, average=None, zero_division=0)
    }
    
    test_metrics = {
        'precision': precision_score(y_test, test_preds, average=None, zero_division=0),
        'recall': recall_score(y_test, test_preds, average=None, zero_division=0),
        'f1': f1_score(y_test, test_preds, average=None, zero_division=0)
    }
    
    # Store results
    hu_str=''
    for x in hidden_units:
        hu_str+=str(x)+","
    results['hidden_units'].append(f'[{hu_str }]')
    results['train_f1'].append(np.mean(train_metrics['f1']))
    results['test_f1'].append(np.mean(test_metrics['f1']))
    results['class_metrics'].append({
        'train': train_metrics,
        'test': test_metrics
    })
    
    # Print per-class metrics
    f.write(f"\nHidden Units: {hidden_units}\n")
    f.write("Train Data Metrics:\n")
    for cls in range(len(train_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {train_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {train_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {train_metrics['f1'][cls]:.3f}\n")
    
    f.write("\nTest Data Metrics:")
    for cls in range(len(test_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {test_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {test_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {test_metrics['f1'][cls]:.3f}\n")
    f.flush()

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(results['hidden_units'], results['train_f1'], 'b-o', label='Train')
plt.plot(results['hidden_units'], results['test_f1'], 'r-o', label='Test')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Average F1 Score')
plt.title('Model Performance vs Hidden Layer Size')
plt.legend()
plt.grid()
plt.show()



import os
import pandas as pd
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.neural_network import MLPClassifier


# Define base path (adjust if needed)
BASE_PATH = 'data/Q2/'



def load_dataset():
    """Load training and test data with labels"""
    # Validate directory structure
    required_dirs = {
        'train': os.path.join(BASE_PATH, 'train'),
        'test': os.path.join(BASE_PATH, 'test'),
        'test_labels': os.path.join(BASE_PATH, 'test_labels.csv')
    }
    
    # Check if all required paths exist
    for name, path in required_dirs.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"Required path not found: {path}")

    # Load training data
    train_data = []
    train_labels = []
    train_dir = required_dirs['train']
    
    for label in os.listdir(train_dir):
        label_path = os.path.join(train_dir, label)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                try:
                    img= cv2.imread(img_path)
                    train_data.append(np.array(img))
                    train_labels.append(int(label))
                except Exception as e:
                    print(f"Skipping corrupt image {img_path}: {e}")

    # Load test data with labels
    test_data = []
    test_labels = []
    test_dir = required_dirs['test']
    
    # Read test labels from CSV
    labels_df = pd.read_csv(required_dirs['test_labels'])
    label_map = dict(zip(labels_df['image'], labels_df['label']))
    
    for img_file in os.listdir(test_dir):
        img_path = os.path.join(test_dir, img_file)
        try:
            img= cv2.imread(img_path)
            test_data.append(np.array(img))
            test_labels.append(label_map.get(img_file, -1))  # -1 for missing labels
        except Exception as e:
            print(f"Skipping corrupt image {img_path}: {e}")

    return (
        np.array(train_data), 
        np.array(train_labels),
        np.array(test_data),
        np.array(test_labels)
    )

    

f=open('nn_scikit_results.txt', 'w')
f.write("Loading the Dataset\n")
f.flush()
    
X_train, y_train, X_test, y_test =load_dataset()
input_size = 28 * 28 * 3  # Flattened image dimensions (28x28x3)
num_classes = 43  # Should match your dataset's class count

# Normalize and flatten the input data
X_train = X_train.reshape(-1, input_size).astype(np.float32) / 255.0
X_test = X_test.reshape(-1, input_size).astype(np.float32) / 255.0

# Verify label dimensions
y_train = y_train.astype(int)
y_test = y_test.astype(int)

# Example Usage
input_size = 2352  # Flattened image size (28x28x3)
num_classes = 43

# Results storage
hidden_layer_sizes = [[512],[512,256],[512,256,128],[512,256,128,64]]
results = {
    'hidden_units': [],
    'train_f1': [],
    'test_f1': [],
    'class_metrics': []
}

# Run experiments
for hidden_units in hidden_layer_sizes:
    f.write(f"\nTraining with {hidden_units} hidden units...\n")
    
    # Initialize and train network
    mlp = MLPClassifier(
            hidden_layer_sizes=hidden_units,
            activation='relu',
            solver='sgd',
            alpha=0,
            batch_size=32,
            learning_rate='invscaling',
            learning_rate_init=0.01,
            early_stopping=True,
            validation_fraction=0.2,
            max_iter=200,
            random_state=42
        )
    
    mlp.fit(X_train, y_train)
    
    # Predictions
    train_preds = mlp.predict(X_train)
    test_preds = mlp.predict(X_test)
    
    # Calculate metrics
    train_metrics = {
        'precision': precision_score(y_train, train_preds, average=None, zero_division=0),
        'recall': recall_score(y_train, train_preds, average=None, zero_division=0),
        'f1': f1_score(y_train, train_preds, average=None, zero_division=0)
    }
    
    test_metrics = {
        'precision': precision_score(y_test, test_preds, average=None, zero_division=0),
        'recall': recall_score(y_test, test_preds, average=None, zero_division=0),
        'f1': f1_score(y_test, test_preds, average=None, zero_division=0)
    }
    
    # Store results
    hu_str=''
    for x in hidden_units:
        hu_str+=str(x)+","
    results['hidden_units'].append(f'[{hu_str }]')
    results['train_f1'].append(np.mean(train_metrics['f1']))
    results['test_f1'].append(np.mean(test_metrics['f1']))
    results['class_metrics'].append({
        'train': train_metrics,
        'test': test_metrics
    })
    
    # Print per-class metrics
    f.write(f"\nHidden Units: {hidden_units}\n")
    f.write("Train Data Metrics:\n")
    for cls in range(len(train_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {train_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {train_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {train_metrics['f1'][cls]:.3f}\n")
    
    f.write("\nTest Data Metrics:")
    for cls in range(len(test_metrics['f1'])):
        f.write(f"Class {cls}:\n")
        f.write(f"  Precision: {test_metrics['precision'][cls]:.3f}\n")
        f.write(f"  Recall: {test_metrics['recall'][cls]:.3f}\n")
        f.write(f"  F1: {test_metrics['f1'][cls]:.3f}\n")
    f.flush()

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(results['hidden_units'], results['train_f1'], 'b-o', label='Train')
plt.plot(results['hidden_units'], results['test_f1'], 'r-o', label='Test')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Average F1 Score')
plt.title('Model Performance vs Hidden Layer Size')
plt.legend()
plt.grid()
plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
