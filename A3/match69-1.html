<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_G8GO6.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_QCOFJ.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[98]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')
from decision_tree import DecisionTree


# # Imports and data
# 

# In[49]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import time


# In[50]:


test_data = pd.read_csv("../data/Q1/test.csv")
train_data = pd.read_csv("../data/Q1/train.csv")
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match69-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

valid_data = pd.read_csv("../data/Q1/valid.csv")


# In[51]:


X_train = train_data.drop("income", axis=1)
y_train = train_data["income"]

X_valid = valid_data.drop("income", axis=1)
y_valid = valid_data["income"]
</FONT>
X_test = test_data.drop("income", axis=1)
y_test = test_data["income"]


# # Part 1)

# ## Solution

# In[17]:


train_accuracy = []
validation_accuracy = []
test_accuracy = []
#tree_heights = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55]
tree_heights = [5,10,15,20]

for tree_height in tree_heights:
    print("------------------------------------------------------------------------------------------------")

    tree = DecisionTree(max_depth=tree_height)
    start_time = time.time()
    tree.fit(X_train, y_train)
    train_time = time.time() - start_time

    predictions_train = tree.predict(X_train)
    accuracy = np.sum(np.array(predictions_train) == np.array(y_train)) / len(y_train)
    print(f"Training accuracy for tree height \t{tree_height}\t : {accuracy} \t in {train_time}")
    train_accuracy.append(accuracy)


    start_time = time.time()
    predictions_valid = tree.predict(X_valid)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_valid) == np.array(y_valid))/len(y_valid)
    print(f"Validation accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    validation_accuracy.append(accuracy)



    start_time = time.time()
    predictions_test= tree.predict(X_test)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_test) == np.array(y_test))/len(y_test)
    print(f"Testing accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    test_accuracy.append(accuracy)


    num_nodes = tree.count_nodes()
    print(f"Number of nodes in the tree for height {tree_height} : {num_nodes}")
    
print("-------------------------------------------------------------------------------------------------")

#plot the results on the same graph
plt.figure(figsize=(10, 6))
plt.plot(tree_heights, train_accuracy, label="Train Accuracy", marker='o')
plt.plot(tree_heights, validation_accuracy, label="Validation Accuracy", marker='o')
plt.plot(tree_heights, test_accuracy, label="Test Accuracy", marker='o')
plt.title("Decision Tree Accuracy vs Tree Height")
plt.xlabel("Tree Height")
plt.ylabel("Accuracy")
plt.xticks(tree_heights)
plt.legend()
plt.grid()

plt.show()

    


# ## Irrelavant stuff

# In[189]:


tree = DecisionTree(max_depth=5)

features = ['age','workclass','fnlwgt','education','education.num','marital.status','occupation','relationship','race','sex','capital.gain','capital.loss','hours.per.week','native.country','income']

tree.fit(X_train,y_train)
tree.print_tree()

print("-------------------------------------------------------------------------------------------------")

X_list = X_train.values.tolist()
categorial_index = tree.get_categorical_features(X_list)

for i in categorial_index:
    print(f"Categorical feature: {features[i]}")

print("-------------------------------------------------------------------------------------------------")


continous_index = tree.get_continuous_features(X_list)

for i in continous_index:
    print(f"Continous feature: {features[i]}")

print("-------------------------------------------------------------------------------------------------")


# In[190]:


import networkx as nx
import matplotlib.pyplot as plt

def build_nx_graph(tree):
    G = nx.DiGraph()
    node_id_counter = [0]  # Mutable to track node ids
    
    def add_nodes_edges(node, parent_id=None, label=""):
        node_id = node_id_counter[0]
        node_id_counter[0] += 1

        if node.is_leaf_node():
            G.add_node(node_id, label=f"Leaf: {node.value}")
        elif isinstance(node.threshold, (int, float)):
            G.add_node(node_id, label=f"{node.feature} â‰¤ {node.threshold}")
        else:
            G.add_node(node_id, label=f"{node.feature} = ?")

        if parent_id is not None:
            G.add_edge(parent_id, node_id, label=label)

        if not node.is_leaf_node():
            if isinstance(node.threshold, (int, float)):
                add_nodes_edges(node.left, node_id, "True")
                add_nodes_edges(node.right, node_id, "False")
            else:
                for val, child in node.branches.items():
                    add_nodes_edges(child, node_id, str(val))

        return G

    return add_nodes_edges(tree.root)

def hierarchy_pos(G, root=None, width=1.0, vert_gap=0.2, vert_loc=0, xcenter=0.5):

    def _hierarchy_pos(G, root, leftmost, width, vert_gap, vert_loc, xcenter, pos, parent=None):
        children = list(G.successors(root))
        if not children:
            pos[root] = (leftmost[0], vert_loc)
            leftmost[0] += width
        else:
            mid = leftmost[0] + width * (len(children) - 1) / 2
            pos[root] = (mid, vert_loc)
            for child in children:
                _hierarchy_pos(G, child, leftmost, width, vert_gap, vert_loc - vert_gap, xcenter, pos, root)
        return pos

    if root is None:
        root = [n for n, d in G.in_degree() if d == 0][0]

    return _hierarchy_pos(G, root, [0], width, vert_gap, vert_loc, xcenter, {})

def visualize_tree(tree):
    G = build_nx_graph(tree)
    pos = hierarchy_pos(G)

    labels = nx.get_node_attributes(G, "label")
    edge_labels = nx.get_edge_attributes(G, "label")

    plt.figure(figsize=(20, 10))
    nx.draw(G, pos, with_labels=True, labels=labels, node_size=500, node_color="lightgreen", font_size=9)
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color="red", font_size=9)
    plt.title("Decision Tree (Hierarchical View)", fontsize=14)
    plt.axis("off")
    plt.show()


tree = DecisionTree(max_depth=2)

tree.fit(X_train,y_train)

visualize_tree(tree)


# # Part 2)

# ## Code

# In[27]:


def one_hot_encode(data, categorical_columns):
    # Collect all unique values for each categorical column
    unique_vals = {col: set() for col in categorical_columns}
    for row in data:
        for col in categorical_columns:
            unique_vals[col].add(row[col])

    # Sort for consistent order
    for col in unique_vals:
        unique_vals[col] = sorted(list(unique_vals[col]))

    # One-hot encode each row
    new_data = []
    for row in data:
        new_row = {}
        for key, value in row.items():
            if key in categorical_columns:
                for v in unique_vals[key]:
                    new_row[f"{key}_{v}"] = 1 if value == v else 0
            else:
                new_row[key] = value
        new_data.append(new_row)

    return new_data, unique_vals

def get_categorical_columns(data):
    categorical_columns = []
    for key in data[0].keys():
        if isinstance(data[0][key], str):
            categorical_columns.append(key)
    return categorical_columns


# In[28]:


test_data = pd.read_csv("../data/Q1/test.csv")
train_data = pd.read_csv("../data/Q1/train.csv")
valid_data = pd.read_csv("../data/Q1/valid.csv")


# In[29]:


categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
categorial_columns.remove("income")
_X_train = train_data.drop("income", axis=1).to_dict(orient='records')
_y_train = train_data["income"].tolist()
_X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
_y_valid = valid_data["income"].tolist()
_X_test = test_data.drop("income", axis=1).to_dict(orient='records')
_y_test = test_data["income"].tolist()   
X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


X_train_encoded = pd.DataFrame(X_train_encoded)
X_valid_encoded = pd.DataFrame(X_valid_encoded)
X_test_encoded = pd.DataFrame(X_test_encoded)
y_test = pd.Series(_y_test)
y_valid = pd.Series(_y_valid)    
y_train = pd.Series(_y_train)


# In[17]:


print(X_train_encoded.shape)
print(X_valid_encoded.shape)
print(X_test_encoded.shape)
print(y_train.shape)
print(y_valid.shape)
print(y_test.shape)
X_test_encoded


# ## Testing

# In[18]:


train_accuracy = []
validation_accuracy = []
test_accuracy = []
#tree_heights = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55]
tree_heights = [25,35,45,55]
for tree_height in tree_heights:
    print("------------------------------------------------------------------------------------------------")

    tree = DecisionTree(max_depth=tree_height)
    start_time = time.time()
    tree.fit(X_train_encoded, y_train)
    train_time = time.time() - start_time

    predictions_train = tree.predict(X_train_encoded)
    accuracy = np.sum(np.array(predictions_train) == np.array(y_train)) / len(y_train)
    print(f"Training accuracy for tree height \t{tree_height}\t : {accuracy} \t in {train_time}")
    train_accuracy.append(accuracy)


    start_time = time.time()
    predictions_valid = tree.predict(X_valid_encoded)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_valid) == np.array(y_valid))/len(y_valid)
    print(f"Validation accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    validation_accuracy.append(accuracy)

    start_time = time.time()
    predictions_test= tree.predict(X_test_encoded)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_test) == np.array(y_test))/len(y_test)
    print(f"Testing accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    test_accuracy.append(accuracy)
    #count number of nodes in the tree
    num_nodes = tree.count_nodes()
    print(f"Number of nodes in the tree for height {tree_height} : {num_nodes}")
print("-------------------------------------------------------------------------------------------------")

#plot the results on the same graph
plt.figure(figsize=(10, 6))
plt.plot(tree_heights, train_accuracy, label="Train Accuracy", marker='o')
plt.plot(tree_heights, validation_accuracy, label="Validation Accuracy", marker='o')
plt.plot(tree_heights, test_accuracy, label="Test Accuracy", marker='o')
plt.title("Decision Tree Accuracy vs Tree Height")
plt.xlabel("Tree Height")
plt.ylabel("Accuracy")
plt.xticks(tree_heights)
plt.legend()
plt.grid()

plt.show()


# # Part 3)
# 

# In[59]:


test_data = pd.read_csv("../data/Q1/test.csv")
train_data = pd.read_csv("../data/Q1/train.csv")
valid_data = pd.read_csv("../data/Q1/valid.csv")


# In[115]:


categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
categorial_columns.remove("income")
_X_train = train_data.drop("income", axis=1).to_dict(orient='records')
_y_train = train_data["income"].tolist()
_X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
_y_valid = valid_data["income"].tolist()
_X_test = test_data.drop("income", axis=1).to_dict(orient='records')
_y_test = test_data["income"].tolist()   
X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


X_train_encoded = pd.DataFrame(X_train_encoded)
X_valid_encoded = pd.DataFrame(X_valid_encoded)
X_test_encoded = pd.DataFrame(X_test_encoded)
y_test = pd.Series(_y_test)
y_valid = pd.Series(_y_valid)    
y_train = pd.Series(_y_train)
X_train_encoded, X_valid_encoded = X_train_encoded.align(X_valid_encoded, join='outer', axis=1, fill_value=0)
X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)
X_valid_encoded, X_test_encoded = X_valid_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)
X_train_encoded = X_train_encoded.fillna(0)
X_valid_encoded = X_valid_encoded.fillna(0)
X_test_encoded = X_test_encoded.fillna(0)


# In[61]:


print(X_train_encoded.shape)
print(X_valid_encoded.shape)
print(X_test_encoded.shape)
print(y_train.shape)
print(y_valid.shape)
print(y_test.shape)
X_test_encoded


# In[101]:


def plot_pruning_curve(nodes, train_accs, val_accs, test_accs):
    plt.figure(figsize=(10, 6))
    if train_accs:
        plt.plot(nodes, train_accs, label="Train Accuracy")
    if val_accs:
        plt.plot(nodes, val_accs, label="Validation Accuracy")
    if test_accs:
        plt.plot(nodes, test_accs, label="Test Accuracy")
    plt.xlabel("Number of Nodes in Tree")
    plt.ylabel("Accuracy")
    plt.title("Accuracy vs Number of Nodes During Pruning")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# In[103]:


train_accuracy = []
validation_accuracy = []
test_accuracy = []
tree_heights = [5,10,15,20,25,30, 35,40, 45,50, 55]
#tree_heights = [4]
for tree_height in tree_heights:
    print("------------------------------------------------------------------------------------------------")

    tree = DecisionTree(max_depth=tree_height)
    start_time = time.time()
    tree.fit(X_train_encoded, y_train)
    train_time = time.time() - start_time
    nodes, train_accs, val_accs, test_accs = tree.prune(X_valid_encoded, y_valid,X_train_encoded, y_train,X_test_encoded, y_test)


    predictions_train = tree.predict(X_train_encoded)
    accuracy = np.sum(np.array(predictions_train) == np.array(y_train)) / len(y_train)
    print(f"Training accuracy for tree height \t{tree_height}\t : {accuracy} \t in {train_time}")
    train_accuracy.append(accuracy)


    start_time = time.time()
    predictions_valid = tree.predict(X_valid_encoded)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_valid) == np.array(y_valid))/len(y_valid)
    print(f"Validation accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    validation_accuracy.append(accuracy)

    start_time = time.time()
    predictions_test= tree.predict(X_test_encoded)
    validation_time = time.time() - start_time

    accuracy = np.sum(np.array(predictions_test) == np.array(y_test))/len(y_test)
    print(f"Testing accuracy for tree height \t{tree_height}\t : {accuracy} \t in {validation_time}")
    test_accuracy.append(accuracy)
    print()

    num_nodes = tree.count_nodes()
    print(f"Number of nodes in the tree for height {tree_height} : {num_nodes}")



    print()
    #plot_pruning_curve(nodes, train_accs, val_accs, test_accs)

    


print("-------------------------------------------------------------------------------------------------")

#plot the results on the same graph
plt.figure(figsize=(20, 6))
plt.plot(tree_heights, train_accuracy, label="Train Accuracy", marker='o')
plt.plot(tree_heights, validation_accuracy, label="Validation Accuracy", marker='o')
plt.plot(tree_heights, test_accuracy, label="Test Accuracy", marker='o')
plt.title("Decision Tree Accuracy vs Tree Height")
plt.xlabel("Tree Height")
plt.ylabel("Accuracy")
plt.xticks(tree_heights)
plt.legend()
plt.grid()

plt.show()


# # Part 4)

# In[85]:


import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt


# In[86]:


test_data4 = pd.read_csv("../data/Q1/test.csv")
train_data4 = pd.read_csv("../data/Q1/train.csv")
valid_data4 = pd.read_csv("../data/Q1/valid.csv")


categorial_columns4 = get_categorical_columns(train_data4.to_dict(orient='records'))
categorial_columns4.remove("income")
_X_train4 = train_data4.drop("income", axis=1).to_dict(orient='records')
_y_train4 = train_data4["income"].tolist()
_X_valid4 = valid_data4.drop("income", axis=1).to_dict(orient='records')
_y_valid4 = valid_data4["income"].tolist()
_X_test4 = test_data4.drop("income", axis=1).to_dict(orient='records')
_y_test4 = test_data4["income"].tolist()   
X_train_encoded4, unique_vals4 = one_hot_encode(_X_train4, categorial_columns4)
X_valid_encoded4, _ = one_hot_encode(_X_valid4, categorial_columns4)
X_test_encoded4, _ = one_hot_encode(_X_test4, categorial_columns4)


X_train_encoded4 = pd.DataFrame(X_train_encoded4)
X_valid_encoded4 = pd.DataFrame(X_valid_encoded4)
X_test_encoded4 = pd.DataFrame(X_test_encoded4)
y_test4 = pd.Series(_y_test4)
y_valid4 = pd.Series(_y_valid4)    
y_train4 = pd.Series(_y_train4)



# In[87]:



# make columns in X_train_encoded4 and X_valid_encoded4 and X_test_encoded4 same
X_train_encoded4, X_valid_encoded4 = X_train_encoded4.align(X_valid_encoded4, join='outer', axis=1, fill_value=0)
X_train_encoded4, X_test_encoded4 = X_train_encoded4.align(X_test_encoded4, join='outer', axis=1, fill_value=0)
X_valid_encoded4, X_test_encoded4 = X_valid_encoded4.align(X_test_encoded4, join='outer', axis=1, fill_value=0)
X_train_encoded4 = X_train_encoded4.fillna(0)
X_valid_encoded4 = X_valid_encoded4.fillna(0)
X_test_encoded4 = X_test_encoded4.fillna(0)
print(X_train_encoded4.shape)
print(X_valid_encoded4.shape)
print(X_test_encoded4.shape)
print(y_train4.shape)
print(y_valid4.shape)
print(y_test4.shape)
X_test_encoded4


# In[91]:


#max_depths = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55]
max_depths = [25,35,45,55]
train_accs = []
val_accs = []
test_accs = []

for depth in max_depths:
    clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
    clf.fit(X_train_encoded4, y_train4)
    
    train_pred = clf.predict(X_train_encoded4)
    val_pred = clf.predict(X_valid_encoded4)
    test_pred = clf.predict(X_test_encoded4)
    print("-----------------------------------------------------------------")

    train_accs.append(accuracy_score(y_train4, train_pred))
    print(f"Train accuracy for max depth \t{depth}\t : {train_accs[-1]}")
    val_accs.append(accuracy_score(y_valid4, val_pred))
    print(f"Validation accuracy for max depth \t{depth}\t : {val_accs[-1]}")

    test_acc = accuracy_score(y_test4, test_pred)
    print(f"Test accuracy for max depth \t{depth}\t : {test_acc}")
    test_accs.append(test_acc)
print("-----------------------------------------------------------------")

plt.plot(max_depths, train_accs, label='Train Accuracy', marker='o')
plt.plot(max_depths, val_accs, label='Validation Accuracy', marker='o')
plt.plot(max_depths, test_accs, label='Test Accuracy', marker='o')
plt.xlabel('Max Depth')
<A NAME="1"></A><FONT color = #00FF00><A HREF="match69-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.ylabel('Accuracy')
plt.title('Accuracy vs Max Depth')
plt.legend()
plt.grid(True)
plt.show()

best_depth = max_depths[np.argmax(val_accs)]
print(f"Best max_depth based on validation set: {best_depth}")


# In[ ]:





# In[105]:


ccp_alphas = [0.001, 0.01, 0.1, 0.2]
train_accs = []
</FONT>val_accs = []
test_accs = []

for alpha in ccp_alphas:
    clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha, random_state=42)
    clf.fit(X_train_encoded, y_train)

    train_pred = clf.predict(X_train_encoded)
    val_pred = clf.predict(X_valid_encoded)
    test_pred = clf.predict(X_test_encoded)

    train_accs.append(accuracy_score(y_train, train_pred))
    val_accs.append(accuracy_score(y_valid, val_pred))
    test_accs.append(accuracy_score(y_test, test_pred))


    print(f"alpha {alpha} Train accuracy: {train_accs[-1]}")
    print(f"alpha {alpha} Validation accuracy: {val_accs[-1]}")
    print(f"alpha {alpha} Test accuracy: {test_accs[-1]}")
    print("-----------------------------------------------------------------")

plt.plot(ccp_alphas, train_accs, label='Train Accuracy', marker='o')
plt.plot(ccp_alphas, val_accs, label='Validation Accuracy', marker='o')
plt.plot(ccp_alphas, test_accs, label='Test Accuracy', marker='o')
plt.xlabel('ccp_alpha')
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match69-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.ylabel('Accuracy')
plt.title('Accuracy vs ccp_alpha')
plt.legend()
plt.grid(True)
plt.show()

best_alpha = ccp_alphas[np.argmax(val_accs)]
</FONT>print(f"Best ccp_alpha based on validation set: {best_alpha}")


# In[21]:


X_combined = pd.concat([X_train_encoded, X_valid_encoded])
y_combined = pd.concat([y_train, y_valid])

final_depth_clf = DecisionTreeClassifier(criterion='entropy', max_depth=best_depth, random_state=42)
final_depth_clf.fit(X_combined, y_combined)
test_acc_depth = accuracy_score(y_test, final_depth_clf.predict(X_test_encoded))
print(f"Test Accuracy (best max_depth={best_depth}): {test_acc_depth}")

final_alpha_clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=best_alpha, random_state=42)
final_alpha_clf.fit(X_combined, y_combined)
test_acc_alpha = accuracy_score(y_test, final_alpha_clf.predict(X_test_encoded))
print(f"Test Accuracy (best ccp_alpha={best_alpha}): {test_acc_alpha}")


# # part 5)

# In[30]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np


# In[31]:


test_data = pd.read_csv("../data/Q1/test.csv")
train_data = pd.read_csv("../data/Q1/train.csv")
valid_data = pd.read_csv("../data/Q1/valid.csv")


categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
categorial_columns.remove("income")
_X_train = train_data.drop("income", axis=1).to_dict(orient='records')
_y_train = train_data["income"].tolist()
_X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
_y_valid = valid_data["income"].tolist()
_X_test = test_data.drop("income", axis=1).to_dict(orient='records')
_y_test = test_data["income"].tolist()   
X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


X_train_encoded = pd.DataFrame(X_train_encoded)
X_valid_encoded = pd.DataFrame(X_valid_encoded)
X_test_encoded = pd.DataFrame(X_test_encoded)
y_test = pd.Series(_y_test)
y_valid = pd.Series(_y_valid)    
y_train = pd.Series(_y_train)


X_train_encoded = X_train_encoded.reindex(columns=X_valid_encoded.columns, fill_value=0)
X_valid_encoded = X_valid_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)
X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)


# In[32]:


X_combined = pd.concat([X_train_encoded, X_valid_encoded])
y_combined = pd.concat([y_train, y_valid])


# In[33]:


param_grid = {
    'n_estimators': [50, 150, 250, 350],
    'max_features': [0.1, 0.3, 0.5, 0.7, 1.0],
    'min_samples_split': [2, 4, 6, 8, 10]
}


# In[109]:


best_oob_score = 0
best_params = None
best_model = None

results = []

for params in ParameterGrid(param_grid):
    clf = RandomForestClassifier(
        criterion='entropy',
        oob_score=True,
        n_estimators=params['n_estimators'],
        max_features=params['max_features'],
        min_samples_split=params['min_samples_split'],
        bootstrap=True,
        random_state=42,
        n_jobs=-1
    )
    
    clf.fit(X_train_encoded, y_train)
    
    if hasattr(clf, 'oob_score_'):
        oob = clf.oob_score_
    else:
        continue

    val_acc = accuracy_score(y_valid, clf.predict(X_valid_encoded))
    train_acc = accuracy_score(y_train, clf.predict(X_train_encoded))

    results.append({
        'params': params,
        'train_acc': train_acc,
        'oob_acc': oob,
        'val_acc': val_acc
    })
    
    if oob &gt; best_oob_score:
        best_oob_score = oob
        best_params = params
        best_model = clf

    
    print(f"Parameters: {params}")
    print(f"Train accuracy: {train_acc}")
    print(f"OOB accuracy: {oob}")
    print(f"Validation accuracy: {val_acc}")
    print("-----------------------------------------------------------------")


# In[110]:


results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by='oob_acc', ascending=False)
print("Top configurations:")
print(results_df.head())

test_acc = accuracy_score(y_test, best_model.predict(X_test_encoded))
print(f"\nBest Params: {best_params}")
print(f"Train Accuracy: {accuracy_score(y_train, best_model.predict(X_train_encoded))}")
print(f"OOB Accuracy: {best_oob_score}")
print(f"Validation Accuracy: {accuracy_score(y_valid, best_model.predict(X_valid_encoded))}")
print(f"Test Accuracy: {test_acc}")


# In[114]:


results_df.head()





#!/usr/bin/env python
# coding: utf-8

# In[14]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')
from neural_network import NeuralNetwork
from neural_network import load_flat_test_data
from neural_network import load_gtsrb_data
from neural_network import save_image_from_array


# # Imports

# In[9]:


import os
import numpy as np
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
import pandas as pd


# # part 2)

# In[10]:


train_data_path = '../data/Q2/train'
test_data_path = '../data/Q2/test'
output_path = '../data/Q2'
X_train, y_train = load_gtsrb_data(train_data_path)
X_test,X_labels = load_flat_test_data(test_data_path)
y_test = []


import csv
test_labels = open("../data/Q2/test_labels.csv", "r")
test_labels_csv = csv.reader(test_labels)
test_labels_dict_actual = {}
test_labels_dict = {}
for row in test_labels_csv:
    if row[1]!='label':
        test_labels_dict_actual[row[0]] = int(row[1])

for i in range(len(X_labels)):
    y_test.append(test_labels_dict_actual[X_labels[i]])
test_labels.close()
y_test = np.array(y_test)


# In[11]:


hidden_sizes = [1, 5, 10, 50, 100]
#hidden_sizes = [10]
avg_f1_scores_train = []
avg_f1_scores_test = []

for h in hidden_sizes:
    print(f"Training with {h} hidden units")
    model = NeuralNetwork(num_features=2352, hidden_layers=[h], num_classes=43, batch_size=32)
    model.train(X_train, y_train, epochs=50, learning_rate=0.01)

    
    train_preds = np.argmax(model.forward_propagation(X_train)[0][-1], axis=1)
    test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)

    print(f"Classification Report for {h} units - Train:")
    print(classification_report(y_train, train_preds, zero_division=0))
    print(f"Classification Report for {h} units - Test:")
    print(classification_report(y_test, test_preds, zero_division=0))

    avg_f1_scores_train.append(f1_score(y_train, train_preds, average='macro'))
    avg_f1_scores_test.append(f1_score(y_test, test_preds, average='macro'))

plt.figure(figsize=(10, 5))
plt.plot(hidden_sizes, avg_f1_scores_train, label='Train Avg F1')
plt.plot(hidden_sizes, avg_f1_scores_test, label='Test Avg F1')
plt.xlabel('Number of Hidden Units')
plt.ylabel('Average F1 Score')
plt.title('F1 Score vs Number of Hidden Units')
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(output_path, 'part_b_f1_plot.png'))
plt.show()


# # Part 3

# In[107]:


train_data_path = '../data/Q2/train'
test_data_path = '../data/Q2/test'
output_path = '../data/Q2'
X_train, y_train = load_gtsrb_data(train_data_path)
X_test,X_labels = load_flat_test_data(test_data_path)
y_test = []


import csv
test_labels = open("../data/Q2/test_labels.csv", "r")
test_labels_csv = csv.reader(test_labels)
test_labels_dict_actual = {}
test_labels_dict = {}
for row in test_labels_csv:
    if row[1]!='label':
        test_labels_dict_actual[row[0]] = int(row[1])

for i in range(len(X_labels)):
    y_test.append(test_labels_dict_actual[X_labels[i]])
test_labels.close()
y_test = np.array(y_test)


# In[108]:


architectures = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
avg_f1_scores_train = []
avg_f1_scores_test = []

for arch in architectures:
    print(f"Training with architecture: {arch}")
    model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32)
    model.train(X_train, y_train, epochs=50, learning_rate=0.01)

    train_preds = np.argmax(model.forward_propagation(X_train)[0][-1], axis=1)
    test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)

    print(f"Classification Report - Train for {arch}:")
    print(classification_report(y_train, train_preds, zero_division=0))
    print(f"Classification Report - Test for {arch}:")
    print(classification_report(y_test, test_preds, zero_division=0))

    avg_f1_scores_train.append(f1_score(y_train, train_preds, average='macro'))
    avg_f1_scores_test.append(f1_score(y_test, test_preds, average='macro'))

depths = [len(arch) for arch in architectures]
plt.figure(figsize=(10, 5))
plt.plot(depths, avg_f1_scores_train, label='Train Avg F1')
plt.plot(depths, avg_f1_scores_test, label='Test Avg F1')
plt.xlabel('Network Depth')
plt.ylabel('Average F1 Score')
plt.title('F1 Score vs Network Depth')
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(output_path, 'part_c_f1_plot.png'))
plt.show()


# # Part 4

# In[109]:


train_data_path = '../data/Q2/train'
test_data_path = '../data/Q2/test'
output_path = '../data/Q2'
X_train, y_train = load_gtsrb_data(train_data_path)
X_test,X_labels = load_flat_test_data(test_data_path)
y_test = []


import csv
test_labels = open("../data/Q2/test_labels.csv", "r")
test_labels_csv = csv.reader(test_labels)
test_labels_dict_actual = {}
test_labels_dict = {}
for row in test_labels_csv:
    if row[1]!='label':
        test_labels_dict_actual[row[0]] = int(row[1])

for i in range(len(X_labels)):
    y_test.append(test_labels_dict_actual[X_labels[i]])
test_labels.close()
y_test = np.array(y_test)


# In[110]:


architectures = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]
avg_f1_scores_train = []
avg_f1_scores_test = []

for arch in architectures:
    print(f"Training with architecture (adaptive lr): {arch}")
    model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32)
    model.train(X_train, y_train, epochs=70, learning_rate=0.01, adaptive_lr=True)

    train_preds = np.argmax(model.forward_propagation(X_train)[0][-1], axis=1)
    test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)

    print(f"Classification Report - Train for {arch}:")
    print(classification_report(y_train, train_preds, zero_division=0))
    print(f"Classification Report - Test for {arch}:")
    print(classification_report(y_test, test_preds, zero_division=0))

    avg_f1_scores_train.append(f1_score(y_train, train_preds, average='macro'))
    avg_f1_scores_test.append(f1_score(y_test, test_preds, average='macro'))

depths = [len(arch) for arch in architectures]
plt.figure(figsize=(10, 5))
plt.plot(depths, avg_f1_scores_train, label='Train Avg F1')
plt.plot(depths, avg_f1_scores_test, label='Test Avg F1')
plt.xlabel('Network Depth')
plt.ylabel('Average F1 Score')
plt.title('F1 Score vs Network Depth with Adaptive LR')
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(output_path, 'part_d_f1_plot.png'))
plt.show()


# # Part 5

# In[13]:


train_data_path = '../data/Q2/train'
test_data_path = '../data/Q2/test'
output_path = '../data/Q2'
X_train, y_train = load_gtsrb_data(train_data_path)
X_test,X_labels = load_flat_test_data(test_data_path)
y_test = []


import csv
test_labels = open("../data/Q2/test_labels.csv", "r")
test_labels_csv = csv.reader(test_labels)
test_labels_dict_actual = {}
test_labels_dict = {}
for row in test_labels_csv:
    if row[1]!='label':
        test_labels_dict_actual[row[0]] = int(row[1])

for i in range(len(X_labels)):
    y_test.append(test_labels_dict_actual[X_labels[i]])
test_labels.close()
y_test = np.array(y_test)


# In[17]:


architectures = [[512],[512, 256], [512, 256, 128], [512, 256, 128, 64]]
avg_f1_scores_train = []
avg_f1_scores_test = []

for arch in architectures:
    print(f"Training with architecture (ReLU + adaptive lr): {arch}")
    model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32, activation='relu')
    model.train(X_train, y_train, epochs=20, learning_rate=0.01, adaptive_lr=True)

    train_preds = np.argmax(model.forward_propagation(X_train)[0][-1], axis=1)
    test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)

    print(f"Classification Report - Train for {arch}:")
    print(classification_report(y_train, train_preds, zero_division=0))
    print(f"Classification Report - Test for {arch}:")
    print(classification_report(y_test, test_preds, zero_division=0))

    avg_f1_scores_train.append(f1_score(y_train, train_preds, average='macro'))
    avg_f1_scores_test.append(f1_score(y_test, test_preds, average='macro'))

depths = [len(arch) for arch in architectures]
plt.figure(figsize=(10, 5))
plt.plot(depths, avg_f1_scores_train, label='Train Avg F1')
plt.plot(depths, avg_f1_scores_test, label='Test Avg F1')
plt.xlabel('Network Depth')
plt.ylabel('Average F1 Score')
plt.title('F1 Score vs Network Depth with ReLU + Adaptive LR')
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(output_path, 'part_e_f1_plot.png'))
plt.show()


# # Part 6

# In[4]:


train_data_path = '../data/Q2/train'
test_data_path = '../data/Q2/test'
output_path = '../data/Q2'
X_train, y_train = load_gtsrb_data(train_data_path)
X_test,X_labels = load_flat_test_data(test_data_path)
y_test = []


import csv
test_labels = open("../data/Q2/test_labels.csv", "r")
test_labels_csv = csv.reader(test_labels)
test_labels_dict_actual = {}
test_labels_dict = {}
for row in test_labels_csv:
    if row[1]!='label':
        test_labels_dict_actual[row[0]] = int(row[1])

for i in range(len(X_labels)):
    y_test.append(test_labels_dict_actual[X_labels[i]])
test_labels.close()
y_test = np.array(y_test)


# In[ ]:


architectures = [[512, 256, 128], [512, 256, 128, 64]]
avg_f1_scores_train = []
avg_f1_scores_test = []

""" for arch in architectures:
    print(f"Training MLPClassifier with architecture: {arch}")
    clf = MLPClassifier(hidden_layer_sizes=tuple(arch), activation='relu', solver='sgd',
                        alpha=0, batch_size=32, learning_rate='invscaling', max_iter=100)
    clf.fit(X_train, y_train)

    train_preds = clf.predict(X_train)
    test_preds = clf.predict(X_test)

    print(f"Classification Report - Train for {arch}:")
    print(classification_report(y_train, train_preds, zero_division=0))
    print(f"Classification Report - Test for {arch}:")
    print(classification_report(y_test, test_preds, zero_division=0))

    avg_f1_scores_train.append(f1_score(y_train, train_preds, average='macro'))
    avg_f1_scores_test.append(f1_score(y_test, test_preds, average='macro')) """

depths = [len(arch) for arch in architectures]
plt.figure(figsize=(10, 5))
plt.plot(depths, avg_f1_scores_train, label='Train Avg F1')
plt.plot(depths, avg_f1_scores_test, label='Test Avg F1')
plt.xlabel('Network Depth')
plt.ylabel('Average F1 Score')
plt.title('F1 Score vs Network Depth using MLPClassifier')
plt.legend()
plt.grid(True)
plt.savefig(os.path.join(output_path, 'part_f_f1_plot.png'))
plt.show()


# In[13]:


#open two different csv files and check the accuracy of their mathing 

from sklearn.metrics import accuracy_score
import pandas as pd
y_test = pd.read_csv("../data/Q2/test_labels.csv")
y_test = y_test["label"].tolist()
y_test = pd.Series(y_test)
#r = ['b','c','d','e']
r = ['b','c','d']
for part in r:
    y_test_encoded = pd.read_csv(f"../data/Q2/prediction_{part}.csv")
    y_test_encoded = y_test_encoded["prediction"].tolist()
    y_test_encoded = pd.Series(y_test_encoded)
    
    print(f"Test accuracy for test data: {accuracy_score(y_test, y_test_encoded)}")





import math
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import time
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

class DecisionTreeNode:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, branches=None, is_categorical=False , is_leaf_node=False):
        
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value
        self.branches = branches  
        self.parent = None
        self.correct_prediction = { " &lt;=50K":0, " &gt;50K":0 }
        self.incorrect_prediction = { " &lt;=50K":0, " &gt;50K":0}
        self.is_categorical = is_categorical    
        self.is_leaf_node = is_leaf_node

class DecisionTree:
    def __init__(self, max_depth):
        self.max_depth = max_depth
        self.root = None
        self.columns = None

    def fit(self, X, y):
        self.columns = list(X.columns) 
        self.root = self._build_tree(X.values.tolist(), y.tolist(), depth=0)

    def _build_tree(self, X, y, depth):

        num_samples = len(y)
        num_labels = len(set(y))

        if depth &gt;= self.max_depth or num_labels == 1:
            node = DecisionTreeNode(value=self._most_common_label(y),is_leaf_node=True)
            return node

        best_feat, best_thresh, is_categorical = self._best_split(X, y)

        if best_feat is None:
            node = DecisionTreeNode(value=self._most_common_label(y),is_leaf_node=True)
            return node

        if is_categorical:
            branches = {}
            unique_values = set(row[best_feat] for row in X)
            
            for val in unique_values:
                X_sub = [row for i, row in enumerate(X) if row[best_feat] == val]
                y_sub = [y[i] for i in range(len(X)) if X[i][best_feat] == val]
                branches[val] = self._build_tree(X_sub, y_sub, depth + 1)
            node =  DecisionTreeNode(feature=best_feat, branches=branches, is_categorical=True , value = self._most_common_label(y), is_leaf_node=False, threshold=None )

            for b_node in branches.keys():
                branches[b_node].parent = node
            return node
        else:
            left_indices = [i for i in range(len(X)) if X[i][best_feat] &lt;= best_thresh]
            right_indices = [i for i in range(len(X)) if X[i][best_feat] &gt; best_thresh]
            left = self._build_tree([X[i] for i in left_indices], [y[i] for i in left_indices], depth + 1)
            right = self._build_tree([X[i] for i in right_indices], [y[i] for i in right_indices], depth + 1)
            node = DecisionTreeNode(feature=best_feat, threshold=best_thresh, left=left, right=right,value = self._most_common_label(y), is_leaf_node=False, is_categorical=False)
            left.parent = node
            right.parent = node
            return node
        
    def prune(self, X_val, y_val, X_train=None, y_train=None, X_test=None, y_test=None):
        X_val_encoded = X_val.copy()
        X_val = X_val.values.tolist()
        val_accs = []
        train_accs = []
        test_accs = []
        nodes = []

        for i in range(len(X_val)):
            x = X_val[i]
            y_true = y_val[i]
            self._update_prediction(x, y_true)

        def evaluate_accuracy(X, y):
            preds = self.predict(X)
            return sum(p == t for p, t in zip(preds, y)) / len(y)

        while True:
            nodes.append(self.count_nodes())
            if X_train is not None:
                train_accs.append(evaluate_accuracy(X_train, y_train))
            if X_val_encoded  is not None:
                val_accs.append(evaluate_accuracy(X_val_encoded , y_val))
            if X_test is not None:
                test_accs.append(evaluate_accuracy(X_test, y_test))

            best_node, best_gain = self._find_best_marginal_gain_node()
            if best_node is None:
                break
            current = best_node
            V = current.value
            W = " &gt;50K" if V == " &lt;=50K" else " &lt;=50K"
            while current.parent is not None:
                parent = current.parent
                parent.correct_prediction[W] -= best_node.correct_prediction[W]
                parent.incorrect_prediction[W] += best_node.correct_prediction[W]
                parent.correct_prediction[V] += best_node.incorrect_prediction[V]
                parent.incorrect_prediction[V] -= best_node.incorrect_prediction[V]
                current = parent
            best_node.is_leaf_node = True
            best_node.left = None
            best_node.right = None
            best_node.branches = None

        # Final entry
        nodes.append(self.count_nodes())
        if X_train is not None:
            train_accs.append(evaluate_accuracy(X_train, y_train))
        if X_val_encoded  is not None:
            val_accs.append(evaluate_accuracy(X_val_encoded , y_val))
        if X_test is not None:
            test_accs.append(evaluate_accuracy(X_test, y_test))

        return nodes, train_accs, val_accs, test_accs



    def _find_best_marginal_gain_node(self):

        best_node = None
        best_gain = 0

        def traverse(node):
            if node.is_leaf_node:
                return
            nonlocal best_node, best_gain
            if node is None or node.is_leaf_node:
                return

            predicted = node.value
            opposite = " &gt;50K" if predicted == " &lt;=50K" else " &lt;=50K"
            gain = node.incorrect_prediction[predicted] - node.correct_prediction[opposite]

            if gain &gt; best_gain:
                best_gain = gain
                best_node = node

            if node.is_categorical:
                for child in node.branches.values():
                    traverse(child)
            else:
                traverse(node.left)
                traverse(node.right)


        traverse(self.root)
        return best_node, best_gain

    def _update_prediction(self, x, y_true):


        node = self.root
        while not node.is_leaf_node:
            if node.is_categorical:
                key = x[node.feature]
                if key in node.branches:
                    node = node.branches[key]
                else:
                    break  # unknown category, use current node
            else:
                if x[node.feature] &lt;= node.threshold:
                    node = node.left
                else:
                    node = node.right

        y_pred = node.value
        current = node
        if y_pred == y_true:
            while current is not None:
                current.correct_prediction[y_true] += 1
                
                current = current.parent
        else:
            while current is not None:
                current.incorrect_prediction[y_true] += 1
                current = current.parent

    def get_categorical_features(self, X):
        categorical_features = []
        for i in range(len(X[0])):
            if isinstance(X[0][i], str):
                categorical_features.append(i)
        return categorical_features
    
    def get_continuous_features(self, X):
        continuous_features = []
        for i in range(len(X[0])):
            if isinstance(X[0][i], (int, float)):
                continuous_features.append(i)
        return continuous_features
    
    def _best_split(self, X, y):
        best_gain = -1
        best_feat = None
        best_thresh = None
        is_categorical = False

        num_features = len(X[0])
        for feat_idx in range(num_features):
            values = [row[feat_idx] for row in X]
            if isinstance(values[0], (int, float)):
                # Continuous attribute
                median = sorted(values)[len(values) // 2]
                left_y = [y[i] for i in range(len(X)) if X[i][feat_idx] &lt;= median]
                right_y = [y[i] for i in range(len(X)) if X[i][feat_idx] &gt; median]
                if not left_y or not right_y:
                    continue
                gain = self._information_gain(y, left_y, right_y)
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feat = feat_idx
                    best_thresh = median
                    is_categorical = False
            else:
                # Categorical attribute
                subsets = {}
                for i, row in enumerate(X):
                    key = row[feat_idx]
                    subsets.setdefault(key, []).append(y[i])
                gain = self._entropy(y)
                for subset in subsets.values():
                    gain -= len(subset) / len(y) * self._entropy(subset)
                if gain &gt; best_gain:
                    best_gain = gain
                    best_feat = feat_idx
                    best_thresh = None
                    is_categorical = True

        return best_feat, best_thresh, is_categorical

    def _information_gain(self, parent, left, right):
        weight_left = len(left) / len(parent)
        weight_right = len(right) / len(parent)
        return self._entropy(parent) - (weight_left * self._entropy(left) + weight_right * self._entropy(right))

    def _entropy(self, y):
        counts = Counter(y)
        entropy = 0.0
        for lbl in counts:
            prob = counts[lbl] / len(y)
            entropy -= prob * math.log2(prob)
        return entropy
    
    def print_tree(self, node=None, depth=0):
        if node is None:
            node = self.root

        if node.is_leaf_node:
            print(f"{' ' * depth * 2}Leaf: {node.value}")
            return

        if node.is_categorical:
            for key, branch in node.branches.items():
                print(f"{' ' * depth * 2}Feature {node.feature} = {key}:")
                self.print_tree(branch, depth + 1)
        else:
            print(f"{' ' * depth * 2}Feature {node.feature} &lt;= {node.threshold}:")
            self.print_tree(node.left, depth + 1)
            print(f"{' ' * depth * 2}Feature {node.feature} &gt; {node.threshold}:")
            self.print_tree(node.right, depth + 1)
        print(f"{' ' * depth * 2}Feature {node.feature} &gt; {node.threshold}:")
    
    def count_nodes(self, node=None):
        if node is None:
            node = self.root

        if node.is_leaf_node:
            return 1

        count = 1
        if node.is_categorical:
            for branch in node.branches.values():
                count += self.count_nodes(branch)
        else:
            count += self.count_nodes(node.left)
            count += self.count_nodes(node.right)

        return count

    def _most_common_label(self, y):
        if not y:
            return None
        return Counter(y).most_common(1)[0][0]

    def predict(self, X):
        for col in self.columns:
            if col not in X.columns:
                X[col] = 0
        X = X[self.columns]  
        return [self._predict(inputs, self.root) for inputs in X.values.tolist()]

    def _predict(self, inputs, node):
        
        if node.is_leaf_node:
            return node.value

        if node.is_categorical:
            key = inputs[node.feature]
            if key in node.branches:
                return self._predict(inputs, node.branches[key])
            else:
                return node.value  # unseen category (e.g. Japan)
        else:
            if inputs[node.feature] &lt;= node.threshold:
                return self._predict(inputs, node.left)
            else:
                return self._predict(inputs, node.right)

def one_hot_encode(data, categorical_columns):

    unique_vals = {col: set() for col in categorical_columns}
    for row in data:
        for col in categorical_columns:
            unique_vals[col].add(row[col])

    for col in unique_vals:
        unique_vals[col] = sorted(list(unique_vals[col]))

    new_data = []
    for row in data:
        new_row = {}
        for key, value in row.items():
            if key in categorical_columns:
                for v in unique_vals[key]:
                    new_row[f"{key}_{v}"] = 1 if value == v else 0
            else:
                new_row[key] = value
        new_data.append(new_row)

    return new_data, unique_vals

def get_categorical_columns(data):
    categorical_columns = []
    for key in data[0].keys():
        if isinstance(data[0][key], str):
            categorical_columns.append(key)
    return categorical_columns

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Neural Network Training')
    parser.add_argument('train_data_path', type=str, help='Path to training data CSV')
    parser.add_argument('validation_data_path', type=str, help='Path to training data CSV')
    parser.add_argument('test_data_path', type=str, help='Path to testing data CSV')
    parser.add_argument('output_folder_path', type=str, help='Folder path to save prediction output')
    parser.add_argument('question_part', type=str, help='Controls the configuration (e.g., architecture depth, adaptive learning rate, activation)')
    args = parser.parse_args()


    if args.question_part == "a":
        test_data = pd.read_csv(args.test_data_path)
        train_data = pd.read_csv(args.train_data_path)
        valid_data = pd.read_csv(args.validation_data_path)
        X_train = train_data.drop("income", axis=1)
        y_train = train_data["income"]

        X_valid = valid_data.drop("income", axis=1)
        y_valid = valid_data["income"]
        X_test = test_data

        tree_heights = [20]

        for tree_height in tree_heights:
            tree = DecisionTree(max_depth=tree_height)
            tree.fit(X_train, y_train)


            prediction_test = tree.predict(X_test)
            test_data["prediction"] = prediction_test

            test_data = test_data[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_a.csv", index=False)
            
    elif args.question_part == "b":
        
        
        test_data = pd.read_csv(args.test_data_path)
        train_data = pd.read_csv(args.train_data_path)
        valid_data = pd.read_csv(args.validation_data_path)

        categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
        categorial_columns.remove("income")
        _X_train = train_data.drop("income", axis=1).to_dict(orient='records')
        _y_train = train_data["income"].tolist()
        _X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
        _y_valid = valid_data["income"].tolist()
        _X_test = test_data.to_dict(orient='records')
  
        X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
        X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
        X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


        X_train_encoded = pd.DataFrame(X_train_encoded)
        X_valid_encoded = pd.DataFrame(X_valid_encoded)
        X_test_encoded = pd.DataFrame(X_test_encoded)

        y_valid = pd.Series(_y_valid)    
        y_train = pd.Series(_y_train)


        tree_heights = [55]
        for tree_height in tree_heights:
      
            tree = DecisionTree(max_depth=tree_height)
            start_time = time.time()
            tree.fit(X_train_encoded, y_train)
            train_time = time.time() - start_time

            prediction_test = tree.predict(X_test_encoded)
            test_data["prediction"] = prediction_test

            test_data = test_data[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_b.csv", index=False)

    elif args.question_part == "c":
        test_data = pd.read_csv(args.test_data_path)
        train_data = pd.read_csv(args.train_data_path)
        valid_data = pd.read_csv(args.validation_data_path)

        categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
        categorial_columns.remove("income")
        _X_train = train_data.drop("income", axis=1).to_dict(orient='records')
        _y_train = train_data["income"].tolist()
        _X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
        _y_valid = valid_data["income"].tolist()
        _X_test = test_data.to_dict(orient='records')
 
        X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
        X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
        X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


        X_train_encoded = pd.DataFrame(X_train_encoded)
        X_valid_encoded = pd.DataFrame(X_valid_encoded)
        X_test_encoded = pd.DataFrame(X_test_encoded)

        y_valid = pd.Series(_y_valid)    
        y_train = pd.Series(_y_train)
        X_train_encoded, X_valid_encoded = X_train_encoded.align(X_valid_encoded, join='outer', axis=1, fill_value=0)
        X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)
        X_valid_encoded, X_test_encoded = X_valid_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)
        X_train_encoded = X_train_encoded.fillna(0)
        X_valid_encoded = X_valid_encoded.fillna(0)
        X_test_encoded = X_test_encoded.fillna(0)
        tree_heights = [55]
        #tree_heights = [4]
        for tree_height in tree_heights:

            tree = DecisionTree(max_depth=tree_height)
            start_time = time.time()
            tree.fit(X_train_encoded, y_train)
            train_time = time.time() - start_time
            nodes, train_accs, val_accs, test_accs = tree.prune(X_valid_encoded, y_valid)


            prediction_test = tree.predict(X_test_encoded)
            test_data["prediction"] = prediction_test

            test_data = test_data[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_c.csv", index=False)
    
    elif args.question_part == "d":
        test_data4 = pd.read_csv(args.test_data_path)
        train_data4 = pd.read_csv(args.train_data_path)
        valid_data4 = pd.read_csv(args.validation_data_path)


        categorial_columns4 = get_categorical_columns(train_data4.to_dict(orient='records'))
        categorial_columns4.remove("income")
        _X_train4 = train_data4.drop("income", axis=1).to_dict(orient='records')
        _y_train4 = train_data4["income"].tolist()
        _X_valid4 = valid_data4.drop("income", axis=1).to_dict(orient='records')
        _y_valid4 = valid_data4["income"].tolist()
        _X_test4 = test_data4.to_dict(orient='records')

        X_train_encoded4, unique_vals4 = one_hot_encode(_X_train4, categorial_columns4)
        X_valid_encoded4, _ = one_hot_encode(_X_valid4, categorial_columns4)
        X_test_encoded4, _ = one_hot_encode(_X_test4, categorial_columns4)


        X_train_encoded4 = pd.DataFrame(X_train_encoded4)
        X_valid_encoded4 = pd.DataFrame(X_valid_encoded4)
        X_test_encoded4 = pd.DataFrame(X_test_encoded4)

        y_valid4 = pd.Series(_y_valid4)    
        y_train4 = pd.Series(_y_train4)
        X_train_encoded4, X_valid_encoded4 = X_train_encoded4.align(X_valid_encoded4, join='outer', axis=1, fill_value=0)
        X_train_encoded4, X_test_encoded4 = X_train_encoded4.align(X_test_encoded4, join='outer', axis=1, fill_value=0)
        X_valid_encoded4, X_test_encoded4 = X_valid_encoded4.align(X_test_encoded4, join='outer', axis=1, fill_value=0)
        X_train_encoded4 = X_train_encoded4.fillna(0)
        X_valid_encoded4 = X_valid_encoded4.fillna(0)
        X_test_encoded4 = X_test_encoded4.fillna(0)

        max_depths = [25]


        for depth in max_depths:
            clf = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=42)
            clf.fit(X_train_encoded4, y_train4)
            

            test_pred = clf.predict(X_test_encoded4)
            
            prediction_test = test_pred
            test_data4["prediction"] = prediction_test

            test_data = test_data4[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_d.csv", index=False)
            
        ccp_alphas = [0.001]


        for alpha in ccp_alphas:
            clf = DecisionTreeClassifier(criterion='entropy', ccp_alpha=alpha, random_state=42)
            clf.fit(X_train_encoded4, y_train4)


            test_pred = clf.predict(X_test_encoded4)

            prediction_test = test_pred
            test_data4["prediction"] = prediction_test

            test_data = test_data4[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_d.csv", index=False)
       
    elif args.question_part == "e":
        test_data = pd.read_csv(args.test_data_path)
        train_data = pd.read_csv(args.train_data_path)
        valid_data = pd.read_csv(args.validation_data_path)
        categorial_columns = get_categorical_columns(train_data.to_dict(orient='records'))
        categorial_columns.remove("income")
        _X_train = train_data.drop("income", axis=1).to_dict(orient='records')
        _y_train = train_data["income"].tolist()
        _X_valid = valid_data.drop("income", axis=1).to_dict(orient='records')
        _y_valid = valid_data["income"].tolist()
        _X_test = test_data.to_dict(orient='records')
      

        X_train_encoded, unique_vals = one_hot_encode(_X_train, categorial_columns)
        X_valid_encoded, _ = one_hot_encode(_X_valid, categorial_columns)
        X_test_encoded, _ = one_hot_encode(_X_test, categorial_columns)


        X_train_encoded = pd.DataFrame(X_train_encoded)
        X_valid_encoded = pd.DataFrame(X_valid_encoded)
        X_test_encoded = pd.DataFrame(X_test_encoded)

        y_valid = pd.Series(_y_valid)    
        y_train = pd.Series(_y_train)


        X_train_encoded = X_train_encoded.reindex(columns=X_valid_encoded.columns, fill_value=0)
        X_valid_encoded = X_valid_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)
        X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)
        X_combined = pd.concat([X_train_encoded, X_valid_encoded])
        y_combined = pd.concat([y_train, y_valid])

        param_grid = {
            'n_estimators': [150],
            'max_features': [0.3],
            'min_samples_split': [10]
        }


        for params in ParameterGrid(param_grid):
            clf = RandomForestClassifier(
                criterion='entropy',
                oob_score=True,
                n_estimators=params['n_estimators'],
                max_features=params['max_features'],
                min_samples_split=params['min_samples_split'],
                bootstrap=True,
                random_state=42,
                n_jobs=-1
            )
            
            clf.fit(X_train_encoded, y_train)
            
            if hasattr(clf, 'oob_score_'):
                oob = clf.oob_score_
            else:
                continue

            prediction_test = clf.predict(X_test_encoded)
            

            test_data["prediction"] = prediction_test

            test_data = test_data[["prediction"]]
            test_data["prediction"] = prediction_test

            test_data.to_csv(args.output_folder_path + "/prediction_e.csv", index=False)

    else:
        raise ValueError("Invalid question part. Use 'a', 'b', 'c', or 'd'.")

    
        



if __name__ == "__main__":
    main()



import os
import numpy as np
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
import os
import numpy as np
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
import pandas as pd

<A NAME="2"></A><FONT color = #0000FF><A HREF="match69-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def load_gtsrb_data(data_dir):
    X, y = [], []
    
    for class_id in sorted(os.listdir(data_dir)):
        class_path = os.path.join(data_dir, class_id)
        if not os.path.isdir(class_path):
            continue
        
        for img_name in tqdm(os.listdir(class_path), desc=f"Loading class {class_id}"):
</FONT>            img_path = os.path.join(class_path, img_name)
            try:
                img = Image.open(img_path)
                img_array = np.asarray(img, dtype=np.float32).flatten() / 255.0 
                X.append(img_array)
                y.append(int(class_id))
            except Exception as e:
                print(f"Error loading {img_path}: {e}")
    
    X, y = np.array(X), np.array(y)
    if X.size == 0:
        raise ValueError(f"No images loaded from {data_dir}. Check the path or contents.")
    return X, y

def load_flat_test_data(data_dir):
    X = {}

    for img_name in tqdm(os.listdir(data_dir), desc=f"Loading test images"):
        img_path = os.path.join(data_dir, img_name)
        try:
            img = Image.open(img_path)
            img_array = np.asarray(img, dtype=np.float32).flatten() / 255.0
            X[img_path] = img_array

        except Exception as e:
            print(f"Error loading {img_path}: {e}")

    X= dict(sorted(X.items()))
    
    XX = []
    for k in X:
        XX.append(X[k])
    XX = np.array(XX)
    if XX.size == 0:
        raise ValueError(f"No images loaded from {data_dir}. Check the path or contents.")
    
    return XX

class NeuralNetwork:
    def __init__(self, num_features, hidden_layers, num_classes, batch_size, activation='sigmoid'):
        self.num_features = num_features
        self.hidden_layers = hidden_layers
        self.num_classes = num_classes
        self.batch_size = batch_size
        self.activation = activation
        self.weights = self.initialize_weights()
        self.biases = self.initialize_biases()

    def initialize_weights(self):
        layer_sizes = [self.num_features] + self.hidden_layers + [self.num_classes]
        weights = []

        for i in range(len(layer_sizes) - 1):
            fan_in = layer_sizes[i]
            fan_out = layer_sizes[i + 1]

            if self.activation == 'sigmoid':
                # Xavier Initialization
                limit = np.sqrt(6 / (fan_in + fan_out))
                W = np.random.uniform(-limit, limit, (fan_in, fan_out))
            elif self.activation == 'relu':
                # He Initialization
                stddev = np.sqrt(2 / fan_in)
                W = np.random.randn(fan_in, fan_out) * stddev
            else:
                # Default to small random values
                W = np.random.randn(fan_in, fan_out) * 0.01

            weights.append(W)

        return weights

    def initialize_biases(self):
        layer_sizes = [self.num_features] + self.hidden_layers + [self.num_classes]
        biases = [np.zeros((1, size)) for size in layer_sizes[1:]]
        return biases

    def sigmoid(self, z):
<A NAME="0"></A><FONT color = #FF0000><A HREF="match69-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return 1 / (1 + np.exp(-z))

    def sigmoid_derivative(self, a):
        return a * (1 - a)

    def relu(self, z):
        return np.maximum(0, z)

    def relu_derivative(self, z):
        return (z &gt; 0).astype(float)

    def softmax(self, z):
        exps = np.exp(z - np.max(z, axis=1, keepdims=True))
</FONT>        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward_propagation(self, X):
        activations = [X]
        pre_activations = []

        A = X
        for i in range(len(self.hidden_layers)):
            Z = np.dot(A, self.weights[i]) + self.biases[i]
            pre_activations.append(Z)
            A = self.sigmoid(Z) if self.activation == 'sigmoid' else self.relu(Z)
            activations.append(A)

        Z_out = np.dot(A, self.weights[-1]) + self.biases[-1]
        pre_activations.append(Z_out)
        A_out = self.softmax(Z_out)
        activations.append(A_out)

        return activations, pre_activations

    def compute_loss(self, y_true, y_pred):
        m = y_true.shape[0]
        y_one_hot = np.zeros((m, self.num_classes))
        y_one_hot[np.arange(m), y_true] = 1
        log_probs = -np.log(y_pred[range(m), y_true] + 1e-9)
        return np.mean(log_probs)

    def backpropagation(self, activations, pre_activations, X, y_true, learning_rate):
        m = X.shape[0]
        y_one_hot = np.zeros((m, self.num_classes))
        y_one_hot[np.arange(m), y_true] = 1

        delta = activations[-1] - y_one_hot
        for i in reversed(range(len(self.weights))):
            dW = np.dot(activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db

            if i &gt; 0:
                if self.activation == 'sigmoid':
                    delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(activations[i])
                else:
                    delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(pre_activations[i-1])

    def train(self, X, y, epochs=100, learning_rate=0.01, adaptive_lr=False):
        for epoch in range(1, epochs + 1):
            lr = learning_rate / (epoch**0.5) if adaptive_lr else learning_rate
            permutation = np.random.permutation(len(X))
<A NAME="5"></A><FONT color = #FF0000><A HREF="match69-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            X_shuffled, y_shuffled = X[permutation], y[permutation]

            for i in range(0, len(X), self.batch_size):
                X_batch = X_shuffled[i:i+self.batch_size]
</FONT>                y_batch = y_shuffled[i:i+self.batch_size]
                activations, pre_activations = self.forward_propagation(X_batch)
                self.backpropagation(activations, pre_activations, X_batch, y_batch, lr)

    def predict(self, X):
        output = self.forward_propagation(X)[0][-1]
        return np.argmax(output, axis=1)

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Neural Network Training')
    parser.add_argument('train_data_path', type=str, help='Path to training data CSV')
    parser.add_argument('test_data_path', type=str, help='Path to testing data CSV')
    parser.add_argument('output_folder_path', type=str, help='Folder path to save prediction output')
    parser.add_argument('question_part', type=str, help='Controls the configuration (e.g., architecture depth, adaptive learning rate, activation)')
    args = parser.parse_args()

    print(args.train_data_path)
    print(args.test_data_path)
    X_train, y_train = load_gtsrb_data(args.train_data_path)
    X_test = load_flat_test_data(args.test_data_path)

    
    if args.question_part == 'b':
        hidden_sizes = [ 100]
        avg_f1_scores_train = []
        avg_f1_scores_test = []

        for h in hidden_sizes:
            print(f"Training with {h} hidden units")
            
            model = NeuralNetwork(num_features=2352, hidden_layers=[h], num_classes=43, batch_size=32)
            model.train(X_train, y_train, epochs=50, learning_rate=0.01)

            test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)

            test_preds_df = pd.DataFrame(test_preds, columns=['prediction'])
            test_preds_df.to_csv(os.path.join(args.output_folder_path, 'prediction_b.csv'), index=False)    

    if args.question_part == 'c':
        architectures = [[512, 256, 128, 64]]


        for arch in architectures:
            print(f"Training with architecture: {arch}")
            model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32)
            model.train(X_train, y_train, epochs=50, learning_rate=0.01)


            test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)
            test_preds_df = pd.DataFrame(test_preds, columns=['prediction'])
            test_preds_df.to_csv(os.path.join(args.output_folder_path, 'prediction_c.csv'), index=False)
            

    if args.question_part == 'd':
        architectures = [[512, 256, 128, 64]]


        for arch in architectures:
            print(f"Training with architecture (adaptive lr): {arch}")
            model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32)
            model.train(X_train, y_train, epochs=50, learning_rate=0.01, adaptive_lr=True)

            test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)
            test_preds_df = pd.DataFrame(test_preds, columns=['prediction'])
            test_preds_df.to_csv(os.path.join(args.output_folder_path, 'prediction_d.csv'), index=False)

    if args.question_part == 'e':
        architectures = [[512, 256, 128, 64]]

        for arch in architectures:
            print(f"Training with architecture (ReLU + adaptive lr): {arch}")
            model = NeuralNetwork(num_features=2352, hidden_layers=arch, num_classes=43, batch_size=32, activation='relu')
            model.train(X_train, y_train, epochs=50, learning_rate=0.01, adaptive_lr=True)

            test_preds = np.argmax(model.forward_propagation(X_test)[0][-1], axis=1)
            test_preds_df = pd.DataFrame(test_preds, columns=['prediction'])
            test_preds_df.to_csv(os.path.join(args.output_folder_path, 'prediction_e.csv'), index=False)

    if args.question_part == 'f':
        architectures = [[512, 256, 128, 64]]

        for arch in architectures:
            print(f"Training MLPClassifier with architecture: {arch}")
            clf = MLPClassifier(hidden_layer_sizes=tuple(arch), activation='relu', solver='sgd',
                                alpha=0, batch_size=32, learning_rate='invscaling', max_iter=100)
            clf.fit(X_train, y_train)

            test_preds = clf.predict(X_test)

            test_preds_df = pd.DataFrame(test_preds, columns=['prediction'])
            test_preds_df.to_csv(os.path.join(args.output_folder_path, 'prediction_f.csv'), index=False)


if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
