<HTML>
<HEAD>
<TITLE>./A3_processed_new_hash/combined_BKVDM.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A3_processed_new_hash/combined_HJFII.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[480]:


import sys 
import numpy as np 
import pandas as pd 
import math
import os
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier


# In[464]:


import decision_tree


# In[465]:


from decision_tree import tree


# In[466]:


import importlib


# In[467]:


importlib.reload(decision_tree)
from decision_tree import tree


# In[ ]:





# In[468]:


train_df = pd.read_csv("COL774 Assignment-3 Dataset\\train.csv",skipinitialspace=True)
valid_df = pd.read_csv("C:\\Users\\gaura\\Downloads\\ass3\\COL774 Assignment-3 Dataset\\valid.csv",skipinitialspace=True)
test_df = pd.read_csv("C:\\Users\\gaura\\Downloads\\ass3\\COL774 Assignment-3 Dataset\\test.csv",skipinitialspace=True)


# In[469]:


train_df['income'] = train_df['income'].map({'&lt;=50K':0,'&gt;50K':1})
test_df['income'] = test_df['income'].map({'&lt;=50K':0,'&gt;50K':1})
valid_df['income'] = valid_df['income'].map({'&lt;=50K':0,'&gt;50K':1})


# In[446]:


is_numeric ={}
for cols in train_df.columns:
    if train_df[cols].dtype in ['int64','float64'] :
        is_numeric[cols] = 1 
    else:
        is_numeric[cols] = 0


# In[447]:


model = tree()
model.train(max_height =5 ,df= train_df,is_numeric=is_numeric)
        


# In[448]:


<A NAME="5"></A><FONT color = #FF0000><A HREF="match6-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_pred = model.predict(train_df)
valid_pred = model.predict(valid_df)
test_pred= model.predict(test_df)


# In[449]:


train_accuracy = np.mean(train_pred == train_df['income'])
</FONT>valid_accuracy = np.mean(valid_pred == valid_df['income'])
test_accuracy = np.mean(test_pred == test_df['income'])
print(f"train accuracy : {train_accuracy}")
print(f"valid accuracy : {valid_accuracy}")
print(f"test accuracy : {test_accuracy}")


# In[450]:


# 

depths = [5, 10, 15, 20]
<A NAME="10"></A><FONT color = #FF0000><A HREF="match6-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_accuracies = []
test_accuracies = []

for d in depths:
    model = tree()
    model.train(max_height=d, df=train_df, is_numeric=is_numeric)
</FONT>    train_pred = model.predict(train_df)
    test_pred = model.predict(test_df)
    train_acc = np.mean(train_pred == train_df['income'])
    test_acc = np.mean(test_pred == test_df['income'])
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)
print(train_accuracies)
print(test_accuracies)
plt.plot(depths, train_accuracies, label='Train Accuracy')
plt.plot(depths, test_accuracies, label='Test Accuracy')
plt.xlabel('Tree Depth (root is taken as 0 depth)')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig("Q1a.png")
plt.show()


# In[546]:


importlib.reload(decision_tree)
from decision_tree import tree


# In[ ]:





# In[547]:


selected = []
for feat in  train_df.columns:
    if is_numeric[feat] == 0 and feat!='income' and train_df[feat].nunique()==2:
                vals = train_df[feat].unique()
                val0, val1 = vals[0],vals[1]
                mapping = {val0: 0, val1: 1}
                train_df[feat] = train_df[feat].map(mapping)
                valid_df[feat] = valid_df[feat].map(mapping)
                test_df[feat] = test_df[feat].map(mapping)
    if is_numeric[feat]==0 and feat!='income' and train_df[feat].nunique() &gt; 2:
        selected.append(feat)
train_onehot_df = pd.get_dummies(train_df,columns=selected,prefix_sep='_')
valid_onehot_df = pd.get_dummies(valid_df,columns=selected,prefix_sep='_')
test_onehot_df = pd.get_dummies(test_df,columns=selected,prefix_sep='_')
train_onehot_df, valid_onehot_df = train_onehot_df.align(valid_onehot_df, join='outer', axis=1, fill_value=0)
train_onehot_df, test_onehot_df  = train_onehot_df.align(test_onehot_df, join='outer', axis=1, fill_value=0)
onehot_columns = [feat for feat in train_onehot_df.columns if any(f"{feature}_" in feat for feature in selected)]
for feat in onehot_columns:
    train_onehot_df[feat] = train_onehot_df[feat].astype(int)
    valid_onehot_df[feat] = valid_onehot_df[feat].astype(int) 
    test_onehot_df[feat] = test_onehot_df[feat].astype(int)


# In[548]:


is_numeric_encoded = {}
for feat in train_onehot_df.columns:
    if feat == 'income':
        continue
    if train_onehot_df[feat].dtype !=object:
        is_numeric_encoded[feat] = 1
    else:
        print(feat)
        is_numeric_encoded[feat] = 0


# In[504]:


# print(train_onehot_df['education_10th'])


# In[522]:


depths = [25, 35, 45, 55]
<A NAME="11"></A><FONT color = #00FF00><A HREF="match6-0.html#11" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_accuracies = []
test_accuracies = []

for d in depths:
    model = tree()
    model.train(max_height=d, df=train_onehot_df, is_numeric=is_numeric_encoded)
</FONT>    train_pred = model.predict(train_onehot_df)
    test_pred = model.predict(test_onehot_df)
    train_acc = np.mean(train_pred == train_onehot_df['income'])
    test_acc = np.mean(test_pred == test_onehot_df['income'])
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)
print(train_accuracies)
print(test_accuracies)
plt.plot(depths, train_accuracies, label='Train Accuracy')
plt.plot(depths, test_accuracies, label='Test Accuracy')
plt.xlabel('Tree Depth (root is taken as 0 depth)')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig("Q2a.png")
plt.show()


# In[527]:


model = tree()
model.train(max_height =55 ,df= train_onehot_df,is_numeric=is_numeric_encoded)


# In[ ]:


stats = model.val_prune_iterative(valid_onehot_df)


# In[554]:



stata=   []
for d in depths:
    model = tree()
    model.train(max_height =d ,df= train_onehot_df,is_numeric=is_numeric_encoded)
    stats = model.val_prune_iterative(valid_onehot_df,train_df =train_onehot_df,test_df  = test_onehot_df)
    # after val_prune_iterative:
    num_nodes_list = [x[0] for x in stats]
    train_acc_list = [x[1] for x in stats]
    val_acc_list   = [x[2] for x in stats]
    test_acc_list  = [x[3] for x in stats]
    stata.append([num_nodes_list,train_acc_list,val_acc_list,test_acc_list])

    plt.figure()
    plt.plot(num_nodes_list, train_acc_list, marker='o', label='Train Accuracy')
    plt.plot(num_nodes_list, val_acc_list, marker='s', label='Validation Accuracy')
    plt.plot(num_nodes_list, test_acc_list, marker='^', label='Test Accuracy')
    plt.xlabel("Number of Nodes")
    plt.ylabel("Accuracy")
    plt.title("Accuracy vs. Number of Nodes During Post-Pruning")
    plt.legend()
    plt.savefig(f"pruning_accuracy_curve_{d}.png")  # or plt.show()
    plt.close()


# In[555]:


for i in range(len(depths)):
    print(stata[i][1][-1],stata[i][2][-1],stata[i][3][-1])


# In[552]:


train_pred = model.predict(train_onehot_df)
valid_pred = model.predict(valid_onehot_df)
test_pred= model.predict(test_onehot_df)


# In[ ]:





# In[553]:


train_accuracy = (train_pred == train_onehot_df['income']).mean()
valid_accuracy = (valid_pred == valid_onehot_df['income']).mean()
test_accuracy = (test_pred == test_onehot_df['income']).mean()
print(f"train accuracy : {train_accuracy}")
print(f"valid accuracy : {valid_accuracy}")
print(f"test accuracy : {test_accuracy}")


# In[537]:


importlib.reload(decision_tree)
from decision_tree import tree


# In[538]:


data_train_df = train_onehot_df.drop(columns=["income"])
label_train_df = train_onehot_df['income'].values 
data_test_df = test_onehot_df.drop(columns=["income"])
label_test_df =  test_onehot_df['income'].values  
data_valid_df = valid_onehot_df.drop(columns=["income"])
label_valid_df =  valid_onehot_df['income'].values


# In[514]:


from sklearn.tree import DecisionTreeClassifier
depth = [25,35,45,55]
<A NAME="0"></A><FONT color = #FF0000><A HREF="match6-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

train_accuracies =[]
test_accuracies =[]
valid_accuracies =[]

for d in depth :
    model  =DecisionTreeClassifier(criterion='entropy',splitter ='best',max_depth=d)
    model.fit(data_train_df,label_train_df)

    train_accuracy = model.score(data_train_df,label_train_df)
    valid_accuracy = model.score(data_valid_df,label_valid_df)
    test_accuracy =model.score(data_test_df,label_test_df)
</FONT>
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    test_accuracies.append(test_accuracy)

best_depth = 25 
maxi = 0
for j in range(len(valid_accuracies)):
    if valid_accuracies[j]&gt;maxi:
        maxi= valid_accuracies[j]
        best_depth = depth[j] 
print(train_accuracies)

print(valid_accuracies)
print(test_accuracies)
plt.figure()
plt.plot(depth, train_accuracies, marker='o', label='Train Accuracy')
plt.plot(depth, valid_accuracies, marker='s', label='Validation Accuracy')
plt.plot(depth, test_accuracies, marker='^', label='Test Accuracy')
plt.xlabel("max_depth")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Decision Tree (information gain) - Varying max_depth")
plt.savefig( "scikit_dt_maxdepth.png")
plt.close()




# In[539]:


ccp_alpha = [0.001, 0.01, 0.1, 0.2]
train_accuracies =[]
test_accuracies =[]
valid_accuracies =[]

for alpha in ccp_alpha:
    model = DecisionTreeClassifier(criterion='entropy',max_depth=None,ccp_alpha=alpha)
    model.fit(data_train_df, label_train_df)

    train_accuracy = model.score(data_train_df, label_train_df)
    valid_accuracy = model.score(data_valid_df, label_valid_df)
    test_accuracy  = model.score(data_test_df, label_test_df)

    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    test_accuracies.append(test_accuracy)

print(train_accuracies)

print(valid_accuracies)
print(test_accuracies)


plt.figure()
plt.plot(ccp_alpha, train_accuracies, marker='o', label='Train Accuracy')
plt.plot(ccp_alpha, valid_accuracies, marker='s', label='Validation Accuracy')
plt.plot(ccp_alpha, test_accuracies, marker='^', label='Test Accuracy')
plt.xlabel("ccp_alpha")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Decision Tree (information gain) - Varying ccp")
plt.savefig( "scikit_dt_ccp.png")
plt.close()


# In[516]:



n_estimators = [50, 150, 250, 350]  
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match6-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

max_features  = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
min_samples_splits = [2, 4, 6, 8, 10]
</FONT>maxi = 0 
besti= None 

results = []

for i in n_estimators:
    for j in max_features:
        for k in min_samples_splits:
            model = RandomForestClassifier(criterion='entropy',n_estimators=i,max_features=j,min_samples_split=k,oob_score=True) 
            model.fit(data_train_df, label_train_df)
            oob_accuracy = model.oob_score_
            results.append({
                'n_estimators': i,
                'max_features': j,
                'min_samples_split': k,
                'oob_accuracy': oob_accuracy
            })

            if oob_accuracy &gt; maxi:
                maxi = oob_accuracy
                besti    = (i, j, k)
        



n_estimator,max_feature,min_split = besti
    


# In[540]:


print(results)


# In[517]:


model = RandomForestClassifier(criterion='entropy',n_estimators=n_estimator,max_features=max_feature,min_samples_split=min_split,oob_score=True) 
model.fit(data_train_df,label_train_df)
print(besti)
train_accuracy = model.score(data_train_df, label_train_df)
valid_accuracy = model.score(data_valid_df, label_valid_df)
test_accuracy  = model.score(data_test_df, label_test_df)
oob_accuracy =model.oob_score_

print("  Train Accuracy:       ", train_accuracy)
print("  Out-of-Bag Accuracy: ", oob_accuracy)
print("  Validation Accuracy:  ", valid_accuracy)
print("  Test Accuracy:        ", test_accuracy)


# In[ ]:








import sys 
import numpy as np 
import pandas as pd 
import math
import os
# from joblib import Parallel, delayed
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer
from sklearn.base import BaseEstimator

class Node():
    def __init__(self, depth=None, feat=None, thresh=None, isLeaf=False, left=None, right=None):
        self.depth = depth
        self.feat = feat
        self.thresh = thresh
        self.isLeaf = isLeaf
        self.left = left
        self.right = right
        self.children = {}
        self.prediction = None
        self.val_label_counts = {0:0, 1:0}
        self.subtree_error = 0
        self.leaf_error = 0
        self.num_leaves_subtree = 1

class tree():
    def __init__(self, root=None):
        self.root = root

    def split(self, df, feat, thresh):
        left_df = df[df[feat] &lt;= thresh]
        right_df = df[df[feat] &gt; thresh]
        return left_df, right_df

    def entropy(self, df):
        cnt = {0:0, 1:0}
        for label in df['income']:
            cnt[label] += 1
        total = len(df)
        ent = 0
        for ct in cnt:
            p = cnt[ct] / total
            if p &gt; 0:
                ent -= p * math.log2(p)
        return ent

    def info_numeric(self, df, feat):
        median = df[feat].median()
        left_df, right_df = self.split(df, feat, median)
        if len(left_df) == 0 or len(right_df) == 0:
            return 0, median
        total = len(df)
        left_entropy = self.entropy(left_df)
        right_entropy = self.entropy(right_df)
        parent_entropy = self.entropy(df)
        curr_entropy = ((len(left_df) * left_entropy) 
                        + (len(right_df) * right_entropy)) / total
        gain = parent_entropy - curr_entropy
        return gain, median

    def info_cat(self, df, feat):
        parent_entropy = self.entropy(df)
        cats = df[feat].unique()
        if len(cats) &lt;= 1:
            return 0
        total = len(df)
        curr_entropy = 0
        for cat in cats:
            dfc = df[df[feat] == cat]
            curr_entropy += self.entropy(dfc) * (len(dfc) / total)
        gain = parent_entropy - curr_entropy
        return gain

    def best_split(self, df, is_numeric):
        maxi = 0
        best_feat = None
        thresh = None
        for feat in df.columns:
            if feat == 'income':
                continue
            if is_numeric[feat] == 1:
                gain, med = self.info_numeric(df, feat)
                if gain &gt; maxi:
                    maxi = gain
                    best_feat = feat
                    thresh = med
            else:
                gain = self.info_cat(df, feat)
                if gain &gt; maxi:
                    maxi = gain
                    best_feat = feat
                    thresh = None
        return best_feat, thresh, maxi

    def recursive_train(self, df, depth, max_height, is_numeric):
        node = Node(depth=depth)
        node.prediction = df['income'].mode()[0]
        if len(df) == 0:
            node.isLeaf = True
            node.prediction = 0
            return node
        if df['income'].nunique() == 1 or depth &gt;= max_height:
            node.isLeaf = True
            return node

        best_feat, thresh, maxi = self.best_split(df, is_numeric)
        if maxi &lt;= 0:
            node.isLeaf = True
            node.prediction = df['income'].mode()[0]
            return node

        node.feat = best_feat
        node.thresh = thresh
        if thresh is not None:
            left_df, right_df = self.split(df, best_feat, thresh)
            node.left = self.recursive_train(left_df, depth+1, max_height, is_numeric)
            node.right = self.recursive_train(right_df, depth+1, max_height, is_numeric)
        else:
            cats = df[best_feat].unique()
            for cat in cats:
                dfc = df[df[best_feat] == cat]
                node.children[cat] = self.recursive_train(dfc, depth+1, max_height, is_numeric)
        return node

    def predict(self, df):
        prediction = []
        for _, row in df.iterrows():
            pred = None
            curr = self.root
            while curr.isLeaf == False:
                if curr.children != {}:
                    feat = curr.feat
                    val = row[feat]
                    if val in curr.children:
                        curr = curr.children[val]
                    else:
                        pred = curr.prediction
                        break
                else:
                    feat = curr.feat
                    val  = row[feat]
                    if val &lt;= curr.thresh:
                        curr = curr.left
                    else:
                        curr = curr.right
            if pred is not None:
                prediction.append(pred)
            else:
                prediction.append(curr.prediction)
        return np.array(prediction)

    def train(self, max_height=5, df=None, is_numeric=None):
        self.root = self.recursive_train(df, 0, max_height, is_numeric)



    def assign_validation_data(self, node, df):
        for _, row in df.iterrows():
            curr = node
            label = row['income']
            while True:
                curr.val_label_counts[label] += 1
                if curr.isLeaf:
                    break
                if curr.children:
                   
                    val = row[curr.feat]
                    if val in curr.children:
                        curr = curr.children[val]
                    else:
                        break
                else:
                    if row[curr.feat] &lt;= curr.thresh:
                        curr = curr.left
                    else:
                        curr = curr.right

    def compute_subtree_error(self, node):
        if node.isLeaf:
            label_counts = node.val_label_counts
            mistakes = label_counts[1 - node.prediction]
            return mistakes
        if node.children:
            total_error = 0
            for child in node.children.values():
                total_error += self.compute_subtree_error(child)
            return total_error
        else:
            return (self.compute_subtree_error(node.left)
                    + self.compute_subtree_error(node.right))

    def compute_leaf_error(self, node):
        label_counts = self.get_subtree_label_counts(node)
        mistakes = label_counts[1 - node.prediction]
        return mistakes

    def get_subtree_label_counts(self, node):
        stack = [node]
        total_counts = {0:0, 1:0}
        while stack:
            cur = stack.pop()
            total_counts[0] += cur.val_label_counts[0]
            total_counts[1] += cur.val_label_counts[1]
            if not cur.isLeaf:
                if cur.children:
                    for ch in cur.children.values():
                        stack.append(ch)
                else:
                    if cur.left:  stack.append(cur.left)
                    if cur.right: stack.append(cur.right)
        return total_counts

    def get_internal_nodes(self, node):
        nodes = []
        if node is None:
            return nodes
        if not node.isLeaf:
            nodes.append(node)
            if node.children:
                for ch in node.children.values():
                    nodes.extend(self.get_internal_nodes(ch))
            else:
                nodes.extend(self.get_internal_nodes(node.left))
                nodes.extend(self.get_internal_nodes(node.right))
        return nodes

    def val_prune_iterative(self, valid_df, train_df=None, test_df=None):
        self.reset_label_counts(self.root)
        self.assign_validation_data(self.root, valid_df)
        prune_stats = []


        def get_accuracies():
           
            n_nodes = self.num_nodes(self.root)
            
            train_acc = self.evaluate(train_df) if train_df is not None else 0.0
       
            val_acc   = self.evaluate(valid_df)
           
            test_acc  = self.evaluate(test_df) if test_df is not None else 0.0
            return (n_nodes, train_acc, val_acc, test_acc)


        prune_stats.append(get_accuracies())

        improved = True
        while improved:
            improved = False
           
            nodes = self.get_internal_nodes(self.root)
            if len(nodes) == 0:
                break
    
            full_tree_error = self.compute_subtree_error(self.root)

            best_gain = 0
            best_node = None
     
            for node_candidate in nodes:
                gain, _ = self.test_prune(node_candidate, full_tree_error)
                if gain &gt; best_gain:
                    best_gain = gain
                    best_node = node_candidate
            
    
            if best_gain &gt; 0 and best_node is not None:
                self.force_prune(best_node)
                improved = True

                
                self.reset_label_counts(self.root)
                self.assign_validation_data(self.root, valid_df)

 
                prune_stats.append(get_accuracies())

        return prune_stats


    def test_prune(self, node, full_tree_error):
        subtree_error = self.compute_subtree_error(node)
        leaf_error    = self.compute_leaf_error(node)
        old_error     = full_tree_error
        new_error     = old_error - subtree_error + leaf_error
        gain = old_error - new_error
        return (gain, node)

    def force_prune(self, node):
        if node.children:
            node.children = {}
        else:
            node.left = None
            node.right = None
        node.isLeaf = True

    def reset_label_counts(self, node):
        if node is None:
            return
        node.val_label_counts = {0:0, 1:0}
        if node.children:
            for ch in node.children.values():
                self.reset_label_counts(ch)
        else:
            if node.left:
                self.reset_label_counts(node.left)
            if node.right:
                self.reset_label_counts(node.right)


    def evaluate(self, df):
        preds = self.predict(df)
        return np.mean(preds == df['income'])

    def num_nodes(self, node):
        
        if node is None:
            return 0
        count = 1
        if node.children:
            for ch in node.children.values():
                count += self.num_nodes(ch)
        else:
            count += self.num_nodes(node.left)
            count += self.num_nodes(node.right)
        return count

def main():
    if len(sys.argv)!=6:
        print(' Expected input format :  python decision_tree.py &lt;train_data_path&gt; &lt;validation_data_path&gt; &lt;test_data_path&gt; &lt;output_folder_path&gt; &lt;question_part&gt;')
        sys.exit(1)
    train_data = sys.argv[1] 
    validation_data = sys.argv[2]
    test_data = sys.argv[3] 
    output_folder = sys.argv[4] 
    question_number = sys.argv[5] 

    train_df = pd.read_csv(train_data,skipinitialspace=True)
    valid_df = pd.read_csv(validation_data,skipinitialspace=True)
    test_df = pd.read_csv(test_data,skipinitialspace=True)

    train_df['income'] = train_df['income'].map({'&lt;=50K':0,'&gt;50K':1})
    test_df['income'] = test_df['income'].map({'&lt;=50K':0,'&gt;50K':1})

    valid_df['income'] = valid_df['income'].map({'&lt;=50K':0,'&gt;50K':1})
    is_numeric ={}
    for cols in train_df.columns:
        if train_df[cols].dtype in ['int64','float64','int32','float32'] :
            is_numeric[cols] = 1 
        else:
            is_numeric[cols] = 0 
    if question_number=='a':
        
        model = tree()
        model.train(max_height =20 ,df= train_df,is_numeric=is_numeric)
        # train_pred = model.predict(train_df)
        # valid_pred = model.predict(valid_df)
        test_pred= model.predict(test_df)
    if question_number=='b' :
        selected = []
        for feat in  train_df.columns:
            if is_numeric[feat]==0 and feat!='income' and train_df[feat].nunique() &gt; 2:
                selected.append(feat)
        train_onehot_df = pd.get_dummies(train_df,columns=selected,prefix_sep='_')
        valid_onehot_df = pd.get_dummies(valid_df,columns=selected,prefix_sep='_')
        test_onehot_df = pd.get_dummies(test_df,columns=selected,prefix_sep='_')
        train_onehot_df, valid_onehot_df = train_onehot_df.align(valid_onehot_df, join='outer', axis=1, fill_value=0)
        train_onehot_df, test_onehot_df  = train_onehot_df.align(test_onehot_df, join='outer', axis=1, fill_value=0)

        is_numeric_encoded = {}
        for col in train_onehot_df.columns:
            # if col == 'income':
            #     continue
            if train_onehot_df[col].dtype in ['int64','float64']:
                is_numeric_encoded[col] = 1
            else:
                is_numeric_encoded[col] = 0
        model = tree()
        model.train(max_height =55 ,df= train_onehot_df,is_numeric=is_numeric_encoded)
        # train_pred = model.predict(train_df)
        # valid_pred = model.predict(valid_df)
        test_pred= model.predict(test_onehot_df)
    elif question_number=='c':
        selected = []
        
        for feat in  train_df.columns:
            if is_numeric[feat] == 0 and feat!='income' and train_df[feat].nunique()==2:
                vals = train_df[feat].unique()
                val0, val1 = vals[0],vals[1]
                mapping = {val0: 0, val1: 1}
                train_df[feat] = train_df[feat].map(mapping)
                valid_df[feat] = valid_df[feat].map(mapping)
                test_df[feat] = test_df[feat].map(mapping)
            elif is_numeric[feat]==0 and feat!='income' and train_df[feat].nunique() &gt; 2:
                selected.append(feat)
        train_onehot_df = pd.get_dummies(train_df,columns=selected,prefix_sep='_').astype(int)
        valid_onehot_df = pd.get_dummies(valid_df,columns=selected,prefix_sep='_').astype(int)
        test_onehot_df = pd.get_dummies(test_df,columns=selected,prefix_sep='_').astype(int)
        train_onehot_df, valid_onehot_df = train_onehot_df.align(valid_onehot_df, join='outer', axis=1, fill_value=0)
        train_onehot_df, test_onehot_df  = train_onehot_df.align(test_onehot_df, join='outer', axis=1, fill_value=0)

        is_numeric_encoded = {}
        for col in train_onehot_df.columns:
            # if col == 'income':
            #     continue
            if train_onehot_df[col].dtype != object :
                is_numeric_encoded[col] = 1
            else:
                # print(col)
                is_numeric_encoded[col] = 0
        model = tree()
        model.train(max_height =55,df= train_onehot_df,is_numeric=is_numeric_encoded)
        model.val_prune_iterative(valid_onehot_df)
        # train_pred = model.predict(train_df)
        # valid_pred = model.predict(valid_df)
        test_pred= model.predict(test_onehot_df)
        pass

            
    elif question_number=='d':
        selected = []
        
        for feat in  train_df.columns:
            if is_numeric[feat] == 0 and feat!='income' and train_df[feat].nunique()==2:
                vals = train_df[feat].unique()
                val0, val1 = vals[0],vals[1]
                mapping = {val0: 0, val1: 1}
                train_df[feat] = train_df[feat].map(mapping)
                valid_df[feat] = valid_df[feat].map(mapping)
                test_df[feat] = test_df[feat].map(mapping)
            elif is_numeric[feat]==0 and feat!='income' and train_df[feat].nunique() &gt; 2:
                selected.append(feat)
        train_onehot_df = pd.get_dummies(train_df,columns=selected,prefix_sep='_').astype(int)
        valid_onehot_df = pd.get_dummies(valid_df,columns=selected,prefix_sep='_').astype(int)
        test_onehot_df = pd.get_dummies(test_df,columns=selected,prefix_sep='_').astype(int)
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match6-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        train_onehot_df, valid_onehot_df = train_onehot_df.align(valid_onehot_df, join='outer', axis=1, fill_value=0)
        train_onehot_df, test_onehot_df  = train_onehot_df.align(test_onehot_df, join='outer', axis=1, fill_value=0)

        data_train_df = train_onehot_df.drop(columns=["income"])
</FONT>        label_train_df = train_onehot_df['income'].values 
        data_test_df = test_onehot_df.drop(columns=["income"])
        label_test_df =  test_onehot_df['income'].values  
        data_valid_df = valid_onehot_df.drop(columns=["income"])
        label_valid_df =  valid_onehot_df['income'].values 
        
        model= DecisionTreeClassifier(criterion='entropy',max_depth=None, ccp_alpha=0.001)
        model.fit(data_train_df,label_train_df)
        test_pred = model.predict(data_test_df)



    elif question_number=='e':
        selected = []
        
        for feat in  train_df.columns:
            if is_numeric[feat] == 0 and feat!='income' and train_df[feat].nunique()==2:
                vals = train_df[feat].unique()
                val0, val1 = vals[0],vals[1]
                mapping = {val0: 0, val1: 1}
                train_df[feat] = train_df[feat].map(mapping)
                valid_df[feat] = valid_df[feat].map(mapping)
                test_df[feat] = test_df[feat].map(mapping)
            elif is_numeric[feat]==0 and feat!='income' and train_df[feat].nunique() &gt; 2:
                selected.append(feat)
        train_onehot_df = pd.get_dummies(train_df,columns=selected,prefix_sep='_').astype(int)
        valid_onehot_df = pd.get_dummies(valid_df,columns=selected,prefix_sep='_').astype(int)
        test_onehot_df = pd.get_dummies(test_df,columns=selected,prefix_sep='_').astype(int)
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match6-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        train_onehot_df, valid_onehot_df = train_onehot_df.align(valid_onehot_df, join='outer', axis=1, fill_value=0)
        train_onehot_df, test_onehot_df  = train_onehot_df.align(test_onehot_df, join='outer', axis=1, fill_value=0)

        data_train_df = train_onehot_df.drop(columns=["income"])
</FONT>        label_train_df = train_onehot_df['income'].values 
        data_test_df = test_onehot_df.drop(columns=["income"])
        label_test_df =  test_onehot_df['income'].values  
        data_valid_df = valid_onehot_df.drop(columns=["income"])
        label_valid_df =  valid_onehot_df['income'].values
        # def oob_scorer(estimator, X, y):
        #     return estimator.oob_score_

        # my_oob_scorer = make_scorer(oob_scorer, greater_is_better=True)
        param_grid = {
        'n_estimators':      [50, 150, 250, 350],
        'max_features':      [0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
        'min_samples_split': [2, 4, 6, 8, 10],}
        rf = RandomForestClassifier(criterion='entropy', oob_score=True, random_state=42)
        grid = GridSearchCV(estimator=rf,param_grid=param_grid,scoring='accuracy', cv=10,n_jobs=-1,verbose=1
)
        grid.fit(data_train_df, label_train_df)
        best_model = grid.best_estimator_
        test_pred = best_model.predict(data_test_df)
        # n_estimators = [50, 150, 250, 350]  
        # max_features  = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
        # min_samples_splits = [2, 4, 6, 8, 10]
        # maxi = 0 
        # besti= None 

        # results = []

        # for i in n_estimators:
        #     for j in max_features:
        #         for k in min_samples_splits:
        #             model = RandomForestClassifier(criterion='entropy',n_estimators=i,max_features=j,min_samples_split=k,oob_score=True) 
        #             model.fit(data_train_df, label_train_df)
        #             oob_accuracy = model.oob_score_
        #             results.append({
        #                 'n_estimators': i,
        #                 'max_features': j,
        #                 'min_samples_split': k,
        #                 'oob_accuracy': oob_accuracy
        #             })

        #             if oob_accuracy &gt; maxi:
        #                 maxi = oob_accuracy
        #                 besti    = (i, j, k)
        # n_estimator,max_feature,min_split = besti

        # model = RandomForestClassifier(criterion='entropy',n_estimators=n_estimator,max_features=max_feature,min_samples_split=min_split,oob_score=True) 
        # model.fit(data_train_df,label_train_df)
        # test_pred = model.predict(data_test_df)


        

    output_path  = os.path.join(output_folder,f"prediction_{question_number}.csv")
    out_df = pd.DataFrame({'prediction':test_pred})
    out_df.to_csv(output_path,index=False)



if __name__ == "__main__":
    main()




import sys
import os
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.neural_network import MLPClassifier

def load_train(path, sz=(28,28)):
    Xlist = []
    ylist = []
    dirs = sorted(os.listdir(path))
    for d in dirs:
        p = os.path.join(path, d)
        if not os.path.isdir(p):
            continue
        c = int(d)
        files = sorted(os.listdir(p))
        for f in files:
            fp = os.path.join(p, f)
            im = Image.open(fp).convert('RGB')
            arr = np.array(im).astype(np.float32)/255.0
            flt = arr.flatten()
            Xlist.append(flt)
            ylist.append(c)
    X = np.array(Xlist, dtype=np.float32)
    Y = np.array(ylist, dtype=int)
    return X, Y

def load_test(path, sz=(28,28)):
    Xlist = []
    files = sorted(os.listdir(path))
    for f in files:
    
        fp = os.path.join(path, f)
        im = Image.open(fp).convert('RGB')
        arr = np.array(im).astype(np.float32)/255.0
        flt = arr.flatten()
        Xlist.append(flt)
    X = np.array(Xlist, dtype=np.float32)
    return X
class NN:
    def __init__(self, nf, hid, nc, mb=32, lr=0.01):
        self.nf = nf
        self.hid = hid
        self.nc = nc
        self.mb = mb
        self.lr = lr
        lz = [nf]+hid+[nc]
        self.ws = []
        self.bs = []
        for i in range(len(lz)-1):
            f1 = lz[i]
            f2 = lz[i+1]
            lim = np.sqrt(6.0/(f1+f2))
            W = np.random.uniform(-lim, lim, (f1, f2))
            B = np.zeros((1, f2))
            self.ws.append(W)
            self.bs.append(B)
    def sg(self, z):
        return 1.0/(1.0+np.exp(-z))
    def sgp(self, a):
        return a*(1.0-a)
    def sm(self, z):
        e = np.exp(z - np.max(z, axis=1, keepdims=True))
        return e/np.sum(e, axis=1, keepdims=True)
    def fwd(self, X):
        aa = [X]
        zz = []
        for i in range(len(self.ws)):
            z = aa[-1]@self.ws[i] + self.bs[i]
            zz.append(z)
            if i == len(self.ws)-1:
                a = self.sm(z)
            else:
                a = self.sg(z)
            aa.append(a)
        return zz, aa
    def ce(self, yt, yp):
        eps = 1e-10
        lp = np.log(yp+eps)
        return -np.sum(yt*lp)/yt.shape[0]
    def bwd(self, zz, aa, oh):
        gw = [None]*len(self.ws)
        gb = [None]*len(self.bs)
        m = oh.shape[0]
        d = (aa[-1] - oh)/m
        gw[-1] = aa[-2].T@d
        gb[-1] = np.sum(d, axis=0, keepdims=True)
        for i in range(len(self.ws)-2, -1, -1):
            d = d@self.ws[i+1].T
            d = d*self.sgp(aa[i+1])
            gw[i] = aa[i].T@d
            gb[i] = np.sum(d, axis=0, keepdims=True)
        return gw, gb

    def upd(self, gw, gb):
        for i in range(len(self.ws)):
            self.ws[i] -= self.lr*gw[i]
            self.bs[i] -= self.lr*gb[i]

    def oh(self, y, c):
        M = y.shape[0]
        tmp = np.zeros((M, c))
        for i in range(M):
            tmp[i, y[i]] = 1.0
        return tmp

    def train(self, X, y, epochs=20):
        N = X.shape[0]
        c = self.nc
        for e in range(epochs):
            pm = np.random.permutation(N)
            Xs = X[pm]
            ys = y[pm]
            i = 0
            while i &lt; N:
                Xb = Xs[i:i+self.mb]
                yb = ys[i:i+self.mb]
                ohb = self.oh(yb, c)
                zz, aa = self.fwd(Xb)
                gw, gb = self.bwd(zz, aa, ohb)
                self.upd(gw, gb)
                i += self.mb
            pdp = self.prd(X)
            acc = np.mean(pdp==y)
            print(f"Epoch {e+1}/{epochs}, Train Acc = {acc:.4f}")

    def prd(self, X):
        _, aa = self.fwd(X)
        return np.argmax(aa[-1], axis=1)
def main():
    if len(sys.argv)!=5:
        print("Usage: python neural_network.py &lt;train_dir&gt; &lt;test_dir&gt; &lt;output_folder&gt; &lt;question_part&gt;")
        sys.exit(1)
    trd  = sys.argv[1]
    ted  = sys.argv[2]
    outp = sys.argv[3]
    qp   = sys.argv[4]
    
    Xt, Yt = load_train(trd, (28,28))
    Xv = load_test(ted,(28,28))
    
    nf = Xt.shape[1]
    nc = len(np.unique(Yt))
    mb = 32
    lr = 0.01
    if qp =='b':
        hid = [100]
        ep = 40
        net = NN(nf, hid, nc, mb, lr)
        net.train(Xt, Yt, ep)
        pte = net.prd(Xv)
        opf = os.path.join(outp, f"prediction_{qp}.csv")
        pd.DataFrame({'prediction': pte}).to_csv(opf, index=False)
    elif qp =='c' or qp=='d' or qp=='e':
        hid = [512, 256, 128, 64]
        ep =40
        net = NN(nf, hid, nc, mb, lr)
        net.train(Xt, Yt, ep)
        pte = net.prd(Xv)
        opf = os.path.join(outp, f"prediction_{qp}.csv")
        pd.DataFrame({'prediction': pte}).to_csv(opf, index=False)
    elif qp=='f':
        model_clf = MLPClassifier(hidden_layer_sizes=[512, 256, 128, 64],activation='relu',solver='sgd',alpha=0,batch_size=32,learning_rate='invscaling',max_iter=50, shuffle=True,random_state=42)
        model_clf.fit(Xt,Yt)
        pte=  model_clf.predict(Xv) 
    
    opf = os.path.join(outp, f"prediction_{qp}.csv")
    pd.DataFrame({'prediction': pte}).to_csv(opf, index=False)  
        

if __name__=="__main__":
    main()




#!/usr/bin/env python
# coding: utf-8

# In[20]:


import sys
import os
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.metrics import classification_report
import importlib
import matplotlib.pyplot as plt


# In[3]:


import neural_network


# In[10]:


importlib.reload(neural_network)
import neural_network
from neural_network import NN,load_test,load_train


# In[11]:


X_train, y_train  = load_train("train")


# In[16]:


X_test  = load_test("test")


# In[ ]:


<A NAME="4"></A><FONT color = #FF00FF><A HREF="match6-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

nf = X_train.shape[1]
nc = len(np.unique(y_train))
hlu = [1,5,10,50,100]
train_f1_list = []
test_f1_list = []
y_test = "test_labels.csv"
</FONT>df_labels = pd.read_csv("test_labels.csv", skipinitialspace=True)
y_test = df_labels.iloc[:,1].values
for hid in hlu:
    model_nn = NN(nf, [hid], nc, 32, 0.01)
    model_nn.train(X_train,y_train,40)
    ptr = model_nn.prd(X_train)
    tracc = np.mean(ptr==y_train)
    pte = model_nn.prd(X_test)
   
    train_report_dict = classification_report(y_train, ptr, digits = 4 ,output_dict=True)
    test_report_dict = classification_report( y_test, pte, digits = 4 ,output_dict=True)
    
   
    train_macro_f1 = train_report_dict['macro avg']['f1-score']
    test_macro_f1  = test_report_dict['macro avg']['f1-score']
    
    train_f1_list.append(train_macro_f1)
    test_f1_list.append(test_macro_f1)
    
    # Print the full textual reports
    print("Train Classification Report:")
    print(classification_report(y_train, ptr, digits=4))
    print("Test Classification Report:")
    print(classification_report(y_test, pte, digits=4))


plt.figure()
plt.plot(hlu, train_f1_list, marker='o', label='Train Macro-F1')
plt.plot(hlu, test_f1_list,  marker='s', label='Test  Macro-F1')
plt.xlabel("Hidden Layer Units")
plt.ylabel("Macro-F1 Score")
plt.title("Average F1 vs Number of Hidden Units")
plt.legend()
plt.savefig("f1_vs_hidden_units.png")
plt.close()


# In[ ]:


print(train_f1_list)
print(test_f1_list)


# In[21]:


<A NAME="6"></A><FONT color = #00FF00><A HREF="match6-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.figure()
plt.plot(hlu, train_f1_list, marker='o', label='Train Macro-F1')
plt.plot(hlu, test_f1_list,  marker='s', label='Test  Macro-F1')
plt.xlabel("Hidden Layer Units")
plt.ylabel("Macro-F1 Score")
</FONT>plt.title("Average F1 vs Number of Hidden Units")
plt.legend()
plt.savefig("f1_vs_hidden_units.png")
plt.close()


# In[ ]:


from sklearn.neural_network import MLPClassifier
<A NAME="1"></A><FONT color = #00FF00><A HREF="match6-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

nf = X_train.shape[1]
nc = len(np.unique(y_train))
hlu = [[512],[512,216],[512,256,128],[512,256,128,64]]
hlu_idx = [1,2,3,4]
</FONT>train_f1_list_f = []
test_f1_list_f = []
y_test = "test_labels.csv"
df_labels = pd.read_csv("test_labels.csv", skipinitialspace=True)
y_test = df_labels.iloc[:,1].values
for hid in hlu:
    model_nn = MLPClassifier(hidden_layer_sizes=hid,activation='relu',solver='sgd',alpha=0,batch_size=32,learning_rate='invscaling',max_iter=50, shuffle=True,random_state=42)
    model_nn.fit(X_train,y_train)
    ptr = model_nn.predict(X_train)
    pte = model_nn.predict(X_test)
   
    train_report_dict = classification_report(y_train, ptr, digits = 4 ,output_dict=True)
    test_report_dict = classification_report( y_test, pte, digits = 4 ,output_dict=True)
    
   
    train_macro_f1 = train_report_dict['macro avg']['f1-score']
    test_macro_f1  = test_report_dict['macro avg']['f1-score']
    
    train_f1_list_f.append(train_macro_f1)
    test_f1_list_f.append(test_macro_f1)
    
    # Print the full textual reports
    print("Train Classification Report:")
    print(classification_report(y_train, ptr, digits=4))
    print("Test Classification Report:")
    print(classification_report(y_test, pte, digits=4))


plt.figure()
plt.plot(hlu_idx, train_f1_list_f, marker='o', label='Train Macro-F1')
plt.plot(hlu_idx, test_f1_list_f,  marker='s', label='Test  Macro-F1')
plt.xlabel("Hidden Layer Units")
plt.ylabel("Macro-F1 Score")
plt.title("Average F1 vs Number of Hidden Units")
plt.legend()
plt.savefig("f1_vs_hidden_units.png")
plt.close()


# In[26]:


hlu_idx = [1,2,3,4]
<A NAME="7"></A><FONT color = #0000FF><A HREF="match6-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.figure()
plt.plot(hlu_idx, train_f1_list_c, marker='o', label='Train Macro-F1')
plt.plot(hlu_idx, test_f1_list_c,  marker='s', label='Test  Macro-F1')
plt.xlabel("Hidden Layer Units")
plt.ylabel("Macro-F1 Score")
</FONT>plt.title("Average F1 vs Number of Hidden Units")
plt.legend()
plt.savefig("f1_vs_hidden_units.png")
plt.close()


# In[ ]:


<A NAME="2"></A><FONT color = #0000FF><A HREF="match6-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

nf = X_train.shape[1]
nc = len(np.unique(y_train))
hlu = [[512],[512,216],[512,256,128],[512,256,128,64]]
train_f1_list_c = []
</FONT>test_f1_list_c = []
y_test = "test_labels.csv"
df_labels = pd.read_csv("test_labels.csv", skipinitialspace=True)
y_test = df_labels.iloc[:,1].values
for hid in hlu:
    model_nn = NN(nf, hid, nc, 32, 0.01)
    model_nn.train(X_train,y_train,20)
    ptr = model_nn.prd(X_train)
    tracc = np.mean(ptr==y_train)
    pte = model_nn.prd(X_test)
   
    train_report_dict = classification_report(y_train, ptr, digits = 4 ,output_dict=True)
    test_report_dict = classification_report( y_test, pte, digits = 4 ,output_dict=True)
    
   
    train_macro_f1 = train_report_dict['macro avg']['f1-score']
    test_macro_f1  = test_report_dict['macro avg']['f1-score']
    
    train_f1_list_c.append(train_macro_f1)
    test_f1_list_c.append(test_macro_f1)
    
    # Print the full textual reports
    print("Train Classification Report:")
    print(classification_report(y_train, ptr, digits=4))
    print("Test Classification Report:")
    print(classification_report(y_test, pte, digits=4))



</PRE>
</PRE>
</BODY>
</HTML>
