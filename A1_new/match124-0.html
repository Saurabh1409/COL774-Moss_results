<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_1HEUC.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.maxInterations = 100000 
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        
        for iter in range(self.maxInterations):
            y_pred = XWithIntercept.dot(self.theta)
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            oldTheta = self.theta.copy()
            self.theta -= learning_rate * gradient
            if(np.linalg.norm(oldTheta - self.theta)&lt;1e-6):
                # print("Converged at iteration: ", iter)
                break
        else:
            # print("Did not converge")
            pass
        return self.theta
    
    def initialize_theta(self, n_features):
        self.theta = np.zeros(n_features + 1)

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        return XWithIntercept.dot(self.theta)[0]
    
    def fit_with_theta_details(self, X, y,learning_rate=0.01):
        
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        prev_time = time.time()
        time.sleep(0.2)
        theta0 = []
        theta1 = []
        m = []
        for iter in range(self.maxInterations):
            y_pred = XWithIntercept.dot(self.theta)
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            oldTheta = self.theta.copy()
            self.theta -= learning_rate * gradient
            m.append(self.calculate_cost(X, y))
            theta0.append(self.theta[0])
            theta1.append(self.theta[1])
            
            if(np.linalg.norm(oldTheta - self.theta)&lt;1e-6):
                # print("Converged at iteration: ", iter)
                break
        else:
            # print("Did not converge")
            pass
        
        return (theta0, theta1, m)
    
    def calculate_cost(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        error = y_pred - y[:,0]
        return (1/(2*len(y))) * np.sum(error**2)
    
    def plot_cost_function(self, X, y, theta_range=((-50, 50),(-50, 50)), resolution=100):
        # Preparing data
        theta_vals = np.linspace(theta_range[0][0], theta_range[0][1], resolution), np.linspace(theta_range[1][0], theta_range[1][1], resolution)

        theta = np.meshgrid(theta_vals[0], theta_vals[1])

        J_vals = np.zeros(theta[0].shape)

        for i in range(len(theta_vals[0])):
            for j in range(len(theta_vals[1])):
                theta0 = theta[0][i, j]
                theta1 = theta[1][i, j]
                predictions = theta0 + theta1 * X
                errors = y - predictions
<A NAME="2"></A><FONT color = #0000FF><A HREF="match124-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                J_vals[i, j] = (1 / (2 * len(y))) * np.sum(errors ** 2)
        


        # Plot the cost function surface
        fig = plt.figure(figsize=(10, 7))

        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(theta[0], theta[1], J_vals, cmap='viridis', edgecolor='none',alpha=0.6)
</FONT>
        theta_details = self.fit_with_theta_details(X, y)
        ax.plot(theta_details[0], theta_details[1], theta_details[2], color='b', marker='o', linestyle='dashed', linewidth=2, markersize=5)

        ax.set_xlabel(r'$\theta 0$')
        ax.set_ylabel(r'$\theta 1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Cost Function Surface')

        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)

        plt.show()

    def plot_contours(self, X, y,learning_rate=0.01, theta_range=((-50, 50),(-50, 50)), resolution=100):
        # Preparing data
        theta_vals = np.linspace(theta_range[0][0], theta_range[0][1], resolution), np.linspace(theta_range[1][0], theta_range[1][1], resolution)

        theta = np.meshgrid(theta_vals[0], theta_vals[1])

        J_vals = np.zeros(theta[0].shape)

        for i in range(len(theta_vals[0])):
            for j in range(len(theta_vals[1])):
                theta0 = theta[0][i, j]
                theta1 = theta[1][i, j]
                predictions = theta0 + theta1 * X
                errors = y - predictions
                J_vals[i, j] = (1 / (2 * len(y))) * np.sum(errors ** 2)
        
        # Plot the cost function contours
        plt.figure(figsize=(10, 7))
        plt.contour(theta[0], theta[1], J_vals, levels=np.logspace(-2, 3, 20))
        plt.xlabel(r'$\theta 0$')
        plt.ylabel(r'$\theta 1$')
        plt.title('Cost Function Contours')
        
        theta_details = self.fit_with_theta_details(X, y,learning_rate)
        plt.scatter(theta_details[0], theta_details[1], color='green', s=10)

        plt.show()

    def drawHypothesis(self, X, y):

        plt.scatter(X[:,0], y[:,0], color='#003366')
      
        self.fit(X, y)
        
        line_x = np.linspace(np.min(X[:,0])-0.25, np.max(X[:,0])+0.25)
        line_y = self.theta[0] + self.theta[1] * line_x 
        plt.plot(line_x, line_y, color='#3D7317')
        plt.xlabel("X axis")
        plt.ylabel("Y axis")
       
        plt.show()

# X = pd.read_csv('data/Q1/linearX.csv').values
# y = pd.read_csv('data/Q1/linearY.csv').values

# lr = LinearRegressor()
# print(lr.fit(X, y, 0.01))

# lr.drawHypothesis(X, y)
# lr.plot_cost_function(X, y)
# lr.plot_contours(X, y,0.01)

# for x in (0.001,0.025, 0.1):
#     lr.plot_contours(X, y, x)




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match124-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise

    return np.column_stack((x1, x2)), y
</FONT>
def split_train_test(N, X, y, train_ratio=0.8):
    data = pd.DataFrame({'x1': X[:,0], 'x2': X[:,1], 'y': y})

    data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index

    train_size = int(train_ratio * N)

    train_data = data.iloc[:train_size]
    test_data = data.iloc[train_size:]

    return (train_data[['x1', 'x2']].to_numpy(), train_data['y'].to_numpy()), (test_data[['x1', 'x2']].to_numpy(), test_data['y'].to_numpy())

class StochasticLinearRegressor:
    def __init__(self):
        self.r = 80
        self.theta = None
        self.maxInterations = 1000
    
    def updateIteration(self,val):
        if(val &lt; 1000):
            self.maxInterations = 1000
        else:
            self.maxInterations = 100000

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        self.updateIteration(self.r)
        prev_cost = 0
        indices = range(m)
        X_shuffled = XWithIntercept[indices]
        y_shuffled = y[indices]
        for iter in range(self.maxInterations):
            cost = 0
            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                
                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)
                cost += np.mean((error)**2)
                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                # print(self.theta)
            
            cost /= (m // self.r)
            # if np.linalg.norm(oldTheta - self.theta) &lt; 1e-5:
            #     print("Converged at iteration:", iter)
            #     return self.theta
            if iter &gt; 0 and np.linalg.norm(prev_cost - cost) &lt; 1e-6:
                # print("Converged at iteration:", iter)
                return self.theta
            prev_cost = cost
        
        # print("Did not converge")

        return self.theta
    
    def initialize_theta(self, n_features):
        self.theta = np.zeros(n_features + 1)

    def fit_with_batch_size(self, X, y, learning_rate=0.001, batch_size=1):
        self.r = batch_size
        return self.fit(X, y, learning_rate)
    
    def fit_get_all_thetas(self, X, y, learning_rate=0.001):
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        all_thetas = []
        self.updateIteration(self.r)
        indices = np.random.permutation(m)
        X_shuffled = XWithIntercept[indices]
        y_shuffled = y[indices]
        prev_cost = 0
        for iter in range(self.maxInterations):
            cost = 0
            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                
                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)
                cost += np.mean((error)**2)
                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                all_thetas.append(self.theta.copy())
            cost /= (m // self.r)
            if iter &gt; 0 and np.linalg.norm(prev_cost - cost) &lt; 1e-6:
                # print("Converged at iteration:", iter)
                return np.array(all_thetas)
            prev_cost = cost
        return np.array(all_thetas)

    def draw_plot_3d_theta_movement(self, X, y, number_of_points=100, learning_rate=0.001):
        batch_sizes = [1, 80, 8000, 800000]
        color = {1: 'r', 80: 'g', 8000: 'b', 800000: 'y'}

        for i in batch_sizes:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111, projection='3d')
            self.r = i
            all_thetas = self.fit_get_all_thetas(X, y, learning_rate)

            theta_0 = [theta[0] for theta in all_thetas]
            theta_1 = [theta[1] for theta in all_thetas]
            theta_2 = [theta[2] for theta in all_thetas]

            ax.plot(theta_0, theta_1, theta_2, c=color[i], marker='o', label='Theta Movement ' + str(i))
            ax.scatter(self.theta[0], self.theta[1], self.theta[2], c=color[i], marker='x')

            ax.set_xlabel(r'$\theta_0$')
            ax.set_ylabel(r'$\theta_1$')
            ax.set_zlabel(r'$\theta_2$')
            ax.set_title(f'3D Plot of Theta Updates for Batch Size {i}')

            # Ensure label is only added once
            if len(all_thetas) &gt; 1:
                ax.legend()

        plt.show()

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match124-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit_with_closed_form(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        self.theta = np.linalg.inv(XWithIntercept.T.dot(XWithIntercept)).dot(XWithIntercept.T).dot(y)
</FONT>        return self.theta

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return y_pred
    
    def test(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return np.mean((y_pred - y)**2)
    

# X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), np.sqrt(2))

# train, test = split_train_test(1000000, X, y)

# slr = StochasticLinearRegressor()

# theta_with_closed_form = slr.fit_with_closed_form(X, y)
# print(theta_with_closed_form)
# for i in [1,80,8000,800000]:
#     print(i)

#     theta_with_slr = slr.fit_with_batch_size(train[0], train[1], learning_rate=0.001, batch_size=i)
   
#     mse_test_data = slr.test(test[0], test[1])
#     mse_train_data = slr.test(train[0], train[1])

#     print(theta_with_slr)
#     print(mse_test_data)
#     print(mse_train_data)

# slr.draw_plot_3d_theta_movement(train[0], train[1])



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        norm_X = self.normalize(X)
        self.theta = np.zeros(norm_X.shape[1] + 1)
        XWithIntercept = np.c_[np.ones((norm_X.shape[0], 1)), norm_X]
        m = XWithIntercept.shape[0]
        for iter in range(1000):
            y_pred = self.sigmoid(XWithIntercept.dot(self.theta))
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            R = np.diag((y_pred * (1 - y_pred)).flatten()) 
            H = XWithIntercept.T.dot(R).dot(XWithIntercept)
            oldTheta = self.theta.copy()
            self.theta -= np.linalg.inv(H).dot(gradient)

        return self.theta

    def sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    def normalize(self, X):
        return (X - X.mean(axis=0))/X.std(axis=0)
        
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # norm_X = self.normalize(X)
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        probabilities = self.sigmoid(np.array(XWithIntercept.dot(self.theta)))
        return (probabilities &gt;= 0.5).astype(int)


    def draw_plot(self,X,y):
        theta = self.fit(X,y)
        for i in range(len(y)):
            if y[i] == 0:
                plt.scatter(X[i,0], X[i,1], color='red', marker='o')
            else:
                plt.scatter(X[i,0], X[i,1], color='blue', marker='x')
        # draw using theta on the same plot
        x1 = np.linspace(0, 10, 100)
        x2 = (-theta[0] - theta[1]*x1)/theta[2]
        plt.plot(x1, x2, color='black')

        plt.show()
        

# X = pd.read_csv('data/Q3/logisticX.csv').values
# y = pd.read_csv('data/Q3/logisticY.csv').values

# lr = LogisticRegressor()
# theta = lr.fit(X, y)
# print(theta)
# lr.draw_plot(X,y)
# print(lr.predict(np.array([[1,2]]))==0)
# print(lr.predict(np.array([[5,20]]))==0)
# print(lr.predict(np.array([[10,2]]))==0)




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.initialize_parameters()
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.initialize_parameters()
        standardized_features = (X - X.mean(axis=0)) / (X.std(axis=0) )

        self.X_mean = X.mean(axis=0)
        self.X_std = X.std(axis=0)

        # Split data by class labels
        features_class_0, features_class_1 = standardized_features[y == 0], standardized_features[y == 1]
        self.mean_vector_class_0 = features_class_0.mean(axis=0)
        self.mean_vector_class_1 = features_class_1.mean(axis=0)

        # Compute covariance matrices for both classes
        num_samples_class_0, num_samples_class_1 = len(features_class_0), len(features_class_1)
        self.covariance_matrix_class_0 = np.cov(features_class_0.T, bias=True)
        self.covariance_matrix_class_1 = np.cov(features_class_1.T, bias=True)

        # Compute shared covariance matrix if assumption holds
        if assume_same_covariance:
            self.shared_covariance_matrix = np.array((
                num_samples_class_0 * self.covariance_matrix_class_0 + 
                num_samples_class_1 * self.covariance_matrix_class_1
            ) / (num_samples_class_0 + num_samples_class_1))

        # Compute prior probability for class 1
        self.prior_probability_class_1 = y.mean()

        if assume_same_covariance:
            return self.mean_vector_class_0, self.mean_vector_class_1, self.shared_covariance_matrix
        else:
            return self.mean_vector_class_0, self.mean_vector_class_1, self.covariance_matrix_class_0, self.covariance_matrix_class_1
        
    def initialize_parameters(self):
        self.mean_vector_class_0 = None
        self.mean_vector_class_1 = None
        self.shared_covariance_matrix = None
        self.covariance_matrix_class_0 = None
        self.covariance_matrix_class_1 = None
        self.prior_probability_class_1 = None
        self.X_mean = None
        self.X_std = None
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Standardize using training mean and std
<A NAME="0"></A><FONT color = #FF0000><A HREF="match124-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = (X - self.X_mean) / self.X_std  
        
        if(self.shared_covariance_matrix is not None):
            # Compute the inverse of the shared covariance matrix
            inv_cov = np.linalg.inv(self.shared_covariance_matrix)

            # Compute quadratic discriminant scores (delta functions)
            delta_0 = -0.5 * np.sum((X - self.mean_vector_class_0) @ inv_cov * (X - self.mean_vector_class_0), axis=1) + np.log(1 - self.prior_probability_class_1)
            delta_1 = -0.5 * np.sum((X - self.mean_vector_class_1) @ inv_cov * (X - self.mean_vector_class_1), axis=1) + np.log(self.prior_probability_class_1)
</FONT>
            # Predict class 1 if delta_1 &gt; delta_0, otherwise class 0
            return (delta_1 &gt; delta_0).astype(int)

def plot_data(X, y, title="Salmon Growth Ring Diameters by Region"):
    """
    Plot the training data with different symbols for each class.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input features (growth ring diameters)
    y : numpy array of shape (n_samples,)
        The target labels (0 for Alaska, 1 for Canada)
    title : str, optional
        The title for the plot
    """
    plt.figure(figsize=(10, 6))
    
    # Plot Alaska samples (class 0) with blue circles
    plt.scatter(X[y == 0, 0], X[y == 0, 1], 
               c='green', marker='o', label='Alaska', 
               alpha=0.6)
    
    # Plot Canada samples (class 1) with red triangles
    plt.scatter(X[y == 1, 0], X[y == 1, 1], 
               c='red', marker='^', label='Canada', 
               alpha=0.6)
    
    # Add labels and title
    plt.xlabel('Growth Ring Diameter (Fresh Water)')
    plt.ylabel('Growth Ring Diameter (Marine Water)')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()

def decision_boundary(X, y, mu_0, mu_1, sigma, prior_prob_1, X_mean, X_std):
    """
    Plot the data points and decision boundary for GDA.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, 2)
        The input features (original, non-normalized data)
    y : numpy array of shape (n_samples,)
        The target labels (0 for Alaska, 1 for Canada)
    mu_0 : numpy array of shape (2,)
        Mean vector for class 0 (Alaska) in normalized space
    mu_1 : numpy array of shape (2,)
        Mean vector for class 1 (Canada) in normalized space
    sigma : numpy array of shape (2, 2)
        Shared covariance matrix in normalized space
    prior_prob_1 : float
        Prior probability of class 1 (Canada)
    X_mean : numpy array of shape (2,)
        Mean used for normalization
    X_std : numpy array of shape (2,)
        Standard deviation used for normalization
    """
    plt.figure(figsize=(10, 6))
    
    # Plot original data points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], 
               c='green', marker='o', label='Alaska', 
               alpha=0.6)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], 
               c='red', marker='^', label='Canada', 
               alpha=0.6)
    
    # Calculate decision boundary
    sigma_inv = np.linalg.inv(sigma)
    
    # Get the range for plotting in original scale
<A NAME="1"></A><FONT color = #00FF00><A HREF="match124-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    
    # Generate points for the decision boundary
    x1_grid = np.linspace(x_min, x_max, 100)
    
    # Transform grid points to normalized space
    x1_grid_norm = (x1_grid - X_mean[0]) / X_std[0]
</FONT>    
    # Calculate the decision boundary in normalized space
    diff_mu = mu_1 - mu_0
    w = sigma_inv @ diff_mu
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)
    b += np.log(prior_prob_1 / (1 - prior_prob_1))
    
    # Calculate x2 in normalized space
    x2_grid_norm = (-w[0] * x1_grid_norm - b) / w[1]
    
    # Transform back to original space
    x2_grid = x2_grid_norm * X_std[1] + X_mean[1]
    
    # Plot decision boundary
    plt.plot(x1_grid, x2_grid, 'blue', label='Decision Boundary')
    
    # Add labels and title
    plt.xlabel('Growth Ring Diameter (Fresh Water)')
    plt.ylabel('Growth Ring Diameter (Marine Water)')
    plt.title('GDA Decision Boundary for Salmon Classification')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()

def quadratic_decision_boundary(X, y, gda_linear, gda_quadratic):
    """
    Plot both linear and quadratic decision boundaries along with normalized data.
    Uses explicit quadratic equation: X^T A X + B X + C = 0
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, 2)
        The input features
    y : numpy array of shape (n_samples,)
        The target labels
    gda_linear : GaussianDiscriminantAnalysis
        Fitted GDA model with shared covariance
    gda_quadratic : GaussianDiscriminantAnalysis
        Fitted GDA model with separate covariances
    """
    plt.figure(figsize=(10, 8))
    
    # Normalize the data for plotting
    X_normalized = (X - gda_quadratic.X_mean) / gda_quadratic.X_std
    
    # Plot normalized data points
    plt.scatter(X_normalized[y == 0, 0], X_normalized[y == 0, 1], 
               c='red', marker='o', label='Alaska')
    plt.scatter(X_normalized[y == 1, 0], X_normalized[y == 1, 1], 
               c='blue', marker='x', label='Canada')
    
    # Create a grid of points
    x_min, x_max = X_normalized[:, 0].min() - 0.5, X_normalized[:, 0].max() + 0.5
    y_min, y_max = X_normalized[:, 1].min() - 0.5, X_normalized[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), 
                        np.linspace(y_min, y_max, 100))
    
    # Calculate quadratic boundary parameters
    Sigma0_inv = np.linalg.inv(gda_quadratic.covariance_matrix_class_0)
    Sigma1_inv = np.linalg.inv(gda_quadratic.covariance_matrix_class_1)
    mu0 = gda_quadratic.mean_vector_class_0
    mu1 = gda_quadratic.mean_vector_class_1
    phi = gda_quadratic.prior_probability_class_1
    
    # Calculate A, B, C matrices for quadratic equation
    A = Sigma0_inv - Sigma1_inv
    B = -2 * (mu0.dot(Sigma0_inv) - mu1.dot(Sigma1_inv))
    C = (mu0.dot(Sigma0_inv).dot(mu0) - 
         mu1.dot(Sigma1_inv).dot(mu1) - 
         2 * np.log(((1/phi) - 1) * 
         np.sqrt(np.linalg.det(gda_quadratic.covariance_matrix_class_1) / 
                np.linalg.det(gda_quadratic.covariance_matrix_class_0))))
    
    # Calculate quadratic boundary values
    Z_quad = np.zeros_like(xx)
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            X_point = np.array([xx[i,j], yy[i,j]])
            Z_quad[i,j] = X_point.dot(A).dot(X_point) + B.dot(X_point) + C
    
    # Calculate linear decision boundary using shared covariance
    Sigma_shared_inv = np.linalg.inv(gda_linear.shared_covariance_matrix)
    mu0_linear = gda_linear.mean_vector_class_0
    mu1_linear = gda_linear.mean_vector_class_1
    phi_linear = gda_linear.prior_probability_class_1
    
    # Calculate linear boundary values
    Z_linear = np.zeros_like(xx)
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            X_point = np.array([xx[i,j], yy[i,j]])
            Z_linear[i,j] = (X_point - 0.5*(mu0_linear + mu1_linear)).dot(Sigma_shared_inv).dot(
                mu1_linear - mu0_linear) + np.log(phi_linear/(1-phi_linear))
    
    # Plot decision boundaries
    plt.contour(xx, yy, Z_linear, colors='green', levels=[0], 
                linestyles='-', label='Linear Boundary')
    plt.contour(xx, yy, Z_quad, colors='red', levels=[0], 
                linestyles='-', label='Quadratic Boundary')
    
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Linear vs Quadratic Decision Boundaries')
    plt.legend()
    plt.grid(True)
    plt.show()   


# X = pd.read_csv('data/Q4/q4x.dat').values
# y = pd.read_csv('data/Q4/q4y.dat').values


# X = np.array([list(map(int, item[0].split())) for item in X])

# y = np.where(y == 'Alaska', 0, 1)[:,0]

# gda_linear = GaussianDiscriminantAnalysis()
# gda_quadratic = GaussianDiscriminantAnalysis()

# mu_0, mu_1, sigma = gda_linear.fit(X, y, assume_same_covariance=True)

# # Print parameter estimates
# print("\nParameter Estimates:")
# print("μ₀ (Alaska):", mu_0)
# print("μ₁ (Canada):", mu_1)
# print("\nΣ (Alaska and Canada):\n", sigma)

# plot_data(X, y)

# # Fit GDA model


    
# # Plot data and decision boundary
# decision_boundary(X, y, mu_0, mu_1, sigma, gda_linear.prior_probability_class_1,gda_linear.X_mean, gda_linear.X_std)

# mu_0, mu_1, sigma_0, sigma_1 = gda_quadratic.fit(X, y, assume_same_covariance=False)

# # Print parameter estimates
# print("\nParameter Estimates:")
# print("μ₀ (Alaska):", mu_0)
# print("μ₁ (Canada):", mu_1)
# print("\nΣ₀ (Alaska):\n", sigma_0)
# print("\nΣ₁ (Canada):\n", sigma_1)

# # Plot the data and decision boundary
# quadratic_decision_boundary(X, y, gda_linear,gda_quadratic)

</PRE>
</PRE>
</BODY>
</HTML>
