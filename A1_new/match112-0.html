<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_D73IC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_D73IC.py<p><PRE>


# Imports - you can add any other permitted libraries
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match112-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.X = []
</FONT>        self.y = []
        self.load_data('../data/Q1/linearX.csv', '../data/Q1/linearY.csv')
        self.n_features = len(self.X[0] )
        self.params = [0.0 for _ in range( self.n_features)]
        self.eps = 1e-12
        pass
    
    def load_data(self, file_path_X, file_path_y):
        with open(file_path_X, 'r') as file:
            for line in file:
                values = [1.0] + list(map(float, line.strip().split()))
                self.X.append(values)
        with open(file_path_y, 'r') as file:
            for line in file:
                self.y.append(float(line.strip()))
                
    def plot_data(self, X, y):
        plt.scatter([x[1] for x in X], y, color='blue', label='Data points')
        
        # Plot the learned line
        x_values = np.array([x[1] for x in X])
        y_values = np.dot(X, self.params)
        plt.plot(x_values, y_values, color='red', label='Fitted line')
        
        plt.xlabel('X')
        plt.ylabel('y')
        plt.legend()
        plt.savefig('img.png')
        pass
    
    
    def compute_cost(self, X, y, params):
        # m = len(X)
        m = len(y)
        error = 0.0
        for i in range(m):
            y_i = y[i]
            x_i = X[i]
            param_tarnspose_x_i = 0.0
            for j in range(self.n_features):
                param_tarnspose_x_i += x_i[j]*params[j]
            error += (y_i - param_tarnspose_x_i)**2
        error = error/(2*m)
        return error
    
    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        prev_error = 100.00
        curr_error = 200.00
        iter = 0
        
        fig, ax, theta0_vals, theta1_vals = self.plot_contour(X, y)
        red_dot, = ax.plot([self.params[0]], [self.params[1]], 'ro')
        
        
        while abs(prev_error - curr_error) &gt; self.eps:
            iter += 1
            m = len(X)
            error = 0.0
            gradient = [0.0 for _ in range(self.n_features)]
            curr_param = self.params
            for i in range(m):
                y_i = y[i]
                x_i = X[i]
                param_tarnspose_x_i = 0.0
                for j in range(self.n_features):
                    param_tarnspose_x_i += x_i[j]*curr_param[j]
                error += (y_i - param_tarnspose_x_i)**2
                for j in range(self.n_features):
                    gradient[j] += -1*x_i[j]*(y_i - param_tarnspose_x_i)
            error = error/(2*m)
            for j in range(self.n_features):
                curr_param[j] = curr_param[j] - learning_rate*gradient[j]/m
            self.params = curr_param
            print(iter, self.params, error)
            
            
            # Recompute the cost function values
            J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
            print(J_vals)
            for i, theta0 in enumerate(theta0_vals):
                for j, theta1 in enumerate(theta1_vals):
                    params = np.array([theta0, theta1])
                    J_vals[i, j] = self.compute_cost(X, y, params)
                    
            # Update the contour plot
            ax.clear()
            contour = ax.contour(theta0_vals, theta1_vals, J_vals.T, cmap='viridis')
            fig.colorbar(contour)
            ax.set_xlabel('θ0')
            ax.set_ylabel('θ1')
            
            # Update the red dot position
            red_dot, = ax.plot([self.params[0]], [self.params[1]], 'ro')
            plt.draw()
            plt.pause(0.2)
            
            
            prev_error = curr_error
            curr_error = error
        plt.show()
        
        
    def plot_contour(self, X, y):
        """
        Plot the contour of the cost function J(θ).
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        fig, ax
            The figure and axis objects for the plot.
        """
        theta0_vals = np.linspace(-2, 10, 100)
        theta1_vals = np.linspace(-10, 45, 100)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                params = np.array([theta0, theta1])
                J_vals[i, j] = self.compute_cost(X, y, params)
        
        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        
        fig, ax = plt.subplots()
        contour = ax.contour(theta0_vals, theta1_vals, J_vals.T, cmap='viridis')
        fig.colorbar(contour)
        
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        
        return fig, ax, theta0_vals, theta1_vals
                
        
    def plot_cost_function(self, X, y):
        """
        Plot the cost function J(θ) as a 3D mesh.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        None
        """
        theta0_vals = np.linspace(2, 10, 25)
        theta1_vals = np.linspace(20, 40, 25)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                params = np.array([theta0, theta1])
                J_vals[i, j] = self.compute_cost(X, y, params)
        
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match112-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis')
</FONT>        
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_zlabel('J(θ)')
        
        return fig, ax
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        y_pred = []
        for x_i in X:
            y_i = 0.0
            for j in range(self.n_features):
                y_i += x_i[j]*self.params[j]
            y_pred.append(y_i)
        return y_pred
    
    
regressor = LinearRegressor()
regressor.load_data('../data/Q1/linearX.csv', '../data/Q1/linearY.csv')
print(regressor.params)
regressor.fit(regressor.X, regressor.y)
# regressor.plot_data(regressor.X, regressor.y)
# regressor.plot_cost_function(regressor.X, regressor.y)
print(regressor.params)
print(regressor.predict([[1.0, -1.1368]]))



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    X_with_intercept = np.hstack((np.ones((N, 1)), X))
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    y = X_with_intercept.dot(theta) + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.params = [0, 0, 0]
        self.batch_size = 8000
        pass
    
    

    def fit_normal_equation(self, X, y):
        """
        Fit the linear regression model to the data using the normal equation.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        theta : numpy array of shape (n_features+1,)
            The parameters of the linear regression model.
        """
        n_samples, n_features = X.shape
        X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
        
        X_transpose = X_with_intercept.T
        theta = np.linalg.inv(X_transpose.dot(X_with_intercept)).dot(X_transpose).dot(y)
        self.params = theta
        return theta
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        batch_size = self.batch_size
        n_samples, n_features = X.shape
        theta = np.zeros(n_features + 1)
        X_with_intercept = np.hstack((np.ones((n_samples, 1)), X))
        theta_list = [theta.copy()]
        prev_error = 100.00
        curr_error = 200.00
        eps = 1e-6
        flag = False
        while(True):
            # Shuffle the dataset
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_with_intercept = X_with_intercept[indices]
            y = y[indices]
            for start in range(0, n_samples, batch_size):
                end = start + batch_size
                X_batch = X_with_intercept[start:end]
                y_batch = y[start:end]

                # Compute predictions
                predictions = X_batch.dot(theta)
                # Compute the gradient
                gradient = -1 * X_batch.T.dot(y_batch - predictions) / batch_size
                # Update parameters
                theta -= learning_rate * gradient
                # Compute the error
                curr_error = np.sum((y_batch - predictions)**2) / (2*batch_size)
                
                # Store the parameters
                theta_list.append(theta.copy())
                print(theta, curr_error)
                
                self.params = theta
                if abs(prev_error - curr_error) &lt; eps:
                    print('Converged')
                    print(prev_error, curr_error)
                    flag = True
                    break
                prev_error = curr_error
            if flag:
                break
        return np.array(theta_list)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
<A NAME="5"></A><FONT color = #FF0000><A HREF="match112-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        """
        
        X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))
        return X_with_intercept.dot(self.params)
    
    def calculate_error(self, X, y):
</FONT>        """
        Calculate the mean squared error of the model on the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        error : float
            The mean squared error of the model on the input data.
        """
        predictions = self.predict(X)
        return np.sum((y - predictions)**2) / (2*X.shape[0])
    
    
    def plot(self, X, y):
        """
        Plot the data and the model's predictions.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
        """
        theta_array = self.fit(X, y)
        epochs = theta_array.shape[0]
        max_points = 150
        distance = epochs//max_points
        tmp = []
        for i in range(0, epochs, distance):
            tmp.append(theta_array[i])
        tmp = np.array(tmp)
        theta_array = tmp
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.plot(theta_array[:, 0], theta_array[:, 1], theta_array[:, 2], marker='o')
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_zlabel('Theta 2')
        ax.set_title(f'Movement of Theta for Batch Size {self.batch_size}')
        plt.show()
        
    
# N = 1000000
# X, y = generate(N, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), 2)
# X_train = X[:N*8//10]
# y_train = y[:N*8//10]
# X_test = X[N*8//10:]
# y_test = y[N*8//10:]
# np.save('X_train.npy', X_train)
# np.save('y_train.npy', y_train)
# np.save('X_test.npy', X_test)
# np.save('y_test.npy', y_test)
X_train = np.load('X_train.npy')
y_train = np.load('y_train.npy')
X_test = np.load('X_test.npy')
y_test = np.load('y_test.npy')
model = StochasticLinearRegressor()
# theta_list = model.fit(X_train, y_train)
# print(theta_list)
# print(model.calculate_error(X_test, y_test))
model.plot(X_train, y_train)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
        
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))
        self.theta = np.zeros((X_with_intercept.shape[1], 1))
        thetas = [self.theta]
        gradient = np.ones((X_with_intercept.shape[1], 1))
        iter = 0
        while(np.linalg.norm(gradient) &gt; 1e-12):
            # Compute the predictions
            z = np.dot(X_with_intercept, self.theta)
            h = self.sigmoid(z).reshape(-1, 1)
            
            # Compute the gradient
            gradient = np.zeros((X_with_intercept.shape[1], 1))
            for i in range(X_with_intercept.shape[0]):
                gradient += (h[i] - y[i]) * X_with_intercept[i].reshape(-1, 1)
            gradient /= y.size
            
            # Compute the Hessian
            H = np.zeros((X_with_intercept.shape[1], X_with_intercept.shape[1]))
            for i in range(X_with_intercept.shape[0]):
                xi = X_with_intercept[i].reshape(-1, 1)
                H += (h[i] * (1 - h[i])) * np.dot(xi, xi.T)
            H /= y.size
            
            # Update the parameters
            self.theta -= np.dot(np.linalg.inv(H), gradient)
            thetas.append(self.theta.copy())
            
            # print(iter)
            # iter += 1
            
        # print(self.theta)
        return np.array(thetas)
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))
        z = np.dot(X_with_intercept, self.theta)
        h = self.sigmoid(z)
        y_pred = np.round(h)
        return y_pred
    
    def plot_data_and_decision_boundary(self, X, y, model):
        """
        Plot the training data and the decision boundary.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels.
            
        model : LogisticRegression
            The trained logistic regression model.
        """
        # Plot the training data
        plt.scatter(X[y.flatten() == 0][:, 0], X[y.flatten() == 0][:, 1], marker='o', label='Class 0')
<A NAME="1"></A><FONT color = #00FF00><A HREF="match112-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(X[y.flatten() == 1][:, 0], X[y.flatten() == 1][:, 1], marker='x', label='Class 1')
        
        # # Plot the decision boundary
        x_values = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
</FONT><A NAME="0"></A><FONT color = #FF0000><A HREF="match112-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_values = - (model.theta[0] + model.theta[1] * x_values) / model.theta[2]
        plt.plot(x_values, y_values, label='Decision Boundary', color='red')
            
        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend()
</FONT>        plt.show()
    
def load_data_from_csv(file_path):
    """
    Load data from a CSV file.
    
    Parameters
    ----------
    file_path : str
        The path to the CSV file.
        
    Returns
    -------
    data : numpy array
        The data loaded from the CSV file.
    """
    data = pd.read_csv(file_path, header=None).values
    return data
    

# Do not change the code below
model = LogisticRegressor()
X = load_data_from_csv('../data/Q3/logisticX.csv')
y = load_data_from_csv('../data/Q3/logisticY.csv')
print(X.shape, y.shape)
model.fit(X, y)
model.plot_data_and_decision_boundary(X, y, model)
# y_pred = model.predict(X)
# print(y_pred)



# Imports - you can add any other permitted libraries
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
<A NAME="6"></A><FONT color = #00FF00><A HREF="match112-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
</FONT>        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
         # Separate the data by class
        X0 = X[y == 0]
<A NAME="2"></A><FONT color = #0000FF><A HREF="match112-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X1 = X[y == 1]

        # Compute the means
        self.mu0 = np.mean(X0, axis=0)
        self.mu1 = np.mean(X1, axis=0)

        # Compute the shared covariance matrix
        if assume_same_covariance:
            n = X.shape[0]
</FONT>            X_diff = np.where(y[:, None] == 1, X - self.mu1, X - self.mu0)
            self.sigma = np.matmul(X_diff.T, X_diff) / n
            return self.mu0, self.mu1, self.sigma
        else:
            n0 = X0.shape[0]
            n1 = X1.shape[0]
            X_diff0 = X0 - self.mu0
            X_diff1 = X1 - self.mu1
            self.sigma0 = np.matmul(X_diff0.T, X_diff0) / n0
            self.sigma1 = np.matmul(X_diff1.T, X_diff1) / n1
            return self.mu0, self.mu1, self.sigma0, self.sigma1
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        pass
    
    def plot(self, X, y):
        """
        Plot the input data and the decision boundary learned by the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        """
        
        
        fig, axes = plt.subplots()
        axes.set_title('Classification of Salmons')
        axes.set_xlabel('$x_1$ (diameter in freshwater)')
        axes.set_ylabel('$x_2$ (diameter in marine water)')
        axes.scatter(*X[np.where(y == 1), :][0].T, c='blue',
                    marker='x', label='Alaska')
        axes.scatter(*X[np.where(y == 0), :][0].T, c='green',
                    marker='o', label='Canada')
        
        # mu0, mu1, sigma = self.fit(X, y, True)
        # phi = np.mean(y)
        # x1, x2 = np.meshgrid(*np.linspace([-2, -2], [2, 2], 10).T)
        # z = np.apply_along_axis(lambda x: self._plot_linear_boundary(x, phi, mu0, mu1,
        #                     sigma), 2,
        #                     np.stack([x1, x2], axis=-1))
        # axes.contour(x1, x2, z, 0, colors='red')
        # axes.plot([], [], color='red', label='linear separator')
        
        
        # mu0, mu1, sigma0, sigma1 = self.fit(X, y, False)
        # phi = np.mean(y)
        # x1, x2 = np.meshgrid(*np.linspace([-2, -2], [2, 2], 10).T)
        # z = np.apply_along_axis(lambda x: self._plot_quadratic_boundary(x, phi, mu0, mu1,
        #                     sigma0, sigma1),
        #                     2, np.stack([x1, x2], axis=-1))
        # axes.contour(x1, x2, z, 0, colors='red')
        # axes.plot([], [], color='red', label='linear separator')
        
        
        
        
        axes.legend()
        plt.show()
        
        
        
    def _plot_linear_boundary(self, x, phi, mu0, mu1, sigma):
        sigma_inv = np.linalg.inv(sigma)
        return (np.log((1 - phi) / phi)
            - np.matmul(np.matmul((mu1 - mu0).T, sigma_inv), x)
            + ((np.matmul(np.matmul(mu1.T, sigma_inv), mu1)
                + np.matmul(np.matmul(mu0.T, sigma_inv), mu1))) / 2)
        

    def _plot_quadratic_boundary(self, x, phi, mu0, mu1, sigma0, sigma1):
        sigma0_inv = np.linalg.inv(sigma0)
        sigma1_inv = np.linalg.inv(sigma1)
        return (np.log(np.sqrt(np.linalg.det(sigma0_inv)
                           / np.linalg.det(sigma1_inv))
                   *  phi / (1 - phi))
            + np.matmul(np.matmul(x.T, sigma1_inv - sigma0_inv), x) / 2
            + np.matmul(np.matmul(mu1.T, sigma1_inv)
                        - np.matmul(mu0.T, sigma0_inv), x)
            + (np.matmul(np.matmul(mu0.T, sigma1_inv), mu0)
               - np.matmul(np.matmul(mu1.T, sigma0_inv), mu1)) / 2)[0][0]

        
    
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

# Normalize the data
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_normal = (X - X_mean) / X_std

gda = GaussianDiscriminantAnalysis()
mu0, mu1, sigma = gda.fit(X_normal, y, True)

print("Mean µ0:", mu0)
print("Mean µ1:", mu1)
print("Covariance matrix Σ:", sigma)


gda.plot(X_normal, y)


# Fit the model without assuming same covariance
mu0, mu1, sigma0, sigma1 = gda.fit(X_normal, y, False)

print("Mean µ0:", mu0)
print("Mean µ1:", mu1)
print("Covariance matrix Σ0:", sigma0)
print("Covariance matrix Σ1:", sigma1)

# Plot the data and quadratic decision boundary

</PRE>
</PRE>
</BODY>
</HTML>
