<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_KQ3JZ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_XU0MN.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match52-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from matplotlib import cm
import time

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.theta_history = []
</FONT>        self.learning_rate = 0.01
        self.num_iterations = 10000
        self.tolerance = 1e-6
    
    def compute_cost(self, X, y, theta=None):
        """Function to compute cost

        Args: X,  y,  theta

        Returns: cost
        """
        if theta is None:
            theta = self.theta
        m = len(y)
        predictions = X.dot(theta)
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost
    
    def _gradient_descent(self, X, y):
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match52-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        m, n = X.shape
        X = np.c_[np.ones(m), X] #intercept term
        self.theta = np.zeros(n+1)
        
        for i in range (self.num_iterations):
            predictions = X.dot(self.theta)
</FONT>            error = predictions - y
            gradient = (1/m) * X.T.dot(error)
            self.theta -= self.learning_rate * gradient
            
            cost = self.compute_cost(X, y)
            self.cost_history.append(cost)
            self.theta_history.append(self.theta.copy())
            
            if i &gt; 0 and abs(self.cost_history[-2] - cost) &lt; self.tolerance:
                break
            
        return np.array(self.theta_history)
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        self.learning_rate = learning_rate
        return self._gradient_descent(X, y)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.c_[np.ones(m), X] #intercept term
        
        return X.dot(self.theta)
    
    def plot_graph(self, X ,y):
<A NAME="2"></A><FONT color = #0000FF><A HREF="match52-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.figure(figsize=(10, 6))
        plt.scatter(X, y, color='blue', label='Orignal Data')
        plt.plot(X, self.predict(X), color='red', label='Hypothesis')
        plt.xlabel('Acidity of wine')
        plt.ylabel('Density of wine')
        plt.title('Linear Regression')
        plt.legend()
        plt.show()
</FONT>        plt.savefig('linear_regression.png', dpi=300)
        
    def plot_3d_mesh(self, X, y):
        theta_0_vals = np.linspace(-30, 60, 50)
        theta_1_vals = np.linspace(-30, 60, 50)
        J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add intercept term

        for i, t0 in enumerate(theta_0_vals):
            for j, t1 in enumerate(theta_1_vals):
                t = np.array([t0, t1])
                J_vals[i, j] = self.compute_cost(X_b, y, theta=t)

        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

        fig = plt.figure(figsize=(10,8))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
        
        for idx, (theta, cost) in enumerate(zip(self.theta_history, self.cost_history)):
            ax.scatter(theta[0], theta[1], cost, color='r')
            ax.text2D(0.05, 0.95, f'Iteration: {idx+1}, Error: {cost:.4f}', transform=ax.transAxes, fontsize=10, backgroundcolor='white')
            plt.pause(0.5)
            
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_zlabel('Cost J(θ)')
        ax.set_title('3D Mesh of the Cost Function')
        
        plt.show()
        plt.savefig('3d_mesh.png')
        
    def plot_contours(self, X, y):
        theta_0_vals = np.linspace(-70, 100, 500)
        theta_1_vals = np.linspace(-50, 110, 500)
        J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add intercept term

        for i, t0 in enumerate(theta_0_vals):
            for j, t1 in enumerate(theta_1_vals):
                t = np.array([t0, t1])
                J_vals[i, j] = self.compute_cost(X_b, y, theta=t)

        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

        fig = plt.figure(figsize= (15, 9))
        ax = fig.add_subplot(111)
        cp = ax.contour(T0, T1, J_vals.T, levels=50, cmap='viridis')
        
        plt.colorbar(cp)
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_title('Contour Plot of the Cost Function')

        for idx, (theta, cost) in enumerate(zip(self.theta_history, self.cost_history)):
            ax.scatter(theta[0], theta[1], color='r', s = 7)
            ax.text(0.05, 0.95, f'Iteration: {idx+1}, Error: {cost:.4f}', transform=ax.transAxes, fontsize=10, backgroundcolor='white')
            plt.pause(0.5)
        
        plt.show()
        plt.savefig('contour_plot.png')
        
        
def main():
    
    X = np.loadtxt('../data/Q1/linearX.csv', delimiter=',')
    y = np.loadtxt('../data/Q1/linearY.csv', delimiter=',')
    
    if X.ndim == 1:
        X = X.reshape(-1, 1)
        
    model = LinearRegressor()
    model.fit(X, y , learning_rate=0.1)
    
    print("Final parameters (theta):", model.theta)

    model.plot_graph(X, y)
    
    model.plot_3d_mesh(X, y)
    
    model.plot_contours(X, y)

    
if __name__ == '__main__':
    main()



import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    X1 = np.random.normal(input_mean[0] , input_sigma[0], N)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match52-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X2 = np.random.normal(input_mean[1] , input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    
    y = theta[0] + theta[1]*X1 + theta[2]*X2 + noise
    X = np.column_stack((X1, X2))
    
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
</FONT>        self.theta_history = []
        self.learning_rate = 0.01
        self.num_iterations = 100000
        self.tolerance = 1e-6
        self.batch_size = 1
        self.iterations_to_converge = 0
        self.moving_avg_window = 10
        
    def _stochastic_gradient_descent(self, X, y):
        m, n = X.shape
        X = np.c_[np.ones(m), X]  #intercept
        self.theta = np.zeros(n+1)
        iteration_count = 0
        
        cost_buffer = []
        prev_moving_avg = None
                
        for epoch in range(self.num_iterations):
            indexes = np.random.permutation(m)
            X_shuffled = X[indexes]
            y_shuffled = y[indexes]
            
            for i in range(0, m, self.batch_size):
                X_batch = X_shuffled[i:i+self.batch_size]
                y_batch = y_shuffled[i:i+self.batch_size]
                
                predictions = X_batch.dot(self.theta)
                error = predictions - y_batch
                gradient = (1/len(y_batch)) * X_batch.T.dot(error)
                self.theta -= self.learning_rate * gradient
                
                cost = self.compute_cost(X , y)
                self.cost_history.append(cost)
                self.theta_history.append(self.theta.copy())
                iteration_count += 1
                
                cost_buffer.append(cost)
                
                if len(cost_buffer) &gt; self.moving_avg_window:
                    cost_buffer.pop(0)
                
                if len(cost_buffer) == self.moving_avg_window:
                    current_mavg = np.mean(cost_buffer)
                    if prev_moving_avg is not None and abs(prev_moving_avg- current_mavg) &lt; self.tolerance:
                        self.iterations_to_converge = iteration_count
                        return np.array(self.theta_history)
                    prev_moving_avg = current_mavg
                                    
        self.iterations_to_converge = iteration_count
        return np.array(self.theta_history)
        
    def compute_cost(self, X, y):
        m = len(y)
        predictions = X.dot(self.theta)
        cost = (1/(2*m)) * np.sum((predictions - y)**2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        self.learning_rate = learning_rate
        return self._stochastic_gradient_descent(X, y)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        m = X.shape[0]
        X = np.c_[np.ones(m), X] #intercept
        
        return X.dot(self.theta)
    
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def plot_theta_history(theta_history, batch_size):
    fig = plt.figure(figsize=(20, 10))
    ax = fig.add_subplot(111, projection='3d')
    theta_history = np.array(theta_history)
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], label='Trajectory', color='b')
    
    ax.plot(theta_history[:, 0], theta_history[:, 1], np.zeros_like(theta_history[:, 1]), label='Projection onto XY', linestyle='--', color='grey')
    ax.plot(np.zeros_like(theta_history[:, 0]), theta_history[:, 1], theta_history[:, 2], label='Projection onto YZ', linestyle='--', color='grey')
    ax.plot(theta_history[:, 0], np.zeros_like(theta_history[:, 1]), theta_history[:, 2], label='Projection onto XZ', linestyle='--', color='grey')

    ax.set_xlabel('θ0')
    ax.set_ylabel('θ1')
    ax.set_zlabel('θ2')
    ax.set_title(f'Parameter Movement with Projections (Batch Size: {batch_size})')

    ax.legend()
    plt.show()
    plt.savefig(f"theta_history_{batch_size}.png")
    
def main():
    N = 1000000
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([4,4])
    noise_sigma = np.sqrt(2)
    
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
    split_index = int(0.8*N)
    
    X_train, y_train = X[:split_index], y[:split_index]
    X_test, y_test = X[split_index:], y[split_index:]
    
    pd.DataFrame(np.column_stack((X_train, y_train)), columns=['X1', 'X2', 'y']).to_csv('train.csv', index=False)
    pd.DataFrame(np.column_stack((X_test, y_test)), columns=['X1', 'X2', 'y']).to_csv('test.csv', index=False)
    
    batch_sizes = [1, 80, 8000, 800000]
    for batch_size in batch_sizes:
        start = time.perf_counter()
        model = StochasticLinearRegressor()
        model.batch_size = batch_size
        theta_history = model.fit(X_train, y_train, learning_rate=0.001)
        plot_theta_history(theta_history, batch_size)
        end = time.perf_counter()
        print(f"Batch size: {batch_size}, Learned theta: {model.theta}, Iterations to converge: {model.iterations_to_converge} , time taken: {end-start}")
        
    X_b = np.c_[np.ones(X_train.shape[0]), X_train]
    theta_closed_form = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
    print(f"Closed form theta: {theta_closed_form}")
    
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)
    
    train_mse = mean_squared_error(y_train, train_preds)
    test_mse = mean_squared_error(y_test, test_preds)
    
    print(f"Train MSE: {train_mse}")
    print(f"Test MSE: {test_mse}")
    
if __name__ == "__main__":
    main()
    



import numpy as np
import matplotlib.pyplot as plt


class LogisticRegressor:
    def __init__(self):
        
        self.theta = None
        self.mean = None
        self.std = None
        
    def normalize(self, X):
        
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def compute_gradient(self, X, y, theta):
        z = np.dot(X, theta)
        h = self.sigmoid(z)
        diff = y -h
        gradient = np.dot(X.T, diff)
        
        return gradient
    
    def compute_hessian(self, X, y, theta):
        h = self.sigmoid(np.dot(X, theta))
        D = np.diag(h * (1 - h))
        hessian = -np.dot(X.T, np.dot(D, X))
    
        return hessian
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        X = self.normalize(X)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        self.theta = np.zeros(X.shape[1])
        max_iter = 10000
        epsilon = 1e-6
        theta_history = []
        
        for _ in range(max_iter):
            gradient = self.compute_gradient(X, y, self.theta)
            hessian = self.compute_hessian(X, y, self.theta)
            
<A NAME="1"></A><FONT color = #00FF00><A HREF="match52-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            delta_theta = np.linalg.solve(hessian, gradient)
            
            self.theta -= delta_theta
            
            theta_history.append(self.theta.copy())
            
            if np.linalg.norm(delta_theta) &lt; epsilon:
                break
            
        return np.array(theta_history)
    
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        X = (X - self.mean) / self.std
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        probablities = self.sigmoid(np.dot(X, self.theta))
        
        return (probablities &gt;= 0.5).astype(int)
    
def plot_decision_boundaries(X, y, theta):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Class 1')
    
    x_values = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), num = 100)
    y_values = -(theta[0] + theta[1] * x_values) / theta[2]
    
    
    plt.plot(x_values, y_values, color='green', label='Decision Boundary')
    
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Logistic Regression')
    plt.legend()
    print("theta0 ",theta[0], " theta1 ", theta[1], " theta2 ", theta[2])
    plt.savefig('decision_boundary.png')
    plt.show()
    
    
def main():
    
    X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
    y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')
    
    regressor = LogisticRegressor()
    theta_history = regressor.fit(X, y)
    
    theta = theta_history[-1]
    
    plot_decision_boundaries(X, y, theta)   
    
    
if __name__ == '__main__': 
    main()



import numpy as np
import matplotlib.pyplot as plt
import string

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None 
        self.sigma_0 = None
        self.sigma_1 = None
        self.prior_0 = None
        self.prior_1 = None
        self.mean = None
        self.std = None
        
        self.assume_same_covariance = None
        
        
    def normalize(self, X):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std
    
    
    def fit(self, X, y, assume_same_covariance=True):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X_normalized = self.normalize(X)
        m = X_normalized.shape[0]
        
        indices_0 = np.where(y == 0)[0]
        indices_1 = np.where(y == 1)[0]
        n0 = len(indices_0)
        n1 = len(indices_1)
        
        self.prior_0 = float(n0) / m
        self.prior_1 = float(n1) / m
        
        self.mu_0 = np.mean(X_normalized[indices_0], axis=0)
        self.mu_1 = np.mean(X_normalized[indices_1], axis=0)
        
        if assume_same_covariance:
            self.assume_same_covariance = True
            n_features = X_normalized.shape[1]
            sigma = np.zeros((n_features, n_features))
            
            for idx in indices_0:
                diff = X_normalized[idx, :] - self.mu_0
                sigma += np.outer(diff, diff)
                
            for idx in indices_1:
                diff = X_normalized[idx, :] - self.mu_1
                sigma += np.outer(diff, diff)
            
            self.sigma = sigma / m
            return (self.mu_0, self.mu_1, self.sigma)
        
        else:
            self.assume_same_covariance = False
            n_features = X_normalized.shape[1]
            sigma_0 = np.zeros((n_features, n_features))
            sigma_1 = np.zeros((n_features, n_features))
            
            for idx in indices_0:
                diff = X_normalized[idx, :] - self.mu_0
                sigma_0 += np.outer(diff, diff)
                
            for idx in indices_1:
                diff = X_normalized[idx, :] - self.mu_1
                sigma_1 += np.outer(diff, diff)
            
            self.sigma_0 = sigma_0 / n0
            self.sigma_1 = sigma_1 / n1
            
            return (self.mu_0, self.mu_1, sigma_0, sigma_1)
            
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_normalized = (X - self.mean) / self.std
        m = X_normalized.shape[0]
        predictions = np.zeros(m, dtype=int)
        
        if self.assume_same_covariance:
            inv_sigma = np.linalg.inv(self.sigma)
            for i in range(m):
                x = X_normalized[i, :]
                diff0 = x - self.mu_0
                diff1 = x - self.mu_1
                
                g0 = -0.5*np.dot(diff0, np.dot(inv_sigma, diff0)) + np.log(self.prior_0)
                g1 = -0.5*np.dot(diff1, np.dot(inv_sigma, diff1)) + np.log(self.prior_1)
                
                if g1 &gt; g0:
                    predictions[i] = 1
                else :
                    predictions[i] = 0
                
            return predictions
        
        else:
            inv_sigma0 = np.linalg.inv(self.sigma_0)
            inv_sigma1 = np.linalg.inv(self.sigma_1)
            
            for i in range(m):
                x = X_normalized[i, :]
                diff0 = x - self.mu_0
                diff1 = x - self.mu_1
<A NAME="5"></A><FONT color = #FF0000><A HREF="match52-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                det0 = np.linalg.det(self.sigma_0)
                det1 = np.linalg.det(self.sigma_1)
                
                g0 = -0.5*np.dot(diff0, np.dot(inv_sigma0, diff0)) -0.5*np.log(det0) + np.log(self.prior_0)
</FONT>                g1 = -0.5*np.dot(diff1, np.dot(inv_sigma1, diff1)) -0.5*np.log(det1) + np.log(self.prior_1)

                if g1 &gt; g0:
                    predictions[i] = 1
                else:
                    predictions[i] = 0
                    
            return predictions

def plot_gda_decision_boundary(X, y, model, same_cov):
                    
    x_min, x_max = np.min(X[:, 0]) - 0.5, np.max(X[:, 0]) + 0.5
    y_min, y_max = np.min(X[:, 1]) - 0.5, np.max(X[:, 1]) + 0.5
    
    grid_x = np.linspace(x_min, x_max, 200)
    grid_y = np.linspace(y_min, y_max, 200)
    xx, yy = np.meshgrid(grid_x, grid_y)
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    Z = model.predict(grid_points)
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    
    plt.contour(xx, yy, Z, levels=[2], colors="black", linewidths=0.1)
    
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', marker='o', label='Class 0 (Alaska)')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', marker='o', label='Class 1 (Canada)')
    
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.title("GDA Decision Boundary (Shared Covariance)")
    plt.legend()
    if same_cov:
        plt.savefig("Gaussian_same_cov.png")
    else:
        plt.savefig("Gaussian_diff_cov.png")
    plt.show()
    
    
def main():
    X = np.loadtxt("../data/Q4/q4x.dat")
    y_str = np.genfromtxt("../data/Q4/q4y.dat", dtype=str)
    
    label_map = {"Alaska": 0, "Canada": 1}
    y = np.array([label_map[label] for label in y_str])
    
    model = GaussianDiscriminantAnalysis()
    assume_same_covariance = False
    params = model.fit(X, y, assume_same_covariance)
    
    print("Learned Parameters:")
    print("mu_0 =", model.mu_0)
    print("mu_1 =", model.mu_1)
    if assume_same_covariance:
        print("sigma =", model.sigma)
    else:
        print("sigma0 =", model.sigma_0)
        print("sigma1 =", model.sigma_1)
    
    plot_gda_decision_boundary(X, y, model, assume_same_covariance)

if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
