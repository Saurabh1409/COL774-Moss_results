<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_089GD.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_GR2T2.py<p><PRE>


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import csv
from mpl_toolkits.mplot3d import Axes3D  # Import 3D module


class LinearRegressor:
    def __init__(self):
        self.theta = None  # Model parameters

    def loss(self, X, y, theta):
        m, n = X.shape  # Get number of samples and features
        i = ((1/(2*m)) * np.sum((X @ theta - y) ** 2)).item()
        return i

    def fit(self, X, y, learning_rate=0.01, tol=1e-6):
        m, n = X.shape  # Get number of samples and features
        X = np.c_[np.ones(m), X]
        self.theta = np.zeros((n + 1, 1))
        theta_list = []  # To store theta after each iteration
        old_loss = self.loss(X, y, self.theta)
        theta_list.append(self.theta)
        iterations = 0
        while (True):
            iterations += 1
            y_pred = X @ self.theta
            gradient = (1/m)*(X.T @ (y_pred - y))  # Shape: (n+1,1)
            val = learning_rate * gradient
            self.theta = self.theta - val
            loss = self.loss(X, y, self.theta)
            theta_list.append(self.theta.copy())
            if (abs(old_loss-loss) &lt; tol):
                break
            old_loss = loss
        print(self.theta)
        # print(theta_list)
        z= np.array(theta_list)
        z = z.reshape(-1, n+1,)
        return z

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match236-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def predict(self, X):
        m = X.shape[0]  # Number of samples
        X = np.c_[np.ones(m), X]  # Add intercept term
        y_pred = X @ self.theta  # Compute predictions
</FONT>        y_pred = y_pred.ravel()
        return y_pred
    
    def plot_data_and_hypothesis(self, X, y):
        # plot the data points
        m = X.shape[0]
        Z = X.copy()
        self.fit(X, y, learning_rate=0.01)
        y_pred = self.predict(X)
        plt.scatter(Z, y, color='red')
        plt.plot(X, y_pred)
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Data points and Hypothesis Function')
        plt.savefig('q1.png')

    def plot_3d_mesh(self, X,y):
        theta_0_range = np.linspace(-35, 35, 1000)
        theta_1_range = np.linspace(-70, 70, 1000)
        theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)
    
        J_values = np.zeros_like(theta_0)
        r,s = 10000000, 10000000
        val = 100000000
        for i in range(theta_0.shape[0]):
            for j in range(theta_0.shape[1]):
                theta = np.array([[theta_0[i, j]], [theta_1[i, j]]])
                # print(theta, type(theta))
                new_X = np.hstack((np.ones((X.shape[0], 1)), X))
                J_values[i, j] = self.loss(new_X, y, theta)
                if (J_values[i,j]&lt;val):
                    val = J_values[i,j]
                    r,s = i,j
        # print(J_values)
        # print(theta_0[r,s], theta_1[r,s])
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta_0, theta_1, J_values, cmap='viridis', alpha=0.8)
        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope)')
        ax.set_zlabel(r'$J(\theta)$ (loss)')
        ax.set_title('3D Mesh of loss Function with Gradient Descent')

        # gradient(X, y, ax, learning_rate=0.01)
        theta_path = self.fit(X, y)

        for theta in theta_path:
            theta_0_val, theta_1_val = theta[0, 0], theta[1, 0]
            J_theta = self.loss(np.hstack((np.ones((X.shape[0], 1)), X)), y, theta)
            ax.scatter(theta_0_val, theta_1_val, J_theta, color='r', marker='o', s=50)
            plt.draw()
            plt.pause(0.0001)
        print("done")
        plt.show()

    def plot_2d_contour(self, X, y, learning_rate):
        theta_0_range = np.linspace(-35, 35, 100)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match236-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_1_range = np.linspace(-70, 70, 100)
        theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)

        J_values = np.zeros_like(theta_0)

        # Compute the loss function values for the contour plot
        for i in range(theta_0.shape[0]):
</FONT>            for j in range(theta_0.shape[1]):
                theta = np.array([[theta_0[i, j]], [theta_1[i, j]]])
                new_X = np.hstack((np.ones((X.shape[0], 1)), X))
                J_values[i, j] = self.loss(new_X, y, theta)

        fig, ax = plt.subplots(figsize=(8, 6))

        # Plot contour map of loss function
        contour = ax.contourf(theta_0, theta_1, J_values, cmap='viridis', levels=100)
        plt.colorbar(contour)
        
        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope)')
        ax.set_title('Contour Plot of loss Function with learning rate = {}'.format(learning_rate))

        # Run Gradient Descent and plot updates
        theta_path = self.fit(X, y, learning_rate)

        # Plot gradient descent updates with animation
        for theta in theta_path:
            theta_0_val, theta_1_val = theta[0, 0], theta[1, 0]
            ax.scatter(theta_0_val, theta_1_val, color='r', marker='o', s=50)
            # print(theta)
            plt.draw()
            # plt.pause(0.2)

        plt.savefig(f'q1.4_{learning_rate}.png')




if __name__=="__main__":
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None))
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None))
    Z = X.copy()
    model = LinearRegressor()
    i = model.fit(Z, y)
    print(type(i), i.shape)
    y_pred = model.predict(X)
    print(type(y_pred), y_pred.shape)
    # model.plot_data_and_hypothesis(X, y)
    # model.plot_3d_mesh(X, y)
    # model.plot_2d_contour(X, y, learning_rate=0.001)
    # print(y_pred)





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

    

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)

    # Add Gaussian noise
    noise = np.random.normal(0, noise_sigma, N)

    # Compute y using the true theta values: y = θ0 + θ1 * x1 + θ2 * x2 + noise
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    y.ravel()
    # Stack x1 and x2 into input matrix X
    X = np.column_stack((x1, x2))
    return X, y

def convergence(batch_size, loss, loss_history):
    if len(loss_history) &gt;= 2 * batch_size:
        # Compare the last batch_size losses with the batch before that
        recent_loss = np.mean(loss_history[-batch_size:])  
        previous_loss = np.mean(loss_history[-2*batch_size:-batch_size])
        
        if (previous_loss - recent_loss) &lt; 1e-6:
            return True
    return False

class StochasticLinearRegressor:
    def __init__(self):
        self._theta = None
        self.learning_rate = 0.001
        self.theta_lk = None
        self.num_iter = []

    def cost(self, X, y, theta):
        """Compute Mean Squared Error Cost."""
        m = X.shape[0]
        return (1 / (2 * m)) * np.sum((X @ theta - y) ** 2)
    # def convergence(batch_size, loss, old_loss, loss_history):
    def train_test_split(self, X, y, train_ratio=0.8):
        """
        Splits the dataset into training and testing sets.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        
        y : numpy array of shape (n_samples,)
            The target values.
        
        train_ratio : float, optional
            The proportion of data to use for training (default is 0.8).
        
        Returns
        -------
        X_train, X_test, y_train, y_test : numpy arrays
            The split training and testing data.
        """
        m = X.shape[0]
        indices = np.random.permutation(m)  # Shuffle indices
        train_size = int(train_ratio * m)   # Compute training set size
        
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]

        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
        
        return X_train, X_test, y_train, y_test

    def func(self, X, y, learning_rate=0.001, batch_size=8000, max_epochs=10000, tol=1e-6):
        """Fit the model using stochastic gradient descent."""
        X, i, y, j = self.train_test_split(X, y)
        m, n = X.shape
        X = np.c_[np.ones(m), X]  # Add bias term
        y = y.reshape(-1, 1)

        theta = np.zeros((n + 1, 1))
        theta_list = []
        loss_history = []
        theta_list.append(theta.copy())
        loss_history.append(self.cost(X, y, theta))
        for epoch in range(max_epochs):
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            for i in range(0, m, batch_size):
                X_batch = X_shuffled[i:min(i + batch_size, m)]
                y_batch = y_shuffled[i:min(i + batch_size, m)]

                # Compute gradient
                gradient = (1 / len(y_batch)) * (X_batch.T @ (X_batch @ theta - y_batch))
                theta -= learning_rate * gradient

            loss = self.cost(X, y, theta)
            loss_history.append(loss)
            theta_list.append(theta.copy())
            if convergence(batch_size, loss, loss_history):
                self.num_iter.append(epoch)
                self.theta_lk.append(theta.copy())
                break
        self._theta = theta
        return np.array(theta_list)
    
    def fit (self, X, y, learning_rate=0.001):
        L = [1, 80, 8000, 800000]
        answer = []
        self.theta_lk = []
        self.num_iter = []
        for i in L:
            answer.append(self.func(X, y, learning_rate=0.001, batch_size=i, max_epochs=10000, tol=1e-6))
        return answer
    def get_val(self, X, theta):
        """Predict target values."""
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        y =  X @ theta
        return y
    def predict(self, X):
        """Predict target values."""
        answer = []
        for i in self.theta_lk:
            answer.append(self.get_val(X, i).ravel())
        print( type(answer), type(answer[0]), answer[0].shape)
        return answer

    def closed_form(self, X, y):
        """Compute closed-form solution."""
        y = y.reshape(-1, 1)
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        return np.linalg.inv(X.T @ X) @ (X.T @ y)

# Generate data
X, y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), 2**0.5)

# Train model
model = StochasticLinearRegressor()
# thetas = model.fit(X, y)
a,b,c,d = model.train_test_split(X, y)
# y = model.predict(X)
# print(model.theta_lk)
print()
# print(model.num_iter)
print()
print(type(y), y.shape)
k = model.closed_form(X, y)
print(k, type(k), k.shape)
print()
# a = np.c_[np.ones(a.shape[0]), a] 
# b = np.c_[np.ones(b.shape[0]), b]
# print(model.cost(a,c, k))
# # train
# print(model.cost(b, d, k))




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def logLikelihoodLoss(self, X, y, theta):
        h = self.sigmoid(X @ theta)
        return -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        max_iter = 1000000
        theta_history = []
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        print("normalised X")
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        self.theta = np.zeros(X.shape[1])
        old_ll_Loss = self.logLikelihoodLoss(X, y, self.theta)
        theta_history.append(self.theta.copy())
        for _ in range(max_iter):
            h = self.sigmoid(X @ self.theta)
            y = y.reshape(-1)
            gradient = (X.T) @ (h - y) # (n, m) @ (m,) = (n,)
            z = (h*(1-h))
            D = np.diag(z)
            H = X.T @ D @ X
            delta = np.linalg.inv(H) @ gradient # (n, n) @ (n,) = (n,)
            self.theta -= delta
            theta_history.append(self.theta.copy())
            ll_Loss = self.logLikelihoodLoss(X, y, self.theta)
            if np.abs(old_ll_Loss - ll_Loss) &lt; (1e-6):
                break
            old_ll_Loss = ll_Loss
        y =  np.array(theta_history)
        y = y.reshape(-1, X.shape[1])
        return y
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        return ((self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)).reshape(-1)
    
def plot_decision_boundary(X, y, theta):
    """
    Plots the training data and the decision boundary.

    Parameters:
    -----------
    X : numpy array of shape (n_samples, n_features)
        The input features.
    y : numpy array of shape (n_samples,)
        The labels (0 or 1).
    theta : numpy array of shape (n_features+1,)
        The logistic regression parameters.
    """
    # Separate data points by class
    class_0 = X[y == 0]
    class_1 = X[y == 1]
    # Plot training data points
    plt.scatter(class_0[:, 0], class_0[:, 1], marker='o', color='red', label='Class 0')
    plt.scatter(class_1[:, 0], class_1[:, 1], marker='x', color='blue', label='Class 1')

    # # Decision boundary equation: theta_0 + theta_1 * x1 + theta_2 * x2 = 0
    # x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)  # Generate x1 values
    # x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]  # Solve for x2

    # # Plot decision boundary
    # plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')

    # # Labels and legend
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend()
    plt.title("Logistic Regression data plot")
    plt.savefig("data_plot.png")

# Example Usage
# Assume X is already loaded and normalized, and theta is obtained from Newton's method
# X = np.array([...])  # Shape (n_samples, 2)
# y = np.array([...])  # Shape (n_samples,)
# theta = np.array([...])  # Shape (3,)

# Add intercept term to X
# X = np.hstack([np.ones((X.shape[0], 1)), X])

# plot_decision_boundary(X, y, theta)

    
if __name__=="__main__":
    X = np.array(pd.read_csv('../data/Q3/logisticX.csv', header=None))
    y = np.array(pd.read_csv('../data/Q3/logisticY.csv', header=None))
    
    Z = X.copy()
    model = LogisticRegressor()
    y = y.ravel()
    l = model.fit(Z, y)
    print(l.shape, type(l))
    print(model.theta)

    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    plot_decision_boundary(X, y, model.theta)
    # print(model.theta)

    # y_pred = model.predict(X)
    # model.plot_data_and_hypothesis(X, y)




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.phi = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.assume_same_covariance = False

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        self.phi = np.mean(y)
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        if assume_same_covariance:
            self.assume_same_covariance = True
            m = X.shape[0]
            self.sigma = (1 / m) * ((X[y == 0] - self.mu_0).T @ (X[y == 0] - self.mu_0) +
                                     (X[y == 1] - self.mu_1).T @ (X[y == 1] - self.mu_1))
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.assume_same_covariance = False
            self.sigma_0 = np.cov(X[y == 0].T, bias=True)
            self.sigma_1 = np.cov(X[y == 1].T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    
    def predict(self, X):
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match236-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            w = sigma_inv @ (self.mu_1 - self.mu_0)
            b = (-0.5 * self.mu_1.T @ sigma_inv @ self.mu_1 +
                 0.5 * self.mu_0.T @ sigma_inv @ self.mu_0 +
                 np.log(self.phi / (1 - self.phi)))
</FONT><A NAME="4"></A><FONT color = #FF00FF><A HREF="match236-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            return (X @ w + b &gt; 0).astype(int)
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
</FONT>            term1 = np.diag(-0.5 * X @ (sigma_1_inv - sigma_0_inv) @ X.T)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match236-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            term2 = X @ (sigma_1_inv @ self.mu_1 - sigma_0_inv @ self.mu_0)
            term3 = (-0.5 * self.mu_1.T @ sigma_1_inv @ self.mu_1 +
                     0.5 * self.mu_0.T @ sigma_0_inv @ self.mu_0 +
                     np.log(self.phi / (1 - self.phi)))
</FONT>            return ((term1 + term2 + term3) &gt; 0).astype(int)

def plot_data(X, y):
    y = y.ravel()  # Flatten y if needed

    # Scatter plot
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska', color='blue', alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', label='Canada', color='red', alpha=0.7)
    plt.xlabel('x1 (Freshwater Growth Rings)')
    plt.ylabel('x2 (Marine Water Growth Rings)')
    plt.legend()
    plt.title('training data')
    plt.savefig('data_plot.png')




def plot_data_and_linear_boundary(X, y, mu_0, mu_1, sigma, phi):
    y = y.ravel()  # Flatten y if needed

    # Scatter plot
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska', color='blue', alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', label='Canada', color='red', alpha=0.7)

    # Compute decision boundary
    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, (mu_1 - mu_0))
    b = 0.5 * (np.dot(mu_0.T, np.dot(sigma_inv, mu_0)) - np.dot(mu_1.T, np.dot(sigma_inv, mu_1))) + np.log((1 - phi) / phi)

    # Generate x1 values and compute corresponding x2 values
    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_range_ = (-w[0] * x1_range - b) / w[1]  # Solve for x2

    plt.plot(x1_range, x2_range_, 'k--', label='LinearDecision Boundary')

    # Labels and legend
    plt.xlabel('x1 (Freshwater Growth Rings)')
    plt.ylabel('x2 (Marine Water Growth Rings)')
    plt.legend()
    plt.title('GDA Decision Boundary')
    plt.savefig('linear_decision_boundary.png')


def plot_data_and_quadratic_boundary(X, y, mu_0, mu_1, sigma_0, sigma_1, phi):
    y = y.ravel()  # Flatten y if needed

    # Scatter plot
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska', color='blue', alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', label='Canada', color='red', alpha=0.7)

    # Compute necessary matrices
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    
    A = 0.5 * (sigma_1_inv - sigma_0_inv)  # Quadratic term
    B = -(mu_1.T @ sigma_1_inv - mu_0.T @ sigma_0_inv)  # Linear term
    C = 0.5 * (mu_1.T @ sigma_1_inv @ mu_1 - mu_0.T @ sigma_0_inv @ mu_0) + np.log(((1 - phi) * np.linalg.det(sigma_1)**0.5) / (phi * np.linalg.det(sigma_0)**0.5))

    # Generate grid
    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
    X1, X2 = np.meshgrid(x1_range, x2_range)
    
    # Compute decision boundary function
    Z = np.zeros_like(X1)
    for i in range(X1.shape[0]):
        for j in range(X1.shape[1]):
            x_vec = np.array([X1[i, j], X2[i, j]])
            Z[i, j] = x_vec.T @ A @ x_vec + B @ x_vec + C

    # Contour plot for decision boundary
    plt.contour(X1, X2, Z, levels=[0], colors='black', label='Decision Boundary')

    # Labels and legend
    plt.xlabel('x1 (Freshwater Growth Rings)')
    plt.ylabel('x2 (Marine Water Growth Rings)')
    plt.legend()
    plt.title('GDA Quadratic Decision Boundary')
    plt.savefig('quadratic-boundary.png')

import numpy as np
import matplotlib.pyplot as plt

def plot_combined_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1, phi):
    y = y.ravel()  # Flatten y if needed

    plt.figure(figsize=(8, 6))

    # Scatter plot
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska', color='blue', alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', label='Canada', color='red', alpha=0.7)

    # Linear Decision Boundary (LDA)
    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, (mu_1 - mu_0))
    b = 0.5 * (np.dot(mu_0.T, np.dot(sigma_inv, mu_0)) - np.dot(mu_1.T, np.dot(sigma_inv, mu_1))) + np.log((1 - phi) / phi)

    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_range_linear = (-w[0] * x1_range - b) / w[1]  # Solve for x2

    plt.plot(x1_range, x2_range_linear, 'k--', label='Linear Boundary')

    # Quadratic Decision Boundary (QDA)
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    
    A = 0.5 * (sigma_1_inv - sigma_0_inv)  # Quadratic term
    B = -(mu_1.T @ sigma_1_inv - mu_0.T @ sigma_0_inv)  # Linear term
    C = 0.5 * (mu_1.T @ sigma_1_inv @ mu_1 - mu_0.T @ sigma_0_inv @ mu_0) + np.log(((1 - phi) * np.linalg.det(sigma_1)**0.5) / (phi * np.linalg.det(sigma_0)**0.5))

    x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
    X1, X2 = np.meshgrid(x1_range, x2_range)
    
    Z = np.zeros_like(X1)
    for i in range(X1.shape[0]):
        for j in range(X1.shape[1]):
            x_vec = np.array([X1[i, j], X2[i, j]])
            Z[i, j] = x_vec.T @ A @ x_vec + B @ x_vec + C

    plt.contour(X1, X2, Z, levels=[0], colors='purple', linestyles='solid', label='Quadratic Boundary')

    # Labels, legend, and title
    plt.xlabel('x1 (Freshwater Growth Rings)')
    plt.ylabel('x2 (Marine Water Growth Rings)')
    plt.legend()
    plt.title('GDA: Linear vs Quadratic Decision Boundaries')
    plt.savefig('combined_decision_boundaries.png')
    plt.show()

if __name__ == "__main__":
    X = np.array(pd.read_csv('../data/Q4/q4x.dat', delim_whitespace=True, header=None))
    y = np.array(pd.read_csv('../data/Q4/q4y.dat', header = None))
    y = np.where(y == "Alaska", 0, 1)  # Canada -&gt; 0, Alaska -&gt; 1
    print(X.dtype, y.dtype)
    y =  y.reshape(-1)
    print(X.shape, y.shape)
    model = GaussianDiscriminantAnalysis()
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    # plot_data(X, y)
    # mu_0, mu_1, sigma_0, sigma_1= model.fit(X, y, assume_same_covariance=False)
    mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)
    N = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma = N.fit(X, y, assume_same_covariance=True)
    # y_pred = model.predict(X)
    # print(y_pred)
    # plot_data_and_linear_boundary(X,y, mu_0, mu_1, sigma, model.phi)
    # plot_data_and_quadratic_boundary(X, y, mu_0, mu_1, sigma_0, sigma_1, model.phi)
    plot_combined_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1, model.phi)

</PRE>
</PRE>
</BODY>
</HTML>
