<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_G8GO6.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_Z4Z0A.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation

class LinearRegressor:
    def __init__(self, input_path: str, output_path: str):
        self.X = pd.read_csv(input_path, header=None)
        self.X.columns = ['x1']
        self.X['x0'] = 1
        self.X = self.X.T.values  
        self.y = pd.read_csv(output_path, header=None).T.values.flatten()
        self.loss_history = []  
        self.theta_history = []  

    def fit(self, X, y, learning_rate=0.01):
        self.theta = np.zeros(np.shape(X)[0])  
        n_iter = 100000  
        latest_loss = float("inf")
        # self.theta_history.append(self.theta.copy())

        for i in range(n_iter + 1):
            current_loss, gradient = self.loss(X, y)
            
            if abs(latest_loss - current_loss) &lt; 0.001 * current_loss:
                break  
            
            latest_loss = current_loss

            if i % 4 == 0:
                self.loss_history.append(current_loss)
                self.theta_history.append(self.theta.copy())
                print(f"Iteration: {i}, Loss: {current_loss:.6f}")

            self.theta = self.theta - learning_rate * gradient

    def loss(self, X, y):
        error = y - np.matmul(self.theta.T, X)  
        mean_squared_error = np.dot(error, error.T).mean() / (2 * X.shape[1])  
        gradient = -np.dot(error, X.T) / X.shape[1]  
        return mean_squared_error, gradient

    def compute_loss(self, a, b):
        theta = np.array([a, b])
        error = self.y - np.dot(theta, self.X)
        return np.dot(error, error.T).mean() / (2 * self.X.shape[1])

    def animate_contour(self):
        a_vals = np.linspace(self.theta_history[0][0] - 20, self.theta_history[-1][0] + 30, 100)
        b_vals = np.linspace(self.theta_history[0][1] - 20, self.theta_history[-1][1] + 30, 100)
        A, B = np.meshgrid(a_vals, b_vals)

        zs = np.array([self.compute_loss(a, b) for a, b in zip(np.ravel(A), np.ravel(B))])
        loss_vals = zs.reshape(A.shape)

        fig, ax = plt.subplots(figsize=(8, 6))
        contour = ax.contour(A, B, loss_vals, levels=20, cmap="viridis")
        ax.clabel(contour, inline=True, fontsize=8)
        ax.set_xlabel("Theta 0 (a)")
        ax.set_ylabel("Theta 1 (b)")
        ax.set_title("Gradient Descent Animation on Contour Plot")

        point, = ax.plot([], [], 'ro-', markersize=3)
        
        a_history = [theta[0] for theta in self.theta_history]
        b_history = [theta[1] for theta in self.theta_history]
        
        def init():
            point.set_data([], [])
            return point,
        
        def update(frame):
            point.set_data(a_history[:frame], b_history[:frame])
            return point,
        
        anim = animation.FuncAnimation(fig, update, frames=len(self.theta_history), init_func=init, interval=200, blit=True)
        anim.save("contour_0.1.gif", writer="imagemagick", fps=5)

if __name__ == "__main__":
    model = LinearRegressor("../data/Q1/linearX.csv", "../data/Q1/linearY.csv")
    model.fit(model.X, model.y, learning_rate=0.1)
    print("Final Parameters: ", model.theta)
    model.animate_contour()



# Imports - you can add any other permitted libraries
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match224-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

class LinearRegressor:
    def __init__(self, input_path: str, output_path: str):
</FONT>        """
        Reading the input and output data from the given paths.
        """
        self.X = pd.read_csv(input_path, header=None)
        self.X.columns = ['x1']  # Renaming the column to x1 for adding the bias term later
        self.X['x0'] = 1  # Adding the bias/intercept term
        self.X = self.X.T.values  # Converted to (n_features, n_samples) shape
        self.y = pd.read_csv(output_path, header=None).T.values.flatten()
        self.loss_history = []  # To store loss at each step
        self.theta_history = []  # To store theta values at each step

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match224-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        """
        self.theta = np.zeros(np.shape(X)[0])  # Initialize weights
</FONT>        n_iter = 10000000  # Max iterations
        latest_loss = float("inf")

        for i in range(n_iter + 1):
            current_loss, gradient = self.loss(X, y)
            
            if abs(latest_loss - current_loss) &lt; 0.001 * current_loss:
                break  # Stop if loss improvement is less than 0.1%
            
            # print(gradient)
            self.theta = self.theta - learning_rate * gradient
            latest_loss = current_loss

            if i % 4 == 0:
                self.loss_history.append(current_loss)
                self.theta_history.append(self.theta.copy())
                print(f"Iteration: {i}, Loss: {current_loss:.6f}")

    def loss(self, X, y):
        """
        Calculate the loss and gradient.
        """
        error = y - np.matmul(self.theta.T, X)  # Error calculation
        mean_squared_error = np.dot(error, error.T).mean() / (2 * X.shape[1])  # Mean Squared Error
        gradient = -np.dot(error, X.T) / X.shape[1]  # Gradient calculation
        return mean_squared_error, gradient

    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        return np.dot(self.theta, X)


    def plot_loss_surface(self, save_as_gif=True):
        """
        Plot the 3D surface and gradient descent path with animation.
        
        Parameters:
        - save_as_gif (bool): If True, saves the animation as a GIF file.
        """
        a_vals = np.linspace(self.theta_history[0][0] - 2, self.theta_history[-1][0] + 30, 100)
        b_vals = np.linspace(self.theta_history[0][1] - 30, self.theta_history[-1][1] + 20, 100)
        A, B = np.meshgrid(a_vals, b_vals)

        # Compute loss for each pair of parameters
        zs = np.array([self.compute_loss(a, b) for a, b in zip(np.ravel(A), np.ravel(B))])
        loss_vals = zs.reshape(A.shape)

        # Create figure and axis
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection="3d")

        # Plot surface and contour (static elements)
        ax.plot_surface(A, B, loss_vals, cmap="viridis", alpha=0.8, rstride=3, cstride=3, edgecolor='none')
        ax.contour(A, B, loss_vals, levels=20, cmap="viridis", linestyles="solid", offset=np.min(loss_vals))

        # Initialize path data
        a_history = [theta[0] for theta in self.theta_history]
        b_history = [theta[1] for theta in self.theta_history]
        loss_history = self.loss_history

        # Initialize plots for animation
        path_line, = ax.plot([], [], [], 'black', lw=2)
        current_point = ax.scatter([], [], [], color="blue", s=50)

        # Function to update each frame of the animation
        def update(frame):
            path_line.set_data(a_history[:frame + 1], b_history[:frame + 1])
            path_line.set_3d_properties(loss_history[:frame + 1])
            current_point._offsets3d = (a_history[frame:frame + 1], b_history[frame:frame + 1], loss_history[frame:frame + 1])
            return path_line, current_point

        # Set labels and title
        ax.set_xlabel('a')
        ax.set_ylabel('b')
        ax.set_zlabel('Loss', rotation="vertical")
        ax.set_title('Loss Function Surface')
        ax.view_init(50, 240)

        # Create animation
        anim = FuncAnimation(fig, update, frames=len(a_history), interval=200, blit=False)

        if save_as_gif:
            anim.save("gradient_descent_0.1.gif", writer="imagemagick", fps=5)
        else:
            plt.show()

    def compute_loss(self, a, b):
        """
        Compute loss for given parameters.
        """
        theta = np.array([a, b])
        error = self.y - np.dot(theta, self.X)
        return np.dot(error, error.T).mean() / (2 * self.X.shape[1])


if __name__ == "__main__":
    model = LinearRegressor("../data/Q1/linearX.csv", "../data/Q1/linearY.csv")
    model.fit(model.X, model.y, learning_rate=0.1)
    print("Final Parameters: ", model.theta)
    model.plot_loss_surface()



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X : np.array, y : np.array, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        y = y.flatten()
        if self.theta is None:
            self.theta = np.zeros(np.shape(X)[1])
        n_iter = 1000000
        latest_loss = float('inf')
        params = np.array([self.theta])
        for _ in range(0, n_iter):
            current_loss, gradient = self.loss(X, y)
            if abs(latest_loss - current_loss) &lt; 0.001 * current_loss:
                break
            self.theta = self.theta - learning_rate * gradient
            latest_loss = current_loss
            if _%10 == 0:
                print("Iteration: ", _, " Loss: ", current_loss)
            params = np.vstack((params, self.theta))
        
        print(params.shape)
        return params
    
    def loss(self, X, y):
        """
        Computes the loss and gradient for the given data and parameters.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features + 1(bias term))
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        Returns
        -------
        current_loss : float
            The current loss for the data.

        gradient : numpy array of shape (n_features + 1,)
            The gradient of the loss function at the current parameters.
        """

        y_pred = np.matmul(X, self.theta)
        error = y - y_pred
        current_loss = np.dot(error, error) / (2 * X.shape[0])
        gradient = - np.matmul(error.T, X) / X.shape[0]
        gradient = np.array(gradient).flatten()
        return current_loss, gradient
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.matmul(X, self.theta)



import pandas as pd

import matplotlib.pyplot as plt

# Load the data from linearX.csv and linearY.csv
x = pd.read_csv("../data/Q1/linearX.csv")
y = pd.read_csv("../data/Q1/linearY.csv")

# Plotting the line y = 29.04281159*x + 6.21401678 (parameters obtained from linear_regression.py)
m = 29.04281159
c = 6.21401678

line_y = m * x + c

#plotting the data
plt.scatter(x, y, label="Data")
plt.plot(x, line_y, color="red", label="Line: y = {}*x + {}".format(m, c))
plt.xlabel("x")
plt.ylabel("y")
plt.title("Scatter Plot with Straight Line")
plt.legend()

plt.show()



import numpy as np
import pandas as pd
import linear_regression as lr

if __name__ == '__main__':
    X = pd.read_csv('../data/Q1/linearX.csv', header=None)
    y = pd.read_csv('../data/Q1/linearY.csv', header=None)
    X['bias'] = 1
    X = np.array(X)
    y = np.array(y)
    y = y.flatten()
    print(y.shape)
    print(X.shape)
    model = lr.LinearRegressor()
    model.fit(X, y)
    print(model.theta)
    print(model.predict(X))



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import math
import time as t
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N) #generating random values for x1 ~ N(3, 4)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N) #generating random values for x2 ~ N(-1, 4)
    X = pd.DataFrame() #creating a dataframe to store the values of x1 and x2
    X['x1'] = x1
    X['x2'] = x2
    X['x0'] = 1 #adding a column of 1's to the dataframe for intercept
    X = X[['x0', 'x1', 'x2']] #transposing the dataframe to get the desired shape
    epsilon = np.random.normal(0, noise_sigma, N) #generating random values for epsilon ~ N(0, 2)
    Y = np.matmul(X, theta) + epsilon #generating the target values
    # print("Shape of X: ", X.shape) # ensuring the X has desired shape
    # print("Shape of Y: ", Y.shape) # ensuring that Y has desired shape
    # print("X: ", X)
    # print("Y: ", Y)
    # print("epsilon: ", epsilon)
    # print((input_mean[0], input_sigma[0], N))
    # print((input_mean[1], input_sigma[1], N))

    return X, Y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None

    def loss(self, X, Y, theta):

        error = Y - np.matmul(X, theta) # calculating the error
        current_loss = np.dot(error.T, error) / (2 * X.shape[0]) # calculating the loss
        gradient = -np.matmul(error.T, X) / X.shape[0] # calculating the gradient
        gradient = np.array(gradient).flatten() # flattening the gradient
        return current_loss, gradient
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        if self.theta is None:
            self.theta = np.zeros(X.shape[1])

        split = int(0.2 * X.shape[0]) # defining the split value

        indices = np.random.permutation(X.shape[0])
        """
        shuffling the indices just in case the data has some ordering bias
        """
        X = X[indices] # shuffling the input data
        y = y[indices] # shuffling the target values
        X_train = X[split:]
        X_test = X[:split]
        y_train = y[split:]
        y_test = y[:split]

        params = np.array(self.theta)
        losses = np.array([])
        
        latest_average_loss = float('inf')
        epochs = 1000

        batch_size = 8000

        f = True

        for _ in range(epochs):
            indices = np.random.permutation(X_train.shape[0])
            X_train = X_train[indices]
            y_train = y_train[indices]
            for sub_epoch in range(0, math.ceil(X_train.shape[0] / batch_size)):
                start = sub_epoch * batch_size
                end = min((sub_epoch + 1) * batch_size, X_train.shape[0])
                X_batch = X_train[start:end]
                y_batch = y_train[start:end]
                current_loss, gradient = self.loss(X_batch, y_batch, self.theta)
                self.theta = self.theta - learning_rate * gradient

                params = np.vstack((params, self.theta))
                losses = np.append(losses, current_loss)
                if len(losses) &gt; math.floor(2 * X_train.shape[0] / batch_size):
                    losses = losses[1:]
                current_average_loss = np.mean(losses)
                
                if abs(latest_average_loss - current_average_loss) &lt; 0.0000001 * current_average_loss:
                    f = False
                    break
                latest_average_loss = current_average_loss
            if not f:
                break

        print(params.shape)
        print(losses[-1])
        return params
    
    def closed_form(self, X, y):
        """
        Fits the linear regression model to the data using the closed form solution.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        theta : numpy array of shape (n_features,)
            The parameters of the linear regression model.
        """
        theta = np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y)) #calculating the parameters using the closed form solution
        """
        theta = (X^T * X)^-1 * X^T * Y
        """
        print(theta)
        return theta

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.matmul(X, self.theta) #returning the predicted values
        
        


if __name__ == "__main__":
    start = t.time()
    X, y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), 2)
    X = np.array(X)
    y = np.array(y)
    model = StochasticLinearRegressor()
    params = model.fit(X, y, 0.01)
    print("The time taken is: ", t.time() - start)
    theta0 = params[:, 0]
    theta1 = params[:, 1]
    theta2 = params[:, 2]

    fig = plt.figure(figsize=(5.6, 5.6))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot(theta0, theta1, theta2, marker='o', linestyle='-', markersize=3, label="θ updates")
    ax.scatter(theta0[0], theta1[0], theta2[0], color='red', marker='o', s=100, label="Start (θ_init)")
    ax.scatter(theta0[-1], theta1[-1], theta2[-1], color='green', marker='o', s=100, label="Converged (θ_final)")

    # Labels
    ax.set_xlabel("θ0 (Bias)")
    ax.set_ylabel("θ1 (Feature 1)")
    ax.set_zlabel("θ2 (Feature 2)")
    ax.set_title("Gradient Path for Stochastic Gradient Descent")
    ax.legend()

    plt.savefig("theta_8000.png")



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import math

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N) #generating random values for x1 ~ N(3, 4)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N) #generating random values for x2 ~ N(-1, 4)
    X = pd.DataFrame() #creating a dataframe to store the values of x1 and x2
    X['x1'] = x1
    X['x2'] = x2
    X['x0'] = 1 #adding a column of 1s for the bias term
    X = X[['x0', 'x1', 'x2']] #transposing the dataframe to get the desired shape
    epsilon = np.random.normal(0, noise_sigma, N) #generating random values for epsilon ~ N(0, 2)
    Y = np.matmul(X, theta) + epsilon #generating the target values
    print("Shape of X: ", X.shape) # ensuring the X has desired shape
    print("Shape of Y: ", Y.shape) # ensuring that Y has desired shape

    X_return = np.array(X[['x1', 'x2']]) # X_return is matrix having N rows and 2 columns of x1 and x2
    # print("X: ", X)
    # print("Y: ", Y)
    # print("epsilon: ", epsilon)
    # print((input_mean[0], input_sigma[0], N))
    # print((input_mean[1], input_sigma[1], N))

    return X_return, Y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta_1 = None
        self.theta_80 = None
        self.theta_8000 = None
        self.theta_800000 = None

    def loss(self, X, Y, theta):

        error = Y - np.matmul(X, theta) #calculating the error term
        current_loss = np.dot(error.T, error) / (2 * X.shape[0]) #calculating the loss
        gradient = -np.matmul(error.T, X) / X.shape[0] #calculating the gradient
        gradient = np.array(gradient).flatten() #flattening the gradient for ease of use in update rule
        return current_loss, gradient
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        print("Shape of X: ", X.shape) # This should print (n_samples, n_features + 1) or (1000000, 3) in our case.

        if self.theta_1 is None:
            self.theta_1 = np.zeros(X.shape[1]) #initializing the parameters to 0 if not already initialized
        if self.theta_80 is None:
            self.theta_80 = np.zeros(X.shape[1])
        if self.theta_8000 is None:
            self.theta_8000 = np.zeros(X.shape[1])
        if self.theta_800000 is None:
            self.theta_800000 = np.zeros(X.shape[1])

        split = int(0.2 * X.shape[0]) # defining the split value

        indices = np.random.permutation(X.shape[0]) # shuffling the indices just in case the data has some ordering bias
        """
        shuffling the indices just in case the data has some ordering bias
        """
        X = X[indices] # shuffling the input data
        y = y[indices] # shuffling the target values
        X_train = X[split:]
        X_test = X[:split]
        y_train = y[split:]
        y_test = y[:split]

        params_1 = np.array(self.theta_1) #initializing the parameters array
        losses_1 = np.array([]) #initializing the losses array

        params_80 = np.array(self.theta_80)
        losses_80 = np.array([])

        params_8000 = np.array(self.theta_8000)
        losses_8000 = np.array([])

        params_800000 = np.array(self.theta_800000)
        losses_800000 = np.array([])

        
        epochs = 10000 #defining the number of epochs

        batch_sizes = [1, 80, 8000, 800000] #defining the batch size


        for num, batch_size in enumerate(batch_sizes):
            f = True
            latest_average_loss = float('inf') #initializing the latest average loss to infinity
            for _ in range(epochs):
                indices = np.random.permutation(X_train.shape[0])
                X_train = X_train[indices]
                y_train = y_train[indices]
                for sub_epoch in range(0, math.ceil(X_train.shape[0] / batch_size)):
                    start = sub_epoch * batch_size
                    end = min((sub_epoch + 1) * batch_size, X_train.shape[0]) #defining the start and end indices for the batch
                    X_batch = X_train[start:end]
                    y_batch = y_train[start:end]
                    if num == 0:
                        current_loss, gradient = self.loss(X_batch, y_batch, self.theta_1) #calculating the loss and gradient
                        self.theta_1 = self.theta_1 - learning_rate * gradient #updating the parameters using the update rule

                        params_1 = np.vstack((params_1, self.theta_1))
                        losses_1 = np.append(losses_1, current_loss)
                        if len(losses_1) &gt; 20: #calculating the average loss over the last 20 epochs
                            losses_1 = losses_1[1:]
                        current_average_loss = np.mean(losses_1)
                        
                        if abs(latest_average_loss - current_average_loss) &lt; 0.0000001 * current_average_loss: #checking for convergence using the change in average loss
                            f = False
                            break
                        latest_average_loss = current_average_loss

                    elif num == 1:
                        current_loss, gradient = self.loss(X_batch, y_batch, self.theta_80)
                        self.theta_80 = self.theta_80 - learning_rate * gradient

                        params_80 = np.vstack((params_80, self.theta_80))
                        losses_80 = np.append(losses_80, current_loss)
                        if len(losses_80) &gt; 20:
                            losses_80 = losses_80[1:]
                        current_average_loss = np.mean(losses_80)

                        if abs(latest_average_loss - current_average_loss) &lt; 0.0000001 * current_average_loss:
                            f = False
                            break
                        latest_average_loss = current_average_loss

                    elif num == 2:
                        current_loss, gradient = self.loss(X_batch, y_batch, self.theta_8000)
                        self.theta_8000 = self.theta_8000 - learning_rate * gradient

                        params_8000 = np.vstack((params_8000, self.theta_8000))
                        losses_8000 = np.append(losses_8000, current_loss)
                        if len(losses_8000) &gt; 20:
                            losses_8000 = losses_8000[1:]
                        current_average_loss = np.mean(losses_8000)

                        if abs(latest_average_loss - current_average_loss) &lt; 0.0000001 * current_average_loss:
                            f = False
                            break
                        latest_average_loss = current_average_loss

                    elif num == 3:
                        current_loss, gradient = self.loss(X_batch, y_batch, self.theta_800000)
                        self.theta_800000 = self.theta_800000 - learning_rate * gradient

                        params_800000 = np.vstack((params_800000, self.theta_800000))
                        losses_800000 = np.append(losses_800000, current_loss)
                        if len(losses_800000) &gt; 20:
                            losses_800000 = losses_800000[1:]
                        current_average_loss = np.mean(losses_800000)

                        if abs(latest_average_loss - current_average_loss) &lt; 0.0000001 * current_average_loss:
                            f = False
                            break
                        latest_average_loss = current_average_loss
                if not f:
                    break

        print(params_1.shape, params_80.shape, params_8000.shape, params_800000.shape)

        return [params_1, params_80, params_8000, params_800000]
    
    def closed_form(self, X, y):
        """
        Fits the linear regression model to the data using the closed form solution.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        theta : numpy array of shape (n_features,)
            The parameters of the linear regression model.
        """
        theta = np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y)) #calculating the parameters using the closed form solution
        """
        theta = (X^T * X)^-1 * X^T * Y
        """
        print(theta)
        return theta

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return [X @ self.theta_1, X @ self.theta_80, X @ self.theta_8000, X @ self.theta_800000] #returning the predicted values
        
        



import numpy as np
import pandas as pd
import sampling_sgd as sgd
import time as t
import matplotlib.pyplot as plt


if __name__ == '__main__':
    start = t.time()
    X, y = sgd.generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), 2)
    X = np.array(X)
    # add bias term in X
    X = np.insert(X, 0, 1, axis=1)
    y = np.array(y)
    model = sgd.StochasticLinearRegressor()
    params = model.fit(X, y)
    y_pred = model.predict(X)
    print(y.shape, y_pred[0].shape)
    print("MSE: ", np.mean((y - y_pred[0]) ** 2))
    print(model.theta_1, model.theta_80, model.theta_8000, model.theta_800000)
    print("Time taken: ", t.time() - start)

    print(model.closed_form(X, y))
    # print(model.predict(X))
    # theta0 = params[:, 0]
    # theta1 = params[:, 1]
    # theta2 = params[:, 2]

    # fig = plt.figure(figsize=(5.6, 5.6))
    # ax = fig.add_subplot(111, projection='3d')

    # ax.plot(theta0, theta1, theta2, marker='o', linestyle='-', markersize=3, label="θ updates")
    # ax.scatter(theta0[0], theta1[0], theta2[0], color='red', marker='o', s=100, label="Start (θ_init)")
    # ax.scatter(theta0[-1], theta1[-1], theta2[-1], color='green', marker='o', s=100, label="Converged (θ_final)")

    # # Labels
    # ax.set_xlabel("θ0 (Bias)")
    # ax.set_ylabel("θ1 (Feature 1)")
    # ax.set_zlabel("θ2 (Feature 2)")
    # ax.set_title("Gradient Path for Stochastic Gradient Descent")
    # ax.legend()

    # plt.savefig("theta_1.png")



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None

<A NAME="1"></A><FONT color = #00FF00><A HREF="match224-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def loss(self, X, y):
        """
        Compute the loss function for logistic regression.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
</FONT>            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        Returns
        -------
        loss : float
            The loss value.
        
        gradient : numpy array of shape (n_features,)
            The gradient of the loss with respect to the parameters.
        """

        m = X.shape[0]
        # Compute predictions using the sigmoid function.
        h_theta = np.matmul(X, self.theta)  # shape: (m,)
        h = 1 / (1 + np.exp(-h_theta))       # shape: (m,)
        
        # Compute the loss (negative log-likelihood)
        current_loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
        
        # Compute the gradient (ensure shape consistency)
        gradient = (X.T @ (h - y)) / m  # shape: (n,)

        # Compute Hessian
        # Create a diagonal matrix with h*(1-h)
        w = h * (1 - h)               # shape: (m,)
        W = np.diag(w)                # shape: (m, m)
        Hessian = (X.T @ W @ X) / m     # shape: (n, n)
        Hessian = np.linalg.inv(Hessian)

        print("Current Loss: ", current_loss)
        print("Gradient: ", gradient)
        print("Hessian: ", Hessian)
        
        return current_loss, gradient, Hessian

    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        """
        Normalization of the input data
        """

        X_mean = np.mean(X, axis=0)
        y_mean = np.mean(y, axis=0)
        X_error = X - X_mean
        y_error = y - y_mean
        X_error = np.sum(X_error ** 2, axis=0) / X.shape[0]
        y_error = np.sum(y_error ** 2, axis=0) / y.shape[0]
        X_error = np.sqrt(X_error)
        y_error = np.sqrt(y_error)
        # just normalize 2nd and 3rd columns of X since the first column is the bias term and std deviation is 0 for it
        X[:, 1] = (X[:, 1] - X_mean[1]) / X_error[1] # normalize the 2nd column (x1)
        X[:, 2] = (X[:, 2] - X_mean[2]) / X_error[2] # normalize the 3rd column (x2)
        """I added the bias term previously, then after clarification, I took it to be the first column of X"""
        # add a column for the bias term
        # X = np.hstack((X, np.ones((X.shape[0], 1))))
        # print(X)
        # print(y)

        if self.theta is None:
            self.theta = np.zeros(X.shape[1])

        y = np.array(y).flatten()
        n_iter = 10000
        latest_loss = float('inf')
        params = np.array([self.theta])
        for i in range(n_iter):
            current_loss, gradient, Hessian = self.loss(X, y)
            self.theta -= Hessian @ gradient # update rule for Newton's Method that is theta = theta - Hessian^-1 * gradient
            if abs(latest_loss - current_loss) &lt; 0.0001 * current_loss: # stopping criterion
                break
            latest_loss = current_loss
            params = np.vstack((params, self.theta))
            print("Iteration: ", i, "Theta: ", self.theta)
        print("Final Theta: ", self.theta)
        return params

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        print(X.shape)
        print(self.theta.shape)

        return np.round(1 / (1 + np.exp(-np.matmul(X, self.theta)))) # returning the predicted values as 0 or 1



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Loading data
X = pd.read_csv("../data/Q3/logisticX.csv", header=None).to_numpy()
y = pd.read_csv("../data/Q3/logisticY.csv", header=None).to_numpy().flatten()

# calculated theta values
theta = np.array([2.5885477, -2.72558849, 0.40125316])

# Separating points based on labels
X1 = X[y == 1]
X0 = X[y == 0]

# Plotting data points with different markers for different labels
plt.scatter(X1[:, 0], X1[:, 1], marker='o', color='blue', label='Label 1')
plt.scatter(X0[:, 0], X0[:, 1], marker='x', color='red', label='Label 0')

# Plotting decision boundary: x2 = -(theta_0 + theta_1*x1) / theta_2
x1_vals = np.linspace(min(X[:, 0]) - 1, max(X[:, 0]) + 1, 100)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match224-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x2_vals = -(theta[2] + theta[0] * x1_vals) / theta[1]
plt.plot(x1_vals, x2_vals, color='green', label='Decision Boundary')

# Labels and legend
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
plt.grid(True)
</FONT>
# Show the plot
plt.show()




import numpy as np
import pandas as pd
import logistic_regression as lr

if __name__ == "__main__":
    X = pd.read_csv("../data/Q3/logisticX.csv", header=None)
    y = pd.read_csv("../data/Q3/logisticY.csv", header=None)
    X['x0'] = 1
    # print(X.columns)
    X = X[['x0', 0, 1]]
    X = np.array(X)
    y = np.array(y)
    model = lr.LogisticRegressor()
    model.fit(X, y)

    y_pred = model.predict(X)

    print("Predictions: ", y_pred)

    print("Done with the code.")



import numpy as np
class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi_0 = None
        self.phi_1 = None

    def fit(self, X, y, assume_same_covariance=False):
        """
<A NAME="5"></A><FONT color = #FF0000><A HREF="match224-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        Fit the Gaussian Discriminant Analysis model to the data.
        Normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
</FONT><A NAME="2"></A><FONT color = #0000FF><A HREF="match224-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        assume_same_covariance : bool
            If True, assume that both classes share the same covariance matrix.
</FONT>        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # normalizing X
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        self.phi_1 = np.sum(y, axis=0, where=y[:] == 1) # calculating phi_1, probability of y = 1
        self.phi_0 = (X.shape[0] - self.phi_1) # calculating phi_0, probability of y = 0

        self.phi_0 /= X.shape[0] # normalizing phi_0
        self.phi_1 /= X.shape[0] # normalizing phi_1

        self.mu_1 = np.sum(X, axis=0, where=y[:] == 1) / np.sum(y, axis=0, where=y[:] == 1) # calculating mu_1
        self.mu_0 = np.sum(X, axis=0, where=y[:] == 0) / (X.shape[0] - np.sum(y, axis=0, where=y[:] == 1)) # calculating mu_0

        """
        reshaping mu_0 and mu_1 for easier operations later in the code
        """
        self.mu_0 = self.mu_0.reshape(1, -1) 
        self.mu_1 = self.mu_1.reshape(1, -1)

        # print(self.mu_0, self.mu_1)

        if assume_same_covariance:
            self.sigma = (X[:] - self.mu_0).T @ ((X[:] - self.mu_0) * (y[:] == 0)) + (X[:] - self.mu_1).T @ ((X[:] - self.mu_1) * (y[:] == 1))
            self.sigma /= X.shape[0]

            """I used the sigma_inv to get theta_1 and theta_0 to plot the decision boundary."""

            # sigma_inv = np.linalg.inv(self.sigma)
            # print(sigma_inv)

            # theta_1 = sigma_inv @ (self.mu_1 - self.mu_0).T
            # theta_0 = np.log2(self.phi_1 / self.phi_0) - 0.5 * (self.mu_1 @ sigma_inv @ self.mu_1.T - self.mu_0 @ sigma_inv @ self.mu_0.T)

            # print(theta_1, theta_0)

            return self.mu_0, self.mu_1, self.sigma
        
        else:
            self.sigma_0 = (X[:] - self.mu_0).T @ ((X[:] - self.mu_0) * (y[:] == 0)) / np.sum(y[:] == 0)
            self.sigma_1 = (X[:] - self.mu_1).T @ ((X[:] - self.mu_1) * (y[:] == 1)) / np.sum(y[:] == 1)

            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize X using training data statistics (mean and std)
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        if self.sigma is not None:  # Shared covariance case (linear decision boundary)
            sigma_inv = np.linalg.inv(self.sigma)
            m = sigma_inv @ (self.mu_1 - self.mu_0).T # slope vector
            b = -0.5 * (self.mu_1 @ sigma_inv @ self.mu_1.T - self.mu_0 @ sigma_inv @ self.mu_0.T) + np.log(self.phi_1 / (self.phi_0)) # bias term
            
            logits = X @ m + b #decision boundary (straight line)

            ans = (logits &gt; 0).astype(int)
            return ans.flatten()
        
        else:  # Separate covariance case (quadratic decision boundary)
            preds = []
            for x in X:
                p_y0 = -0.5 * ((x - self.mu_0) @ np.linalg.inv(self.sigma_0) @ (x - self.mu_0).T) - 0.5 * np.log(np.linalg.det(self.sigma_0)) + np.log(self.phi_0) # calculating p(y=0|x)
                p_y1 = -0.5 * ((x - self.mu_1) @ np.linalg.inv(self.sigma_1) @ (x - self.mu_1).T) - 0.5 * np.log(np.linalg.det(self.sigma_1)) + np.log(self.phi_1) # calculating p(y=1|x)
                preds.append(1 if p_y1 &gt; p_y0 else 0)
            
            return np.array(preds).flatten()




import numpy as np
import matplotlib.pyplot as plt
import gda as g
import pandas as pd

# Load data
X = pd.read_csv("../data/Q4/q4x.dat", header=None, sep='\s+')
y = pd.read_csv("../data/Q4/q4y.dat", header=None)

X = np.array(X, dtype=float)
# convert y to 0 and 1 from string
y = np.array(y.replace(['Alaska', 'Canada'], [0, 1]), dtype=float)

X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Fit GDA model
gda = g.GaussianDiscriminantAnalysis()
print("hello")
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
print("hello")

# Compute decision boundary parameters
sigma_inv = np.linalg.inv(sigma)
print(sigma_inv.shape, mu_1.shape, mu_0.shape)
mu_0 = mu_0.T
mu_1 = mu_1.T
w = sigma_inv @ (mu_1 - mu_0)
b = - 0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)

# Generating x1 values for the decision boundary
x1_vals = np.linspace(min(X[:, 0]) - 1, max(X[:, 0]) + 1, 100)
x2_vals = -(w[0] * x1_vals + b) / w[1]  # From w1*x1 + w2*x2 + b = 0
x2_vals = x2_vals.T

y = y.flatten()

# Plot data points
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='o', color='blue', label='Canada')
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='x', color='red', label='Alaska')

# Plot decision boundary
plt.plot(x1_vals, x2_vals, color='green', label='GDA Decision Boundary')

# Labels and legend
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Gaussian Discriminant Analysis Decision Boundary')
plt.grid(True)

# Show the plot
plt.show()




import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import gda as g  # assuming your gda module is available in your PYTHONPATH

# Loading and preprocess data
X = pd.read_csv("../data/Q4/q4x.dat", header=None, sep='\s+')
y = pd.read_csv("../data/Q4/q4y.dat", header=None)

X = np.array(X, dtype=float)
# Converting y labels from strings "Alaska", "Canada" to numeric 0 and 1
y = np.array(y.replace(['Alaska', 'Canada'], [0, 1]), dtype=float)

# Normalizing the features
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Fitting the GDA model assuming different covariance for each class (quadratic case)
gda_model = g.GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda_model.fit(X, y, assume_same_covariance=False)

# Retrieve=ing class priors from the fitted model (already normalized as fractions)
phi_0 = gda_model.phi_0
phi_1 = gda_model.phi_1

# Flattenning means into 1D arrays for easier manipulation
mu0 = mu_0.flatten()
mu1 = mu_1.flatten()

# Preparing a grid for contour plotting the quadratic decision boundary
x1_range = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)
x2_range = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 200)
xx, yy = np.meshgrid(x1_range, x2_range)

# Precomputing inverses and log determinants for each covariance matrix
sigma0_inv = np.linalg.inv(sigma_0)
sigma1_inv = np.linalg.inv(sigma_1)
log_det_sigma0 = np.log(np.linalg.det(sigma_0))
log_det_sigma1 = np.log(np.linalg.det(sigma_1))

# Define a function for the discriminant difference delta(x) = g(x|y=1) - g(x|y=0)
def discriminant_diff(x):
    # x should be a 2D point, small vector of length 2
    diff1 = x - mu1
    diff0 = x - mu0
    # Compute quadratic forms
    d1 = -0.5 * (diff1.T @ sigma1_inv @ diff1) - 0.5 * log_det_sigma1 + np.log(phi_1)
    d0 = -0.5 * (diff0.T @ sigma0_inv @ diff0) - 0.5 * log_det_sigma0 + np.log(phi_0)
    return d1 - d0

# Evaluating the discriminant difference on the grid
grid_points = np.c_[xx.ravel(), yy.ravel()]
dvals = np.array([discriminant_diff(point) for point in grid_points])
dvals = dvals.reshape(xx.shape)

# Plotting the data and the quadratic decision boundary
plt.figure(figsize=(8, 6))
# Scattering plot for each class (flatten y for boolean mask)
plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1],
            marker='o', color='blue', label='Canada')
plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1],
            marker='x', color='red', label='Alaska')

# Plotting the decision boundary: contour where discriminant difference is zero.
contour = plt.contour(xx, yy, dvals, levels=[0], colors='green', linewidths=2)
plt.clabel(contour, inline=1, fontsize=10)

plt.xlabel('x1')
plt.ylabel('x2')
plt.title('GDA Decision Boundary (Quadratic) with Separate Covariances')
plt.legend()
plt.grid(True)
plt.show()




import numpy as np
import pandas as pd
import gda as gda

if __name__ == "__main__":
    X = pd.read_csv("../data/Q4/q4x.dat", header=None, sep='\s+')
    y = pd.read_csv("../data/Q4/q4y.dat", header=None)
    X = np.array(X, dtype=float)
    # convert y to 0 and 1 from string
    y = np.array(y.replace(['Alaska', 'Canada'], [0, 1]), dtype=float)

    # print(X)
    # print(y)

    gda = gda.GaussianDiscriminantAnalysis()
    mu1, mu2, sigma = gda.fit(X, y, assume_same_covariance=True)
    # mu1, mu2, sigma1, sigma2 = gda.fit(X, y, assume_same_covariance=False)
    print(mu1)
    print(mu2)
    print(sigma)

    # print(sigma1)
    # print(sigma2)
    # print(gda.predict(X))

    y_pred = gda.predict(X)
    print(y_pred.shape)

    # print(y)
    # print(y_pred == y)

    # y = y.flatten()
    # y_pred = y_pred.flatten()

    # print(y)
    # print(y_pred)
    # print(np.sum(y_pred == y) / y.shape[0])

</PRE>
</PRE>
</BODY>
</HTML>
