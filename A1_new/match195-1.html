<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_8OSK3.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_SGPW2.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.cm as cm
import matplotlib.animation as animation
import time
from linear_regression import LinearRegressor  

def load_data(x_file, y_file):
    X = pd.read_csv(x_file, header=None).values  
    y = pd.read_csv(y_file, header=None).values.flatten()  
    return X, y

X, y = load_data("../data/Q1/linearX.csv", "../data/Q1/linearY.csv")

model = LinearRegressor()
theta_his = model.fit(X, y)
print(f"final parameters learned: {theta_his[-1]}")

y_pred = model.predict(X)

plt.scatter(X, y, label="Data Points", color="blue", alpha=0.6)
plt.plot(X, y_pred, label="Regression Line", color="red")
plt.xlabel("X (Feature)")
plt.ylabel("y (Target)")
plt.title("Linear Regression Fit")
plt.legend()
plt.show()

def find_cost(X, y, theta):
    m = len(y)
    predictions = X @ theta
    error = predictions - y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

def plot1(X, y, theta_history, cost_history):
    theta_history = np.array(theta_history)

    theta_0_vals = np.linspace(-5, 6, 50)  
    theta_1_vals = np.linspace(0, 30, 50)  
    Theta0, Theta1 = np.meshgrid(theta_0_vals, theta_1_vals)
    CostVals = np.zeros(Theta0.shape)

    for i in range(Theta0.shape[0]):
        for j in range(Theta0.shape[1]):
            theta = np.array([Theta0[i, j], Theta1[i, j]])
            X_tmp = np.hstack((np.ones((X.shape[0], 1)), X)) 
            CostVals[i, j] = find_cost(X_tmp, y, theta)

    fig = plt.figure(figsize=(10, 7))
<A NAME="2"></A><FONT color = #0000FF><A HREF="match195-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax = fig.add_subplot(111, projection='3d')

    ax.plot_surface(Theta0, Theta1, CostVals, cmap='viridis', alpha=0.7)

    norm = plt.Normalize(min(cost_history), max(cost_history))
</FONT>    colors = cm.viridis(norm(cost_history))  

    scatter = ax.scatter([], [], [], c=[], cmap='viridis', edgecolor='k', s=50)

    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$J(\theta)$')
    ax.set_title("Gradient Descent Path on Cost Function")

    def init():
        scatter._offsets3d = ([], [], [])
        return scatter,

    def update(frame):
        ax.cla()  
        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title("Gradient Descent Path on Cost Function")

        ax.plot_surface(Theta0, Theta1, CostVals, cmap='viridis', alpha=0.7)

        ax.scatter(theta_history[:frame, 0], theta_history[:frame, 1], cost_history[:frame], 
                   color='red', edgecolor='k', s=50)

        time.sleep(0.2)
        return scatter,

    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), init_func=init, blit=False, repeat=False)
    
    plt.show()

def plot2(X, y, theta_history, cost_history):
    theta_history = np.array(theta_history)

    theta_0_vals = np.linspace(-10, 20, 100)  
    theta_1_vals = np.linspace(-4, 60, 100)  
    Theta0, Theta1 = np.meshgrid(theta_0_vals, theta_1_vals)
    CostVals = np.zeros(Theta0.shape)

    for i in range(Theta0.shape[0]):
        for j in range(Theta0.shape[1]):
            theta = np.array([Theta0[i, j], Theta1[i, j]])
            X_tmp = np.hstack((np.ones((X.shape[0], 1)), X))  
            CostVals[i, j] = find_cost(X_tmp, y, theta)

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contour(Theta0, Theta1, CostVals, levels=50, cmap='viridis')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_title("Gradient Descent Path on Contour Plot")

    path, = ax.plot([], [], 'ro-', markersize=5) 

    def init():
        path.set_data([], [])
        return path,

    def update(frame):
        path.set_data(theta_history[:frame, 0], theta_history[:frame, 1])
        time.sleep(0.2)  
        return path,

    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), init_func=init, blit=False, repeat=False)
    
    plt.show()

def plot3(X, y, theta_history, cost_history, learning_rate):
    theta_history = np.array(theta_history)

    theta_0_vals = np.linspace(-10, 20, 100) 
    theta_1_vals = np.linspace(-4, 60, 100) 
    Theta0, Theta1 = np.meshgrid(theta_0_vals, theta_1_vals)
    CostVals = np.zeros(Theta0.shape)

    for i in range(Theta0.shape[0]):
        for j in range(Theta0.shape[1]):
            theta = np.array([Theta0[i, j], Theta1[i, j]])
            X_tmp = np.hstack((np.ones((X.shape[0], 1)), X)) 
            CostVals[i, j] = find_cost(X_tmp, y, theta)

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contour(Theta0, Theta1, CostVals, levels=50, cmap='viridis')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_title(f"Gradient Descent Path (η = {learning_rate})")

    path, = ax.plot([], [], 'ro-', markersize=5)

    def init():
        path.set_data([], [])
        return path,

    def update(frame):
        path.set_data(theta_history[:frame, 0], theta_history[:frame, 1])
        time.sleep(0.2) 
        return path,

    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), init_func=init, blit=False, repeat=False)
    
    plt.show()

if len(model.theta_history) &gt; 1:
    plot1(X, y, model.theta_history, model.cost_history)
    plot2(X, y, model.theta_history, model.cost_history)

print("Final Cost:", model.cost_history[-1])

learning_rates = [0.001, 0.025, 0.1]

for eta in learning_rates:
    print(f"\nTraining model with learning rate η = {eta} ...")
    model = LinearRegressor()
    model.fit(X, y, learning_rate=eta)
    plot3(X, y, model.theta_history, model.cost_history, eta)




import numpy as np

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
        self.cost_history = []
    
    def compute_cost(self, X, y):
        m = len(y)
        predictions = X @ self.theta
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        delta=1e-8
        iterations=5000
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X)) 
        self.theta = np.zeros(n + 1)  
        
        
        for iter in range(iterations):
            gradient = (1 / m) * (X.T @ (X @ self.theta - y))
            self.theta -= learning_rate * gradient

            cost = self.compute_cost(X, y)
            # print(f"Cost at iteration {iter}: {cost}")

            if len(self.cost_history) &gt; 0 and abs(self.cost_history[-1] - cost) &lt; delta:
                print(f"Converging at iteration {iter}")
                break 

            self.theta_history.append(self.theta.copy())
            self.cost_history.append(cost)
        
        # return np.array(theta_history), np.array(cost_history)
        return np.array(self.theta_history)
    
    def predict(self, X):
        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))  
        return X @ self.theta



import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from sampling_sgd import generate, StochasticLinearRegressor, closed_form_solution

theta_list = []

def plot_theta_history(theta_history):
    theta_history = np.array(theta_history)

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', linestyle='-', color='blue')

    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title("Parameters Variation")

    formatter = FuncFormatter(lambda x, _: f'{x:.8f}')
    ax.xaxis.set_major_formatter(formatter)
    ax.yaxis.set_major_formatter(formatter)

    plt.show()

true_theta = np.array([3, 1, 2])  
input_mean = np.array([3, -1])  
input_sigma = np.array([2, 2])  
noise_sigma = np.sqrt(2)  

X, y = generate(1000000, true_theta, input_mean, input_sigma, noise_sigma)

m, n = X.shape
indices = np.random.permutation(m)
X_shuffled = X[indices]
y_shuffled = y[indices]

X_training, X_testing = X_shuffled[:800000], X_shuffled[800000:]
y_training, y_testing = y_shuffled[:800000], y_shuffled[800000:]

model = StochasticLinearRegressor()
model.fit(X_training, y_training, learning_rate=0.001)
theta_list.append(model.theta_list)

theta_closed = closed_form_solution(X,y)
print(theta_closed)

y_pred_train = model.predict(X_training)
print(len(y_pred_train[0]),len(y_pred_train[1]),len(y_pred_train[2]),len(y_pred_train[3]))

print("train error bs: 1-&gt;")
train_error = np.sum((y_pred_train[0] - y_training) ** 2) / 1600000
print(train_error)

print("train error bs: 80-&gt;")
train_error = np.sum((y_pred_train[1] - y_training) ** 2) / 1600000
print(train_error)

print("train error bs: 8000-&gt;")
train_error = np.sum((y_pred_train[2] - y_training) ** 2) / 1600000
print(train_error)

print("train error bs: 800000-&gt;")
train_error = np.sum((y_pred_train[3] - y_training) ** 2) / 1600000
print(train_error)


y_pred_test = model.predict(X_testing)

print("test error bs: 1-&gt;")
test_error = np.sum((y_pred_test[0] - y_testing) ** 2) / 400000
print(test_error)

print("test error bs: 80-&gt;")
test_error = np.sum((y_pred_test[1] - y_testing) ** 2) / 400000
print(test_error)

print("test error bs: 8000-&gt;")
test_error = np.sum((y_pred_test[2] - y_testing) ** 2) / 400000
print(test_error)

print("test error bs: 800000-&gt;")
test_error = np.sum((y_pred_test[3] - y_testing) ** 2) / 400000
print(test_error)

plot_theta_history(model.theta_history)
# print (theta_list)










import numpy as np

<A NAME="0"></A><FONT color = #FF0000><A HREF="match195-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise
</FONT>
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.theta_history = []
        self.batch_size = None
        self.theta_list = []
        self.list_of_theta_history =[]

    def fit(self, X, y, learning_rate=0.001):

        tolerance = 1e-6        
        batch_sizes = [1,80,8000, 800000]        
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match195-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))  

        self.theta = np.zeros(n+1)  
        self.list_of_theta_history = []  
</FONT>
        for b_size in batch_sizes:
            epochs = 10 if b_size == 1 else 50 
            self.theta_history = [] 
            
            prev_theta = self.theta.copy()  
            
            for epoch in range(epochs):
                indices = np.random.permutation(m)
                X_shuffled = X[indices]
                y_shuffled = y[indices]
                
                for batch_start in range(0, m, b_size):
                    batch_end = min(batch_start + b_size, m)
                    X_batch = X_shuffled[batch_start:batch_end]
                    y_batch = y_shuffled[batch_start:batch_end]

                    predictions = X_batch @ self.theta
                    errors = predictions - y_batch
                    gradient = (1 / b_size) * (X_batch.T @ errors)

                    self.theta -= learning_rate * gradient
                    self.theta_history.append(self.theta.copy())

                theta_change = np.linalg.norm(self.theta - prev_theta)  
                # print(f"Epoch {epoch+1}/{epochs}, Theta Change: {theta_change}, Theta: {self.theta}")

                if theta_change &lt; tolerance:
                    print(f"Converged at epoch {epoch+1} for batch size {b_size}.")
                    break  
                
                prev_theta = self.theta.copy()  
            self.list_of_theta_history.append(self.theta_history)

        return self.list_of_theta_history


    def predict(self, X):
        batch_sizes = [1, 80, 8000, 800000]
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X)) 

        predictions_list = []  

        for batch_size in batch_sizes:
            y_pred = np.zeros(m)  
            
            for i in range(0, m, batch_size):
                X_batch = X[i:i + batch_size] 
                y_pred[i:i + batch_size] = X_batch @ self.theta  
            
            predictions_list.append(y_pred) 

        return predictions_list
    

def closed_form_solution(X, y):
    
    m, n = X.shape
    X = np.hstack((np.ones((m, 1)), X))  

    theta = np.linalg.inv(X.T @ X) @ X.T @ y
    print(f"Closed-form solution for θ: {theta}")
    return theta















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from logistic_regression import LogisticRegressor

def plot_decision_boundary(model, X, y):
    
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='red', label='Class 0')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='o', color='blue', label='Class 1')

    theta = model.theta
    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2] 

    plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

    plt.xlabel('Feature 1 (x1)')
    plt.ylabel('Feature 2 (x2)')
    plt.title('Logistic Regression Decision Boundary')
    plt.legend()
    plt.grid()
    plt.show()


X = pd.read_csv("../data/Q3/logisticX.csv", header=None).values 
y = pd.read_csv("../data/Q3/logisticY.csv", header=None).values.flatten()  

model = LogisticRegressor()
theta_final = model.fit(X, y)
print("Final coefficients:", theta_final)

y_pred = model.predict(X)
# print("Incorrect Predictions:", y_pred-y)

plot_decision_boundary(model, X, y)



import numpy as np
import pandas as pd

class LogisticRegressor:
    def __init__(self):
        self.theta = None  

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def compute_hessian(self, X, h):
        R = np.diag(h * (1 - h))  
        H = X.T @ R @ X  
        return H
    
    def fit(self, X, y, learning_rate = 0.01):
        tol=1e-12
        max_iter=5000       
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)        
        X = np.c_[np.ones(X.shape[0]), X]          
        self.theta = np.zeros(X.shape[1])     

        for iter in range(max_iter):
            h = self.sigmoid(X @ self.theta)            
            gradient = X.T @ (h - y)            
            H = self.compute_hessian(X, h)            
            delta_theta = np.linalg.inv(H) @ gradient
            self.theta -= learning_rate * delta_theta
            
            if np.linalg.norm(delta_theta) &lt; tol:
                print(f"Converged in {iter} iterations with del theta: {delta_theta}")
                break
        
        return self.theta
    
    def predict(self, X):
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)        
        X = np.c_[np.ones(X.shape[0]), X]        
        probs = self.sigmoid(X @ self.theta)       
        y_pred = (probs &gt;= 0.5).astype(int)
        return y_pred
    



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

def plot_decision_boundary(X, y, gda):
    
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    X_alaska = X[y == 0]  
    X_canada = X[y == 1]  

    plt.figure(figsize=(8, 6))
    plt.scatter(X_canada[:, 0], X_canada[:, 1], marker='o', color='b', label="Canada")  
    plt.scatter(X_alaska[:, 0], X_alaska[:, 1], marker='o', color='r', label="Alaska")  

    Sigma_inv = np.linalg.inv(gda.sigma)  
    w = Sigma_inv @ (gda.mu_1 - gda.mu_0) 
    b = -0.5 * (gda.mu_1.T @ Sigma_inv @ gda.mu_1 - gda.mu_0.T @ Sigma_inv @ gda.mu_0)  

    x1_vals = np.linspace(min(X[:, 0])-1, max(X[:, 0])+1, 100)
    x2_vals_linear = (-w[0] * x1_vals - b) / w[1]

    plt.plot(x1_vals, x2_vals_linear, 'g-', label="Linear Decision Boundary (GDA)")
    
    # if not gda.same_cov:
    Sigma_0_inv = np.linalg.inv(gda.sigma_0)
    Sigma_1_inv = np.linalg.inv(gda.sigma_1)
    A = Sigma_0_inv - Sigma_1_inv
    B = 2 * (gda.mu_1.T @ Sigma_1_inv - gda.mu_0.T @ Sigma_0_inv)
    C = (gda.mu_0.T @ Sigma_0_inv @ gda.mu_0) - (gda.mu_1.T @ Sigma_1_inv @ gda.mu_1) - np.log(np.linalg.det(gda.sigma_0) / np.linalg.det(gda.sigma_1))

    x2_vals = np.linspace(min(X[:, 1])-1, max(X[:, 1])+3, 100)
    X1, X2 = np.meshgrid(x1_vals, x2_vals)
    Z = np.zeros_like(X1)

    for i in range(X1.shape[0]):
        for j in range(X1.shape[1]):
            x = np.array([X1[i, j], X2[i, j]])
            Z[i, j] = x.T @ A @ x + B @ x + C

    plt.contour(X1, X2, Z, 'g-', levels=[0], colors='black')

    plt.xlabel("x1 (Growth Ring Diameter in Fresh Water)")
    plt.ylabel("x2 (Growth Ring Diameter in Marine Water)")
    plt.title("GDA Decision Boundaries for Alaska vs. Canada")
    plt.legend()
    plt.grid(True)
    
    plt.show()

if __name__ == "__main__":
<A NAME="1"></A><FONT color = #00FF00><A HREF="match195-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X = np.loadtxt('../data/Q4/q4x.dat')
    y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
    y = np.where(y == 'Alaska', 0, 1)
    gda = GaussianDiscriminantAnalysis()
    # mu_0 , mu_1, sigma_0, sigma_1 =gda.fit(X, y, assume_same_covariance=False)
    mu_0 , mu_1, sigma_0 = gda.fit(X, y, assume_same_covariance=True)
    same_cov = gda.same_cov
</FONT>
    # print("Mean for class 0 (Alaska):", mu_0)
    # print("Mean for class 1 (Canada):", mu_1)
    # print("cov1 :", sigma_0)
    # print("cov2:", sigma_1)
    y_pred = gda.predict(X)
    # print("Predictions:", y_pred)
    # print ("actual: ", y)
    plot_decision_boundary(X, y, gda)



import numpy as np

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.same_cov = False

    def fit(self, X, y, assume_same_covariance=False):        
        self.same_cov = assume_same_covariance
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X0 = X[y == 0]  
        X1 = X[y == 1]  

        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)
        self.phi = np.mean(y)

        if assume_same_covariance:
            m = X.shape[0]
            sigma = np.zeros((X.shape[1], X.shape[1]))
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match195-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            for i in range(m):
                x_i = X[i].reshape(-1, 1)
                mu_y = self.mu_0 if y[i] == 0 else self.mu_1
</FONT>                mu_y = mu_y.reshape(-1, 1)
                sigma += (x_i - mu_y) @ (x_i - mu_y).T
            self.sigma = sigma / m

            n0 = X0.shape[0]
            n1 = X1.shape[0]

            self.sigma_0= sum(np.outer(X0[i]-self.mu_0, X0[i]-self.mu_0) for i in range(n0))/n0
            self.sigma_1= sum(np.outer(X1[i]-self.mu_1, X1[i]-self.mu_1) for i in range(n1))/n1
            
            return self.mu_0, self.mu_1, self.sigma

        else:
            n0 = X0.shape[0]
            n1 = X1.shape[0]

            self.sigma_0 = sum(np.outer(X0[i]-self.mu_0, X0[i]-self.mu_0) for i in range(n0))/n0
            self.sigma_1 = sum(np.outer(X1[i]-self.mu_1, X1[i]-self.mu_1) for i in range(n1))/n1

            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        if self.same_cov:
            inv_sigma = np.linalg.inv(self.sigma)
            log_prob_0 = -0.5*np.sum((X-self.mu_0) @ inv_sigma * (X-self.mu_0), axis=1)
            log_prob_1 = -0.5*np.sum((X-self.mu_1) @ inv_sigma * (X-self.mu_1), axis=1)

        else:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            log_prob_0 = -0.5*np.sum((X-self.mu_0) @ inv_sigma_0*(X-self.mu_0), axis=1)
            log_prob_1 = -0.5*np.sum((X-self.mu_1) @ inv_sigma_1*(X-self.mu_1), axis=1)
        
        log_prob_0 += np.log(1 - self.phi)
        log_prob_1 += np.log(self.phi)
        
        # y_pred = np.where(log_prob_1 &gt; log_prob_0, 'Canada', 'Alaska')
        y_pred = np.where(log_prob_1 &gt; log_prob_0, 1, 0)        
        return y_pred


</PRE>
</PRE>
</BODY>
</HTML>
