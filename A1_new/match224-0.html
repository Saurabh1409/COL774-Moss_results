<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_G8GO6.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_G8GO6.py<p><PRE>


import numpy as np 
from linear_regression import LinearRegressor
import sys
import matplotlib.pyplot as plt

if __name__ == '__main__':

    if len(sys.argv) != 3:
        print("Usage: python plot_data1.py &lt;path_to_linearX.csv&gt; &lt;path_to_linearY.csv&gt;")
        sys.exit(1)

    # Load dataset from command line arguments
    features = np.loadtxt(sys.argv[1], delimiter=',').reshape(-1, 1)
    targets = np.loadtxt(sys.argv[2], delimiter=',')
    num_samples = len(features)
    print(num_samples)
    # Train model
    model = LinearRegressor()
    params_history = model.fit(features, targets)
    print("params history: ", params_history)

    def plot_training_data(features, targets, model):
        plt.scatter(features[:], targets, label='Training Data', marker='o')
        plt.plot(features[:], model.predict(features), label='Learned Hypothesis', color='red')
        plt.xlabel('Feature')
        plt.ylabel('Target')
        plt.legend()
        plt.show()

    # Call the function to plot training data
    plot_training_data(features, targets, model)

    # section 1.3 (a) & (b)
    def mesh(features, targets, params_history):
        features_with_bias = np.c_[np.ones(len(features)), features]  # Add bias column to features

        # Compute range of θ_0 and θ_1
        theta_min = np.min(params_history, axis=0)
        theta_max = np.max(params_history, axis=0)
        theta_0_range = (theta_min[0] - 2, theta_max[0] + 2)
        theta_1_range = (theta_min[1] - 2, theta_max[1] + 2)

        # Generate grid of θ_0 and θ_1 values
        theta_0_values = np.linspace(*theta_0_range, 100)
        theta_1_values = np.linspace(*theta_1_range, 100)
        theta_0, theta_1 = np.meshgrid(theta_0_values, theta_1_values)

        # Compute cost values for grid
        cost_values = np.zeros_like(theta_0)
        m = len(targets)
        for i in range(theta_0.shape[0]):
            for j in range(theta_0.shape[1]):
                theta = np.array([theta_0[i, j], theta_1[i, j]])
                predictions = features_with_bias.dot(theta)
                errors = predictions - targets
                cost_values[i, j] = (1 / (2 * m)) * np.sum(errors ** 2)

        # Create a 3D surface plot
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(theta_0, theta_1, cost_values, cmap='viridis', edgecolor='none', alpha=0.8)
        ax.set_xlabel('θ_0 (Bias)')
        ax.set_ylabel('θ_1 (Weight)')
        ax.set_zlabel('Cost J(θ)')
        ax.set_title('Error Function J(θ)')
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)

        # Plot parameter progression during gradient descent
        for i, theta in enumerate(params_history):
            theta_0_val, theta_1_val = theta
            prediction = features_with_bias.dot(theta)
            error = prediction - targets
            cost = (1 / (2 * m)) * np.sum(error ** 2)
            
            ax.scatter(theta_0_val, theta_1_val, cost, color='red', s=50, label='Iteration {}'.format(i) if i == 0 else "")
            
            # Add time delay to observe the change in parameters
            plt.pause(0.2)
        
        plt.show()

    mesh(features, targets, params_history)
    # Section 1.4
    def contour(features, targets, params_history):
        features_with_bias = np.c_[np.ones(len(features)), features]  # Add bias column to features

        # Compute range of θ_0 and θ_1
        theta_min = np.min(params_history, axis=0)
        theta_max = np.max(params_history, axis=0)
        theta_0_range = (theta_min[0] - 2, theta_max[0] + 2)
        theta_1_range = (theta_min[1] - 2, theta_max[1] + 2)

        # Generate grid of θ_0 and θ_1 values
        theta_0_values = np.linspace(*theta_0_range, 100)
        theta_1_values = np.linspace(*theta_1_range, 100)
        theta_0, theta_1 = np.meshgrid(theta_0_values, theta_1_values)

        # Compute cost values for grid
        cost_values = np.zeros_like(theta_0)
        m = len(targets)
        for i in range(theta_0.shape[0]):
            for j in range(theta_0.shape[1]):
                theta = np.array([theta_0[i, j], theta_1[i, j]])
                predictions = features_with_bias.dot(theta)
                errors = predictions - targets
                cost_values[i, j] = (1 / (2 * m)) * np.sum(errors ** 2)

        # Create contour plot
        plt.figure(figsize=(10, 8))
        plt.contour(theta_0, theta_1, cost_values, levels=50, cmap='viridis')
        plt.xlabel('θ_0 (Bias)')
        plt.ylabel('θ_1 (Weight)')
        plt.title('Contours of Error Function J(θ)')

        # Plot parameter progression during gradient descent
        for i, theta in enumerate(params_history):
            theta_0_val, theta_1_val = theta
            prediction = features_with_bias.dot(theta)
            error = prediction - targets
            cost = (1 / (2 * m)) * np.sum(error ** 2)
            
            plt.scatter(theta_0_val, theta_1_val, color='red', s=50, label='Iteration {}'.format(i) if i == 0 else "")
            
            # Add a delay to visualize each iteration
            plt.pause(0.2)
        
        plt.legend()
        plt.show()

    contour(features, targets, params_history)


    def celr(features, targets, learning_rates):
        for rate in learning_rates:
            model = LinearRegressor()
            params_history = model.fit(features, targets, learning_rate=rate)
            contour(features, targets, params_history)
            plt.title(f'Learning Rate: {rate}')
            plt.show()

    learning_rates = [0.1]
    celr(features, targets, learning_rates)




import numpy as np
import sampling_sgd as sgd
import time
from mpl_toolkits.mplot3d import Axes3D

# Analytical solution for linear regression
<A NAME="1"></A><FONT color = #00FF00><A HREF="match224-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def analytical_solution(A, b):
    """
    Compute the analytical solution for linear regression.
    
    Parameters
    ----------
    A : numpy array of shape (n_samples, n_features+1)
</FONT>        The input data.
        
    b : numpy array of shape (n_samples,)
        The target values.
        
    Returns
    -------
    beta : numpy array of shape (n_features+1,)
        The parameters of the linear regression model.
    """
    A_new = np.c_[np.ones(len(A)), A]
    beta = np.linalg.inv(A_new.T.dot(A_new)).dot(A_new.T).dot(b)
    return beta


# Calculate mean squared error
def calculate_mse(actual, predicted):
    """
    Compute the mean squared error between the actual and predicted target values.
    
    Parameters
    ----------
    actual : numpy array of shape (n_samples,)
        The actual target values.
        
    predicted : numpy array of shape (n_samples,)
        The predicted target values.
        
    Returns
    -------
    mse : float
        The mean squared error between the actual and predicted target values.
    """
    mse = np.mean((actual - predicted)**2)
    return mse

X,y = sgd.generate(1000000,[3,1,2],[3,-1],[4,4],np.sqrt(2))
X_train, y_train = X[:800000], y[:800000]
X_test, y_test = X[800000:], y[800000:]

# Fit the linear regression model using SGD
sgd_model = sgd.StochasticLinearRegressor()
sgd_model.fit(X_train, y_train, learning_rate=0.001)
print("SGD Model Training Completed")
print("Training Times: ", sgd_model.training_times, " for batch sizes: ", sgd_model.batch_size)
# Predict using analytical solution
print(sgd_model.params)
beta = analytical_solution(X_train, y_train)
y_pred_beta = np.dot(np.column_stack((np.ones(X_test.shape[0]), X_test)), beta)
mse_test_beta = calculate_mse(y_test, y_pred_beta)
print(f"Mean Squared Error using analytical solution on testing data: {mse_test_beta}")
y_pred_beta = np.dot(np.column_stack((np.ones(X_train.shape[0]), X_train)), beta)
print(f"Mean Squared Error using analytical solution on training data: {calculate_mse(y_train, y_pred_beta)}")

# Predict using SGD
y_pred_sgd = sgd_model.predict(X_test)
for i in range(len(y_pred_sgd)):
    print(f"Mean Squared Error using SGD on testing data: {calculate_mse(y_pred_sgd[i], y_test)} for Batch Size: {sgd_model.batch_size[i]}")
y_pred_sgd = sgd_model.predict(X_train)
for i in range(len(y_pred_sgd)):
    print(f"Mean Squared Error using SGD on training data: {calculate_mse(y_pred_sgd[i], y_train)} for Batch Size: {sgd_model.batch_size[i]}")


# plot for movements
import matplotlib.pyplot as plt

# Plot movement of theta
theta_history = np.array(sgd_model.history[0])

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot movement of theta
ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label='Movement of Theta for batch size -80 ')

# Labels
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')

plt.legend()
plt.show()


# Plot movement of theta
theta_history = np.array(sgd_model.history[1])

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot movement of theta
ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label='Movement of Theta for batch size-8000')

# Labels
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')

plt.legend()
plt.show()



# Plot movement of theta
theta_history = np.array(sgd_model.history[2])

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot movement of theta
ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label='Movement of Theta for batch size-800000')

# Labels
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')

plt.legend()
plt.show()







import numpy as np
import sys
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor  # Ensure this module is correctly implemented

# ========== ARGUMENT CHECK ==========
if len(sys.argv) != 3:
    print("Usage: python plot_data3.py &lt;path_to_X&gt; &lt;path_to_y&gt;")
    sys.exit(1)

data_path_X = sys.argv[1]
data_path_y = sys.argv[2]

# ========== LOAD DATA ==========
data_X = np.loadtxt(data_path_X, delimiter=',')
data_y = np.loadtxt(data_path_y, delimiter=',')

model = LogisticRegressor()
final_theta = model.fit(data_X, data_y)[-1]  # Extract the final parameters

# Normalize data_X (feature scaling)
mean_X = np.mean(data_X, axis=0)
std_X = np.std(data_X, axis=0)
data_X = (data_X - mean_X) / std_X  # Standardize the data

# Add intercept term (bias column of ones)
data_X = np.hstack((np.ones((data_X.shape[0], 1)), data_X))


print("Learned theta: ", final_theta)

# ========== PLOT DECISION BOUNDARY ==========
def plot_decision_boundary(data_X, data_y, final_theta):
    """
    Plots the dataset and the learned decision boundary.
    """
    plt.figure(figsize=(8, 6))

    # Plot data points
    plt.scatter(data_X[data_y == 0, 1], data_X[data_y == 0, 2], label='Class 0', marker='o', color='blue')
    plt.scatter(data_X[data_y == 1, 1], data_X[data_y == 1, 2], label='Class 1', marker='x', color='red')

    # Decision boundary formula: θ₀ + θ₁x₁ + θ₂x₂ = 0  ⟹  x₂ = -(θ₀ + θ₁x₁) / θ₂
    x_values = np.linspace(min(data_X[:, 1]), max(data_X[:, 1]), 100)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match224-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y_values = -(final_theta[0] + final_theta[1] * x_values) / final_theta[2]

    plt.plot(x_values, y_values, label='Decision Boundary', color='green')

    plt.xlabel('Feature 1 (Normalized)')
    plt.ylabel('Feature 2 (Normalized)')
    plt.legend()
    plt.title('Logistic Regression Decision Boundary')
    plt.grid()
    plt.show()
</FONT>
# Call function to plot
plot_decision_boundary(data_X, data_y, final_theta)




from gda import GaussianDiscriminantAnalysis as GDA
import sys
import matplotlib.pyplot as plt
import numpy as np


# Load data from command line arguments
data_path_X = sys.argv[1]
data_path_y = sys.argv[2]

X_data = np.loadtxt(data_path_X)
y_data = np.loadtxt(data_path_y, dtype=str)

# Convert labels to binary
y_data = np.where(y_data == 'Alaska', 0, 1)

# Split data into training and testing sets
X_train, y_train = X_data[:int(0.8 * len(X_data)), :], y_data[:int(0.8 * len(y_data))]
X_test, y_test = X_data[int(0.8 * len(X_data)):, :], y_data[int(0.8 * len(y_data)):]

# Linear boundary ****************************************************
gda_model_linear = GDA()
mean_0, mean_1, cov_shared = gda_model_linear.fit(X_train, y_train, assume_same_covariance=True)

# Calculate w and b
cov_shared_inv = np.linalg.inv(cov_shared)
w_linear = cov_shared_inv @ (mean_1 - mean_0)
b_linear = 0.5 * (mean_0.T @ cov_shared_inv @ mean_0 - mean_1.T @ cov_shared_inv @ mean_1) + np.log((1 - gda_model_linear.phi) / gda_model_linear.phi)

# Generate decision boundary line
x_vals_linear = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)
x_vals_norm_linear = (x_vals_linear - gda_model_linear.mean_X[0]) / gda_model_linear.std_X[0]
y_vals_norm_linear = - (w_linear[0] / w_linear[1]) * x_vals_norm_linear - (b_linear / w_linear[1])
y_vals_linear = y_vals_norm_linear * gda_model_linear.std_X[1] + gda_model_linear.mean_X[1]

# Quadratic boundary ****************************************************
gda_model_quad = GDA()
mean_0, mean_1, cov_0, cov_1 = gda_model_quad.fit(X_train, y_train)

# Compute inverse covariance matrices
cov_0_inv = np.linalg.inv(cov_0)
cov_1_inv = np.linalg.inv(cov_1)

# Compute the quadratic coefficients A, B, and C in normalized space
A_quad = 0.5 * (cov_0_inv - cov_1_inv)
B_quad = (cov_1_inv @ mean_1) - (cov_0_inv @ mean_0)
C_quad = (0.5 * (mean_0.T @ cov_0_inv @ mean_0 - mean_1.T @ cov_1_inv @ mean_1)
          + np.log(np.linalg.det(cov_0) / np.linalg.det(cov_1))
          + np.log((1 - gda_model_quad.phi) / gda_model_quad.phi))

# Generate mesh grid in original scale
x1_vals_quad = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)
x2_vals_quad = np.linspace(np.min(X_train[:, 1]), np.max(X_train[:, 1]), 100)
X1_quad, X2_quad = np.meshgrid(x1_vals_quad, x2_vals_quad)

# Normalize mesh grid
X1_norm_quad = (X1_quad - gda_model_quad.mean_X[0]) / gda_model_quad.std_X[0]
X2_norm_quad = (X2_quad - gda_model_quad.mean_X[1]) / gda_model_quad.std_X[1]

# Compute quadratic decision boundary in normalized space
Z_quad = (A_quad[0, 0] * X1_norm_quad**2 + 2 * A_quad[0, 1] * X1_norm_quad * X2_norm_quad + A_quad[1, 1] * X2_norm_quad**2
          + B_quad[0] * X1_norm_quad + B_quad[1] * X2_norm_quad + C_quad)

# Extract data points for Alaska and Canada
Alaska = X_data[y_data == 0]
Canada = X_data[y_data == 1]

# Plot the training data
plt.figure(figsize=(8, 6))

# Plot Alaska data (use circles) and Canada data (use crosses)
plt.scatter(Alaska[:, 0], Alaska[:, 1], marker='o', color='blue', label='Alaska')
plt.scatter(Canada[:, 0], Canada[:, 1], marker='x', color='brown', label='Canada')

# Plot linear decision boundary
plt.plot(x_vals_linear, y_vals_linear, color='green', linestyle='--')
linear_label = plt.Line2D([0], [0], color='green', linestyle='--', label="Linear Decision Boundary")
# Plot Quadratic decision boundary
quad_contour = plt.contour(X1_quad, X2_quad, Z_quad, levels=[0], colors='red', linestyles='solid')
# Manually add a proxy artist for the quadratic boundary
quadratic_label = plt.Line2D([0], [0], color='red', linestyle='solid', label="Quadratic Decision Boundary")

# Add labels and legend
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Training Data - Gaussian Discriminant Analysis")
# Combine all legend handles into a single legend
plt.legend(handles=[quadratic_label, linear_label,
                    plt.Line2D([0], [0], marker='o', linestyle='None', color='blue', markersize=8, label="Alaska"),
                    plt.Line2D([0], [0], marker='x', linestyle='None', color='brown', markersize=8, label="Canada")],
           loc='upper left')

plt.grid(True)

plt.show()

# Predict and calculate accuracy
y_pred_linear = gda_model_linear.predict(X_test)
accuracy_linear = np.mean(y_pred_linear == y_test) * 100
print(f"Accuracy: {accuracy_linear:.2f}%")




<A NAME="3"></A><FONT color = #00FFFF><A HREF="match224-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class LinearRegressor:
    def __init__(self, convergence_threshold=1e-6):
</FONT>        self.params = None  # Initialize as None until fit() is called
        self.convergence = False
        self.convergence_threshold = convergence_threshold
        self.history = []  # Store parameters for visualization
        self.max_iters = 1000

    def compute_gradient(self, X, y):
        """
        Compute the gradient for batch gradient descent.
        """
        num_samples, num_features = X.shape
        predictions = X @ self.params
        errors = predictions - y
        
        gradient = (X.T @ errors) / num_samples
        
        return gradient

    def check_convergence(self, gradient, learning_rate):
        """
        Check if the gradient descent should stop (convergence condition).
        """
        net_change = np.linalg.norm(learning_rate * gradient)
        if net_change &lt; self.convergence_threshold:
            self.convergence = True

    def fit(self, X, y, learning_rate=0.05):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        num_samples = len(X)
        num_features = len(X[0])
        X_bias = np.c_[np.ones(num_samples), X]  # Add bias term first
        self.params = np.zeros(num_features + 1)  # Initialize parameters (bias + weights)
        iteration = 0
        
        while not self.convergence and iteration &lt; self.max_iters:
            gradient = self.compute_gradient(X_bias, y)
            self.params -= learning_rate * gradient  # Update parameters
            self.history.append(self.params.copy())  # Store history for visualization
            self.check_convergence(gradient, learning_rate)  # Check convergence
            iteration += 1
        
        return np.array(self.history)

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_bias = np.c_[np.ones(X.shape[0]), X]  # Add bias term first
        return np.array(X_bias @ self.params)




import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import time

# ========== DATA GENERATION FUNCTION ==========
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    """
    np.random.seed(42)
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    X_bias = np.column_stack((np.ones(N), X))  # Add bias term
    y = np.dot(X_bias, theta) + np.random.normal(0, noise_sigma, N)
    return X, y


class StochasticLinearRegressor:
    def __init__(self, window_size=10):
        """
        Initialize the regressor with multiple batch sizes and moving average tracking.
        """
        self.batch_size = [80]
        self.max_iter = [500,500,3000]
        self.history = []
        self.params = []
        self.convergence_threshold = 1e-4
        self.window_size = window_size  # Moving average window
        self.training_times = []
        self.loss_history = deque(maxlen=window_size)
    
    def gradient(self, X_batch, y_batch, params, batch_size):
        h_theta = np.dot(X_batch, params)
        # Error
        error = h_theta - y_batch
        gradient = np.dot(X_batch.T, error) / batch_size
        return gradient

    def param_update(self, params, gradient, learning_rate):
        return params - learning_rate * gradient

    def compute_loss(self, X, y, params):
        h_theta = np.dot(X, params)
        error = h_theta - y
        loss = np.mean(error ** 2) / 2
        return loss

    def convergence_function(self):
        if len(self.loss_history) == self.window_size:
            moving_average_loss = np.mean(self.loss_history)
            if abs(self.loss_history[-1] - moving_average_loss) &lt; self.convergence_threshold:
                self.convergence = True

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match224-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model using SGD with different batch sizes.
        """
        num_samples, num_features = X.shape
</FONT>        X = np.column_stack((np.ones(num_samples), X))  # Add bias term
        self.params = [np.zeros(num_features + 1) for _ in range(len(self.batch_size))]

        for i in range(len(self.batch_size)):
            history = [np.zeros(num_features + 1)]
            self.convergence = False
            params = np.zeros(num_features + 1)
            iteration = 0
            t = time.time()
            print(f"Training with batch size: {self.batch_size[i]}")
            while not self.convergence and iteration &lt; self.max_iter[i]:
                random_ness = np.random.permutation(num_samples)
                X, y = X[random_ness], y[random_ness]
                for j in range(0, num_samples, self.batch_size[i]):
                    X_batch = X[j:j+self.batch_size[i]]
                    y_batch = y[j:j+self.batch_size[i]]

                    # Compute the gradient for the current batch
                    gradient = self.gradient(X_batch, y_batch, params, self.batch_size[i])

                    # Update parameters
                    params = self.param_update(params, gradient, learning_rate)
                
                # Compute and store the loss
                loss = self.compute_loss(X, y, params)
                self.loss_history.append(loss)
                self.convergence_function()
                
                history.append(params.copy())
                iteration += 1
                print(f"Iteration {iteration}, Loss: {loss}, Params: {params}")
            self.history.append(np.array(history))
            self.params[i] = params
            self.training_times.append(time.time() - t)
            print(f"Batch Size: {self.batch_size[i]} converged in {iteration} iterations.")
        
        return self.history

    def predict(self, X):
        """
        Predict target values for new input data.
        """
        X = np.column_stack((np.ones(X.shape[0]), X))
        return [np.dot(X, self.params[i]) for i in range(len(self.batch_size))]




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import sys
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.threshold = 1e-6
        self.max_iter = 10000
        self.params = None
        self.convergence = False
    
    def sigmoid(self, z):
        """
        Compute the sigmoid function.
        
        Parameters
        ----------
        z : numpy array of shape (n_samples,)
            The input to the sigmoid function.
        
        Returns
        -------
        sigmoid : numpy array of shape (n_samples,)
            The output of the sigmoid function.
        """

        return 1/(1+np.exp(-z))
    
    def normalize(self, X):
        """
        Normalize the input data X.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        
        Returns
        -------
        X_normalized : numpy array of shape (n_samples, n_features)
            The normalized input data.
        """
        
        return (X - np.mean(X, axis=0))/np.std(X, axis=0)

    def hessian(self, X, theta):
        """
        Compute the Hessian matrix.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        
        theta : numpy array of shape (n_features,)
            The parameters of the model.
        
        Returns
        -------
        hessian : numpy array of shape (n_features, n_features)
            The Hessian matrix.
        """
        
        h = self.sigmoid(np.dot(X, theta))
        D = np.diag(h*(1-h))
        return np.dot(np.dot(X.T, D), X)

    def gradient(self, X, y, theta):
        """
        Compute the gradient.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match224-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        theta : numpy array of shape (n_features,)
            The parameters of the model.
</FONT>        
        Returns
        -------
        gradient : numpy array of shape (n_features,)
            The gradient.
        """
        
        return np.dot(X.T, self.sigmoid(np.dot(X, theta)) - y)

    def update_theta(self, X, y, theta, learning_rate):
        """
        Update the parameters theta using Newton's Method.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        theta : numpy array of shape (n_features,)
            The parameters of the model.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        theta : numpy array of shape (n_features,)
            The updated parameters.
        """
        
        h = self.sigmoid(np.dot(X, theta))
        H_inv = np.linalg.inv(self.hessian(X, theta))
        grad = self.gradient(X, y, theta)
        return theta - learning_rate*np.dot(H_inv, grad)

    
    def check_convergence(self, theta, new_theta):
        """
        Check for convergence by comparing the change in parameters.
        
        Parameters
        ----------
        theta : numpy array of shape (n_features,)
            The parameters of the model.
            
        new_theta : numpy array of shape (n_features,)
            The updated parameters of the model.
        
        Returns
        -------
        convergence : bool
            True if the change in parameters is less than the threshold, False otherwise.
        """
        
        net_change = np.sqrt(sum((theta[i] - new_theta[i]) ** 2 for i in range(len(theta))))
        if net_change &lt; self.threshold:
            self.convergence = True
        

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        X = self.normalize(X)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        theta = np.zeros(X.shape[1])
        self.theta = theta
        params = [theta]
        iteration = 0
        while not self.convergence and iteration &lt; self.max_iter:
            new_theta = self.update_theta(X, y, self.theta, learning_rate)
            self.check_convergence(self.theta, new_theta)
            self.theta = new_theta
            params.append(new_theta)
            iteration += 1
        return params
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        y_pred = []
        X = self.normalize(X)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        for x in X:
            y_pred.append(1 if self.sigmoid(np.dot(x, self.theta)) &gt; 0.5 else 0)
        return np.array(y_pred)




import numpy as np

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.phi = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.mean_X = None
        self.std_X = None

    def normalize(self, X):
        """Normalize the data using stored mean and standard deviation."""
        return (X - self.mean_X) / self.std_X

    def fit(self, X, y, assume_same_covariance=False):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match224-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        """
        Fit the Gaussian Discriminant Analysis model to the data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
</FONT>            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels (0 or 1).
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays (mu_0, mu_1, sigma) 
            If assume_same_covariance = False - 4-tuple of numpy arrays (mu_0, mu_1, sigma_0, sigma_1)
        """
        X = np.array(X)
        y = np.array(y)

        # Compute mean and std for normalization (store for later use)
        self.mean_X = np.mean(X, axis=0)
        self.std_X = np.std(X, axis=0)
        
        # Normalize X internally
        X = self.normalize(X)

        # Compute class priors and means
        self.phi = np.mean(y)
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)

        if assume_same_covariance:
            mean_matrix = X - np.where(y[:, np.newaxis] == 0, self.mu_0, self.mu_1)
            self.sigma_0 = np.dot(mean_matrix.T, mean_matrix) / len(X)
            self.sigma_1 = self.sigma_0
            return self.mu_0, self.mu_1, self.sigma_0
        else:
            X_0 = X[y == 0]
            X_1 = X[y == 1]
            self.sigma_0 = np.dot((X_0 - self.mu_0).T, (X_0 - self.mu_0)) / len(X_0)
            self.sigma_1 = np.dot((X_1 - self.mu_1).T, (X_1 - self.mu_1)) / len(X_1)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def gaussian(self, x, mu, sigma):
        """Compute the Gaussian density function."""
        size = len(mu)
        if np.linalg.det(sigma) == 0:
            sigma += np.eye(size) * 1e-6  # Regularization to avoid singular matrix
        norm_const = 1.0 / (np.power((2 * np.pi), size / 2) * np.sqrt(np.linalg.det(sigma)))
        x_mu = x - mu
        return norm_const * np.exp(-0.5 * np.dot(np.dot(x_mu.T, np.linalg.inv(sigma)), x_mu))

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target labels.
        """
        X = self.normalize(X)  # Normalize input using stored mean/std
        p_0 = np.array([self.gaussian(x, self.mu_0, self.sigma_0) * (1 - self.phi) for x in X])
        p_1 = np.array([self.gaussian(x, self.mu_1, self.sigma_1) * self.phi for x in X])
        return (p_1 &gt; p_0).astype(int)


</PRE>
</PRE>
</BODY>
</HTML>
