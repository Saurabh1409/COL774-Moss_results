<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_2EL0I.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_R0S9Q.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.weights=None
    
    def loss(self, y_pred, y, n_samples):
        loss=(1/(2*n_samples))*np.sum((y_pred-y)**2)
        return loss

    def train(self, X, y, lrate, n_iter, eps):
        n_samples, n_features = X.shape if X.ndim == 2 else (X.shape[0], 1)
        X=np.c_[np.ones(n_samples),X] # Concatenate column of ones to X
        Xt=X.T
        # Shape of X: (n_samples, n_features+1)

        self.W=np.zeros(n_features+1) #Initializing weights to 0
        param_list=[]
        prev_loss = float('inf')

        # loss J(θ)=(1/2n) * Σ(hθ(x(i)) - y(i))^2 
        for i in range(n_iter):
            y_pred=np.dot(X,self.W)
            loss=self.loss(y_pred,y,n_samples)

            if abs(loss-prev_loss)&lt;eps:
                # print("iterations: ", i)
                break

            prev_loss=loss
            gradient=np.dot(Xt,(y_pred-y))*(1/n_samples)
            
            self.W -=lrate*gradient
            param_list.append(self.W.copy())


        return np.array(param_list)        



    def fit(self, X, y, learning_rate=0.025):
        n_iter=1500
        eps=1e-5
        return self.train(X,y,learning_rate,n_iter,eps)
        
    
    def predict(self, X):
       
        n_samples =X.shape[0]
        X=np.c_[np.ones(n_samples),X]
        return np.dot(X,self.W)



import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor  

X = np.loadtxt("./data/Q1/linearX.csv", delimiter=",")  
y = np.loadtxt("./data/Q1/linearY.csv", delimiter=",")  

model = LinearRegressor()
params = model.fit(X, y, 0.025)
n=X.shape[0]

y_pred = model.predict(X)

print("Final Weights:", params[-1])
print("Number of Iterations: ", len(params))
print("Final Loss: ", model.loss(y_pred, y, n))
losses=[]
X_new = np.c_[np.ones(X.shape[0]), X]
for w in params:
    loss = model.loss(np.dot(X_new,w),y,n)
    losses.append(loss)

plt.plot(range(len(losses)), losses)
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title("Loss Curve (Training)")
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor  

X = np.loadtxt('./data/Q1/linearX.csv', delimiter=',')
y = np.loadtxt('./data/Q1/linearY.csv', delimiter=',')

model = LinearRegressor()
model.fit(X, y, learning_rate=0.025)

plt.scatter(X, y, color='blue', label='Data Points',s=10) 
y_pred = model.predict(X)
plt.plot(X, y_pred, color='red', label='Hypothesis')
# plt.xlim(-0.1, 0.1)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor 

X = np.loadtxt('./data/Q1/linearX.csv', delimiter=',')
y = np.loadtxt('./data/Q1/linearY.csv', delimiter=',')


model = LinearRegressor()
params_list = model.fit(X, y, learning_rate=0.025)

theta0_vals = np.linspace(-20, 32, 100)
theta1_vals = np.linspace(-15,70, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

n = X.shape[0]
X = np.c_[np.ones(n), X]

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        theta = np.array([theta0, theta1])
        y_pred = np.dot(X,theta)
        J_vals[i, j] = model.loss(y_pred,y,n)


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
<A NAME="7"></A><FONT color = #0000FF><A HREF="match14-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

surf = ax.plot_surface(T0, T1, J_vals.T, cmap='coolwarm', edgecolor='none', alpha=0.5)
ax.set_xlabel('Theta0 (Intercept)')
ax.set_ylabel('Theta1')
ax.set_zlabel('Cost J(θ)')
ax.set_title('3D Mesh of Cost Function')
</FONT>
# plt.pause(10)
for param in params_list:
    theta0, theta1 = param
    pred=np.dot(X,param)
    cost_val = model.loss(pred,y,n)
    
    point = ax.scatter(theta0, theta1, cost_val, color='red', s=15)
    label = ax.text(theta0, theta1, cost_val, f'Error = {cost_val:.4f}', color='black', fontsize=8, ha='left')
    ax.scatter(theta0, theta1, cost_val, color='red', s=2)
    plt.pause(0.2)
    point.remove()
    label.remove()

theta0, theta1 = params_list[-1]
pred=np.dot(X,params_list[-1])
cost_val = model.loss(pred,y,n)

point = ax.scatter(theta0, theta1, cost_val, color='red', s=15)
label = ax.text(theta0, theta1, cost_val, f'Final Error = {cost_val:.4f}', color='black', fontsize=8, ha='left')
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor

lr=0.025
X = np.loadtxt('./data/Q1/linearX.csv', delimiter=',')
y = np.loadtxt('./data/Q1/linearY.csv', delimiter=',')

model = LinearRegressor()
params_list = model.fit(X, y, learning_rate=lr)

theta0_vals = np.linspace(-20, 32, 100)
theta1_vals = np.linspace(-15, 70, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

n = X.shape[0]
X_bias = np.c_[np.ones(n), X]

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        theta = np.array([theta0, theta1])
        J_vals[i, j] = model.loss(np.dot(X_bias, theta), y, n)

levels = np.concatenate((np.arange(0, 100, 10), np.arange(100, np.max(J_vals), 100)))

plt.figure()
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
plt.contourf(T0, T1, J_vals.T, levels=levels, cmap='coolwarm', alpha=0.8)
cp = plt.contour(T0, T1, J_vals.T, levels=levels, colors='black', linewidths=0.5)

label = [0, 10, 20, 40, 60, 80, 100, 200, 300, 400]
plt.clabel(cp, levels=label, inline=True, fontsize=8, fmt='%1.1f')
plt.xlabel('Theta0 (Intercept)')
plt.ylabel('Theta1')
plt.title('Contour Plot of Cost Function')

for param in params_list:
    theta0, theta1 = param
    cost_val = model.loss(np.dot(X_bias, param), y, n)
    marker = plt.plot(theta0, theta1, 'ro', markersize=5)[0]
    label = plt.text(theta0, theta1, f'Error = {cost_val:.4f}', color='black', fontsize=8, ha='left')
    
    plt.plot(theta0, theta1, 'ro', markersize=1)[0]
    plt.draw()
    plt.pause(0.2)
    marker.remove()
    label.remove()

theta0, theta1 = params_list[-1]
plt.plot(theta0, theta1, 'ro', markersize=1)[0]
plt.text(theta0, theta1, f'Final Error = {model.loss(model.predict(X), y, n):.4f}', color='black', fontsize=8, ha='left')   

plt.show()




# run.py
import numpy as np
from sampling_sgd import generate, StochasticLinearRegressor


N = 1_000_000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2]) 
noise_sigma = np.sqrt(2)     

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

<A NAME="0"></A><FONT color = #FF0000><A HREF="match14-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

indices = np.arange(N)
np.random.shuffle(indices)
train_size = int(0.8 * N)
train_indices = indices[:train_size]
test_indices = indices[train_size:]

X_train, y_train = X[train_indices], y[train_indices]
X_test, y_test = X[test_indices], y[test_indices]
</FONT>
np.savetxt('X_train.csv', X_train, delimiter=',')
np.savetxt('y_train.csv', y_train, delimiter=',')
np.savetxt('X_test.csv', X_test, delimiter=',')
np.savetxt('y_test.csv', y_test, delimiter=',')




import numpy as np
from sampling_sgd import StochasticLinearRegressor

X_train = np.loadtxt("X_train.csv", delimiter=",")  
y_train = np.loadtxt("y_train.csv", delimiter=",")
batch_sizes = [1, 80, 8000, 800000]
lr=0.001
n_epochs=[100,100,200,2000]
eps=[1e-3,1e-4,1e-5,1e-5]


for i in range(4):
    print(f"Batch size: {batch_sizes[i]}")
    model = StochasticLinearRegressor()
    _ = model.fit(X_train, y_train, learning_rate=lr, batch_size=batch_sizes[i], n_epochs=n_epochs[i],epsilon=eps[i])
    print(f"Theta: {model.theta}\n")




import numpy as np
from sampling_sgd import generate, StochasticLinearRegressor

X = np.loadtxt("X_train.csv", delimiter=",")  
y = np.loadtxt("y_train.csv", delimiter=",")

n_samples, n_features = X.shape
X = np.c_[(np.ones(n_samples), X)]

theta_closed = np.dot(np.linalg.inv(np.dot(X.T, X)), (np.dot(X.T,y)))

print(theta_closed)



import numpy as np
from sampling_sgd import generate, StochasticLinearRegressor

X_train = np.loadtxt("X_train.csv", delimiter=",")  
y_train = np.loadtxt("y_train.csv", delimiter=",")
X_test = np.loadtxt("X_test.csv", delimiter=",")  
y_test = np.loadtxt("y_test.csv", delimiter=",")

n_train = X_train.shape[0]
n_test = X_test.shape[0]
X_cl_train = np.column_stack((np.ones(n_train), X_train))
X_cl_test = np.column_stack((np.ones(n_test), X_test))

theta_closed = np.dot(np.linalg.inv(np.dot(X_cl_train.T, X_cl_train)), (np.dot(X_cl_train.T,y_train)))
print(f"Closed Form Theta", theta_closed)
y_train_pred = np.dot(X_cl_train, theta_closed)
y_test_pred = np.dot(X_cl_test, theta_closed)

train_error = np.mean((y_train - y_train_pred) ** 2)
test_error = np.mean((y_test - y_test_pred) ** 2)

print("Training error (MSE):", train_error)
print("Test error (MSE):", test_error)
print("\n")
batch_sizes = [1, 80, 8000, 800000]
lr=0.001
n_epochs=[100,100,200,2000]
eps=[1e-3,1e-4,1e-5,1e-5]


for i in range(4):
    print(f"Batch size: {batch_sizes[i]}")
    model = StochasticLinearRegressor()
    _ = model.fit(X_train, y_train, learning_rate=lr, batch_size=batch_sizes[i], n_epochs=n_epochs[i],epsilon=eps[i])
    print(f"Theta: {model.theta}\n")

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_error = np.mean((y_train - y_train_pred) ** 2)
    test_error = np.mean((y_test - y_test_pred) ** 2)
    print("Training error (MSE):", train_error)
    print("Test error (MSE):", test_error)
    print("\n")






import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import generate, StochasticLinearRegressor
import matplotlib as mpl
mpl.rcParams['agg.path.chunksize'] = 10000
X_train = np.loadtxt("X_train.csv", delimiter=",")  
y_train = np.loadtxt("y_train.csv", delimiter=",")


trajectories = {}
batch_sizes = [1, 80, 8000, 800000]
lr=0.001
n_epochs=[100,100,200,2000]
eps=[1e-3,1e-4,1e-5,1e-5]


for i in range(4):
    model = StochasticLinearRegressor()
    params_history = model.fit(X_train, y_train, learning_rate=lr, batch_size=batch_sizes[i], n_epochs=n_epochs[i],epsilon=eps[i])
    trajectories[batch_sizes[i]] = params_history

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

for batch_size in batch_sizes:
    traj = trajectories[batch_size]
    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], label=f'Batch size {batch_size}')
    ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2], marker='o', color='black', s=50)  
    ax.scatter(traj[-1, 0], traj[-1, 1], traj[-1, 2], marker='X', color='red', s=50) 

ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')
ax.set_title('Trajectory of θ updates in 3D Parameter Space')
ax.legend()
plt.show()





import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    np.random.seed(42)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match14-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)
    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)
    
    epsilon = np.random.normal(loc=0, scale=noise_sigma, size=N)
</FONT><A NAME="1"></A><FONT color = #00FF00><A HREF="match14-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + epsilon
    X = np.column_stack((x1, x2))
    
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
</FONT>    
    def loss(self, y_pred, y, n_samples):
        loss=(1/(2*n_samples))*np.sum((y_pred-y)**2)
        return loss
    
    def fit(self, X, y, learning_rate=0.01, n_epochs=10, batch_size=32,epsilon = 1e-5):
        n_samples, n_features = X.shape
        X=np.c_[np.ones(n_samples),X]
        self.theta = np.zeros(n_features + 1)

        num_batches = int(np.ceil(n_samples / batch_size))
        params_history = []

        
        prev_loss = float('inf')

        for epoch in range(n_epochs):
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            loss = self.loss(np.dot(X,self.theta), y, n_samples)   

            if abs(loss-prev_loss)&lt;epsilon:
                # print(epoch)
                break
            prev_loss=loss
            
            
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match14-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            for batch in range(num_batches):
                start = batch * batch_size
                end = min(start + batch_size, n_samples)

                X_batch = X_shuffled[start:end]
                y_batch = y_shuffled[start:end]

                y_pred = np.dot(X_batch,self.theta)
</FONT>                
                m_batch = X_batch.shape[0]
                gradient = - (1 / (m_batch)) * (X_batch.T).dot(y_batch - y_pred)
                
                self.theta = self.theta - learning_rate * gradient
                
                params_history.append(self.theta.copy())
                

        return np.array(params_history)

    
    def predict(self, X):

        n_samples = X.shape[0]
        X = np.c_[(np.ones(n_samples), X)]
        return np.dot(X,self.theta)




import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std = None
    
    def fit(self, X, y, learning_rate=1,max_iter=100):
        """
        Fit the logistic regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        params_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        self.std[self.std == 0] = 1.0
<A NAME="5"></A><FONT color = #FF0000><A HREF="match14-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = (X - self.mean) / self.std
        
        n_samples, n_features = X.shape

        X = np.c_[np.ones((n_samples, 1)), X]
        theta = np.zeros(n_features + 1)
</FONT>        params_history = []

        eps = 1e-7
        
        for i in range(max_iter):
            z = X.dot(theta)
            p = sigmoid(z)
            
            grad = X.T.dot(p - y)
            
            W = np.diag(p * (1 - p))
            H = X.T.dot(W).dot(X)
            
            try:
                delta = np.linalg.solve(H, grad)
            except np.linalg.LinAlgError:
                delta = np.linalg.pinv(H).dot(grad)
            
            theta_new = theta - learning_rate * delta
            params_history.append(theta_new.copy())
            
            if np.linalg.norm(theta_new - theta, ord=2) &lt; eps:
                theta = theta_new
                # print(i)
                break
            
            theta = theta_new
        
        self.theta = theta
        return np.array(params_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match14-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = (X - self.mean) / self.std
        n_samples = X.shape[0]
        X = np.hstack([np.ones((n_samples, 1)), X])
        
        probs = sigmoid(X.dot(self.theta))
</FONT>        return (probs &gt;= 0.5).astype(int)




import numpy as np
from logistic_regression import LogisticRegressor

X = np.loadtxt("./data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("./data/Q3/logisticY.csv", delimiter=",")

model = LogisticRegressor()

history = model.fit(X, y, learning_rate=1,max_iter=10000)

print("Learned Theta:")
print(model.theta)



import numpy as np
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

def plot_decision_boundary(X, y, model):
    label0 = y == 0
    label1 = y == 1
    
    plt.figure(figsize=(8, 6))
    plt.scatter(X[label0, 0], X[label0, 1], marker='o', color='red', label='Class 0')
    plt.scatter(X[label1, 0], X[label1, 1], marker='x', color='blue', label='Class 1')
    
    theta = model.theta      
    mu = model.mean          
    sigma = model.std    
    
    x1_min, x1_max = X[:, 0].min(), X[:, 0].max()
    x1_range = np.linspace(x1_min, x1_max, 100)
    
    if abs(theta[2]) &gt; 1e-6: 
        x2_range = mu[1] - (sigma[1]/theta[2]) * (theta[0] + theta[1]*((x1_range - mu[0]) / sigma[0]))
        plt.plot(x1_range, x2_range, 'g-', linewidth=2, label='Decision Boundary')
    else:
        x1_boundary = mu[0] - (theta[0]*sigma[0])/theta[1]
        plt.axvline(x=x1_boundary, color='g', linewidth=2, label='Decision Boundary')
    
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Training Data and Logistic Regression Decision Boundary')
    plt.legend()
    plt.show()

X = np.loadtxt("./data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("./data/Q3/logisticY.csv", delimiter=",")

model = LogisticRegressor()
model.fit(X, y, learning_rate=0.01)

plot_decision_boundary(X, y, model)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mean = None
        self.std = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X_norm = (X - self.mean) / self.std

        self.mu_0 = np.mean(X_norm[y == 0], axis=0)
        self.mu_1 = np.mean(X_norm[y == 1], axis=0)

        if assume_same_covariance:
            m = X_norm.shape[0]
            self.sigma = np.zeros((X_norm.shape[1], X_norm.shape[1]))
            for i in range(m):
                x_i = X_norm[i].reshape(-1, 1)
                mu = self.mu_1.reshape(-1, 1) if y[i] == 1 else self.mu_0.reshape(-1, 1)
                self.sigma += (x_i - mu) @ (x_i - mu).T
            self.sigma /= m
            return (self.mu_0, self.mu_1, self.sigma)
        
        else:
            X_0 = X_norm[y == 0]
            X_1 = X_norm[y == 1]
            self.sigma_0 = np.cov(X_0, rowvar=False,bias=True)
            self.sigma_1 = np.cov(X_1, rowvar=False,bias=True)
            
<A NAME="6"></A><FONT color = #00FF00><A HREF="match14-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)

        
    
    def predict(self, X):
        X_norm = (X - self.mean) / self.std
</FONT>        
        if self.sigma is not None:  
            inv_sigma = np.linalg.inv(self.sigma)
            discriminant_score_0 = -0.5 * np.sum((X_norm - self.mu_0) @ inv_sigma * (X_norm - self.mu_0), axis=1)
            discriminant_score_1 = -0.5 * np.sum((X_norm - self.mu_1) @ inv_sigma * (X_norm - self.mu_1), axis=1)
        
        else: 
            discriminant_score_0 = -0.5 * np.sum((X_norm - self.mu_0) @ np.linalg.inv(self.sigma_0) * (X_norm - self.mu_0), axis=1)
            discriminant_score_1 = -0.5 * np.sum((X_norm - self.mu_1) @ np.linalg.inv(self.sigma_1) * (X_norm - self.mu_1), axis=1)

        y_pred = np.where(discriminant_score_0 &gt; discriminant_score_1, 0, 1)
        
        return y_pred



import numpy as np
from gda import GaussianDiscriminantAnalysis

X = np.loadtxt('./data/Q4/q4x.dat')
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match14-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

with open('./data/Q4/q4y.dat', 'r') as f:
    y = np.array([1 if line.strip() == 'Canada' else 0 for line in f])

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, cov_matrix = gda.fit(X, y, assume_same_covariance=True)
</FONT>
print("Normalized Scale:")
print("mu_0 (Alaska):", mu_0)
print("mu_1 (Canada):", mu_1)
print("Shared Covariance Matrix Σ:")    
print(cov_matrix)
mu_0_orig = mu_0 * gda.std + gda.mean
mu_1_orig = mu_1 * gda.std + gda.mean
cov_matrix_orig = cov_matrix * np.outer(gda.std, gda.std)

print("Original Scale:")
print("mu_0 (Alaska):", mu_0_orig)
print("mu_1 (Canada):", mu_1_orig)
print("Shared Covariance Matrix Σ:")
print(cov_matrix_orig)




import numpy as np
import matplotlib.pyplot as plt

X = np.loadtxt('./data/Q4/q4x.dat')
y = np.loadtxt('./data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada')
plt.xlabel('Feature 1 (Freshwater Growth)')
plt.ylabel('Feature 2 (Marine Growth)')
plt.title('Salmon Data from Alaska and Canada (Original)')
plt.legend()
plt.grid(True)
plt.show()


mean = np.mean(X, axis=0)
std = np.std(X, axis=0)
X_norm = (X - mean) / std

plt.figure(figsize=(8, 6))
plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', label='Alaska')
plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', label='Canada')
plt.xlabel('Feature 1 (Freshwater Growth)')
plt.ylabel('Feature 2 (Marine Growth)')
plt.title('Salmon Data from Alaska and Canada (Normalized)')
plt.legend()
plt.grid(True)
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

X = np.loadtxt('./data/Q4/q4x.dat')
y = np.loadtxt('./data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

gda = GaussianDiscriminantAnalysis()
gda.fit(X, y, assume_same_covariance=True)
mu_0 = gda.mu_0
mu_1 = gda.mu_1
cov_matrix = gda.sigma

mu_0_orig = gda.mean + gda.std * mu_0
mu_1_orig = gda.mean + gda.std * mu_1

scaling = np.diag(gda.std)
cov_matrix_orig = scaling @ cov_matrix @ scaling

inv_cov = np.linalg.inv(cov_matrix_orig)
w = np.dot(inv_cov, (mu_1_orig - mu_0_orig))
phi = np.mean(y)
intercept = -0.5 * np.dot(mu_1_orig.T, np.dot(inv_cov, mu_1_orig)) + 0.5 * np.dot(mu_0_orig.T, np.dot(inv_cov, mu_0_orig)) - np.log((1 - phi) / phi)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='green', label='Canada')

x_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
y_vals = -(w[0] * x_vals + intercept) / w[1]
plt.plot(x_vals, y_vals, 'r-', label='Decision Boundary')

plt.xlabel('Freshwater Growth')
plt.ylabel('Marine Growth')
plt.title('GDA Decision Boundary')
plt.legend()
plt.grid(True)
plt.show()




import numpy as np
from gda import GaussianDiscriminantAnalysis

X = np.loadtxt('./data/Q4/q4x.dat')
y = np.loadtxt('./data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y)

print("Normalized Scale:")
print("mu_0 (Alaska):", mu_0)
print("mu_1 (Canada):", mu_1)
print("Sigma_0:\n", sigma_0)
print("Sigma_1:\n", sigma_1)

# Convert back to original scale
mu_0_orig = mu_0 * gda.std + gda.mean
mu_1_orig = mu_1 * gda.std + gda.mean

print("\nOriginal Scale:")
print("mu_0 (Alaska):", mu_0_orig)
print("mu_1 (Canada):", mu_1_orig)
print("Sigma_0:\n", sigma_0 * np.outer(gda.std, gda.std))
print("Sigma_1:\n", sigma_1 * np.outer(gda.std, gda.std))




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

X = np.loadtxt('./data/Q4/q4x.dat')
y = np.loadtxt('./data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y)

mu_0_orig = mu_0 * gda.std + gda.mean
mu_1_orig = mu_1 * gda.std + gda.mean
sigma_0_orig = (gda.std[:, None] * sigma_0 * gda.std)
sigma_1_orig = (gda.std[:, None] * sigma_1 * gda.std)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada')

inv_cov = np.linalg.inv((sigma_0_orig + sigma_1_orig) / 2)
w = np.dot(inv_cov, (mu_1_orig - mu_0_orig))
intercept = -0.5 * np.dot(mu_1_orig.T, np.dot(inv_cov, mu_1_orig)) + 0.5 * np.dot(mu_0_orig.T, np.dot(inv_cov, mu_0_orig))
x_vals = np.linspace(X[:, 0].min()-2, X[:, 0].max()+2, 200)
y_vals = -(w[0] * x_vals + intercept) / w[1]
plt.plot(x_vals, y_vals, 'b--', label='Linear Decision Boundary')

x, y_grid = np.meshgrid(np.linspace(X[:, 0].min()-2, X[:, 0].max()+2, 300), np.linspace(X[:, 1].min()-2, X[:, 1].max()+2, 300))
z = np.zeros(x.shape)
for i in range(x.shape[0]):
    for j in range(x.shape[1]):
        diff_0 = np.array([x[i, j], y_grid[i, j]]) - mu_0_orig
        diff_1 = np.array([x[i, j], y_grid[i, j]]) - mu_1_orig
        z[i, j] = diff_0.T @ np.linalg.inv(sigma_0_orig) @ diff_0 - diff_1.T @ np.linalg.inv(sigma_1_orig) @ diff_1

contour = plt.contour(x, y_grid, z, levels=[0], colors='red', linewidths=2, linestyles='-')
# contour.collections[0].set_label('Quadratic Decision Boundary')
plt.xlabel('Freshwater Growth')
plt.ylabel('Marine Growth')
plt.title('GDA Decision Boundaries (Linear and Quadratic)')
plt.legend()
plt.grid(True)
plt.show()




./data/Q4

</PRE>
</PRE>
</BODY>
</HTML>
