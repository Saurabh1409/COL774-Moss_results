<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_MY1FP.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_MY1FP.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LinearRegressor:
    def __init__(self):
        self.learning_rate = 1
        self.tolerance = 1e-6
        self.max_epochs = 500
        self.losses = []
        self.parameter_hist = []

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        self.parameter_hist = []
        print(f"Learning rate: {learning_rate}")
        print(f"Stopping criteria: Change in loss &lt; {self.tolerance}")

        # Gradient descent
        for epoch in range(self.max_epochs):
            # Predict
            y_predicted = np.dot(X, self.weights) + self.bias

            dw = -(1 / n_samples) * np.dot(X.T, (y - y_predicted))
            db = -(1 / n_samples) * np.sum(y - y_predicted)

            self.losses.append(np.mean(np.square(y - y_predicted)))

            self.weights -= learning_rate * dw
            self.bias -= learning_rate * db

            self.parameter_hist.append(
                np.concatenate((np.array([self.bias]), self.weights), axis=0)
            )

            if epoch &gt; 0 and abs(self.losses[-1] - self.losses[-2]) &lt; self.tolerance:
                break

        print(f"Training stopped at epoch {epoch+1} with loss {self.losses[-1]:.6f}")
        print(f"Final parameters: Bias={self.bias:.6f}, Weights={self.weights}")
        self.parameter_hist = np.array(self.parameter_hist)
        return self.parameter_hist

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match245-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.dot(X, self.weights) + self.bias
</FONT>



import numpy as np
from linear_regression import LinearRegressor
import matplotlib.pyplot as plt


def compute_cost(X, y, theta_0, theta_1):
    predictions = X[:, 0] * theta_1 + theta_0
    return np.mean(np.square(predictions - y))


def generate_graph_prereqs(X, y, parameter_hist):
    biases = [params[0] for params in parameter_hist]
    weights = [params[1] for params in parameter_hist]
    theta_0_vals = np.linspace(min(biases), max(biases), 1000)
    theta_1_vals = np.linspace(min(weights), max(weights), 1000)
    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

    for i in range(len(theta_0_vals)):
        for j in range(len(theta_1_vals)):
            J_vals[i, j] = compute_cost(X, y, theta_0_vals[i], theta_1_vals[j])

    return J_vals, theta_0_vals, theta_1_vals


# Q2
def plot_scatter_with_fit(X, Y):
    lr = LinearRegressor()
    weights = lr.fit(X, Y, learning_rate=1)
    b, w = weights[-1]

    plt.figure(figsize=(8, 6))
    plt.scatter(X, Y, label="Scatter Plot")

    x_line = np.linspace(X.min(), X.max(), 1000)
    y_line = w * x_line + b

    plt.plot(x_line, y_line, color="red", linewidth=2, label="y = w*x + b")

    plt.xlabel("x values")
    plt.ylabel("y values")
    plt.title("Scatter Plot with Linear Fit")
    plt.legend()
    plt.grid(True)

    plt.savefig("Scatter Plot with Linear Fit")


def plot_cost_surface(X, y, parameter_hist):
    J_vals, theta_0_vals, theta_1_vals = generate_graph_prereqs(X, y, parameter_hist)

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection="3d")

    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
    ax.plot_surface(theta_0_vals, theta_1_vals, J_vals, cmap="coolwarm", alpha=0.7)

    for params in parameter_hist:
        theta_0, theta_1 = params[0], params[1]
        J_theta = compute_cost(X, y, theta_0, theta_1)
        ax.scatter(theta_0, theta_1, J_theta, c="black", marker="o", s=50)
        plt.pause(0.2)

    plt.xlabel("Bias (Theta_0)")
    plt.ylabel("Weight (Theta_1)")
    ax.set_zlabel("J(Theta)")
    plt.title("Gradient Descent Path on Cost Function")
    plt.show()


def plot_cost_contours(X, y, parameter_hist, pause=True, header=None):
    J_vals, theta0_vals, theta1_vals = generate_graph_prereqs(X, y, parameter_hist)
    fig, ax = plt.subplots()

    # Plot initial contour map
    contour = ax.contourf(
        theta0_vals, theta1_vals, J_vals.T, cmap="coolwarm", alpha=0.6
    )
    plt.colorbar(contour)
    ax.set_xlabel("Bias (θ0)")
    ax.set_ylabel("Weight (θ1)")
    if not pause:
        ax.set_title(header)

    params = np.array(parameter_hist)
    for i in range(len(params)):
        ax.plot(
            params[: i + 1, 0], params[: i + 1, 1], "ro-", markersize=3, linewidth=1
        )
        if pause:
            plt.pause(0.2)
    if pause:
        plt.show()
    else:
        plt.savefig(header + ".png")


# Q3
def plot_animated_surface_gradient(X, y):
    lr = LinearRegressor()
    parameter_hist = lr.fit(X, y)
    plot_cost_surface(X, y, parameter_hist)


# Q4
def plot_animated_contour_gradient(X, y):
    lr = LinearRegressor()
    parameter_hist = lr.fit(X, y)
    plot_cost_contours(X, y, parameter_hist)


# Q5
def plot_diff_lr_contour_gradient(X, y):
    lr = LinearRegressor()
    learning_rates = [0.001, 0.025, 0.1]
    for rate in learning_rates:
        parameter_hist = lr.fit(X, y, learning_rate=rate)
        plot_cost_contours(
            X, y, parameter_hist, pause=False, header=f"Contour Plot, Rate: {rate}"
        )


# For comparing learning rates
def compare_learning_rates():
    learning_rates = [0.001, 0.025, 0.1, 1]

    plt.figure(figsize=(8, 6))

    for lr_value in learning_rates:
        lr = LinearRegressor()
        lr.fit(X, Y, learning_rate=lr_value)
        losses = [lr.losses[i] for i in range(0, len(lr.losses))]
        plt.plot(losses, label=f"Learning Rate: {lr_value}")

    plt.xlabel("Iteration (every 10th)")
    plt.ylabel("Loss")
    plt.title("Loss over Iterations for Different Learning Rates")
    plt.legend()
    plt.grid(True)

    plt.savefig("Loss over Iterations for Different Learning Rates")
    plt.show()


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
Y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

if len(X.shape) == 1:
    X = X.reshape(X.shape[0], 1)


# plot_scatter_with_fit(X, Y)
# plot_animated_surface_gradient(X, Y)
# plot_animated_contour_gradient(X, Y)
# plot_diff_lr_contour_gradient(X, Y)
# compare_learning_rates()


def testing(X, Y):
    lr = LinearRegressor()
    theta = lr.fit(X, Y)
    print(type(theta))
    print(theta.shape)

    preds = lr.predict(X)
    print(len(X))
    print(type(preds))
    print(preds.shape)


# testing(X, Y)




import numpy as np
import matplotlib.pyplot as plt


class LinearRegressor:
    def __init__(self, learning_rate=0.01, tolerance=1e-6, max_epochs=500):
        self.learning_rate = learning_rate
        self.tolerance = tolerance
        self.max_epochs = max_epochs
        self.losses = []
        self.parameter_hist = []

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for epoch in range(self.max_epochs):
            y_predicted = np.dot(X, self.weights) + self.bias
            loss = np.mean((y - y_predicted) ** 2)

            self.losses.append(loss)
            self.parameter_hist.append((self.weights.copy(), self.bias))

            dw = -(1 / n_samples) * np.dot(X.T, (y - y_predicted))
            db = -(1 / n_samples) * np.sum(y - y_predicted)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

            if epoch &gt; 0 and abs(self.losses[-1] - self.losses[-2]) &lt; self.tolerance:
                print(f"Converged at epoch {epoch}")
                break

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

    def plot_multiple_metrics(results):
        plt.figure(figsize=(10, 5))
        for label, losses in results.items():
            plt.plot(range(len(losses)), losses, label=label)
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.title("Loss vs. Epochs for Different Learning Rates and Tolerances")
        plt.legend()
        plt.savefig("learning_rate_v_loss.png")
        plt.show()


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")
X = X.reshape(-1, 1)

learning_rates = [0.001, 1, 0.1]
tolerances = [1e-6]
results = {}

for lr in learning_rates:
    for tol in tolerances:
        regressor = LinearRegressor(learning_rate=lr, tolerance=tol, max_epochs=500)
        regressor.fit(X, y)
        results[f"lr={lr}, tol={tol}"] = regressor.losses

LinearRegressor.plot_multiple_metrics(results)




import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

from sampling_sgd import (
    generate,
    train_test_split,
    StochasticLinearRegressor,
    FixedPointLinearRegressor,
)

xs, ys = generate(
    1000000,
    np.array([3, 1, 2]).reshape((3,)),
    np.array([3, -1]).reshape((2,)),
    np.array([2, 2]).reshape((2,)),
    np.sqrt(2.0),
)

x_train, x_test, y_train, y_test = train_test_split(xs, ys)


def train_all_batch_sizes(X_train, y_train):
    sgd_regressor = StochasticLinearRegressor()
    sgd_regressor.fit(X_train, y_train)
    return sgd_regressor


def predict_for_batch(model, X, batch_size):
    theta = model.results[batch_size]["theta"]
    ones = np.ones((X.shape[0], 1))
    X_bias = np.concatenate((ones, X), axis=1)
    return np.dot(X_bias, theta).reshape(-1)


def compute_mse_for_batch(model, X, y, batch_size):
    y_pred = predict_for_batch(model, X, batch_size)
    return np.mean((y_pred - y) ** 2)


def plot_trajectory(traj, title="3D Trajectory", line_color="b"):
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection="3d")
    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=line_color, label="Trajectory")

    ax.scatter(
        traj[0, 0],
        traj[0, 1],
        traj[0, 2],
        color=line_color,
        marker="o",
        s=50,
        edgecolor="k",
        label="Start",
    )

    ax.set_xlabel(r"$\theta_0$")
    ax.set_ylabel(r"$\theta_1$")
    ax.set_title(title)
    ax.legend()
    plt.show()


def compare_movment(model):
    trajectories = {r: model.results[r]["param_hist"] for r in model.batch_sizes}
    theta_true = [3, 1, 2]
    colors = {1: "r", 80: "g", 8000: "b", 800000: "m"}
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection="3d")
    for r in model.batch_sizes:
        traj = trajectories[r]
        ax.plot(
            traj[:, 0],
            traj[:, 1],
            traj[:, 2],
            label=f"Batch size = {r}",
            color=colors[r],
        )
        ax.scatter(
            traj[0, 0],
            traj[0, 1],
            traj[0, 2],
            color=colors[r],
            marker="o",
            s=50,
            edgecolor="k",
        )

    ax.scatter(
        theta_true[0],
        theta_true[1],
        theta_true[2],
        color="k",
        marker="*",
        s=200,
        label="True Parameters",
    )
    ax.set_xlabel(r"$\theta_0$")
    ax.set_ylabel(r"$\theta_1$")
    ax.set_zlabel(r"$\theta_2$")
    ax.set_title("Trajectory of $\\theta$ in 3D Space for Various Batch Sizes")
    ax.legend()
    plt.show()


# sgd_model = train_all_batch_sizes(x_train, y_train)

# for r in sgd_model.batch_sizes:
#     train_mse = compute_mse_for_batch(sgd_model, x_train, y_train, r)
#     test_mse = compute_mse_for_batch(sgd_model, x_test, y_test, r)
#     print(f"\nSGD with batch size {r}:")
#     print("Learned theta:", sgd_model.results[r]["theta"].flatten())
#     print(f"Time taken: {sgd_model.results[r]["time_taken"]}")
#     print(f"Iterations Taken: {sgd_model.results[r]['epochs']}")
#     print(f"Training MSE: {train_mse}")
#     print(f"Test MSE: {test_mse}")

# fixed_model = FixedPointLinearRegressor()
# fixed_model.fit(x_train, y_train)
# theta_closed = fixed_model.theta.flatten()
# train_mse_closed = np.mean((fixed_model.predict(x_train) - y_train) ** 2)
# test_mse_closed = np.mean((fixed_model.predict(x_test) - y_test) ** 2)
# print("\nClosed-Form Solution:")
# print("Learned theta:", theta_closed)
# print(f"Training MSE: {train_mse_closed}")
# print(f"Test MSE: {test_mse_closed}")


# compare_movment(sgd_model)
def testing():
    xs, ys = generate(
        1000000,
        np.array([3, 1, 2]).reshape((3,)),
        np.array([3, -1]).reshape((2,)),
        np.array([2, 2]).reshape((2,)),
        np.sqrt(2.0),
    )
    print(type(xs))
    print(type(ys))
    print(xs.shape)
    print(ys.shape)
    print(np.mean(xs))
    print(np.mean(ys))

    sgd = StochasticLinearRegressor()
    results = sgd.fit(xs, ys)
    print(type(results))
    print(len(results))
    print(results[0].shape, results[1].shape)
    print(results[0][-1], results[1][-1])


# testing()




# Imports - you can add any other permitted libraries
import numpy as np
import time

np.random.seed(0)
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def train_test_split(xs, ys, test_size=0.2):
    indices = np.arange(xs.shape[0])
    np.random.shuffle(indices)

    split_index = int(xs.shape[0] * (1 - test_size))

    X_train, X_test = xs[indices[:split_index]], xs[indices[split_index:]]
    y_train, y_test = ys[indices[:split_index]], ys[indices[split_index:]]

    return X_train, X_test, y_train, y_test


def shuffle(X, y):
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    return X[indices], y[indices]


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.

    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.

    input_mean : numpy array of shape (2,)
        The mean of the input data.

    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.

    noise_sigma : float
        The standard deviation of the Gaussian noise.

    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.

    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=(1, N))
    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=(1, N))
    ones = np.ones(shape=(1, N))
    epsilon = np.random.normal(loc=0, scale=noise_sigma, size=(1, N))
    random_vars = np.concatenate((ones, x1, x2), axis=0)
    xs = np.concatenate((x1, x2), axis=0).T
    ys = theta.T @ random_vars + epsilon
    return xs, ys.reshape((N,))


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_sizes = [1, 80, 8000, 800000]
        # self.batch_sizes = [1, 80]
        self.results = {
            bs: {
                "theta": None,
                "param_hist": [],
                "loss_hist": [],
                "epochs": 0,
                "time_taken": 0,
            }
            for bs in self.batch_sizes
        }

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        ones = np.ones(shape=(X.shape[0], 1))
        X = np.concatenate((ones, X), axis=1)
        y = y.reshape((y.shape[0], 1))

        total_epochs = int(1e4)
        C = 1e-3

        for r in self.batch_sizes:
            start_time = time.time()
            theta = np.zeros(shape=(X.shape[1], 1))
            param_hist = []
            loss_hist = []
            epsilon = C * (1 / r)

            print(f"Running for batch size: {r}")
            for epoch in range(total_epochs):
                X, y = shuffle(X, y)
                epoch_loss = 0

                for b in range(0, X.shape[0], r):
                    X_batch = X[b : min(b + r, X.shape[0]), :]
                    y_batch = y[b : min(b + r, X.shape[0]), :]

                    y_pred = np.dot(X_batch, theta)
                    dw = -(1 / r) * np.dot(X_batch.T, (y_batch - y_pred))
                    loss = (1 / 2) * np.average((y_batch - y_pred) ** 2)
                    epoch_loss += loss
                    theta -= learning_rate * dw

                epoch_loss /= X.shape[0] / r

                if (
                    len(loss_hist) &gt;= 5
                    and abs(epoch_loss - np.mean(loss_hist[-5:])) &lt; epsilon
                ):
                    print(f"Stopping early at epoch {epoch} for batch size {r}")
                    break

                loss_hist.append(epoch_loss)
                param_hist.append(theta.flatten())

            self.results[r]["theta"] = theta
            self.results[r]["param_hist"] = np.array(param_hist)
            self.results[r]["loss_hist"] = loss_hist
            self.results[r]["epochs"] = epoch
            self.results[r]["time"] = time.time() - start_time

        self.theta = {r: self.results[r]["theta"] for r in self.batch_sizes}

        return [self.results[r]["param_hist"] for r in self.batch_sizes]

    def predict(self, X):
        """
        Predict the target values for the input data using the model trained for a specific batch size.
        """

        predictions = []
        X = np.concatenate((np.ones(shape=(X.shape[0], 1)), X), axis=1)
        for r in self.batch_size:
            predictions.append(
                np.dot(X, self.results[r]["theta"]).reshape((X.shape[0],))
            )
        return predictions


class FixedPointLinearRegressor:
    def __init__(self):
        self.theta = None

    def fit(self, X, y):
        ones = np.ones(shape=(X.shape[0], 1))
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X = np.concatenate((ones, X), axis=1)
        theta = np.zeros(shape=(X.shape[1], 1))
        y = y.reshape((y.shape[0], 1))
        theta = np.linalg.inv(np.dot(X.T, X)) @ X.T @ y
        self.theta = theta
        return theta

    def predict(self, X):
        X = np.concatenate((np.ones(shape=(X.shape[0], 1)), X), axis=1)
        return np.dot(X, self.theta).reshape((X.shape[0],))




import numpy as np
from logistic_regression import LogisticRegressor

markers = ["o", "s"]


def f1_score(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0
    recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0

    f1 = (
        2 * (precision * recall) / (precision + recall)
        if (precision + recall) &gt; 0
        else 0
    )
    return f1


def accuracy(y_true, y_pred):
    return np.sum(y_true == y_pred) / len(y_true)


def train_test_split(xs, ys, test_size=0.2):
    indices = np.arange(xs.shape[0])
    np.random.shuffle(indices)

    split_index = int(xs.shape[0] * (1 - test_size))

    X_train, X_test = xs[indices[:split_index]], xs[indices[split_index:]]
    y_train, y_test = ys[indices[:split_index]], ys[indices[split_index:]]

    return X_train, X_test, y_train, y_test


<A NAME="3"></A><FONT color = #00FFFF><A HREF="match245-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

model = LogisticRegressor()
theta = model.fit(X, y)
</FONT>
print("\nFinal coefficients from Newton's method (including intercept):")
print(theta[-1])

x_train, x_test, y_train, y_test = train_test_split(X, y)
model = LogisticRegressor()
theta = model.fit(x_train, y_train)

y_pred_train = model.predict(x_train)
y_pred_test = model.predict(x_test)
print("Train F1 Score: ", f1_score(y_train, y_pred_train))
print("Train Accuracy: ", accuracy(y_train, y_pred_train))
print("Test F1 Score: ", f1_score(y_test, y_pred_test))
print("Test Accuracy: ", accuracy(y_test, y_pred_test))


import matplotlib.pyplot as plt

X1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
X2 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
X1, X2 = np.meshgrid(X1, X2)
classes = [0, 1]

Z = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z[i, j] = np.ceil(
            model.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2)) - 0.5
        )[0]

for label, color, marker in zip([0, 1], ["blue", "red"], markers):
    plt.scatter(
        X[y == label, 0],
        X[y == label, 1],
        color=color,
        label=classes[label],
        edgecolor="k",
        marker=marker,
    )
plt.contourf(X1, X2, Z, alpha=0.3, cmap="coolwarm")
plt.legend()
plt.xlabel("x1")
plt.ylabel("x2")
plt.savefig("contour_plot.png")


def testing(X, Y):
    lr = LogisticRegressor()
    theta = lr.fit(X, Y)
    print(type(theta))
    print(theta.shape)
    print(len(X))
    predictions = lr.predict(X)
    print(type(predictions))
    print(predictions.shape)


<A NAME="4"></A><FONT color = #FF00FF><A HREF="match245-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

testing(X, y)




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LogisticRegressor:
    def __init__(self):
        self.theta = None

    def fit(self, X, y, learning_rate=0.01):
</FONT>        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X = (X - self.mean) / self.std

        X = np.hstack((np.ones((X.shape[0], 1)), X))
        epochs = 5000
        tol = 1e-6

        theta = np.zeros(X.shape[1])
        self.theta_list = []
        prev_norm = float("-inf")
        for epoch in range(epochs):
            z = X @ theta
            sigmoid = 1 / (1 + np.exp(-z))
            gradient = X.T @ (y - sigmoid)

            hessian = X.T @ np.diag(sigmoid * (1 - sigmoid)) @ X + 1e-9 * np.eye(
                X.shape[1]
            )

            theta = theta + learning_rate * np.linalg.inv(hessian) @ gradient

            self.theta_list.append(theta.flatten())
            if abs(np.linalg.norm(theta) - prev_norm) &lt; tol:
                break

            prev_norm = np.linalg.norm(theta)

        self.theta = theta
        self.theta_list = np.array(self.theta_list)

        return self.theta_list

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - self.mean) / self.std
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        z = X @ self.theta
        sigmoid = 1 / (1 + np.exp(-z))
        classes = np.ceil(sigmoid - 0.5)
        return classes.reshape((-1,))




# Imports - you can add any other permitted libraries
<A NAME="1"></A><FONT color = #00FF00><A HREF="match245-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
</FONT>
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.assume_same_covariance = assume_same_covariance
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)

        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        m, n = X_norm.shape

        phi = np.mean(y == 1)

<A NAME="0"></A><FONT color = #FF0000><A HREF="match245-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X0 = X_norm[y == 0]
        X1 = X_norm[y == 1]
        mu_0 = np.mean(X0, axis=0)
        mu_1 = np.mean(X1, axis=0)
</FONT>
        self.phi = phi
        self.mu_0 = mu_0
        self.mu_1 = mu_1

        if assume_same_covariance:
            self.sigma = (
                np.dot((X0 - mu_0).T, (X0 - mu_0)) + np.dot((X1 - mu_1).T, (X1 - mu_1))
            ) / m
            return mu_0, mu_1, self.sigma
        else:
            self.sigma0 = np.dot((X0 - mu_0).T, (X0 - mu_0)) / len(X0)
            self.sigma1 = np.dot((X1 - mu_1).T, (X1 - mu_1)) / len(X1)
            return mu_0, mu_1, self.sigma0, self.sigma1

    def get_gaussian_probability(self, x, mu, inv_sigma, sigma):
        d = x.shape[0]
        diff = x - mu
        exponent = -0.5 * diff.T @ inv_sigma @ diff
        # denom = np.sqrt(((2 * np.pi) ** d) * np.linalg.det(sigma))
        return np.exp(exponent)  # / denom

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - self.mean) / self.std
        y_pred = np.zeros((X.shape[0],))
        if self.assume_same_covariance:
            inv_sigma = np.linalg.inv(self.sigma)
        else:
            inv_sigma0 = np.linalg.inv(self.sigma0)
            inv_sigma1 = np.linalg.inv(self.sigma1)

        for i in range(X.shape[0]):
            if self.assume_same_covariance:
                px_y0 = self.get_gaussian_probability(
                    X[i], self.mu_0, inv_sigma, self.sigma
                ) * (1 - self.phi)
                px_y1 = (
                    self.get_gaussian_probability(
                        X[i], self.mu_1, inv_sigma, self.sigma
                    )
                    * self.phi
                )

            else:
                px_y0 = self.get_gaussian_probability(
                    X[i], self.mu_0, inv_sigma0, self.sigma0
                ) * (1 - self.phi)
                px_y1 = (
                    self.get_gaussian_probability(
                        X[i], self.mu_1, inv_sigma1, self.sigma1
                    )
                    * self.phi
                )

            y_pred[i] = int(px_y0 &lt; px_y1)

        return y_pred




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

mapping = {"Alaska": 0, "Canada": 1}
classes = ["Alaska", "Canada"]
markers = ["o", "s"]


def get_accuracy(y, y_pred):
    return np.count_nonzero(y == y_pred) / len(y)


def train_test_split(xs, ys, test_size=0.2):
    indices = np.arange(xs.shape[0])
    np.random.shuffle(indices)

    split_index = int(xs.shape[0] * (1 - test_size))

    X_train, X_test = xs[indices[:split_index]], xs[indices[split_index:]]
    y_train, y_test = ys[indices[:split_index]], ys[indices[split_index:]]

    return X_train, X_test, y_train, y_test


X = np.loadtxt("../data/Q4/q4x.dat")
Y = np.genfromtxt("../data/Q4/q4y.dat", dtype=str)
Y = np.array([mapping[label] for label in Y])


gda = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma = gda.fit(X, Y, assume_same_covariance=True)

y_pred = gda.predict(X)


X1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
X2 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
X1, X2 = np.meshgrid(X1, X2)


Z = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z[i, j] = gda.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2))[0]


for label, color, marker in zip([0, 1], ["blue", "red"], markers):
    plt.scatter(
        X[Y == label, 0],
        X[Y == label, 1],
        color=color,
        label=classes[label],
        edgecolor="k",
        marker=marker,
    )
plt.xlabel("x1")
plt.ylabel("x2")
plt.contourf(X1, X2, Z, alpha=0.3, cmap="coolwarm")
plt.legend()

plt.savefig("same_sigma_contour.png")
plt.clf()

gda = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma0, sigma1 = gda.fit(X, Y)


Z = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z[i, j] = gda.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2))[0]


for label, color, marker in zip([0, 1], ["blue", "red"], markers):
    plt.scatter(
        X[Y == label, 0],
        X[Y == label, 1],
        color=color,
        label=classes[label],
        edgecolor="k",
        marker=marker,
    )
plt.xlabel("x1")
plt.ylabel("x2")
plt.contourf(X1, X2, Z, alpha=0.3, cmap="coolwarm")
plt.legend()

plt.savefig("diff_sigma_contour.png")
plt.clf()

gda = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma = gda.fit(X, Y, assume_same_covariance=True)

y_pred = gda.predict(X)


X1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
X2 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
X1, X2 = np.meshgrid(X1, X2)


Z_same = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z_same[i, j] = gda.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2))[0]

gda = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma0, sigma1 = gda.fit(X, Y)


Z = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z[i, j] = gda.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2))[0]

for label, color, marker in zip([0, 1], ["blue", "red"], markers):
    plt.scatter(
        X[Y == label, 0],
        X[Y == label, 1],
        color=color,
        label=classes[label],
        edgecolor="k",
        marker=marker,
    )
plt.contourf(X1, X2, Z, alpha=0.3, cmap="coolwarm")
plt.contourf(X1, X2, Z_same, alpha=0.3, cmap="coolwarm")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()

plt.savefig("combined_contour.png")
plt.clf()

gda = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma0, sigma1 = gda.fit(X, Y)

X1 = np.linspace(-500, 600, 200)
X2 = np.linspace(-200, 450, 200)
X1, X2 = np.meshgrid(X1, X2)

Z = np.zeros(X1.shape)
for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        Z[i, j] = gda.predict(np.array([X1[i, j], X2[i, j]]).reshape(-1, 2))[0]


# for label, color, marker in zip([0, 1], ["blue", "red"], markers):
#     plt.scatter(
#         X[Y == label, 0],
#         X[Y == label, 1],
#         color=color,
#         label=classes[label],
#         edgecolor="k",
#         marker=marker,
#     )
plt.contourf(X1, X2, Z, alpha=0.3, cmap="coolwarm")
# plt.legend()

plt.xlabel("x1")
plt.ylabel("x2")
plt.savefig("test.png")
plt.clf()

X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std

print("Feature means:", X_mean)
print("Feature std deviations:", X_std)

<A NAME="5"></A><FONT color = #FF0000><A HREF="match245-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X_train, X_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.2)

gda_same = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda_same.fit(X_train, y_train, assume_same_covariance=True)

print("mu_0:", mu_0)
print("mu_1:", mu_1)
print("sigma:", sigma)
</FONT>
y_train_pred_same = gda_same.predict(X_train)
y_test_pred_same = gda_same.predict(X_test)

print("Train Accuracy (same covariance):", get_accuracy(y_train, y_train_pred_same))
print("Test Accuracy (same covariance):", get_accuracy(y_test, y_test_pred_same))

gda_diff = GaussianDiscriminantAnalysis()
mu_0_diff, mu_1_diff, sigma0, sigma1 = gda_diff.fit(X_train, y_train)

print("mu_0 (diff covariance):", mu_0_diff)
print("mu_1 (diff covariance):", mu_1_diff)
print("sigma0:", sigma0)
print("sigma1:", sigma1)

y_train_pred_diff = gda_diff.predict(X_train)
y_test_pred_diff = gda_diff.predict(X_test)

print("Train Accuracy (diff covariance):", get_accuracy(y_train, y_train_pred_diff))
print("Test Accuracy (diff covariance):", get_accuracy(y_test, y_test_pred_diff))


def testing(X, Y):
    gda = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma = gda.fit(X, Y, assume_same_covariance=True)
    mu_0, mu_1, sigma1, sigma2 = gda.fit(X, Y, assume_same_covariance=False)

    preds = gda.predict(X)
    print(type(preds))
    print(preds.shape)


testing(X, Y)


</PRE>
</PRE>
</BODY>
</HTML>
