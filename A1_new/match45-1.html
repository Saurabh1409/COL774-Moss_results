<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_DALFY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_G9KZF.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import argparse

def plot(m,c,linear_X,linear_Y):
    x_values = np.linspace(np.min(linear_X), np.max(linear_X), 100)  
    y_values = m * x_values + c  
    plt.scatter(linear_X, linear_Y, color='blue', s=20, label='Data Points')
    plt.plot(x_values, y_values, color='orange', linewidth=2, label=f'y = {m:.2f}x + {c:.2f}')
    plt.xlabel("X values")
    plt.ylabel("Y values")
    plt.title("Scatter Plot with Regression Line")
    plt.legend()
    plt.savefig('q2.png')
    plt.show()

def three_D_plot_save(data, label):
    linear_XX = pd.read_csv(data, header=None).values.flatten()
    linear_YY = pd.read_csv(label, header=None).values.flatten()
    theta0_min, theta1_min = 6, 29
    theta0_range = np.linspace(0, theta0_min+1, 100)
    theta1_range = np.linspace(0, theta1_min+1, 100)
    T0, T1 = np.meshgrid(theta0_range, theta1_range)
    J_values = np.zeros_like(T0)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta0 = T0[i, j]
            theta1 = T1[i, j]
            J_values[i, j] = np.sum((linear_YY - (theta1 * linear_XX + theta0)) ** 2)

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(T0, T1, J_values, cmap='coolwarm', alpha=0.9)
    ax.contourf(T0, T1, J_values, zdir='z', offset=np.min(J_values) - 100, cmap='coolwarm')
    ax.set_xlabel(r'$\theta_0$ (Bias)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_zlabel(r'$J(\theta)$ (Error Function)')
    ax.set_title('3D Half-Bowl of Error Function)')
    plt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)
    plt.savefig('J_theta.png')
    plt.show()

def three_D_plot_show(data, label, theta_values):
    linear_XX = pd.read_csv(data, header=None).values.flatten()
    linear_YY = pd.read_csv(label, header=None).values.flatten()
    theta0_min, theta1_min = 6, 29
    theta0_range = np.linspace(0, theta0_min+1, 100)
    theta1_range = np.linspace(0, theta1_min+1, 100)

    T0, T1 = np.meshgrid(theta0_range, theta1_range)
    J_values = np.zeros_like(T0)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta0 = T0[i, j]
            theta1 = T1[i, j]
            J_values[i, j] = np.sum((linear_YY - (theta1 * linear_XX + theta0)) ** 2)

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(T0, T1, J_values, cmap='coolwarm', alpha=0.9)
    ax.set_xlabel(r'$\theta_0$ (Bias)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_zlabel(r'$J(\theta)$ (Error Function)')
    ax.set_title('3D Half-Bowl of Error Function)')

    plt.colorbar(surf, ax=ax, shrink=0.5, aspect=5)
    J_values_added_points = []
    for (theta0, theta1) in theta_values:
        if not plt.fignum_exists(fig.number):  
            break
        J_theta = np.sum((linear_YY - (theta1 * linear_XX + theta0)) ** 2)
        J_values_added_points.append(J_theta)
        ax.scatter(theta0, theta1, J_theta, color='black', s=30)
        plt.pause(0.2) 
        
    plt.show()

    
def contour_plot(data, label, theta_values):
    linear_XX = pd.read_csv(data, header=None).values.flatten() 
    linear_YY = pd.read_csv(label, header=None).values.flatten() 
    
    theta1_optimal = np.sum(linear_XX * linear_YY) / np.sum(linear_XX**2) 
    theta0_optimal = np.mean(linear_YY) - theta1_optimal * np.mean(linear_XX)  

    theta0_range = np.linspace(theta0_optimal - 50, theta0_optimal + 50, 100)  
    theta1_range = np.linspace(theta1_optimal - 50, theta1_optimal + 50, 100)  

    T0, T1 = np.meshgrid(theta0_range, theta1_range)
    J_values = np.zeros_like(T0)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta0 = T0[i, j]
            theta1 = T1[i, j]
            J_values[i, j] = np.sum((linear_YY - (theta1 * linear_XX + theta0)) ** 2) 

    fig = plt.figure(figsize=(10, 7))
    plt.contourf(T0, T1, J_values, levels=50, cmap='coolwarm')
    plt.colorbar(label=r'$J(	heta)$ (Error Function)')
    plt.xlabel(r'$\theta_0$ (Bias)')
    plt.ylabel(r'$\theta_1$ (Slope)')
    plt.title('Contour Plot of Error Function with Gradient Descent Path')

    for (theta0, theta1) in theta_values:
        if not plt.fignum_exists(fig.number):  
            break  
        plt.scatter(theta0, theta1, color='red', s=5)
        plt.pause(0.2) 

    plt.show()

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)   
 
if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="Command Line Arguments")
    try:
        parser.add_argument("function", type=str, help="Name of Function")
        parser.add_argument("data", type=str, help="Path to data.csv file")
        parser.add_argument("label", type=str, help="Path to labels.csv file")
        
        args = parser.parse_args()
    
        X = pd.read_csv(args.data, header=None)
        y = pd.read_csv(args.label, header=None)
        X = X.to_numpy()
        y = y.to_numpy()

        l = LinearRegressor()
        learning_rate = 0.1
        ans = l.fit(X,y,learning_rate)
        print("parameter vector : ", ans[-1])
        print("Hops : ", len(ans))
        if (args.function == "parameters"):
            ones_col = np.ones((X.shape[0],1))
            X = np.hstack((ones_col,X))
            print("Loss : ", l.J_theta(ans[-1],X,y))
        elif (args.function == "plot"):
            plot(ans[-1][1],ans[-1][0],X, y)
        elif (args.function == "show"):
            three_D_plot_show(args.data,args.label,ans)
        elif (args.function == "save"):
            three_D_plot_save(args.data, args.label)
        elif (args.function == "contour"):
            contour_plot(args.data,args.label,ans)
        elif (args.function == "MSE"):
            predictions = l.predict(X)
            mse = mean_squared_error(y, predictions)
            print("Parameter vector : ",ans[-1])
            print("MSE : ",mse)
        else:
            pass
        
    except Exception as e:
        print(e)
        print("Usage : python3 analysis.py &lt;Function&gt; &lt;path_to_data&gt; &lt;path_to_labels&gt;.")
      
        
        



# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = []
        self.J_theta_values =[]
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match45-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
</FONT>        m, n = X.shape
        theta_t = np.zeros(n)
        theta_t_1 = np.zeros(n)
        self.theta = theta_t_1
        feature_matrix = []
        
        for _ in range(2000):
            theta_t_1 = theta_t - (learning_rate * self.grad(theta_t,X,y))
            feature_matrix.append(theta_t_1)
            self.J_theta_values.append(self.J_theta(theta_t_1,X,y))
            self.theta = theta_t_1
            if (self.convergence(theta_t,theta_t_1, X, y)):
                break
            theta_t = theta_t_1
        
        # print("Jtheta" ,self.J_theta_values)
        return np.array(feature_matrix)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="1"></A><FONT color = #00FF00><A HREF="match45-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
        y = np.dot(X,self.theta)
</FONT>        return y
    
    def grad(self,theta_t,X,y):
        m, n = X.shape
        y = y.reshape(-1)
        ans = (-1/len(X))*(X.T @ (y-(X @ theta_t)))
        return ans
    
    def convergence(self, theta_t,theta_t_1, X, y, epislon = 1e-10):
        norm = abs(self.J_theta(theta_t,X, y) - self.J_theta(theta_t_1, X, y))
        if norm &lt; epislon:
            return True
        return False

    def J_theta(self, theta_t, X, y):
        j_theta = 0
        m, n = X.shape
        for i in range(m):
            j_theta += (y[i] - np.dot(theta_t,X[i]))**2
        j_theta = (j_theta/(2*m))
        
        return j_theta 
    
        
        



import numpy as np
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor, generate
import argparse

def theoretical_theta(X,y):
    ones_col = np.ones((X.shape[0],1))
    X = np.hstack((ones_col,X))
    theta_closed = np.linalg.inv(X.T @ X) @ (X.T @ y)
    return theta_closed

def plot_theta_trajectory(theta_array,number):
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    theta0, theta1, theta2 = theta_array[:, 0], theta_array[:, 1], theta_array[:, 2]

    ax.plot3D(theta0, theta1, theta2, 'b-', marker='o',markersize=0.1)
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    ax.set_title('Theta Trajectory in 3D Space')

    if (number == 0):
        plt.savefig('plot_1.png')
    elif (number == 1):
        plt.savefig('plot_80.png')
    elif (number == 2):
        plt.savefig('plot_8000.png')
    else:
        plt.savefig('plot_800000.png')
    
    plt.show()
    

def train_test_split(X, y, threshold):
    size = X.shape[0]
    indices = np.arange(size)
    np.random.shuffle(indices)

    split_idx = int(threshold * size)

    train_idx = indices[:split_idx]
    test_idx = indices[split_idx:]
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    return X_train, y_train, X_test, y_test

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match45-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def Loss(theta,X,y):
    ones_col = np.ones((X.shape[0],1))
    X = np.hstack((ones_col,X))
    ans = np.mean((y - (X@ theta))**2)/2
</FONT>    return ans

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="command-line arguments")
    parser.add_argument("function", type=str, help="which function to call ?")
    args = parser.parse_args()
    
    l = StochasticLinearRegressor()
    learning_rate = 0.001
    if (args.function == "theoretical"):
        X, y = generate(1000000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)
        print("Theoretical Theta : [",*theoretical_theta(X, y).flatten(),"]")
    elif (args.function == "plot"):
        X, y = generate(1000000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)
        X_train, y_train, X_test, y_test = train_test_split(X,y,0.8)
        ans = l.fit(X_train,y_train,learning_rate)
        plot_theta_trajectory(l.plot_list[0],0)
        plot_theta_trajectory(l.plot_list[1],1)
        plot_theta_trajectory(l.plot_list[2],2)
        plot_theta_trajectory(l.plot_list[3],3)
    elif (args.function == "MSE"):
        X, y = generate(1000000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)
        X_train, y_train, X_test, y_test = train_test_split(X,y,0.8)
        ans = l.fit(X_train,y_train,learning_rate)
        predictions = l.predict(X_test)
        predictions_train = l.predict(X_train)
        mse_1 = mean_squared_error(y_test, predictions[0])
        mse_train_1 = mean_squared_error(y_train, predictions_train[0])
        mse_80 = mean_squared_error(y_test, predictions[1])
        mse_train_80 = mean_squared_error(y_train, predictions_train[1])
        mse_8000 = mean_squared_error(y_test, predictions[2])
        mse_train_8000 = mean_squared_error(y_train, predictions_train[2])
        mse_800000 = mean_squared_error(y_test, predictions[3])
        mse_train_800000 = mean_squared_error(y_train, predictions_train[3])
        print("Parameter vector : ",ans[2][-1])
        print("MSE Test 1: ",mse_1)
        print("MSE Train 1: ", mse_train_1)
        print("MSE Test 80: ",mse_80)
        print("MSE Train 80: ", mse_train_80)
        print("MSE Test 8000: ",mse_8000)
        print("MSE Train 8000: ", mse_train_8000)
        print("MSE Test 800000: ",mse_800000)
        print("MSE Train 800000: ", mse_train_800000)
    elif (args.function == "parameters"):
        X, y = generate(1000000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)
        X_train, y_train, X_test, y_test = train_test_split(X,y,0.8)
        ans = l.fit(X_train,y_train,learning_rate)
        parameters = [ans[0][-1], ans[1][-1], ans[2][-1], ans[3][-1]]
        print("Parameters : ")
        print(parameters)
        print("Epochs : ")
        print(len(ans[0]), len(ans[1]), len(ans[2]), len(ans[3]))
        print("Hops : ")
        print(len(l.plot_list[0]), len(l.plot_list[1]), len(l.plot_list[2]), len(l.plot_list[3]))
        print("Loss : ")
        print(Loss(ans[0][-1],X,y), Loss(ans[1][-1],X,y), Loss(ans[2][-1],X,y), Loss(ans[3][-1],X,y))
    else:
        pass 





# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], np.sqrt(input_sigma[0]), N)
    x2 = np.random.normal(input_mean[1], np.sqrt(input_sigma[1]), N)
    X = np.vstack([np.ones(N), x1, x2]).T
    y = X @ theta + np.random.normal(0, np.sqrt(noise_sigma), N)
    X = np.column_stack((x1, x2))
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_size = 8000
        self.learning_rate = 0.001
        self.max_epochs = 4000
        self.tol = 1e-5
        self.theta_list = []
        self.plot_list = []
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
        
        n_samples, n_features = X.shape
        self.learning_rate = learning_rate
        batch_sizes = [1,80,8000,800000]
        max_epochs = [3,100,350,12000]
        tolerance = [1e-3,1e-4,1e-4,1e-7]
        iterations = [3,5,5,5]
        result = []
        
        for batch_num in range(4):
            self.batch_size = batch_sizes[batch_num]
            self.max_epochs = max_epochs[batch_num]
            self.tol = tolerance[batch_num]
            self.theta = np.zeros(n_features)
            loss_history = []
            itr = 0
            plot_history = []
            
            convergence_acheived = False
            
            features_matrix = []
            for epoch in range(self.max_epochs):
                
                indices = np.random.permutation(n_samples)
                X_shuffled, y_shuffled = X[indices], y[indices]

                for start_idx in range(0, n_samples, self.batch_size):
                    end_idx = min(start_idx + self.batch_size, n_samples)
                    X_batch, y_batch = X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]

                    gradient = self.grad(X_batch,y_batch)
                    self.theta = self.theta - (self.learning_rate * gradient)
                    plot_history.append(self.theta)
                    
                    loss = self.J_theta(X_batch,y_batch)
                    loss_history.append(loss)
                    

                    if len(loss_history) &gt; 10:
                        rolling_mean_diff = abs(np.mean(loss_history[-10:]) - np.mean(loss_history[-11:-1]))
                        
                        if rolling_mean_diff &lt; self.tol:
                            itr += 1
                            if (itr &gt;= iterations[batch_num]):
                                convergence_acheived = True
                        else:
                            itr = 0
                    
                    if convergence_acheived:
                        break
                
                features_matrix.append(self.theta) 
                if convergence_acheived:
                    break
            
            self.plot_list.append(np.array(plot_history))
            print(self.theta)
            result.append(np.array(features_matrix))
        
        self.theta_list = [result[0][-1], result[1][-1], result[2][-1], result[3][-1]]
        print(result[0][-1], result[1][-1], result[2][-1], result[3][-1])
        # print(len(result))
        return result
    
    def grad(self,X,y):
        ans = (-1 / len(X))*(X.T @(y - (X@self.theta)))
        return ans
    
    def J_theta(self,X,y):
        ans = np.mean((y - (X@ self.theta))**2)/2
        return ans
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
        final_ans = []
        for i in range (4):
            y = np.dot(X,self.theta_list[i])
            final_ans.append(y)
        
        return final_ans
    
    



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor
import argparse


def train_test_split(X, y, threshold):
    size = X.shape[0]
    indices = np.arange(size)
    np.random.shuffle(indices)
    split_idx = int(threshold * size)
    train_idx = indices[:split_idx]
    test_idx = indices[split_idx:]
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    return X_train, y_train, X_test, y_test

def plot(X, y, theta):
    Y = y.flatten()
<A NAME="0"></A><FONT color = #FF0000><A HREF="match45-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    class_0 = X[Y==0]
    class_1 = X[Y==1]
    class_0_x1 = class_0[:,0]
    class_0_x2 = class_0[:,1]
    class_1_x1 = class_1[:,0]
    class_1_x2 = class_1[:,1]
    plt.scatter(class_0_x1, class_0_x2, label="Class 0", marker='o', color ='red', edgecolors='k')
</FONT>    plt.scatter(class_1_x1, class_1_x2, label="Class 1", marker='x', color='blue')
    
    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_vals = - (theta[0] + (theta[1] * x1_vals)) / theta[2]
    eq_line = f"{theta[0]:.2f} + {theta[1]:.2f}x₁ + {theta[2]:.2f}x₂ = 0"
    
    plt.plot(x1_vals, x2_vals, color='green', label=f"Decision Boundary\n({eq_line})")
    plt.xlabel("Feature x1")
    plt.ylabel("Feature x2")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.savefig('plot.png')
    plt.show()
    
def get_accuracy(predictions, y_test):
    ans = 0
    true_pos = 0
    true_neg = 0
    false_pos = 0
    false_neg = 0
    for i in range(len(predictions)):
        if (predictions[i] == y_test[i]):
            ans += 1
            if (predictions[i] == 1):
                true_pos += 1
            else:
                true_neg += 1
        else:
            if (predictions[i] == 1):
                false_pos += 1
            else:
                false_neg += 1
                
    ans = ans/len(predictions)
    print("True Positives : ", true_pos)
    print("False Positives : ", false_pos)
    print("True Negatives : ", true_neg)
    print("False Negatives : ", false_neg)
    return ans

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description="command-line arguments")
    try:
        parser.add_argument("function", type=str, help="Name of Function")
        parser.add_argument("data", type=str, help="Path to data.csv file")
        parser.add_argument("label", type=str, help="Path to labels.csv file")
        
        args = parser.parse_args()
    
        X = pd.read_csv(args.data, header=None)
        y = pd.read_csv(args.label, header=None)
        X = X.to_numpy()
        y = y.to_numpy()

        l = LogisticRegressor()
        ans = l.fit(X,y)
        
        if (args.function == "plot"):
            print("parameter vector : ", ans[-1])
            X = l.normalize(X)
            plot(X, y, ans[-1])
        elif (args.function == "parameters"):
            print("parameter vector : ", ans[-1])
            print("Hops : ",len(ans))
        elif (args.function == "accuracy"):
            print("parameter vector : ", ans[-1])
            predictions = l.predict(X)
            accuracy = get_accuracy(predictions,y)
            print("Accuracy : ",accuracy)      
        else:
            pass
        
    except Exception as e:
        print(e)




# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = np.array([])
        self.normalize_mean = 0
        self.normalize_std = 1
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = self.normalize(X)
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
        m,n = X.shape
        theta_t = np.ones(n)
        theta_t_1 = np.zeros(n)
        self.theta = theta_t_1
        feature_matrix = []
        
        for _ in range(2000):
            h_theta = self.sigmoid(X @ theta_t)  
            grad = self.gradient(h_theta, X, y)
            H = self.hessian(h_theta, X)
            try:
                theta_t_1 = theta_t + (learning_rate * (np.linalg.inv(H) @ grad))
            except:
                H = H + ((1e-10) * np.eye(H.shape[0]))
                theta_t_1 = theta_t + (learning_rate * (np.linalg.inv(H) @ grad))
            feature_matrix.append(theta_t_1)
            self.theta = theta_t_1
            
            if (self.convergence(theta_t_1, theta_t)):
                break
            theta_t = theta_t_1
        
        return np.array(feature_matrix)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X-self.normalize_mean)/self.normalize_std
        ones_col = np.ones((X.shape[0],1))
        X = np.hstack((ones_col,X))
        m,n = X.shape
        h_theta = self.sigmoid(X @ self.theta)
        y = np.zeros(m)
        for i in range(m):
            if (h_theta[i] &gt; 0.5):
                y[i] = 1
            else:
                y[i] = 0
        return y
        
    def sigmoid(self, x):
        ans = 1/(1 + np.exp(-x))
        return ans
    
    def normalize(self, X):
        mean = np.mean(X,axis = 0)
        var = np.std(X,axis = 0)
        self.normalize_mean = mean
        self.normalize_std = var
        X = (X-mean)/var
        return X
    
    def gradient(self,h_theta,X,y):
        m,n = X.shape
        ans = np.zeros(n)
        for i in range(m):
            ans = ans + (X[i] * (y[i] - h_theta[i]))
        return ans
    
    def hessian(self, h_theta, X):
        m,n = X.shape
        ans = np.zeros((n,n))
        for i in range(m):
            X_out = np.outer(X[i],X[i])
            ans = ans + (h_theta[i] * (1-h_theta[i]) * X_out)
        return ans
    
    def convergence(self, theta_t,theta_t_1, epislon = 1e-6):
        norm = np.linalg.norm(theta_t_1 - theta_t, ord = 2)
        if norm &lt; epislon:
            return True
        return False
           



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis
import argparse

def equation(x1,x2, sigma1, sigma0, mu1, mu0, C):
    sigma0_inv = np.linalg.inv(sigma0)
    sigma1_inv = np.linalg.inv(sigma1)
    X = np.array([x1, x2])
    term1 = ((X.T @ (sigma1_inv - sigma0_inv)) @ X)
    term2 = 2*(X.T @ ((sigma0_inv @ mu0) - (sigma1_inv @ mu1)))
    term3 = ((mu1.T @ sigma1_inv) @ mu1) - ((mu0.T @ sigma0_inv) @ mu0)
    return (C-(0.5 * (term1 + term2 + term3)))

def plot_points(X,y):
<A NAME="2"></A><FONT color = #0000FF><A HREF="match45-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    class_0 = X[y==0]
    class_1 = X[y==1]
    class_0_x1 = class_0[:,0]
    class_0_x2 = class_0[:,1]
    class_1_x1 = class_1[:,0]
    class_1_x2 = class_1[:,1]
</FONT>    plt.scatter(class_0_x1, class_0_x2, label="Class 0", marker='o', color ='red', edgecolors='k')
    plt.scatter(class_1_x1, class_1_x2, label="Class 1", marker='x', color='blue')
    plt.xlabel("Feature x1")
    plt.ylabel("Feature x2")
    plt.legend()
    plt.title("Training Data")
    plt.savefig("points.png")
    plt.show()

def plot(X,y,mu0,mu1,sigma0,sigma1,same_cov = False):
    class_0 = X[y==0]
    class_1 = X[y==1]
    class_0_x1 = class_0[:,0]
    class_0_x2 = class_0[:,1]
    class_1_x1 = class_1[:,0]
    class_1_x2 = class_1[:,1]
    plt.scatter(class_0_x1, class_0_x2, label="Class 0", marker='o', color ='red', edgecolors='k')
    plt.scatter(class_1_x1, class_1_x2, label="Class 1", marker='x', color='blue')
    
    phi  = np.sum(y == 1) / len(y)
    det0 = np.linalg.det(sigma0)
    det1 = np.linalg.det(sigma1)
    try: 
        C = np.log(phi/(1-phi)) + (0.5 * np.log(det0/det1))
    except:
        C = np.log(phi/(1-phi))
    
    x1_vals = np.linspace((X[:, 0].min())-1, (X[:, 0].max())+1, 400)  
<A NAME="5"></A><FONT color = #FF0000><A HREF="match45-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2_vals = np.linspace((X[:, 1].min())-1, (X[:, 1].max())+1, 400)  
    
    X1, X2 = np.meshgrid(x1_vals, x2_vals)
    Z = np.zeros_like(X1)
    for i in range(len(x1_vals)):
        for j in range(len(x2_vals)):
</FONT>            Z[j, i] = equation(X1[j, i], X2[j, i],sigma1, sigma0, mu1, mu0, C) 
    contour = plt.contour(X1, X2, Z, levels=[0], colors='blue')
    if (same_cov):
        plt.plot([], [], color='blue', label="Linear Decision Boundary")
        plt.title("Linear Decision Boundary")
    else:
        plt.plot([], [], color='blue', label="Quadratic Decision Boundary")
        plt.title("Quadratic Decision Boundary")
    plt.xlabel('$x_1$')
    plt.ylabel('$x_2$')
    plt.legend()
    if (same_cov):
        plt.savefig("plot_linear.png")
    else:
        plt.savefig("plot_quad.png")
    plt.show()
    
def get_accuracy(predictions, y_test):
    ans = 0
    true_pos = 0
    true_neg = 0
    false_pos = 0
    false_neg = 0
    for i in range(len(predictions)):
        if (predictions[i] == y_test[i]):
            ans += 1
            if (predictions[i] == 1):
                true_pos += 1
            else:
                true_neg += 1
        else:
            if (predictions[i] == 1):
                false_pos += 1
            else:
                false_neg += 1
                
    ans = ans/len(predictions)
    print("True Positives : ", true_pos)
    print("False Positives : ", false_pos)
    print("True Negatives : ", true_neg)
    print("False Negatives : ", false_neg)
    return ans

def plot_both(X,y,mu0,mu1,sigma,sigma0,sigma1):
    phi  = np.sum(y == 1) / len(y)
    C2 = np.log(phi/(1-phi))
    theta = np.linalg.inv(sigma) @ (mu1-mu0)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match45-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta0 = C2-(0.5 * (((mu1.T @ np.linalg.inv(sigma)) @ mu1) - ((mu0.T @ np.linalg.inv(sigma)) @ mu0))) 
    theta = np.concatenate(([theta0], theta))
    y = y.flatten()
    x1_vals_1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
</FONT>    x2_vals_1 = - (theta[0] + (theta[1] * x1_vals_1)) / theta[2]
    
    class_0 = X[y == 0]
    class_1 = X[y == 1]
    class_0_x1 = class_0[:, 0]
    class_0_x2 = class_0[:, 1]
    class_1_x1 = class_1[:, 0]
    class_1_x2 = class_1[:, 1]
    plt.scatter(class_0_x1, class_0_x2, label="Class 0", marker='o', color='red', edgecolors='k')
    plt.scatter(class_1_x1, class_1_x2, label="Class 1", marker='x', color='blue')
    
    C = 0
    det0 = np.linalg.det(sigma0)
    det1 = np.linalg.det(sigma1)
    try: 
        C = np.log(phi/(1-phi)) + (0.5 * np.log(det0/det1))
    except:
        C = np.log(phi/(1-phi))
    x1_vals = np.linspace(-3, 3, 400)  
    x2_vals = np.linspace(-3, 3, 400)  
    X1, X2 = np.meshgrid(x1_vals, x2_vals)
    Z = np.zeros_like(X1)
    for i in range(len(x1_vals)):
        for j in range(len(x2_vals)):
            Z[j, i] = equation(X1[j, i], X2[j, i],sigma1, sigma0, mu1, mu0, C) 
    contour = plt.contour(X1, X2, Z, levels=[0], colors='blue')
    
    plt.plot(x1_vals_1, x2_vals_1, color='green', label="Linear Decision Boundary")
    plt.plot([], [], color='blue', label="Quadratic Decision Boundary")
    plt.xlabel('$x_1$')
    plt.ylabel('$x_2$')
    plt.legend()
    plt.savefig("plot_both.png")
    plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="command-line arguments")
    try:
        parser.add_argument("function", type=str, help="Name of Function")
        parser.add_argument("data", type=str, help="Path to data.csv file")
        parser.add_argument("label", type=str, help="Path to labels.csv file")
        args = parser.parse_args()
    
        X = np.loadtxt(args.data, delimiter=None)
        y = np.loadtxt(args.label, dtype=str)
        y = np.where(y == 'Alaska', 0, 1)

        l = GaussianDiscriminantAnalysis()
        
        if (args.function == "same_cov"):
            ans = l.fit(X,y,True)
            print("mu0 : ",ans[0])
            print("mu1 : ",ans[1])
            print("sigma : ")
            print(ans[2])
        elif (args.function == "plot_points"):
            X = l.normalize(X)
            plot_points(X,y)
        elif (args.function == "plot_linear"):
            ans = l.fit(X,y,True)
            X = l.normalize(X)
            plot(X,y,ans[0],ans[1],ans[2],ans[2],True)
        elif (args.function == "diff_cov"):
            ans = l.fit(X,y,False)
            print("mu0 : ",ans[0])
            print("mu1 : ",ans[1])
            print("sigma0 : ")
            print(ans[2])
            print("sigma1 : ")
            print(ans[3])
        elif (args.function == "plot_quad"):
            ans = l.fit(X,y,False)
            X = l.normalize(X)
            plot(X,y,ans[0],ans[1],ans[2],ans[3],False)
        elif (args.function == "plot_both"):
            ans1 = l.fit(X,y,True)
            ans2 = l.fit(X,y,False)
            X = l.normalize(X)
            plot_both(X,y,ans1[0],ans1[1],ans1[2],ans2[2],ans2[3])
        elif (args.function == "accuracy"):
            print("same cov : ")
            ans1 = l.fit(X,y,True)
            pred = l.predict(X)
            print("Accuracy : ",get_accuracy(pred,y))
            print("diff cov : ")
            ans2 = l.fit(X,y,False)
            pred = l.predict(X)
            print("Accuracy : ",get_accuracy(pred,y)) 
        else:
            pass
        
    except Exception as e:
        print(e)







# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 = []
        self.mu1 = []
        self.sigma = []
        self.sigma1 = []
        self.sigma2 = []
        self.C = 0
        self.assume_same_cov = False
        self.normalize_mean = 0
        self.normalize_var = 1
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        self.assume_same_cov = assume_same_covariance
        phi = np.sum(y == 1) / len(y)
        X = self.normalize(X,True)
        m,n = X.shape
        X0 = X[y==0]
        mu0 = np.mean(X0, axis = 0)
        X1 = X[y==1]
        mu1 = np.mean(X1, axis = 0)
        self.mu0 = mu0
        self.mu1 = mu1
        if (assume_same_covariance):
            sigma = np.zeros((n,n))
            for i in range(m):
                mu = np.zeros(n)
                if (y[i] == 0):
                    mu = mu0
                else:
                    mu = mu1
                sigma += (np.outer(X[i] - mu,X[i]-mu))
            sigma = sigma/m
            self.sigma = sigma
            self.C = np.log(phi/(1-phi))
            return mu0, mu1, sigma
        
        else:
            sigma0 = np.zeros((n,n))
            sigma1 = np.zeros((n,n))
            for i in range(m):
                if (y[i] == 0):
                    sigma0 += (np.outer(X[i] - mu0,X[i]-mu0))
                else:
                    sigma1 += (np.outer(X[i] - mu1,X[i]-mu1))
            if (len(X0) &gt; 0):
                sigma0 = sigma0/len(X0)
            if (len(X1) &gt; 0):
                sigma1 = sigma1/len(X1)
            self.sigma1 = sigma1
            self.sigma0 = sigma0
            det0 = np.linalg.det(sigma0)
            det1 = np.linalg.det(sigma1)
            self.C = np.log(phi/(1-phi)) + (0.5 * np.log(det0/det1))
            return mu0, mu1, sigma0, sigma1

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X-self.normalize_mean)/self.normalize_var
        pred = np.zeros(X.shape[0])
        for i in range(X.shape[0]):
            if (self.assume_same_cov):
                if (self.equation(X[i][0], X[i][1],self.sigma, self.sigma, self.mu1, self.mu0, self.C) &gt; 0):
                    pred[i] = 1
            else:
                if (self.equation(X[i][0], X[i][1],self.sigma1, self.sigma0, self.mu1, self.mu0, self.C) &gt; 0):
                    pred[i] = 1
        return pred
    
    def normalize(self, X, store_val = False):
        mean = np.mean(X,axis = 0)
        var = np.std(X,axis = 0)
        if (store_val):
            self.normalize_mean = mean
            self.normalize_var = var
        X = (X-mean)/var
        return X
    
    def equation(self,x1,x2, sigma1, sigma0, mu1, mu0, C):
        sigma0_inv = np.linalg.inv(sigma0)
        sigma1_inv = np.linalg.inv(sigma1)
        X = np.array([x1, x2])
        term1 = ((X.T @ (sigma1_inv - sigma0_inv)) @ X)
        term2 = 2*(X.T @ ((sigma0_inv @ mu0) - (sigma1_inv @ mu1)))
        term3 = ((mu1.T @ sigma1_inv) @ mu1) - ((mu0.T @ sigma0_inv) @ mu0)
        return (C-(0.5 * (term1 + term2 + term3)))
        

</PRE>
</PRE>
</BODY>
</HTML>
