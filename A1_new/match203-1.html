<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_1L6WT.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_D2IIQ.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
     
    def __init__(self):
        self.theta = None
        self.parameters=None

    def loss_function(self, X, y, theta):
        m = X.shape[0]
        return np.sum(np.square(np.dot(X, theta) - y)) / (2 * m)
    
    def fit(self, X, y, learning_rate=0.01):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        m = X.shape[0]  # Number of samples
        n = X.shape[1]  # Number of features
        theta = np.zeros((n, 1));
        self.parameters = np.empty((0, n));
        i=0
        while True:
            prevTheta = theta
            gradientJTheta = np.dot(X.T,(np.dot(X, theta) - y))/m
            theta = theta - learning_rate * gradientJTheta;
            self.parameters = np.vstack((self.parameters, theta.T))
            gradient_norm = np.linalg.norm(gradientJTheta)
            if(gradient_norm &lt; 0.00001):
                break
        self.theta = self.parameters[-1]
        return self.parameters       
    
    def predict(self, X):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return np.dot(X, self.theta)
    
    def plot_training_set(self, X, y, theta):
        X = X.squeeze()
        y = y.squeeze()
        print(X)
        print(y)
        
        plt.scatter(X, y, color='blue', edgecolor='red', label='Training Data')
        plt.xlabel("Input (X)")
        plt.ylabel("Output (y)")
        plt.title("Training Data Scatter Plot")

        x = np.linspace(np.min(X), np.max(X), 100)
        y = theta[0] + theta[1]*x
        plt.plot(x, y, color='green', label='Regression Line')
        plt.show()

    def meshplot_error_function(self, X, y):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        x_coordinate = np.arange(-30, 30, 0.1)
        y_coordinate = np.arange(-30, 30, 0.1)
        
        X_coordinate, Y_coordinate = np.meshgrid(x_coordinate, y_coordinate)
        Z = np.zeros(X_coordinate.shape)  # Initialize Z

        for i in range(X_coordinate.shape[0]):
            for j in range(X_coordinate.shape[1]):
                theta = np.array([[X_coordinate[i, j]], [Y_coordinate[i, j]]]) # Correct theta
                Z[i, j] = self.loss_function(X, y, theta) # Using X and y
        
        position_at_each_iteration = []
        for parameter in self.parameters:
            loss = self.loss_function(X, y, parameter.reshape(-1, 1))
            position_at_each_iteration.append([parameter[0], parameter[1], loss]);
        
        ax = plt.subplot(projection="3d", computed_zorder=False)
        for curr_pos in position_at_each_iteration:
            ax.plot_surface(X_coordinate, Y_coordinate, Z, cmap='viridis', zorder=0)
            ax.scatter(curr_pos[0], curr_pos[1], curr_pos[2], color='red', marker='o', label='Current Position', zorder=1)
            ax.set_title('Surface Plot of Loss Function')
            ax.set_xlabel('Theta 0')
            ax.set_ylabel('Theta 1')
            ax.set_zlabel('J(theta)')
            plt.pause(0.2)
            ax.clear()

    def contour_plot(self, X, y, parameters):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        theta_0_vals = np.linspace(-30, 30, 100)
        theta_1_vals = np.linspace(-30, 30, 100)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

        Z = np.zeros(T0.shape)
        for i in range(T0.shape[0]):
            for j in range(T0.shape[1]):
                theta = np.array([[T0[i, j]], [T1[i, j]]])
                Z[i, j] = self.loss_function(X, y, theta)

        fig, ax = plt.subplots(figsize=(8, 6))

        # Plot gradient descent path
        path = np.array(parameters)

        for curr_pos in path:
            ax.contour(T0, T1, Z, levels=30, cmap='coolwarm')
            ax.scatter(curr_pos[0], curr_pos[1], color='red', marker='o', zorder=1)
            ax.set_xlabel("Q0")
            ax.set_ylabel("Q1")
            ax.set_title("Contour Plot of Loss Function")
            plt.pause(0.2)
            ax.clear()

        
    





import linear_regression as lr
import numpy as np
import pandas as pd

def test_linear_regression():
    inputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q1\linearX.csv')
    outputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q1\linearY.csv')
    
    X = inputs.iloc[:].values  # Features
    y = outputs.iloc[:].values 

    X_train = X[:int(0.8*X.shape[0])]
    y_train = y[:int(0.8*X.shape[0])]
    X_test = X[int(0.8*X.shape[0]):]
    y_test = y[int(0.8*X.shape[0]):]

    model = lr.LinearRegressor()
    parameters = model.fit(X_train, y_train)
    print(f'Final Parameters: {parameters[-1]}');
    # model.plot_training_set(X_train[:, 1], y_train, parameters[-1])
    predictions = model.predict(X_test)
    print("Predictions:", predictions)
    # model.meshplot_error_function(X_train, y_train)
    model.contour_plot(X_train, y_train, parameters)
    pass

test_linear_regression()



import numpy as np
import matplotlib.pyplot as plt
import random

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(input_mean, input_sigma, size=(N, 2)) 
    noise = np.random.normal(0, noise_sigma, size=N)  
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise  
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.parameters = None
        self.batch_size = 1

    def fit(self, X, y, learning_rate=0.001):
        batch_size = self.batch_size
        m, n = X.shape
        X = np.c_[np.ones(m), X]  
        self.theta = np.zeros(n + 1)  
        self.parameters = np.empty((0, n + 1))  
        prev_loss = float('inf')
        while True:
            for j in range(int(m / batch_size)):
                start = j * batch_size
                end = min((j + 1) * batch_size, m) 
                Xb = X[start:end]
                yb = y[start:end]
                gradient = Xb.T.dot(Xb.dot(self.theta) - yb) / len(yb) 
                self.theta = self.theta - learning_rate * gradient
                if j &lt; 5000 : self.parameters = np.vstack((self.parameters, self.theta))
            curr_loss = np.sum(np.square(X.dot(self.theta) - y)) / (2 * m)
            if np.abs(prev_loss - curr_loss) &lt; 1e-5:
                break
            prev_loss = curr_loss
        return self.theta

    def predict(self, X):
        X = np.c_[np.ones(X.shape[0]), X] 
        return np.dot(X, self.theta)

    def fit_updated(self, X, y, batch_size):
        self.batch_size = batch_size
        return self.fit(X, y); 
    
    def closed_form_solution(self, X, y):
        X = np.c_[np.ones(X.shape[0]), X] 
        return np.linalg.inv(X.T @ X) @ X.T @ y
    
    def mean_squared_error(self, X, y):
        X = np.c_[np.ones(X.shape[0]), X] 
        return np.sum(np.square(X.dot(self.theta) - y))/ (X.shape[0])
    
    def plotting_movement_theta(self):
            ax = plt.figure().add_subplot(111, projection='3d')
            theta = self.parameters
            # print(theta)
            ax.plot(theta[:,0], theta[:,1], theta[:,2], c='c')
            ax.set_xlabel('Theta 0')
            ax.set_ylabel('Theta 1')
            ax.set_zlabel('Theta 2')
            ax.set_title(f'Plot for the theta movement with batch size {self.batch_size}')
            plt.show()




import sampling_sgd as sgd
import numpy as np

def test_sampling_sgd():
  total_data = 1000000
  theta_true = np.array([3, 1, 2])
  x_mean = np.array([3, -1])
  x_sigma = np.array([4, 4])
  noise_sigma = np.sqrt(2)
  X, y = sgd.generate(total_data, theta_true, x_mean, x_sigma, noise_sigma)

  # Split Data
  train_size = int(0.8 * total_data)
  X_train, y_train = X[:train_size], y[:train_size]
  X_test, y_test = X[train_size:], y[train_size:]

  # Train Model with Different Batch Sizes
  model = sgd.StochasticLinearRegressor()
  batch_sizes = [1,80,8000,80000]
  for batch_size in batch_sizes:
      theta_sgd = model.fit_updated(X_train, y_train, batch_size)
      model.plotting_movement_theta()
      print(f"Batch Size {batch_size}, Learned Theta: {theta_sgd}")

  print(f"Closed-Form Theta: {model.closed_form_solution(X_train, y_train)}")

  print(f"Test Error: {model.mean_squared_error(X_test, y_test)}")

  print(f"Training Error: {model.mean_squared_error(X_train, y_train)}")

test_sampling_sgd()



import numpy as np
import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.epochs = 20

<A NAME="1"></A><FONT color = #00FF00><A HREF="match203-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def normalizeInput(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    def fit(self, X, y, learning_rate=0.01):
</FONT>        X = self.normalizeInput(X)
        X = np.c_[np.ones(X.shape[0]), X]

        m, n = X.shape
        self.theta = np.zeros((n, 1))

        y = y.reshape(-1, 1)
        all_parameters = []

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match203-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        while True:
            p = self.sigmoid(X @ self.theta)
            gradient = X.T @ (p - y)
            W = np.diag((p * (1 - p)).flatten())
</FONT>            Hessian = X.T @ W @ X
            np.fill_diagonal(Hessian, np.diagonal(Hessian) + 1e-3)  
            H_inv = np.linalg.inv(Hessian)
            self.theta -= (H_inv @ gradient)*learning_rate
            all_parameters.append(self.theta.flatten().copy())
            if(np.linalg.norm(gradient) &lt; 1e-8): 
                break
        return np.array(all_parameters)

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match203-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def predict(self, X):
        X = self.normalizeInput(X)
        X = np.c_[np.ones(X.shape[0]), X]
</FONT>        predictions = self.sigmoid(X @ self.theta)
        print(predictions.shape)
        for i in range(len(predictions)):
            if predictions[i] &gt;= 0.5:
                predictions[i] = int(1)
            else:
                predictions[i] = int(0)
        return predictions.astype(int).flatten()

    def plot_decision_boundary(self, X, y, theta):
        for i in range(len(y)):
            if(y[i] == 0):
                plt.scatter(X[i][0], X[i][1], marker='o',  edgecolors='orange', color='yellow')
            else:
                plt.scatter(X[i][0], X[i][1], marker='x', color='orange')
        
        x1_values = np.linspace(X[:, 0].min(), X[:, 0].max(), 90)
        x2_values = - (theta[0] + theta[1] * x1_values) / theta[2]
        plt.plot(x1_values, x2_values, 'k-', color='c', label="Decision Boundary")
        plt.title('Plot for Scattering points and Decision Boundary')
        plt.xlabel('feature-1')
        plt.ylabel('feature-2')
        plt.show()



import logistic_regression as lr
import pandas as pd
import numpy as np


def test_logistic_regression():
  model = lr.LogisticRegressor()
  inputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q3\logisticX.csv')
  outputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q3\logisticY.csv')
    
  X = inputs.iloc[:].values  
  y = outputs.iloc[:].values 

  X_train = X[:int(0.8*X.shape[0])]
  y_train = y[:int(0.8*X.shape[0])]
  X_test = X[int(0.8*X.shape[0]):]
  y_test = y[int(0.8*X.shape[0]):]

  parameters = model.fit(X_train, y_train)
  print(parameters[-1])
  predictions = model.predict(X_test)
  print(f'predictions: {predictions}')
  model.plot_decision_boundary(X_train, y_train, parameters[-1])
  
  

test_logistic_regression()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.is_cov_same = False
        self.mu_0 = 0
        self.mu_1 = 0
        self.phi = 0
        self.sigma = 0;
        self.sigma_0 = 0;
        self.sigma_1 = 0;
        self.X0 = 0
        self.X1 = 0

    def normalize_input(self, X):
        mean = np.mean(X, axis=0)  
        std_dev = np.std(X, axis=0) 
        return (X - mean) / std_dev 
    
    def fit(self, X, y, assume_same_covariance=False):
        self.is_cov_same = assume_same_covariance
        X = self.normalize_input(X)
        X0 = X[y == 0]
        X1 = X[y == 1]
        self.X0, self.X1 = X0, X1
        m, n = X.shape
        self.phi = np.sum(y) / m
        self.mu_0 = np.mean(X0, axis=0) 
        self.mu_1 = np.mean(X1, axis=0)
        if assume_same_covariance:
            self.sigma =  self.sigma = ((X0 - self.mu_0).T @ (X0 - self.mu_0) + (X1 - self.mu_1).T @ (X1 - self.mu_1)) / m 
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.cov(X0, rowvar=False)
            self.sigma_1 = np.cov(X1, rowvar=False)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
        
    
    def predict(self, X):
        X = self.normalize_input(X)
        phi, mu_0, mu_1, sigma_0, sigma_1, sigma = self.phi, self.mu_0, self.mu_1, self.sigma_0, self.sigma_1, self.sigma        
        
        if self.is_cov_same:
            sigma_0 = sigma_1 = sigma

        sigma_0_inv = np.linalg.inv(sigma_0)
        sigma_1_inv = np.linalg.inv(sigma_1)
        c = np.log((1-phi)/phi) + 0.5*np.log(np.linalg.det(sigma_1)/np.linalg.det(sigma_0))

        term1 = 0.5 * np.sum(X @ (sigma_1_inv - sigma_0_inv) * X, axis=1)

        term2 = (mu_1 @ sigma_1_inv - mu_0 @ sigma_0_inv) @ X.T

        term3 = 0.5 * (mu_1 @ sigma_1_inv @ mu_1 - mu_0 @ sigma_0_inv @ mu_0)

        logA = term1 - term2 + term3 + c
        probability = self._sigmoid_type(logA) 
        
        for i in range(len(probability)):
            if probability[i] &gt; 0.5:
                probability[i] = int(1)
            else:
                probability[i] = int(0)
        print(probability.astype(int).flatten())
        

    def _sigmoid_type(self, logA):
        return 1 / (1 + np.exp(logA))

    def _plotting_points_and_decision_boundary(self, X, y):
        X = self.normalize_input(X)
        # print(X.flatten())
        for i in range(X.shape[0]):
            if y[i] == 0:
                plt.scatter(X[i][0], X[i][1], c='r', marker='x')
            else:
                plt.scatter(X[i][0], X[i][1], c='g', marker='o')
        x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 90)
        if self.is_cov_same:
            sigma_inv = np.linalg.inv(self.sigma)
            a = sigma_inv @ (self.mu_1 - self.mu_0)
            b = -0.5 * self.mu_1.T @ sigma_inv @ self.mu_1 + 0.5 * self.mu_0.T @ sigma_inv @ self.mu_0 + np.log(self.phi / (1 - self.phi))
            x2 = - (a[0] * x1 + b) / a[1]
            plt.plot(x1, x2, 'k-', label="Decision Boundary")
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            A = 0.5 * (sigma_1_inv - sigma_0_inv)
            b = -(self.mu_1.T @ sigma_1_inv - self.mu_0.T @ sigma_0_inv)
            c = 0.5 * (self.mu_1.T @ sigma_1_inv @ self.mu_1 - self.mu_0.T @ sigma_0_inv @ self.mu_0) + np.log((1-self.phi)/self.phi) + 0.5*np.log(np.linalg.det(self.sigma_1)/np.linalg.det(self.sigma_0))
            
<A NAME="2"></A><FONT color = #0000FF><A HREF="match203-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            x1_range = np.linspace(-2, 2, 100)
            x2_range = np.linspace(-2, 2, 100)
            X1, X2 = np.meshgrid(x1_range, x2_range)

            Z = (A[0, 0] * X1**2 + 2 * A[0, 1] * X1 * X2 + A[1, 1] * X2**2 +
</FONT><A NAME="0"></A><FONT color = #FF0000><A HREF="match203-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

                b[0] * X1 + b[1] * X2 + c)

            plt.contour(X1, X2, Z, levels=[0], colors='b', linestyles='--')
        
        plt.title('Scatter plot of the data points')
</FONT>        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.show()      





import gda as gd
import pandas as pd

def test_gda():
  inputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q4\q4x.dat',sep='\s+', header=None)
  outputs = pd.read_csv(r'D:\Courses\ML\Assignment1\data\Q4\q4y.dat',sep='\s+', header=None)
  X = inputs.iloc[:].values  # Features
  y = outputs.iloc[:, 0].map({'Alaska': 0, 'Canada': 1}).values

  X_train = X[:int(0.8*X.shape[0])]
  y_train = y[:int(0.8*X.shape[0])]
  X_test = X[int(0.8*X.shape[0]):]
  y_test = y[int(0.8*X.shape[0]):]

  model = gd.GaussianDiscriminantAnalysis();
  parameters = model.fit(X_train, y_train, False)
  model._plotting_points_and_decision_boundary(X_train, y_train)
  print(parameters)

test_gda()

</PRE>
</PRE>
</BODY>
</HTML>
