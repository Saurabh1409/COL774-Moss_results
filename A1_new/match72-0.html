<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_IC2J6.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_IC2J6.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from linear_regression import LinearRegressor

X = pd.read_csv('../data/Q1/linearX.csv', header=None).values.reshape(-1, 1)
y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()

# Function to plot the regression line
def plot_regression_line(X, y, model):
    plt.scatter(X, y, color='blue', label='Data points')
    y_pred = model.predict(X)
    plt.plot(X, y_pred, color='red', label='Linear Regression')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Scatter Plot with Regression Line')
    plt.legend()
    plt.savefig("regression_plot.png")
    plt.show()

# Function to plot contour of the error function
def plot_contour(X, y, history):
    m, n = X.shape
    X = np.column_stack((np.ones(m), X))

    optimal_theta = history[-1]
    theta0_vals = np.linspace(optimal_theta[0] - 30, optimal_theta[0] + 30, 100)
    theta1_vals = np.linspace(optimal_theta[1] - 30, optimal_theta[1] + 30, 100)
    J_vals = np.zeros((100, 100))

    for i, theta0 in enumerate(theta0_vals):
        for j, theta1 in enumerate(theta1_vals):
            theta = np.array([theta0, theta1])
            J_vals[i, j] = (1/(2*m)) * np.sum((X @ theta - y) ** 2)

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match72-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig, ax = plt.subplots()
    contour = ax.contourf(theta0_vals, theta1_vals, J_vals, levels=50, cmap='viridis')
    plt.colorbar(contour)
    
    theta_history = np.array(history)
</FONT>    ax.plot(theta_history[:, 0], theta_history[:, 1], 'r-o', markersize=3)
    
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.title('Contour Plot of Loss Function')
    plt.savefig("contour_plot.png")
    plt.show()

# Function to animate the gradient descent path on contour plot
def animate_contour(X, y, history):
    m, n = X.shape
    X = np.column_stack((np.ones(m), X))

    optimal_theta = history[-1]
    theta0_vals = np.linspace(optimal_theta[0] - 30, optimal_theta[0] + 30, 100)
    theta1_vals = np.linspace(optimal_theta[1] - 30, optimal_theta[1] + 30, 100)
    J_vals = np.zeros((100, 100))

    for i, theta0 in enumerate(theta0_vals):
        for j, theta1 in enumerate(theta1_vals):
            theta = np.array([theta0, theta1])
            J_vals[i, j] = (1/(2*m)) * np.sum((X @ theta - y) ** 2)

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

    fig, ax = plt.subplots()
    contour = ax.contourf(theta0_vals, theta1_vals, J_vals, levels=50, cmap='viridis')
    plt.colorbar(contour)
    
    theta_history = np.array(history)
    for i in range(len(history)):
        ax.plot(theta_history[:i+1, 0], theta_history[:i+1, 1], 'r-o', markersize=3)
        plt.pause(0.2)
    
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.title('Animated Gradient Descent Path')
    plt.savefig("animated_contour.png")
    plt.show()

# Function to plot a 3D mesh of the cost function
def plot_3d_mesh(X, y, optimal_theta):
    m, n = X.shape
    X = np.column_stack((np.ones(m), X))

    theta0_vals = np.linspace(optimal_theta[0] - 30, optimal_theta[0] + 30, 100)
    theta1_vals = np.linspace(optimal_theta[1] - 30, optimal_theta[1] + 30, 100)
    J_vals = np.zeros((100, 100))

    for i, theta0 in enumerate(theta0_vals):
        for j, theta1 in enumerate(theta1_vals):
            theta = np.array([theta0, theta1])
            J_vals[i, j] = (1/(2*m)) * np.sum((X @ theta - y) ** 2)

<A NAME="1"></A><FONT color = #00FF00><A HREF="match72-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Loss')
    ax.set_title('3D Mesh of Loss Function')
    plt.savefig("3d_mesh_plot.png")
    plt.show()
</FONT>
# Function to animate 3D gradient descent
def animate_3d_mesh(X, y, history):
    m, n = X.shape
    X = np.column_stack((np.ones(m), X))

    optimal_theta = history[-1]
    theta0_vals = np.linspace(optimal_theta[0] - 30, optimal_theta[0] + 30, 100)
    theta1_vals = np.linspace(optimal_theta[1] - 30, optimal_theta[1] + 30, 100)
    J_vals = np.zeros((100, 100))

    for i, theta0 in enumerate(theta0_vals):
        for j, theta1 in enumerate(theta1_vals):
            theta = np.array([theta0, theta1])
            J_vals[i, j] = (1/(2*m)) * np.sum((X @ theta - y) ** 2)

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis', alpha=0.7)
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Loss')
    ax.set_title('Animated 3D Gradient Descent')

    for i in range(len(history)):
        ax.scatter(history[i, 0], history[i, 1], (1/(2*m)) * np.sum((X @ history[i] - y) ** 2), color='r')
        plt.pause(0.2)
    
    plt.savefig("animated_3d_mesh.png")
    plt.show()


model = LinearRegressor()
history = model.fit(X, y, learning_rate=0.1)
X1 = np.column_stack((np.ones(X.shape[0]), X))
print(history.shape, (1/(2*X.shape[0])) * np.sum((X1 @ history[-1] - y) ** 2))
print(f'Coefficient (slope): {model.theta[1]}')
print(f'Intercept: {model.theta[0]}')

plot_regression_line(X, y, model)         # Part 2
plot_3d_mesh(X, y, model.theta)           # Part 3a
animate_3d_mesh(X, y, history)            # Part 3b
plot_contour(X, y, history)               # Part 4
animate_contour(X, y, history)            # Part 5




# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m,n = X.shape
        X = np.column_stack((np.ones(m), X))
        n+=1
        self.theta = np.zeros(n)
        history = []
        prev_loss = 0

        while True:
            hx = X @ self.theta
            gradients = (1/m) * (X.T @ (hx - y))

            self.theta -= learning_rate * gradients
            
            loss = (1/(2*m)) * np.sum((y - hx) ** 2)
            history.append(self.theta.copy())
            
            if abs(prev_loss - loss) &lt; 1e-7:
<A NAME="5"></A><FONT color = #FF0000><A HREF="match72-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                break
            prev_loss = loss
        
        return np.array(history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
</FONT>        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.column_stack((np.ones(X.shape[0]), X))
        y_pred = X @ self.theta
        return y_pred



import numpy as np
from sampling_sgd import generate, StochasticLinearRegressor

N = 1000000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = 2**0.5

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

train_size = int(0.8 * N)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

X_train_bias = np.column_stack((np.ones(train_size), X_train))
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match72-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

closed_form_theta = np.linalg.inv(X_train_bias.T @ X_train_bias) @ (X_train_bias.T @ y_train)

model = StochasticLinearRegressor()
history = model.fit(X_train, y_train, learning_rate=0.001)
</FONT>
print("True parameters:", theta_true)
print("Closed-form solution:", closed_form_theta)
print("SGD learned parameters:", model.all_theta)

def mean_squared_error(X, y, theta):
    X_bias = np.column_stack((np.ones(X.shape[0]), X))
    y_pred = X_bias @ theta
    return np.mean((y - y_pred) ** 2)

train_error = mean_squared_error(X_train, y_train, closed_form_theta)
test_error = mean_squared_error(X_test, y_test, closed_form_theta)

print("Training MSE:", train_error)
print("Test MSE:", test_error)

for theta in model.all_theta:
    train_error = mean_squared_error(X_train, y_train, closed_form_theta)
    test_error = mean_squared_error(X_test, y_test, closed_form_theta)

    print("Training MSE:", train_error)
    print("Test MSE:", test_error)




# Imports - you can add any other permitted libraries
import numpy as np
# import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    X = np.column_stack((x1, x2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.all_theta = []
        self.loss_history = []
        self.theta_history = []
        self.epoch_loss_history = []
    
    def fit(self, X, y, learning_rate=0.01):
        m, n = X.shape
        X = np.column_stack((np.ones(m), X))
        n += 1
        all_history = []

        for r in [1, 80, 8000, 800000]:
            # start_time = time.time()
            theta = np.zeros(n)
            self.loss_history = []
            self.theta_history = []
            self.epoch_loss_history = []
            history = []

            for _ in range(100000):
                perm = np.random.permutation(m)
                X = X[perm]
                y = y[perm]
                
                for i in range(0, m, r):
                    Xb = X[i: i + r]
                    yb = y[i: i + r]
                    hx = Xb @ theta
                    
                    gradients = (1 / r) * (Xb.T @ (hx - yb))
                    theta -= learning_rate * gradients
                    predictions_epoch = Xb @ theta
                    loss = (1 / (2 * r)) * np.sum((yb - predictions_epoch) ** 2)
                    self.loss_history.append(loss)
                    self.theta_history.append(theta.copy())
                    if r&lt;800000:
                        if self.stopped(r):
                            break

                history.append(theta.copy())
                # print(theta)
                self.epoch_loss_history.append((1/(2*m)) * np.sum((y - X @ theta) ** 2))
                if len(self.epoch_loss_history)&gt;=2:
                    if abs(self.epoch_loss_history[-1]-self.epoch_loss_history[-2])&lt;1e-6:
                        break


            all_history.append(np.array(history))
            self.all_theta.append(theta.copy())

            # end_time = time.time()
            # print(f"For batch size {r}, Time taken: {end_time-start_time:.4f}, No. of epoch: {len(history)}")

            self.plot_parameter_movement(r)
        return all_history

    def stopped(self, r):
        n=1000 if r &lt; 8000 else 100
        epsilon = 1e-5 if r &gt; 1 else 1e-4
        if len(self.loss_history) &lt; 2*n:
            return False
        avg_recent = np.mean(self.loss_history[-n:])
        avg_previous = np.mean(self.loss_history[-2*n:-n])
        return abs(avg_recent - avg_previous) &lt; epsilon

    def predict(self, X):
        X = np.column_stack((np.ones(X.shape[0]), X))
        y_pred = [X @ theta for theta in self.all_theta]
        return y_pred

    def plot_parameter_movement(self, batch_size):
        history = np.array(self.theta_history)
        
        fig = plt.figure(figsize=(10, 6))
        ax = fig.add_subplot(111, projection='3d')
        
        ax.plot(history[:, 0], history[:, 1], history[:, 2], label=r'$\theta_0, \theta_1, \theta_2$', alpha=0.8)
        
        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$\theta_2$')
        ax.set_title(f"Parameter Movement for Batch Size {batch_size}")
        ax.grid(True)
        ax.legend()
        plt.savefig(f"parameter_movement_batch_{batch_size}.png")
        plt.close()



import numpy as np
from logistic_regression import LogisticRegressor

X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

model = LogisticRegressor()
model.fit(X, y)
print("Learned Parameters:", model.theta)
model.plot_decision_boundary(X, y)



import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mu = None
        self.sigma = None
    
    def sigmoid(self, x):
        return 1/(1 + np.exp(-x))
    
    def hessian(self, X, y, h):
        m = X.shape[0]
        S = np.diag(h*(1 - h))
        H = (X.T @ S @ X)
        return H
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        m, n = X.shape
        self.mu = np.mean(X, axis=0)
        self.sigma = np.std(X, axis=0)
        X = (X - self.mu)/self.sigma
        X = np.column_stack((np.ones((m, 1)), X))
        n += 1
        self.theta = np.zeros(n)
        history = []
        
        for _ in range(1000):
            h = self.sigmoid(X @ self.theta)
            gradient = (X.T @ (y - h))
            Hessian = self.hessian(X, y, h)
            try:
                update = np.linalg.inv(Hessian) @ gradient
            except:
                Hessian += (1e-10) * np.eye(n)
                update = np.linalg.inv(Hessian) @ gradient
            if np.linalg.norm(update) &lt; 1e-10:
                break
            
            self.theta += update
            history.append(self.theta.copy())
        
        return np.array(history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m = X.shape[0]
        if self.mu is None or self.sigma is None:
            raise Exception("Train the model first!")
        X = (X - self.mu)/self.sigma
        X = np.column_stack((np.ones((m, 1)), X))
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)
    

    def plot_decision_boundary(self, X, y):
        plt.figure(figsize=(8, 6))

        # Scatter plot of original data
        scatter_class0 = plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Class 0', edgecolors='k')
        scatter_class1 = plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Class 1')

        # Generate a meshgrid in the original feature space
        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xmesh, ymesh = np.meshgrid(np.linspace(x1_min, x1_max, 100),np.linspace(x2_min, x2_max, 100))

        # Normalize using stored training mean and std
        X_grid = np.column_stack(((xmesh.ravel() - self.mu[0]) / self.sigma[0],(ymesh.ravel() - self.mu[1]) / self.sigma[1]))
        X_grid = np.column_stack((np.ones(X_grid.shape[0]), X_grid))

        # Predict probabilities and reshape for contour plot
        z = self.sigmoid(X_grid @ self.theta).reshape(xmesh.shape)

        # Plot decision boundary
        plt.contourf(xmesh, ymesh, z, levels=[0, 0.5, 1], cmap="coolwarm", alpha=0.3)
        plt.contour(xmesh, ymesh, z, levels=[0.5], colors='black', linewidths=2)
        boundary_line = mlines.Line2D([], [], color='black', linewidth=2, label="Decision Boundary")

        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend(handles=[scatter_class0, scatter_class1, boundary_line])
        plt.title('Logistic Regression')
        plt.savefig('logistic.png')
        plt.show()




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
	# Assume Binary Classification
	def __init__(self):
		self.mu_0 = None
		self.mu_1 = None
		self.sigma = None
		self.sigma_0 = None
		self.sigma_1 = None
		self.phi = None
	
	def normalize(self, X):
		return (X - np.mean(X, axis=0)) / np.std(X, axis=0)

	def fit(self, X, y, assume_same_covariance=False):
		"""
		Fit the Gaussian Discriminant Analysis model to the data.
		Remember to normalize the input data X before fitting the model.
		
		Parameters
		----------
		X : numpy array of shape (n_samples, n_features)
			The input data.
			
		y : numpy array of shape (n_samples,)
			The target labels - 0 or 1.
		
		learning_rate : float
			The learning rate to use in the update rule.
		
		Returns
		-------
		Parameters: 
			If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
			If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
			The parameters learned by the model.
		"""
		X = self.normalize(X)
		m = X.shape[0]
		self.phi = np.mean(y)
		self.mu_0 = np.mean(X[y == 0], axis=0)
		self.mu_1 = np.mean(X[y == 1], axis=0)
		
		if assume_same_covariance:
			X_0 = X[y == 0] - self.mu_0
			X_1 = X[y == 1] - self.mu_1
			self.sigma = (1/m)*(np.dot(X_0.T, X_0) + np.dot(X_1.T, X_1))
<A NAME="0"></A><FONT color = #FF0000><A HREF="match72-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

			return self.mu_0, self.mu_1, self.sigma
		else:
			self.sigma_0 = np.cov(X[y == 0].T, bias=True)
			self.sigma_1 = np.cov(X[y == 1].T, bias=True)
			return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
	
	def predict(self, X):
		"""
		Predict the target values for the input data.
</FONT>		
		Parameters
		----------
		X : numpy array of shape (n_samples, n_features)
			The input data.
			
		Returns
		-------
		y_pred : numpy array of shape (n_samples,)
			The predicted target label.
		"""
		X = self.normalize(X)
		try:
			inv_sigma = np.linalg.inv(self.sigma)
		except:
			self.sigma += (1e-10) * np.eye(self.sigma.shape[0])
		
		delta_0 = -0.5 * np.dot(np.dot(self.mu_0.T, inv_sigma), self.mu_0) + np.dot(np.dot(X, inv_sigma), self.mu_0)
		delta_1 = -0.5 * np.dot(np.dot(self.mu_1.T, inv_sigma), self.mu_1) + np.dot(np.dot(X, inv_sigma), self.mu_1)
		
		return (delta_1 &gt; delta_0).astype(int)

	def plot_data(self, X, y):
		X = self.normalize(X)
		plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska')
		plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada')
		plt.xlabel('Growth ring diameter in fresh water')
		plt.ylabel('Growth ring diameter in marine water')
		plt.legend()
		plt.title('Salmon Classification')
		plt.savefig('scatter.png')
		plt.show()

	def plot_decision_boundary(self, X, y):
		plt.figure(figsize=(8, 6))
		X = self.normalize(X)
		self.fit(X, y, assume_same_covariance=True)
		mu_0, mu_1, sigma = self.mu_0, self.mu_1, self.sigma
		
		x1_min, x1_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
		x2_min, x2_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
		xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))
		
		# linear boundary
<A NAME="2"></A><FONT color = #0000FF><A HREF="match72-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

		inv_sigma = np.linalg.inv(sigma)
		w = np.dot(inv_sigma, (mu_1 - mu_0))
		b = -0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))
</FONT>		
		x2_vals = (-b-w[0]*xx1)/w[1]
		plt.plot(xx1[0, :], x2_vals[0, :], 'r-', label='Linear Decision Boundary')
		
		# quadratic boundary
		self.fit(X, y, assume_same_covariance=False)
		inv_sigma_0 = np.linalg.inv(self.sigma_0)
		inv_sigma_1 = np.linalg.inv(self.sigma_1)
		
		A = inv_sigma_0 - inv_sigma_1
		B = 2 * (np.dot(inv_sigma_1, self.mu_1) - np.dot(inv_sigma_0, self.mu_0))
		C = (np.dot(self.mu_0.T, np.dot(inv_sigma_0, self.mu_0)) - np.dot(self.mu_1.T, np.dot(inv_sigma_1, self.mu_1))
			 + np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0)))
		
		grid_points = np.c_[xx1.ravel(), xx2.ravel()]
		Z = np.sum(grid_points @ A * grid_points, axis=1) + grid_points @ B + C
		Z = Z.reshape(xx1.shape)
		
		plt.contour(xx1, xx2, Z, levels=[0], colors='b', linewidths=1.5, label='Quadratic Decision Boundary')
		
		plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska')
		plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada')
		plt.xlabel('Growth ring diameter in fresh water')
		plt.ylabel('Growth ring diameter in marine water')
		plt.legend()
		plt.title('Salmon Classification')
		plt.savefig('boundary.png')
		plt.show()

# Load the data
# X = np.loadtxt('../data/Q4/q4x.dat')
# y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
# y = np.where(y == 'Canada', 1, 0)

# gda = GaussianDiscriminantAnalysis()
# mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
# print(mu_0.shape)
# print("Mu_0:", mu_0)
# print("Mu_1:", mu_1)
# print("Sigma:", sigma)
# mu_0, mu_1, sigma1, sigma2 = gda.fit(X, y, assume_same_covariance=False)
# print("Mu_0:", mu_0)
# print("Mu_1:", mu_1)
# print("Sigma1:", sigma1)
# print("Sigma2:", sigma2)

# gda.plot_data(X, y)
# gda.plot_decision_boundary(X, y)


</PRE>
</PRE>
</BODY>
</HTML>
