<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_1I4TJ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_SZI5H.py<p><PRE>


# -*- coding: utf-8 -*-
"""ML_ASS1_IITD.ipynb

<A NAME="6"></A><FONT color = #00FF00><A HREF="match122-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DgPX4qhfgN-R4MVhNFEs2taAmZ7ywOL3
</FONT>
# Question 1

Import Libraries and Define LinearRegressor Class
"""

import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from IPython import display
import time
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation




class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(self, X, y, learning_rate=0.01, max_iter=1000, tol=1e-6):
        """
        Fit the linear regression model to the data using Batch Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
            The learning rate to use in the update rule.
        max_iter : int
            Maximum number of iterations.
        tol : float
            Tolerance for stopping criteria (change in cost).

        Returns
        -------
        theta : numpy array of shape (n_features + 1,)
            The final set of parameters.
        cost_history : list
            The list of cost values at each iteration.
        """

        X = np.c_[np.ones(X.shape[0]), X]
        n_samples, n_features = X.shape


        self.theta = np.zeros(n_features)
        self.cost_history = []

        for i in range(max_iter):

            y_pred = X @ self.theta


            error = y_pred - y


            gradient = (X.T @ error) / n_samples


            self.theta -= learning_rate * gradient


            cost = np.sum(error**2) / (2 * n_samples)
            self.cost_history.append(cost)


            if i &gt; 0 and abs(self.cost_history[-1] - self.cost_history[-2]) &lt; tol:
                break

        return self.theta, self.cost_history

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        X = np.c_[np.ones(X.shape[0]), X]
        return X @ self.theta

"""Load Data and Fit the Model"""

linearX = np.loadtxt("linearX.csv")
linearY = np.loadtxt("linearY.csv")


model = LinearRegressor()
theta, cost_history = model.fit(linearX, linearY, learning_rate=0.01)

print("Final Parameters (theta):", theta)

"""Report Learning Rate, Stopping Criteria, and Final Parameters"""

learning_rate = 0.01
stopping_criteria = "Change in cost &lt; 1e-6"
print(f"Learning Rate: {learning_rate}")
print(f"Stopping Criteria: {stopping_criteria}")
print(f"Final Parameters (theta): {theta}")

"""Plot Data and Hypothesis Function"""

plt.scatter(linearX, linearY, label="Data Points")
x_values = np.linspace(min(linearX), max(linearX), 100)
y_values = theta[0] + theta[1] * x_values
plt.plot(x_values, y_values, color="red", label="Hypothesis Function")
plt.xlabel("Acidity (X)")
plt.ylabel("Density (Y)")
plt.title("Linear Regression Fit")
plt.legend()
plt.show()

""" 3D Mesh Plot of Error Function"""

def plot_3d_mesh(X, y, theta_history):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection="3d")


    theta0_vals = np.linspace(0, 20, 100)
    theta1_vals = np.linspace(0, 50, 100)
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    cost_grid = np.zeros_like(theta0_grid)


    for i in range(theta0_vals.size):
        for j in range(theta1_vals.size):
            theta_temp = np.array([theta0_vals[i], theta1_vals[j]])
            y_pred = np.c_[np.ones(X.shape[0]), X] @ theta_temp
            cost_grid[i, j] = np.sum((y_pred - y)**2) / (2 * y.size)


    surf = ax.plot_surface(theta0_grid, theta1_grid, cost_grid, cmap=cm.coolwarm, alpha=0.8)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match122-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)


    theta0_history = [theta[0] for theta in theta_history]
    theta1_history = [theta[1] for theta in theta_history]
    cost_history = [np.sum((np.c_[np.ones(X.shape[0]), X] @ theta - y)**2) / (2 * y.size) for theta in theta_history]
</FONT>    ax.scatter(theta0_history, theta1_history, cost_history, color="black", s=50, label="Gradient Descent Path")

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Cost (J)")
    plt.title("3D Mesh of Error Function")
    plt.legend()
    plt.show()


theta_history = [np.array([0, 0])]
for cost in cost_history:
    theta_history.append(model.theta.copy())


plot_3d_mesh(linearX, linearY, theta_history)

"""Dynamic Error Plotting on 3D Mesh"""

def plot_3d_mesh_dynamic(X, y, theta_history, cost_history):
    """
    Dynamically plot the 3D mesh of the error function and the gradient descent path.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
    y : numpy array of shape (n_samples,)
        The target values.
    theta_history : list of numpy arrays
        The history of parameters (theta) during gradient descent.
    cost_history : list of floats
        The history of cost values during gradient descent.
    """
    fig = plt.figure()
    ax = fig.add_subplot(111, projection="3d")


    theta0_vals = np.linspace(0, 20, 100)
    theta1_vals = np.linspace(0, 50, 100)
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    cost_grid = np.zeros_like(theta0_grid)


    for i in range(theta0_vals.size):
        for j in range(theta1_vals.size):
            theta_temp = np.array([theta0_vals[i], theta1_vals[j]])
            y_pred = np.c_[np.ones(X.shape[0]), X] @ theta_temp
            cost_grid[i, j] = np.sum((y_pred - y)**2) / (2 * y.size)


    surf = ax.plot_surface(theta0_grid, theta1_grid, cost_grid, cmap=cm.coolwarm, alpha=0.8)
    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)


    scatter = ax.scatter([], [], [], color="black", s=50, label="Gradient Descent Path")


    def update(i):
        scatter._offsets3d = (theta_history[i][0:1], theta_history[i][1:2], cost_history[i:i+1])
        ax.set_title(f"Iteration {i}: Cost = {cost_history[i]:.4f}")
        return scatter,


    ani = FuncAnimation(fig, update, frames=len(theta_history), interval=200, blit=True)

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Cost (J)")
    plt.legend()
    plt.show()


def calculate_iterations_to_converge(cost_history, tol=1e-6):
    """
    Calculate the number of iterations needed for convergence.

    Parameters
    ----------
    cost_history : list of floats
        The history of cost values during gradient descent.
    tol : float
        Tolerance for stopping criteria (change in cost).

    Returns
    -------
    num_iterations : int
        The number of iterations needed to converge.
    """
    for i in range(1, len(cost_history)):
        if abs(cost_history[i] - cost_history[i-1]) &lt; tol:
            return i + 1
    return len(cost_history)


num_iterations_to_converge = calculate_iterations_to_converge(cost_history)
print(f"Number of iterations to converge: {num_iterations_to_converge}")


plot_3d_mesh_dynamic(linearX, linearY, theta_history[:num_iterations_to_converge], cost_history[:num_iterations_to_converge])

""" Contour Plot of Error Function"""

def plot_contours(X, y, theta_history, learning_rate=None):
    plt.figure()


    theta0_vals = np.linspace(0, 20, 100)
    theta1_vals = np.linspace(0, 50, 100)
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    cost_grid = np.zeros_like(theta0_grid)


    for i in range(theta0_vals.size):
        for j in range(theta1_vals.size):
            theta_temp = np.array([theta0_vals[i], theta1_vals[j]])
            y_pred = np.c_[np.ones(X.shape[0]), X] @ theta_temp
            cost_grid[i, j] = np.sum((y_pred - y)**2) / (2 * y.size)


    plt.contourf(theta0_grid, theta1_grid, cost_grid, levels=50, cmap=cm.coolwarm)
    plt.colorbar()


    theta0_history = [theta[0] for theta in theta_history]
    theta1_history = [theta[1] for theta in theta_history]
    plt.plot(theta0_history, theta1_history, color="black", marker="o", label="Gradient Descent Path")


    plt.xlabel("Theta 0")
    plt.ylabel("Theta 1")


    if learning_rate is not None:
        plt.title(f"Contours of Error Function (Learning Rate = {learning_rate})")
    else:
        plt.title("Contours of Error Function")

    plt.legend()
    plt.show()

plot_contours(linearX, linearY, theta_history ,learning_rate=0.01)

"""Contour Plots for Different Learning Rates"""

learning_rates = [0.001, 0.025, 0.1]
for lr in learning_rates:
    model = LinearRegressor()
    theta, cost_history = model.fit(linearX, linearY, learning_rate=lr)
    theta_history = [np.array([0, 0])]
    for cost in cost_history:
        theta_history.append(model.theta.copy())
    plot_contours(linearX, linearY, theta_history, lr)





# -*- coding: utf-8 -*-
"""Question 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyfqRHWyIWQdI1Y9yHr9_pAPB6aFKafZ

# Question 2

Starter code
"""

<A NAME="5"></A><FONT color = #FF0000><A HREF="match122-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import matplotlib.pyplot as plt



def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
</FONT>    """

    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)


    X = np.column_stack((np.ones(N), x1, x2))


    noise = np.random.normal(0, noise_sigma, N)
    y = X @ theta + noise

    return X[:, 1:], y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None

    def fit(self, X, y, learning_rate=0.01, batch_size=1, max_epochs=1000, tol=1e-5):
        """
        Fit the linear regression model using Stochastic Gradient Descent.
        """

        X = np.column_stack((np.ones(X.shape[0]), X))


<A NAME="2"></A><FONT color = #0000FF><A HREF="match122-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.theta = np.zeros(X.shape[1])
        theta_history = [self.theta.copy()]


        for epoch in range(max_epochs):

            indices = np.arange(X.shape[0])
</FONT>            np.random.shuffle(indices)
            X_shuffled = X[indices]
            y_shuffled = y[indices]


            for i in range(0, X.shape[0], batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]


                y_pred = X_batch @ self.theta
                error = y_pred - y_batch
                gradient = X_batch.T @ error / batch_size


                self.theta -= learning_rate * gradient


            theta_history.append(self.theta.copy())


            if np.linalg.norm(theta_history[-1] - theta_history[-2]) &lt; tol:
                break

        return theta_history

    def predict(self, X):
        """
        Predict target values for input data.
        """

        X = np.column_stack((np.ones(X.shape[0]), X))
        return X @ self.theta

"""Data Generation and Splitting"""

N = 1000000
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)


X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)


split_idx = int(0.8 * N)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print("Train set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)

"""Stochastic Gradient Descent"""

model = StochasticLinearRegressor()


batch_sizes = [1, 80, 8000, 800000]
learning_rate = 0.001
theta_histories = {}


for r in batch_sizes:
    print(f"Training with batch size = {r}")
    theta_history = model.fit(X_train, y_train, learning_rate=learning_rate, batch_size=r)
    theta_histories[r] = theta_history
    print(f"Learned theta: {model.theta}\n")

"""Comparison with Closed Form Solution"""

X_train_with_intercept = np.column_stack((np.ones(X_train.shape[0]), X_train))
theta_closed_form = np.linalg.inv(X_train_with_intercept.T @ X_train_with_intercept) @ X_train_with_intercept.T @ y_train
print("Closed-form theta:", theta_closed_form)


for r in batch_sizes:
    print(f"SGD theta (batch size = {r}): {theta_histories[r][-1]}")

"""Mean Squared Error"""

def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)


y_train_pred = model.predict(X_train)
train_error = mean_squared_error(y_train, y_train_pred)
print("Training error:", train_error)


y_test_pred = model.predict(X_test)
test_error = mean_squared_error(y_test, y_test_pred)
print("Test error:", test_error)

"""Parameter Space Visualization"""

from mpl_toolkits.mplot3d import Axes3D


fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')


for r in batch_sizes:
    theta_history = np.array(theta_histories[r])
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], label=f"Batch Size = {r}")


ax.set_xlabel("Theta 0")
ax.set_ylabel("Theta 1")
ax.set_zlabel("Theta 2")
ax.legend()
plt.title("Parameter Movement in 3D Space")
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match122-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()



# -*- coding: utf-8 -*-
"""Question3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16zA6RZIc6pqQ2pXDxMKMJBcivUEE2cIF
</FONT>
# Question 3

Starter Code Implementation
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd



class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std = None

    def _sigmoid(self, z):
        """Sigmoid function."""
        return 1 / (1 + np.exp(-z))

<A NAME="0"></A><FONT color = #FF0000><A HREF="match122-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def _normalize(self, X):
        """Normalize the input data."""
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std

    def fit(self, X, y, learning_rate=0.01, max_iter=1000, tol=1e-6):
</FONT>        """
        Fit the logistic regression model using Newton's Method.
        """

        X_normalized = self._normalize(X)


        X_normalized = np.column_stack((np.ones(X_normalized.shape[0]), X_normalized))


        self.theta = np.zeros(X_normalized.shape[1])


        for i in range(max_iter):

            z = X_normalized @ self.theta
            h = self._sigmoid(z)


            gradient = X_normalized.T @ (h - y) / y.size


            diag = h * (1 - h)
            H = (X_normalized.T * diag) @ X_normalized / y.size


            delta = np.linalg.inv(H) @ gradient
            self.theta -= learning_rate * delta


            if np.linalg.norm(delta) &lt; tol:
                break

    def predict(self, X):
        """
        Predict the target values for the input data.
        """

        X_normalized = (X - self.mean) / self.std


        X_normalized = np.column_stack((np.ones(X_normalized.shape[0]), X_normalized))


        z = X_normalized @ self.theta
        return (self._sigmoid(z) &gt;= 0.5).astype(int)

"""Load Data and Fit Logistic Regression"""

logisticX = pd.read_csv("logisticX.csv", header=None).values
logisticY = pd.read_csv("logisticY.csv", header=None).values.flatten()


model = LogisticRegressor()
model.fit(logisticX, logisticY)


print("Learned coefficients (theta):", model.theta)

"""Training Data and Decision Boundary"""

plt.figure(figsize=(10, 6))
plt.scatter(logisticX[logisticY == 0, 0], logisticX[logisticY == 0, 1], color="blue", label="Class 0")
plt.scatter(logisticX[logisticY == 1, 0], logisticX[logisticY == 1, 1], color="red", label="Class 1")


x1_min, x1_max = logisticX[:, 0].min(), logisticX[:, 0].max()
x2_min, x2_max = logisticX[:, 1].min(), logisticX[:, 1].max()
x1 = np.linspace(x1_min, x1_max, 100)
x2 = -(model.theta[0] + model.theta[1] * (x1 - model.mean[0]) / model.std[0]) * model.std[1] / model.theta[2] + model.mean[1]
plt.plot(x1, x2, color="green", label="Decision Boundary")


plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Training Data and Decision Boundary")
plt.legend()
<A NAME="1"></A><FONT color = #00FF00><A HREF="match122-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()



# -*- coding: utf-8 -*-
"""Question4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsMKL31AW6vXXkwD6PpgPHQeFTviuwmS
</FONT>
# Question 4

Starter Code
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.mean = None
        self.std = None

    def _normalize(self, X):
        """Normalize the input data."""
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        """

        X_normalized = self._normalize(X)


        y_binary = np.where(y == "Canada", 1, 0)


        X_0 = X_normalized[y_binary == 0]
        X_1 = X_normalized[y_binary == 1]


        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        if assume_same_covariance:

            self.sigma = ((X_0 - self.mu_0).T @ (X_0 - self.mu_0) +
                          (X_1 - self.mu_1).T @ (X_1 - self.mu_1)) / X_normalized.shape[0]
            return self.mu_0, self.mu_1, self.sigma
        else:

            self.sigma_0 = (X_0 - self.mu_0).T @ (X_0 - self.mu_0) / X_0.shape[0]
            self.sigma_1 = (X_1 - self.mu_1).T @ (X_1 - self.mu_1) / X_1.shape[0]
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X, assume_same_covariance=False):
        """
        Predict the target values for the input data.
        """

        X_normalized = (X - self.mean) / self.std

        if assume_same_covariance:

            inv_sigma = np.linalg.inv(self.sigma)
            delta_0 = X_normalized @ inv_sigma @ self.mu_0 - 0.5 * self.mu_0.T @ inv_sigma @ self.mu_0
            delta_1 = X_normalized @ inv_sigma @ self.mu_1 - 0.5 * self.mu_1.T @ inv_sigma @ self.mu_1
        else:

            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            delta_0 = -0.5 * np.sum((X_normalized - self.mu_0) @ inv_sigma_0 * (X_normalized - self.mu_0), axis=1)
            delta_1 = -0.5 * np.sum((X_normalized - self.mu_1) @ inv_sigma_1 * (X_normalized - self.mu_1), axis=1)


        return np.where(delta_1 &gt; delta_0, "Canada", "Alaska")

"""Load Data and Fit GDA with Shared Covariance"""

q4x = pd.read_csv("q4x.dat", header=None, delim_whitespace=True).values
q4y = pd.read_csv("q4y.dat", header=None).values.flatten()


model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = model.fit(q4x, q4y, assume_same_covariance=True)


print("Mean for Alaska (mu_0):", mu_0)
print("Mean for Canada (mu_1):", mu_1)
print("Shared Covariance Matrix (sigma):\n", sigma)

"""Training Data Plot"""

plt.figure(figsize=(10, 6))
plt.scatter(q4x[q4y == "Alaska", 0], q4x[q4y == "Alaska", 1], color="blue", label="Alaska", marker="o")
plt.scatter(q4x[q4y == "Canada", 0], q4x[q4y == "Canada", 1], color="red", label="Canada", marker="o")


plt.xlabel("x1 (Freshwater Growth Ring Diameter)")
plt.ylabel("x2 (Marine Growth Ring Diameter)")
plt.title("Training Data")
plt.legend()
plt.show()

"""Decision Boundary for Shared Covariance"""

q4x_normalized = (q4x - model.mean) / model.std


plt.figure(figsize=(10, 6))
plt.scatter(q4x[q4y == "Alaska", 0], q4x[q4y == "Alaska", 1], color="blue", label="Alaska", marker="o")
plt.scatter(q4x[q4y == "Canada", 0], q4x[q4y == "Canada", 1], color="red", label="Canada", marker="o")


Sigma_inv = np.linalg.inv(sigma)
w = Sigma_inv @ (mu_1 - mu_0)
b = -0.5 * (mu_1 @ Sigma_inv @ mu_1 - mu_0 @ Sigma_inv @ mu_0) + np.log(len(q4y[q4y == "Canada"]) / len(q4y[q4y == "Alaska"]))  # Bias term


x_values_normalized = np.linspace(q4x_normalized[:, 0].min(), q4x_normalized[:, 0].max(), 100)
y_values_normalized = -(w[0] * x_values_normalized + b) / w[1]


x_values_original = x_values_normalized * model.std[0] + model.mean[0]
y_values_original = y_values_normalized * model.std[1] + model.mean[1]


plt.plot(x_values_original, y_values_original, color="green", label="Decision Boundary")


plt.xlabel("x1 (Freshwater Growth Ring Diameter)")
plt.ylabel("x2 (Marine Growth Ring Diameter)")
plt.title("Training Data and Linear Decision Boundary (Shared Covariance)")
plt.legend()
plt.show()

"""Fit GDA with Separate Covariances"""

mu_0, mu_1, sigma_0, sigma_1 = model.fit(q4x, q4y, assume_same_covariance=False)


print("Mean for Alaska (mu_0):", mu_0)
print("Mean for Canada (mu_1):", mu_1)
print("Covariance Matrix for Alaska (sigma_0):\n", sigma_0)
print("Covariance Matrix for Canada (sigma_1):\n", sigma_1)

"""Quadratic Decision Boundary"""

plt.figure(figsize=(10, 6))
plt.scatter(q4x[q4y == "Alaska", 0], q4x[q4y == "Alaska", 1], color="blue", label="Alaska")
plt.scatter(q4x[q4y == "Canada", 0], q4x[q4y == "Canada", 1], color="red", label="Canada")


x1 = np.linspace(q4x[:, 0].min(), q4x[:, 0].max(), 100)
x2 = np.linspace(q4x[:, 1].min(), q4x[:, 1].max(), 100)
X1, X2 = np.meshgrid(x1, x2)
Z = model.predict(np.column_stack((X1.ravel(), X2.ravel())), assume_same_covariance=False)
Z = np.where(Z == "Canada", 1, 0).reshape(X1.shape)
plt.contour(X1, X2, Z, levels=[0.5], colors="green", label="Quadratic Decision Boundary")


plt.xlabel("x1 (Freshwater Growth Ring Diameter)")
plt.ylabel("x2 (Marine Growth Ring Diameter)")
plt.title("Training Data and Quadratic Decision Boundary")
plt.legend()
plt.show()

"""Analysis of Boundaries"""

print("Analysis:")
print("1. The linear boundary assumes equal covariance for both classes, resulting in a straight line.")
print("2. The quadratic boundary allows for different covariances, resulting in a more flexible curve, it fits the data better, especially when the class distributions are not symmetric.")



</PRE>
</PRE>
</BODY>
</HTML>
