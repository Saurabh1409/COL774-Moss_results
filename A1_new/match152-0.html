<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_5PC78.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_5PC78.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.lossList = []
    
    def lossCalculator(self,X,y,theta):
        m = len(y)
        if theta.shape[0]==X.shape[1]+1:
            X=np.column_stack((np.ones(m),X))
        predictedLabels = X.dot(theta)
        error = y-predictedLabels
        loss = (1/(2*m)) * np.sum(error**2)
        return loss
        

    def fit(self, X, y, learning_rate=0.025):
    
        self.theta = np.zeros((X.shape[1]+1,1))
        m = len(y)
        X=np.column_stack((np.ones(m),X))
        y = y.reshape(-1,1)

        maxIters= 10000
        paramList = []

        tolerance = 1e-5
        for i in range(maxIters):
            gradient = (1 / m) * X.transpose().dot((X.dot(self.theta)-y))
            self.theta -= learning_rate * gradient
            loss = self.lossCalculator(X, y,self.theta)
            
            paramList.append(self.theta.flatten())
            self.lossList.append(loss)

            if(i&gt;0 and abs(self.lossList[-2]-self.lossList[-1])&lt;tolerance):
                break
        
        paramArr = np.array(paramList)
        return paramArr
        

    
    def predict(self, X):

        if self.theta.shape[0]==X.shape[1]+1:
            X=np.column_stack((np.ones(X.shape[0]),X))
        yPred = X.dot(self.theta)
        return yPred
    
    def plotDataAndHypothesisFunc(self,X,y):
        indices = np.arange(0, len(y), 10)
        X = np.column_stack((np.ones(X.shape[0]), X))
        XSub = X[indices]
        ySub = y[indices]

        plt.figure(figsize=(24, 15))
        plt.scatter(XSub[:, 1], ySub, color='blue', label="Training Data") 
<A NAME="5"></A><FONT color = #FF0000><A HREF="match152-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.plot(X[:, 1], X.dot(self.theta), color='red', label="Hypothesis (hθ(x))") 
        plt.xlabel("Feature (x)")
        plt.ylabel("Target (y)")
        plt.title("Data and Learned Hypothesis")
        plt.legend()
        plt.show()
    
    def plot3d(self,params,theta0_vals,theta1_vals,J_vals):
</FONT>        fig = plt.figure(figsize=(20, 14))
        ax = fig.add_subplot(111, projection='3d')

        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
        ax.plot_surface(T0, T1, J_vals.T, cmap='coolwarm', alpha=0.7)


        J_history= self.lossList
        for i in range(len(params)):
            ax.scatter(params[i, 0], params[i, 1], J_history[i], color='red', s=50)
            plt.pause(0.2)  

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'Cost Function $J(\theta)$')
        ax.set_title('3D Visualization of Cost Function and Gradient Descent Path')
        plt.show()

    def plot_contour(self, params, theta_vals, J_vals):
        plt.figure(figsize=(24, 18))

        # Plot the contour map
        plt.contour(theta_vals, theta_vals, J_vals, levels=np.logspace(-2, 3, 20), cmap="viridis")
        plt.xlabel(r'$\theta_0$')
        plt.ylabel(r'$\theta_1$')
        plt.title("Contour Plot of Cost Function J(θ)")

        # Overlay gradient descent path
        params = np.array(params)  # Ensure it's a NumPy array
        for i in range(len(params)):
            plt.scatter(params[i, 0], params[i, 1], color='red', s=50)
            plt.pause(0.02)  # Pause to visualize iteration

        plt.show()
    

def loadDataset(xFile, yFile):
    X = pd.read_csv(xFile, header=None).values
    y = pd.read_csv(yFile, header=None).values
    return X,y


def main():

    xFile='../data/Q1/linearX.csv'
    yFile='../data/Q1/linearY.csv'

    lr = LinearRegressor()


    X,y = loadDataset(xFile,yFile)
    params = lr.fit(X,y,0.001)



    df = pd.DataFrame(params)
    df.to_csv('data.csv')

    # lr.plotDataAndHypothesisFunc(X,y)

    theta0_vals = np.linspace(-2, 2, 100)
    theta1_vals = np.linspace(-1, 3, 100)
    theta_vals, theta_vals = np.meshgrid(theta0_vals, theta1_vals)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match152-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

    for i, t0 in enumerate(theta0_vals):
        for j, t1 in enumerate(theta1_vals):
            J_vals[i, j] = lr.lossCalculator(X, y, np.array([[t0], [t1]]))
</FONT>
    
    # lr.plot3d(params,theta0_vals,theta1_vals,J_vals)
    lr.plot_contour(params,theta_vals,J_vals)
    

main()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D 

def main():
    batchSizes = [1, 80, 8000, 800000]
    fileNames = {batch: f"Param{batch}.csv" for batch in batchSizes}

    thetaPaths = {batch: np.loadtxt(file, delimiter=",", usecols=range(1,4)) for batch, file in fileNames.items()}


    fig = plt.figure(figsize=(20, 15))
    ax = fig.add_subplot(111, projection='3d')

    colors = ['r', 'g', 'b', 'purple']


    for i, batchSize in enumerate(batchSizes):
        thetaHistory = thetaPaths[batchSize]
        ax.plot(thetaHistory[:, 0], thetaHistory[:, 1], thetaHistory[:, 2], 
                color=colors[i], label=f'Batch {batchSize}')


    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title("Movement of θ in 3D Parameter Space")
    ax.legend()
    plt.show()

main()



# Imports - you can add any other permitted libraries
import numpy as np 
import pandas as pd
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):

    x1 = np.random.normal(input_mean[0],input_sigma[0],N)
    x2 = np.random.normal(input_mean[1],input_sigma[1],N)

    noise = np.random.normal(0,noise_sigma,N)

    X = np.column_stack((x1,x2))
    Xcalc = np.column_stack((np.ones(N),x1,x2))

    y = Xcalc.dot(theta)+noise

    return X,y


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batchSize = 1

    def lossCalculator(self,X,y):
        m = len(y)
        
        if self.theta.shape[0]==X.shape[1]+1:
            X=np.column_stack((np.ones(X.shape[0]),X))
        predictedLabels = X.dot(self.theta)
        error = y-predictedLabels
        loss = (1/(2*m)) * np.sum(error**2)
        return loss
    
    def fit(self, X, y, learning_rate=0.001):

        self.theta = np.zeros((X.shape[1]+1,1))
        m = len(y)
        X=np.column_stack((np.ones(m),X))
        y = y.reshape(-1,1)

        maxIters= 1000
        paramList = []
        lossList = []
        tolerance = 1e-5
        delta = 1e-2


        for epoch in range(maxIters):

            inds = np.random.permutation(m)
            XPrime = X[inds]
            yPrime = y[inds]

            gradient = None

            for i in range(0,m,self.batchSize):
                XBatch = XPrime[i:i+self.batchSize]
                yBatch = yPrime[i:i+self.batchSize]

                gradient = (1 / self.batchSize) * XBatch.transpose().dot((XBatch.dot(self.theta)-yBatch))
                
                self.theta -= learning_rate * gradient


            loss = self.lossCalculator(XPrime, yPrime)
            
            paramList.append(self.theta.flatten())
            lossList.append(loss)

            if(epoch&gt;0 and abs(lossList[-2]-lossList[-1])&lt;tolerance):
                break

            if np.linalg.norm(gradient) &lt; delta:
                break



        # print(lossList)
        paramArr = np.array(paramList)
        return paramArr

    def closedForm(self,X,y):
        X=np.column_stack((np.ones(X.shape[0]),X))
        cfTheta = np.linalg.inv(X.transpose() @ X) @ X.transpose() @ y
        return cfTheta
    
    def predict(self, X):
    
        if self.theta.shape[0]==X.shape[1]+1:
            X=np.column_stack((np.ones(X.shape[0]),X))
        yPred = X.dot(self.theta)
        return yPred
    


def main():
    theta = np.array([3,1,2])
    inputMean = [3,-1]
    inputSigma =[2,2]
    X,y = generate(1000000,theta,inputMean,inputSigma,np.sqrt(2))

    m = X.shape[0]
    inds = np.random.permutation(m)
    XPrime = X[inds]
    yPrime = y[inds]

    limitInd = int(0.8*m)

    XTraining,yTraining = XPrime[:limitInd], yPrime[:limitInd]
    XTest,yTest = XPrime[limitInd:], yPrime[limitInd:]


    sgd = StochasticLinearRegressor()

    start = time.time()

    paramList = sgd.fit(XTraining,yTraining)

    end = time.time()

    print('Time taken was : ', end-start)
    # predLab = sgd.predict(XTraining)
    # predLabTest = sgd.predict(XTest)


    df = pd.DataFrame(paramList)
    df.to_csv(f'Param{sgd.batchSize}.csv')

    # print(predLab)

    # print(sgd.closedForm(XTraining,yTraining))

    yTraining=yTraining.reshape((-1,1))
    yTest=yTest.reshape((-1,1))
    # dfLabelsTraining = pd.DataFrame(np.hstack((predLab, yTraining)), columns=["Predicted Labels", "Actual Labels"])
    # dfLabelsTesting = pd.DataFrame(np.hstack((predLabTest, yTest)), columns=["Predicted Labels", "Actual Labels"])

    # dfLabelsTraining.to_csv("trainingDataPredictions.csv")
    # dfLabelsTesting.to_csv("testingDataPredictions.csv")

    # lossTraining = sgd.lossCalculator(XTraining,yTraining)
    # lossTesting = sgd.lossCalculator(XTest,yTest)

    # print('Loss in Training',lossTraining)
    # print('Loss in Testing',lossTesting)


main()






# Imports - you can add any other permitted libraries
<A NAME="2"></A><FONT color = #0000FF><A HREF="match152-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        pass

    def normalise(self, X):
</FONT>        XMean = np.mean(X,axis=0)
        XDev = np.std(X,axis=0)

        XNormalised = (X-XMean)/XDev

        return XNormalised
    
    def lossCalculator(self,X,y):
        hTheta = self.predict(X)
        loss = -np.sum(y*np.log(hTheta)+(1-y)*np.log(1-hTheta))
        return loss
    
    def fit(self, X, y, learning_rate=0.01):
        XNorm = self.normalise(X)
        self.theta = np.zeros((X.shape[1]+1,1))
        m = len(y)
        X=np.column_stack((np.ones(m),X))
        XNorm=np.column_stack((np.ones(m),XNorm))
        y = y.reshape(-1,1)

        maxIters= 1000
        paramList = []
        lossList = []
        delta = 1e-6
        for i in range(maxIters):
            hTheta = self.predict(XNorm)
            gradient = XNorm.transpose() @ (y-hTheta)
            diagMat = np.diag((hTheta*(1-hTheta)).flatten())
            hessian = -XNorm.transpose() @ diagMat @ XNorm

            self.theta -= np.linalg.inv(hessian) @ gradient
            loss = self.lossCalculator(XNorm, y)
            
            paramList.append(self.theta.flatten())
            lossList.append(loss)

            # if(i&gt;0 and abs(lossList[-2]-lossList[-1])&lt;tolerance):
            #     break
            if np.linalg.norm(gradient) &lt; delta:
                break

        paramArr = np.array(paramList)
        return paramArr



    
    def predict(self, X):

        #I am assuming that the predict function will recieve the normalised input X

        if self.theta.shape[0]==X.shape[1]+1:
            X=np.column_stack((np.ones(X.shape[0]),X))
        predLabel = 1/(1+np.exp(-1*(X.dot(self.theta))))
        return predLabel
        

def loadDataset(xFile, yFile):
    X = pd.read_csv(xFile, header=None).values
    y = pd.read_csv(yFile, header=None).values
    return X,y

def plot_decision_boundary(theta, X, y):
    plt.figure(figsize=(20, 15))  


    y = y.ravel()


    class_0 = (y == 0)
    class_1 = (y == 1)


    plt.scatter(X[class_0, 0], X[class_0, 1], label="Class 0", color='red', marker='o')
    plt.scatter(X[class_1, 0], X[class_1, 1], label="Class 1", color='blue', marker='x')


    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    

    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]  

    plt.plot(x1_vals, x2_vals, label="Decision Boundary", color='black', linestyle="--")

    plt.xlabel("Feature x1")
    plt.ylabel("Feature x2")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.show()

def main():
    xFile='../data/Q3/logisticX.csv'
    yFile='../data/Q3/logisticY.csv'


    X,y = loadDataset(xFile,yFile)

    logReg = LogisticRegressor()
    params = logReg.fit(X,y)
    df = pd.DataFrame(params)
    df.to_csv('param.csv')


    X = logReg.normalise(X)

    plot_decision_boundary(logReg.theta,X,y)

    # yPred = logReg.predict(X)
    # predDF = pd.DataFrame(np.hstack((yPred,y)),columns=["Predicted Labels", "Actual Labels"])
    # predDF.to_csv('prediction.csv')

main()



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.theta= None
        self.sameCov = False
        self.mu0= None
        self.mu1= None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
        self.phi = None


    def print_gda_parameters(self):
        """Prints the parameters of the GDA model."""

        print("GDA Model Parameters:")

        if self.theta is not None:
            print("Theta (Weights):")
            print(self.theta)
        else:
            print("Theta (Weights): Not trained yet.")


        print(f"Same Covariance: {self.sameCov}")

        if self.mu0 is not None:
            print(f"Mu (Class 0): {self.mu0}")
        else:
            print("Mu (Class 0): Not trained yet.")

        if self.mu1 is not None:
            print(f"Mu (Class 1): {self.mu1}")
        else:
            print("Mu (Class 1): Not trained yet.")

        if self.sigma is not None:
            print(f"Sigma (Shared Covariance): {self.sigma}")
        else:
            print("Sigma (Shared Covariance): Not applicable (different covariance matrices used).")

        if self.sigma0 is not None:
            print(f"Sigma (Class 0): {self.sigma0}")
        else:
            print("Sigma (Class 0): Not trained yet.")

        if self.sigma1 is not None:
            print(f"Sigma (Class 1): {self.sigma1}")
        else:
            print("Sigma (Class 1): Not trained yet.")

        if self.phi is not None:
            print(f"Phi (Class Proportion): {self.phi}")
        else:
            print("Phi (Class Proportion): Not trained yet.")



    def normalise(self, X):
        XMean = np.mean(X,axis=0)
        XDev = np.std(X,axis=0)

        XNormalised = (X-XMean)/XDev

        return XNormalised
    
    def yConvert(self, y):
        y = y.flatten()
        labelMap = {"Alaska": 0, "Canada": 1}
        yPrime = np.array([labelMap[label] for label in y.tolist()])
        return yPrime
    
    def fit(self, X, y, assume_same_covariance=False):
        X = self.normalise(X)
        self.sameCov=assume_same_covariance
        self.theta= np.zeros((X.shape[1],1))
        m = len(y)

        self.phi = np.mean(y)

        self.mu0 = np.mean(X[y==0],axis=0)
        self.mu1 = np.mean(X[y==1],axis=0)

        

        if self.sameCov:
            self.sigma = np.cov(X.transpose(), bias=True)
            return self.mu0,self.mu1,self.sigma
        
        else :
            X0 = X[y == 0] - self.mu0  
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match152-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            X1 = X[y == 1] - self.mu1  
            
            self.sigma0 = (X0.transpose() @ X0) / len(X0)
            self.sigma1 = (X1.transpose() @ X1) / len(X1)
</FONT>
            return self.mu0, self.mu1,self.sigma0,self.sigma1

    
    def predict(self, X):
        #I am assuming that the predict function will recieve the normalised input X


        if self.sameCov:
<A NAME="0"></A><FONT color = #FF0000><A HREF="match152-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            sigmaInv = np.linalg.inv(self.sigma)
            theta = sigmaInv @ (self.mu1 - self.mu0)
            theta0 = -0.5 * (self.mu1.transpose() @ sigmaInv @ self.mu1 - self.mu0.transpose() @ sigmaInv @ self.mu0) + np.log(self.phi / (1 - self.phi))
</FONT>            self.theta = np.array([theta0,*theta.flatten()]).reshape((-1,1))
            print(self.theta)

            if self.theta.shape[0]==X.shape[1]+1:
                XPrime=np.column_stack((np.ones(X.shape[0]),X))
            predLabel = (XPrime.dot(self.theta)&gt;0).astype(int)

            return predLabel
        
        else : 
            pass

def loadDataset(xFile, yFile):
    X = np.loadtxt(xFile)
    y = pd.read_csv(yFile, delimiter=r"\s+", header=None, dtype=str).values
    return X,y

<A NAME="1"></A><FONT color = #00FF00><A HREF="match152-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

def plotGDASameCov(X, y, mu_0, mu_1, sigma):
    plt.figure(figsize=(24, 18))


    class_0 = y == 0  
    class_1 = y == 1  
    
    plt.scatter(X[class_0, 0], X[class_0, 1], label="Alaska", color='red', marker='o')
</FONT>    plt.scatter(X[class_1, 0], X[class_1, 1], label="Canada", color='blue', marker='x')


    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, (mu_1 - mu_0))
    b = -0.5 * np.dot(mu_1.transpose(), np.dot(sigma_inv, mu_1)) + 0.5 * np.dot(mu_0.transpose(), np.dot(sigma_inv, mu_0))

    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_vals = -(w[0] * x1_vals + b) / w[1]  


    plt.plot(x1_vals, x2_vals, label="Decision Boundary", color='black', linestyle="--")

    plt.xlabel("Feature x1")
    plt.ylabel("Feature x2")
    plt.legend()
    plt.title("Gaussian Discriminant Analysis (GDA) Decision Boundary")
    plt.show()

def main():

    xFile='../data/Q4/q4x.dat'
    yFile='../data/Q4/q4y.dat'


    X,y = loadDataset(xFile,yFile)


    gda = GaussianDiscriminantAnalysis()
    y = gda.yConvert(y)
    # mu0,mu1,sigma = gda.fit(X,y,True)
    mu0,mu1,sigma1,sigma2 = gda.fit(X,y,False)

    X = gda.normalise(X)
    # plotGDASameCov(X,y,mu0,mu1,sigma)
    gda.print_gda_parameters()

main()





</PRE>
</PRE>
</BODY>
</HTML>
