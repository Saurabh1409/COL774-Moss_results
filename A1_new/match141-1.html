<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_EU2KU.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_K8RRK.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class LinearRegressor:
    def __init__(self):
        self.theta = None

    def fit(self, X, y, learning_rate=0.01):
        X = np.atleast_2d(X).reshape(-1, 1)
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))
        self.theta = np.zeros(n + 1)
        cost_history = []

        prev_cost = float('inf')
        tol = 1e-6

        for _ in range(10000):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match141-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            predictions = X.dot(self.theta)
            errors = predictions - y
            gradient = (1 / m) * X.T.dot(errors)
            self.theta -= learning_rate * gradient
            cost = (1 / (2 * m)) * np.sum(errors ** 2)
</FONT>            cost_history.append((self.theta.copy(), cost))
            if abs(prev_cost - cost) &lt; tol:
                break
            prev_cost = cost
        return cost_history

    def predict(self, X):
        X = np.atleast_2d(X).reshape(-1, 1)
        if self.theta is None:
            raise ValueError("Model is not trained yet. Call fit first.")
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return X.dot(self.theta)

def plot_data_and_hypothesis(X, y, theta):
    X = np.atleast_2d(X).reshape(-1, 1)
    plt.scatter(X, y, color='blue', label='Data Points')
    plt.plot(X, np.hstack((np.ones((X.shape[0], 1)), X)).dot(theta), color='red', label='Hypothesis')
    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.legend()
    plt.title('Linear Regression Hypothesis')
    plt.show()

def plot_3d_error_surface(X, y, cost_history):
    X = np.atleast_2d(X).reshape(-1, 1)
    theta_0 = np.linspace(-10, 10, 100)
    theta_1 = np.linspace(-10, 10, 100)
    J = np.zeros((100, 100))
    for i, t0 in enumerate(theta_0):
        for j, t1 in enumerate(theta_1):
            t = np.array([t0, t1])
            predictions = np.hstack((np.ones((X.shape[0], 1)), X)).dot(t)
            J[i, j] = (1 / (2 * len(y))) * np.sum((predictions - y) ** 2)
    T0, T1 = np.meshgrid(theta_0, theta_1)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(T0, T1, J, cmap='viridis', alpha=0.7)
    thetas, costs = zip(*cost_history)
    ax.scatter([t[0] for t in thetas], [t[1] for t in thetas], costs, color='red')
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Cost')
    plt.show()

def plot_contours(X, y, cost_histories):
    X = np.atleast_2d(X).reshape(-1, 1)
    theta_0 = np.linspace(-10, 10, 100)
    theta_1 = np.linspace(-10, 10, 100)
    J = np.zeros((100, 100))
    for i, t0 in enumerate(theta_0):
        for j, t1 in enumerate(theta_1):
            t = np.array([t0, t1])
            predictions = np.hstack((np.ones((X.shape[0], 1)), X)).dot(t)
            J[i, j] = (1 / (2 * len(y))) * np.sum((predictions - y) ** 2)
    plt.contour(theta_0, theta_1, J, levels=np.logspace(-2, 3, 20), cmap='viridis')
    for cost_history in cost_histories:
        thetas, _ = zip(*cost_history)
        plt.plot([t[0] for t in thetas], [t[1] for t in thetas], label='Gradient Descent Path')
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.legend()
    plt.show()

def plot_learning_rate_comparison(X, y):
    X = np.atleast_2d(X).reshape(-1, 1)
    learning_rates = [0.001, 0.025, 0.1]
    cost_histories = []
    for lr in learning_rates:
        model = LinearRegressor()
        cost_histories.append(model.fit(X, y, learning_rate=lr))
    plot_contours(X, y, cost_histories)
    print("Observations:")
    print("- Lower learning rates converge slowly but steadily.")
    print("- Higher learning rates converge faster but may overshoot.")
    print("- A moderate learning rate balances speed and stability well.")

if __name__ == "__main__":
    X = np.loadtxt('linearX.csv', delimiter=',')
    y = np.loadtxt('linearY.csv', delimiter=',')

    X = np.atleast_2d(X).reshape(-1, 1)
    model = LinearRegressor()
    print("Training Linear Regression...")
    cost_history = model.fit(X, y)
    print("Learned Coefficients:")
    print(model.theta)

    plot_data_and_hypothesis(X, y, model.theta)
    plot_3d_error_surface(X, y, cost_history)
    plot_contours(X, y, [cost_history])
    plot_learning_rate_comparison(X, y)




import numpy as np
import matplotlib.pyplot as plt

# Q2: Sampling, Closed Form, and Stochastic Gradient Descent

# Part 1:
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(input_mean, input_sigma, size=(N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    X_with_intercept = np.hstack((np.ones((N, 1)), X))
    y = X_with_intercept.dot(theta) + noise
    return X, y

# Part 2:
class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None

    def fit(self, X, y, learning_rate=0.001):
<A NAME="2"></A><FONT color = #0000FF><A HREF="match141-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))
        self.theta = np.zeros(n + 1)
</FONT>        batch_sizes = [1, 80, 8000, 800000]
        self.results = {}

        for batch_size in batch_sizes:
            theta = np.zeros(n + 1)
            for epoch in range(10000):
                indices = np.random.permutation(m)
                X_shuffled, y_shuffled = X[indices], y[indices]
                for i in range(0, m, batch_size):
                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]
                    gradient = X_batch.T.dot(X_batch.dot(theta) - y_batch) / batch_size
                    theta -= learning_rate * gradient
            self.results[batch_size] = theta

        return self.results

    def predict(self, X):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return X.dot(self.theta)

# Part 3:
def closed_form_solution(X, y):
    X = np.hstack((np.ones((X.shape[0], 1)), X))

    print("Observation:")
    print("- Smaller batch sizes (e.g., 1) update parameters frequently and converge quickly but with more iterations.")
    print("- Larger batch sizes take fewer iterations but each iteration is computationally expensive.")
    print("- All batch sizes converge to similar values, but the convergence speed varies.")
    print("- The batch size of 80 provides a balance between speed and computational cost.")

    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

# Part 4:
def compute_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2) / 2

# Part 5:
def plot_parameter_updates(results):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    for batch_size, theta in results.items():
        ax.scatter(theta[0], theta[1], theta[2], label=f'Batch {batch_size}')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    plt.legend()
    plt.title('Parameter Updates for Different Batch Sizes')
    plt.show()

    print("Movement Analysis:")
    print("- Smaller batch sizes show a more erratic path due to frequent updates but converge steadily.")
    print("- Larger batch sizes exhibit smoother paths but take larger steps.")
    print("- The movement shapes align with expectations as smaller batches fluctuate more while large batches move consistently.")





import numpy as np
<A NAME="0"></A><FONT color = #FF0000><A HREF="match141-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_2.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt

# Q3: Logistic Regression

# Part 1,2:
class LogisticRegressor:
    def __init__(self):
        self.theta = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    def fit(self, X, y, learning_rate=0.01):
</FONT>        X = self.normalize(X)
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))
        self.theta = np.zeros(n + 1)

        for _ in range(100):
            h = self.sigmoid(X.dot(self.theta))
            gradient = X.T.dot(h - y)
            H = X.T.dot(np.diag(h * (1 - h))).dot(X)
            self.theta -= np.linalg.inv(H).dot(gradient)

        return self.theta

    def predict(self, X):
        X = self.normalize(X)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return (self.sigmoid(X.dot(self.theta)) &gt;= 0.5).astype(int)

    def plot_data_and_boundary(self, X, y):
        X_norm = self.normalize(X)
        plt.scatter(X_norm[y == 0][:, 0], X_norm[y == 0][:, 1], color='red', label='Class 0')
        plt.scatter(X_norm[y == 1][:, 0], X_norm[y == 1][:, 1], color='blue', label='Class 1')

        x1 = np.linspace(-2, 2, 100)
        x2 = -(self.theta[0] + self.theta[1] * x1) / self.theta[2]
        plt.plot(x1, x2, color='green', label='Decision Boundary')

        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend()
        plt.title('Logistic Regression Data and Decision Boundary')
        plt.show()

if __name__ == "__main__":
    X = np.loadtxt('logisticX.csv', delimiter=',')
    y = np.loadtxt('logisticY.csv', delimiter=',')

    model = LogisticRegressor()
    print("Training Logistic Regression...")
    model.fit(X, y)
    model.plot_data_and_boundary(X, y)
    print("Learned Coefficients:")
    print(model.theta)



import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None

    def normalize(self, X):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match141-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    def fit(self, X, y, assume_same_covariance=False):
</FONT>        X = self.normalize(X)
        X0 = X[y == 0]
        X1 = X[y == 1]

        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)

        if assume_same_covariance:
            self.sigma = np.cov(X.T)
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.cov(X0.T)
            self.sigma_1 = np.cov(X1.T)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        X = self.normalize(X)
        if self.sigma_0 is not None and self.sigma_1 is not None:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            decision = []
            for x in X:
                d0 = x.T @ inv_sigma_0 @ x - 2 * x.T @ inv_sigma_0 @ self.mu_0 + self.mu_0.T @ inv_sigma_0 @ self.mu_0
                d1 = x.T @ inv_sigma_1 @ x - 2 * x.T @ inv_sigma_1 @ self.mu_1 + self.mu_1.T @ inv_sigma_1 @ self.mu_1
                decision.append(int(d1 - d0 &gt; 0))
            return np.array(decision)
        else:
            linear_term = (X @ np.linalg.inv(self.sigma).dot(self.mu_1 - self.mu_0)) - 0.5 * (self.mu_1.T @ np.linalg.inv(self.sigma) @ self.mu_1 - self.mu_0.T @ np.linalg.inv(self.sigma) @ self.mu_0)
            return (linear_term &gt; 0).astype(int)

    def plot_data_and_boundary(self, X, y, assume_same_covariance=True):
        X = self.normalize(X)
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Alaska')
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match141-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Canada')
        x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
</FONT>        if assume_same_covariance:
            slope = np.linalg.inv(self.sigma).dot(self.mu_1 - self.mu_0)
            intercept = 0.5 * (self.mu_0.T @ np.linalg.inv(self.sigma) @ self.mu_0 - self.mu_1.T @ np.linalg.inv(self.sigma) @ self.mu_1)
            plt.plot(x_vals, (-slope[0] * x_vals - intercept) / slope[1], label='Linear Boundary', color='green')
        else:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            x1, x2 = np.meshgrid(np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100), np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100))
<A NAME="5"></A><FONT color = #FF0000><A HREF="match141-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            points = np.c_[x1.ravel(), x2.ravel()]
            Z = []
            for point in points:
                d0 = point.T @ inv_sigma_0 @ point - 2 * point.T @ inv_sigma_0 @ self.mu_0 + self.mu_0.T @ inv_sigma_0 @ self.mu_0
</FONT>                d1 = point.T @ inv_sigma_1 @ point - 2 * point.T @ inv_sigma_1 @ self.mu_1 + self.mu_1.T @ inv_sigma_1 @ self.mu_1
                Z.append(d1 - d0)
            Z = np.array(Z).reshape(x1.shape)
            plt.contour(x1, x2, Z, levels=[0], colors='purple', label='Quadratic Boundary')
        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend()
        plt.title('GDA Data with Linear and Quadratic Boundaries')
        plt.show()

    def analyze_boundaries(self):
        print("Analysis:")
        print("- Linear boundary is a straight line formed when both classes share covariance.")
        print("- Quadratic boundary is more flexible, adapting to varying spreads in each class.")
        print("- Linear boundary is simpler and computationally cheaper.")
        print("- Quadratic boundary is complex but better when class distributions differ significantly.")

if __name__ == "__main__":
    X = np.loadtxt('q4x.dat')
    y = np.loadtxt('q4y.dat', dtype=str)
    y = np.where(y == 'Alaska', 0, 1)

    gda = GaussianDiscriminantAnalysis()
    gda.fit(X, y, assume_same_covariance=True)
    gda.plot_data_and_boundary(X, y, assume_same_covariance=True)

    gda.fit(X, y, assume_same_covariance=False)
    gda.plot_data_and_boundary(X, y, assume_same_covariance=False)

    gda.analyze_boundaries()



</PRE>
</PRE>
</BODY>
</HTML>
