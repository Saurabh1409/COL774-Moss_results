<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_GR2T2.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_V56UV.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
from mpl_toolkits.mplot3d import Axes3D
import pickle
class LinearRegressor:
    def __init__(self):
        self.theta = None  
        self.history = []
        self.theta_history=np.array([])
 
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        max_iter=1000000
        tol=1e-6

        n_samples, n_features = X.shape
        X = np.hstack((np.ones((n_samples, 1)), X))
        
        self.theta = np.zeros(X.shape[1])
        
        for iteration in range(max_iter):
            y_pred = np.dot(X, self.theta)
            
            cost = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)
            
            self.history.append(cost)
            
            gradients = -(1 / n_samples) * np.dot(X.T, (y - y_pred))
            
            np.append(self.theta_history,self.theta.copy())
            self.theta -= learning_rate * gradients
            
            if iteration &gt; 0 and abs(self.history[-1] - self.history[-2]) &lt; tol:
                
                break
        print(f"Stopping at iteration {iteration}, Cost: {cost}")
        return self.theta_history
    def save_model(self, filename):
        """Save the model to a file."""
        model_data = {
            'theta': self.theta,
            'theta_history': self.theta_history,
            'cost_history': self.history
        }
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
     
        
    def compute_cost(self,X, y,theta):
        n_samples = len(X)
        predictions = np.dot(X, theta)
        errors = predictions - y
        return (1 / (2 * n_samples)) * np.sum(errors**2)
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return np.dot(X, self.theta)
    

    




import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pickle
from linear_regression import LinearRegressor
X = np.loadtxt("../data/Q1/linearX.csv")
y = np.loadtxt("../data/Q1/linearY.csv")
if X.ndim == 1:
    X = X.reshape(-1, 1)



def find_learning_rate():

    learning_rates = [0.0001, 0.001, 0.01, 0.1]
    cost_histories = []
  

    for lr in learning_rates:
        model = LinearRegressor()
        model.fit(X, y, learning_rate=lr)
        costs = [cost for _, cost in model.history]
        cost_histories.append(costs)

    for lr, costs in zip(learning_rates, cost_histories):
        plt.plot(costs, label=f"LR = {lr}")
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    plt.legend()
    plt.savefig("learning_rate_analysis.png")

def gradient(X, y, ax,learning_rate=0.01):
    
    max_iter=1000
    tol=1e-6
    
    n_samples, n_features = X.shape
    X = np.hstack((np.ones((n_samples, 1)), X))
    
    theta = np.zeros(X.shape[1])
    
    for iteration in range(max_iter):
        y_pred = np.dot(X, theta)
        
        cost = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)
        
        gradients = -(1 / n_samples) * np.dot(X.T, (y - y_pred))
        
        theta -= learning_rate * gradients
        ax.scatter(theta[0], theta[1], cost, color='red', s=50, label="Current Step" if iteration == 0 else "")
        
        plt.pause(0.2)

        if iteration &gt; 0 and abs(cost - compute_cost(X, y, theta)) &lt; tol:
            break
        
    
    
def compute_cost(X, y,theta):
    
    n_samples = len(X)
    predictions = np.dot(X, theta)
    errors = predictions - y
    return (1 / (2 * n_samples)) * np.sum(errors**2)



def plot_2d(X,y,theta):
    X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)  # Dense X values for a smooth line
    X_plot_with_intercept = np.hstack((np.ones((X_plot.shape[0], 1)), X_plot))  # Add intercept term
    y_pred = np.dot(X_plot_with_intercept, theta)  # Hypothesis

    plt.scatter(X, y, color='blue', label='Data Points')

    plt.plot(X_plot, y_pred, color='red', label='Learned Hypothesis (h_theta(x))')

    plt.xlabel('X (Feature)')
    plt.ylabel('y (Target)')
    plt.title('Data Points and Hypothesis Function')
    plt.legend()
    plt.savefig("2d_plot.png")
def plot_3d_mesh(X,y):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match181-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 5, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)
    

    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
</FONT>        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            new_X = np.hstack((np.ones((X.shape[0], 1)), X))
            J_values[i, j] = compute_cost(new_X, y, theta)

    fig = plt.figure(figsize=(10, 8))
<A NAME="0"></A><FONT color = #FF0000><A HREF="match181-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta_0, theta_1, J_values, cmap='viridis', alpha=0.8)
    ax.set_xlabel(r'$\theta_0$ (Intercept)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_zlabel(r'$J(\theta)$ (Cost)')
    ax.set_title('3D Mesh of Cost Function with Gradient Descent')
</FONT>
    gradient(X, y, ax, learning_rate=0.01)

    plt.savefig("3d_plot.png")
def plot_contour_with_gradient_descent( X, y, learning_rate=0.01):
    """Plots the error function's contour and visualizes gradient descent."""
    
    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 30, 100)

    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)
    X = np.hstack((np.ones((X.shape[0], 1)),X))
    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            J_values[i, j] = compute_cost(X, y, theta)

    fig, ax = plt.subplots(figsize=(8, 6))

    ax.contour(theta_0, theta_1, J_values, levels=np.logspace(-2, 3, 20), cmap="viridis")
    ax.set_xlabel(r'$\theta_0$ (Intercept)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_title("Contour of Cost Function with Gradient Descent")

    max_iter = 1000000
    tol = 1e-6
    n_samples = len(X)
    theta = np.zeros(X.shape[1])

    for iteration in range(max_iter):
        y_pred = np.dot(X, theta)
        cost = compute_cost(X, y, theta)

       

        gradients = -(1 / n_samples) * np.dot(X.T, (y - y_pred))
        theta -= learning_rate * gradients

        ax.scatter(theta[0], theta[1], color='red', s=50, marker='o')

        plt.pause(0.2)  

        if iteration &gt; 0 and abs(cost - compute_cost(X, y, theta)) &lt; tol:
            break

    plt.savefig("contour_gradient_descent.png")
    plt.show()


def plot_contour_learning_rates(X, y, learning_rates=[0.001, 0.025, 0.1]):
    """Plots the error function's contour and visualizes gradient descent for different learning rates."""

    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 30, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)

    X = np.hstack((np.ones((X.shape[0], 1)), X))

    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            J_values[i, j] = compute_cost(X, y, theta)

    for eta in learning_rates:
        fig, ax = plt.subplots(figsize=(8, 6))

        contour_levels = np.logspace(
            np.log10(np.min(J_values)), np.log10(np.max(J_values)), 20
        )
        ax.contour(theta_0, theta_1, J_values, levels=contour_levels, cmap="viridis")
        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope)')
        ax.set_title(f"Contour of Cost Function with Gradient Descent (Î·={eta})")

        theta = np.zeros(X.shape[1])  
        theta_history = []
        prev_cost = compute_cost(X, y, theta)
        max_iter = 100000
        tol = 1e-6

        for iteration in range(max_iter):
            y_pred = np.dot(X, theta)
            gradients = -(1 / len(X)) * np.dot(X.T, (y - y_pred))
            theta -= eta * gradients  
            theta_history.append(theta.copy())

            ax.scatter(theta[0], theta[1], color="red", s=20, marker="o")

            plt.pause(0.2)  

            cost = compute_cost(X, y, theta)
            if abs(cost - prev_cost) &lt; tol:
                break
            prev_cost = cost

        theta_history = np.array(theta_history)

        ax.plot(
            theta_history[:, 0],
            theta_history[:, 1],
            color="blue",
            lw=2,
            label="Actual Path"
        )

        ax.legend()

        plt.savefig(f"contour_gradient_descent_eta_{eta}.png")

X = np.loadtxt("../data/Q1/linearX.csv")
y = np.loadtxt("../data/Q1/linearY.csv")
if X.ndim == 1:
    X = X.reshape(-1, 1)
find_learning_rate(X,y)
model=LinearRegressor()
model.fit(X,y)
model.save_model('linear_reg.pkl')
with open('linear_reg.pkl', 'rb') as f:
    model_data = pickle.load(f)

theta = model_data['theta']
theta_history = model_data['theta_history']
cost_history = model_data['cost_history']



plot_2d(X,y,theta)

plot_3d_mesh(X,y)
plot_contour_with_gradient_descent(X,y)
plot_contour_learning_rates(X,y)




import pickle
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor

import time

import numpy as np
from sampling_sgd import generate




def compare_parameters(X,y,theta):
    model1=StochasticLinearRegressor()
    param1=theta
    param2=model1.closed_form(X,y)
    model2=StochasticLinearRegressor()
    param3=model2.fit(X,y,0.01,8000)
    print("original : ",param1)
    print("closed form: ",param2)
    print("sgd: ",param3[2][-1])
def run_and_plot_sgd(X, y, batch_sizes=[1,80,8000,800000], eta=0.001, tolerance=1e-6,window_size=10):
    """
    Runs SGD for multiple batch sizes, tracks time, and plots cost and theta trajectories.
    
    Parameters:
        sgd_function (function): The SGD function that takes (X, y, batch_size, eta, tolerance) as input 
                                and returns (theta, cost_history).
        X (numpy.ndarray): Input data of shape (N, 2).
        y (numpy.ndarray): Target values of shape (N,).
        batch_sizes (list): List of batch sizes to evaluate.
        eta (float): Learning rate for SGD. Default is 0.001.
        tolerance (float): Convergence tolerance. Default is 1e-6.
    """
    results = {}
    model=StochasticLinearRegressor()

    

    model.fit(X, y, eta,window_size=window_size)

    model.save_model('sgd.pkl')
    with open('sgd.pkl', 'rb') as f:
            model_data = pickle.load(f)
    for batch_size in batch_sizes:
        
        print(f"\nRunning SGD for batch size {batch_size}...")

        theta = model_data['theta']
        theta_history = model_data['theta_history']
        cost_history = model_data['cost_history']
       
        elapsed_time=model_data['time_elapsed']
        time_taken=elapsed_time[batch_size]
        results[batch_size] = {
            "theta": theta[batch_size],
            "cost_history": cost_history[batch_size],
            "time": elapsed_time[batch_size],
            "theta_history":theta_history[batch_size]
           
        }
        print(f"Batch size {batch_size}:")
        print(f"  Learned theta: {theta}")
        print(f"  Time taken: {time_taken:.2f} seconds")
    
   
        plt.figure(figsize=(10, 6))
        
        plt.plot(results[batch_size]["cost_history"])
        plt.xlabel("Iterations")
        plt.ylabel("Cost")
        plt.title(f"Cost vs Iterations for Batch Sizes {batch_size}: window_size {window_size}")
        plt.legend()
        plt.grid(True)
        plt.savefig(f"cost_vs_iter_b{batch_size}_w{window_size}.png")
    
   
    
    
    print("\nTime for convergence (seconds):")
    for batch_size, result in results.items():
        print(f"  Batch size {batch_size} window_size {window_size}: {result['time']:.2f} seconds ")
   
    
    

def plot_theta_trajectory(theta_history, batch_size):
    # theta_history = np.array(theta_history)
    fig = plt.figure(figsize=(20, 17))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label=f'Batch Size {batch_size}')


    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    ax.set_title('Theta Trajectory in Parameter Space with Projections')
    ax.legend()
    plt.savefig(f"3_d_{batch_size}")

def compute_convergence_criteria(X_train,y_train):
    for window_size in [1,10,1000]:
        run_and_plot_sgd(X_train,y_train,window_size=window_size)


N=1000000
theta=[3, 1, 2] 
input_mean=3
input_sigma=2
noise_sigma=np.sqrt(2)
X,y=generate(N, theta, input_mean, input_sigma, noise_sigma)
indices = np.arange(N)
np.random.shuffle(indices)
X = X[indices]
y = y[indices]

train_size = int(0.8 * N)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

compare_parameters(X_train,y_train,theta)
# compute_convergence_criteria(X_train,y_train)


# run_and_plot_sgd(X_train,y_train)


# model=StochasticLinearRegressor()

# theta_history = model.fit(X_train, y_train, 0.001)
# for batch_size in [1, 80, 8000,800000]:
#     if batch_size==1:
#         index=0
#     elif batch_size==80:
#         index=1
#     elif batch_size==8000:
#         index=2
#     elif batch_size==800000:
#         index=3


#     plot_theta_trajectory(theta_history[index], batch_size)
#     y_train_pred=model.predict(X_train)
#     y_test_pred=model.predict(X_test)
#     train_mse=model.mse(y_train,y_train_pred)
#     test_mse=model.mse(y_test,y_test_pred)
#     print(f"batch size: {batch_size}")
#     print("training error: ",train_mse)
#     print("test error: ",test_mse)





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
import matplotlib.pyplot as plt
import time
import pickle

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    x1 = np.random.normal(3, 2, N) 
    x2 = np.random.normal(-1, 2, N) 
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match181-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    epsilon = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + epsilon
    X = np.column_stack((x1, x2)) 
</FONT>    
    return X,y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta={}
        self.theta_history = {}
        self.cost_history={}
        self.iterations=0
        self.time_elapsed={}
    
    def fit(self, X, y, learning_rate=0.01,batch_size=1,tol=1e-6,window_size=10):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        for batch_size in [1, 80, 8000,800000]:
            self.theta_history[batch_size]=[]
            self.cost_history[batch_size]=[]
        X= np.column_stack((np.ones(X.shape[0]), X))
        for batch_size in [1, 80, 8000,800000]:
            start_time = time.time()
        
        
            if batch_size==1:
                tol=1e-4
            
            N = X.shape[0]  
            self.theta[batch_size] = np.zeros(3)  
            
            max_epochs=10000
            moving_average = 0
            
            self.iterations=max_epochs
            for epoch in range(max_epochs):
                indices = np.random.permutation(N)
                X_shuffled = X[indices]
                y_shuffled = y[indices]

                
                
                cost=0
                for i in range(0, N, batch_size):
                    X_batch = X_shuffled[i:i + batch_size]
                    y_batch = y_shuffled[i:i + batch_size]
                    y_pred = np.dot(X_batch,self.theta[batch_size])
                
                    
                    
                    gradient = -(1 / (batch_size)) * np.dot(X_batch.T,y_batch - y_pred)

                    self.theta[batch_size] -= learning_rate * gradient
                y_pred_all = np.dot(X, self.theta[batch_size])
                cost = self.mse(y, y_pred_all)
                
                self.theta_history[batch_size].append(self.theta[batch_size].copy())
                if epoch % 100 == 0:
                    
                    
                    print(f"Epoch {epoch}: Loss {cost}")
                    
                

                self.cost_history[batch_size].append(cost)
              
                if len(self.cost_history[batch_size]) &lt; window_size:
                    moving_average = np.mean(self.cost_history[batch_size])  # Average all costs so far
                else:
                    moving_average = np.mean(self.cost_history[batch_size][-window_size:])
                if epoch&gt;0 and abs(cost - moving_average) &lt; tol:
                    
                    print(f"Stopping at epoch {epoch}, Cost: {cost}")
                    
                    self.iterations=epoch
                    break
            
            # print(f"iter: {self.iterations}")
            # print(f"theta : {self.theta[batch_size]}")
            elapsed_time = time.time() - start_time
            self.time_elapsed[batch_size]=elapsed_time
        result=[]
      
        for batch_size,theta_val in self.theta_history.items():
            theta_val=np.array(theta_val)
            result.append(theta_val)
        
        return result
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X= np.column_stack((np.ones(X.shape[0]), X))
        y_pred=[]
        for batch_size in [1, 80, 8000,800000]:
            
            res=np.dot(X, self.theta[batch_size])
            y_pred.append(res)
    
        return y_pred
    
    def save_model(self, filename):
        """Save the model to a file."""
        model_data = {
            'theta': self.theta,
            'theta_history': self.theta_history,
            'cost_history': self.cost_history,
            
            "time_elapsed":self.time_elapsed
        }
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filename}")
    def closed_form(self,X,y):
        X= np.column_stack((np.ones(X.shape[0]), X))
        theta=(np.linalg.inv(X.T @ X)) @ X.T @ y

        return theta
    def mse(self,y,y_pred):
 
        mse_val=np.mean((y - y_pred) ** 2)
        return mse_val






# Imports - you can add any other permitted libraries
import numpy as np
import pickle
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_gradient(X, y, theta):
    """
    Compute the gradient of the loss function L(theta).
    """
    m = len(y)
    h = sigmoid(np.dot(X,theta))
    gradient = (1 / m) * (X.T @ (h - y))
    return gradient

def compute_hessian(X, y, theta):
    """
    Compute the Hessian matrix of the loss function L(theta).
    """
    m = len(y)
    h = sigmoid(X @ theta)
    diag = h * (1 - h)  
    H = (1 / m) * (X.T @ (np.diag(diag) @ X))
    return H
def normalise(X):

    X_normalized = (X - np.mean(X,axis=0)) / np.std(X,axis=0)
    return X_normalized
class LogisticRegressor:
    def __init__(self):
        self.theta=None
        self.theta_history=np.array([])
    
    def fit(self, X, y, learning_rate=0.01,tol=1e-6):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X=normalise(X)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match181-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = np.hstack([np.ones((X.shape[0], 1)), X])
    
        self.theta = np.zeros(X.shape[1])
        max_iter=100000
</FONT>
        for _ in range(max_iter):
            gradient = compute_gradient(X, y, self.theta)
            hessian = compute_hessian(X, y, self.theta)
            np.append(self.theta_history,self.theta.copy())
            theta_update = np.linalg.inv(hessian) @ gradient
            self.theta -= theta_update

            if np.linalg.norm(theta_update, ord=2) &lt; tol:
                break
        print("fit done")
        return self.theta_history

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = np.hstack([np.ones((X.shape[0], 1)), X])
    
        probabilities = sigmoid(X @ self.theta)
        predictions = (probabilities &gt;= 0.5).astype(int)
        return predictions
    
    def save_model(self, filename):
        """Save the model to a file."""
        model_data = {
            'theta': self.theta,
            
        }
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"Model saved to {filename}")



#!/usr/bin/env python
# coding: utf-8

# In[23]:


import pickle
import matplotlib.pyplot as plt
import importlib
import logistic_regression
importlib.reload(logistic_regression)
from logistic_regression import LogisticRegressor

import time

import numpy as np


# In[30]:


def plot_data(X, y):
    
    positive = y == 1
    negative = y == 0

    
    plt.scatter(X[positive, 0], X[positive, 1], c='b', marker='o', label='Label 1')
    plt.scatter(X[negative, 0], X[negative, 1], c='r', marker='x', label='Label 0')


def plot_decision_boundary(theta, X):

    x1_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]

    plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

def plot(X,y,theta):
    plt.figure(figsize=(20, 15))
    plot_data(X, y)
    plot_decision_boundary(theta, X)

    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.title('Training Data and Decision Boundary')
    plt.grid()
    plt.savefig("logistic_decision_boundary.png")


# In[ ]:


X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=',')
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=',')

model=LogisticRegressor()
model.fit(X,y)
model.save_model('logistic_reg.pkl')
with open('logistic_reg.pkl', 'rb') as f:
    model_data = pickle.load(f)

theta = model_data['theta']
print(theta)


# In[32]:


plot(X,y,theta)





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
def normalise(X):

    X_normalized = (X - np.mean(X,axis=0)) / np.std(X,axis=0)
    return X_normalized
def compute_sigma(X,y,val):
    class_0_indices = (y == val) 
    X_val = X[class_0_indices] 
    m_val = X_val.shape[0] 
    mu_val = np.mean(X_val, axis=0)  
    centered_X_val = X_val - mu_val 

    Sigma = (centered_X_val.T @ centered_X_val) / m_val
    
    return Sigma

def compute_linear_decision_boundary(sigma, mu_0, mu_1):
    """
    Precompute the parameters for the linear decision boundary.
    :param sigma: Shared covariance matrix.
    :param mu_0: Mean vector for class 0.
    :param mu_1: Mean vector for class 1.
    :return: Precomputed terms (inv_sigma, diff_mu, const_term).
    """
    inv_sigma = np.linalg.inv(sigma)  
<A NAME="2"></A><FONT color = #0000FF><A HREF="match181-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    diff_mu = mu_1 - mu_0  
    const_term = 0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))
</FONT>    return inv_sigma, diff_mu, const_term



def compute_quadratic_decision_boundary(X, mu_0, sigma_0, mu_1, sigma_1):
    """
    Compute the decision function for the quadratic decision boundary.
    :param X: Input data points of shape (n_samples, n_features).
    :param mu_0: Mean vector for class 0.
    :param sigma_0: Covariance matrix for class 0.
    :param mu_1: Mean vector for class 1.
    :param sigma_1: Covariance matrix for class 1.
    :return: Decision values for the input points.
    """
    term_0 = np.linalg.inv(sigma_0)
    term_1 = np.linalg.inv(sigma_1)
    
    diff_0 = X - mu_0
    diff_1 = X - mu_1
    
    quadratic_0 = np.einsum('ij,jk,ik-&gt;i', diff_0, term_0, diff_0)  
    quadratic_1 = np.einsum('ij,jk,ik-&gt;i', diff_1, term_1, diff_1)  
    
    log_det_ratio = np.log(np.linalg.det(sigma_0)) - np.log(np.linalg.det(sigma_1))
    
    Z = -quadratic_1 + quadratic_0 -log_det_ratio

    return Z


class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0=None
        self.mu_1=None
        self.sigma=None
        self.same_cov=True
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X=normalise(X)
        y = np.where(y == "Canada", 1, 0)
        self.mu_0 = np.mean(X[y.flatten() == 0], axis=0)
        self.mu_1 = np.mean(X[y.flatten() == 1], axis=0)
        m = X.shape[0]
        self.same_cov=assume_same_covariance
        if assume_same_covariance:
            self.sigma = sum((x - (self.mu_1 if yi == 1 else self.mu_0)).reshape(-1, 1) @ (x - (self.mu_1 if yi == 1 else self.mu_0)).reshape(1, -1) 
            for x, yi in zip(X, y.flatten())) / m
            return self.mu_0,self.mu_1,self.sigma
        else:
            sigma_0=compute_sigma(X,y,0)
            sigma_1=compute_sigma(X,y,1)
            self.sigma=sigma_0,sigma_1
            return self.mu_0,self.mu_1,sigma_0,sigma_1

        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X=normalise(X)
        if self.same_cov:
            inv_sigma, diff_mu, const_term = compute_linear_decision_boundary(self.sigma, self.mu_0, self.mu_1)
            Z = np.dot(X, np.dot(inv_sigma, diff_mu)) - const_term
            return np.array(["Canada" if value &gt; 0 else "Alaska" for value in Z])
        else:
            sigma_0,sigma_1=self.sigma
            Z = compute_quadratic_decision_boundary(X, self.mu_0, sigma_0, self.mu_1, sigma_1)
       
            return np.array(["Canada" if value &gt; 0 else "Alaska" for value in Z])



#!/usr/bin/env python
# coding: utf-8

# In[50]:


import pickle
import matplotlib.pyplot as plt
import importlib
import gda
importlib.reload(gda)
from gda import GaussianDiscriminantAnalysis

import time

import numpy as np


# In[51]:


X = np.loadtxt("../data/Q4/q4x.dat")  
y = np.loadtxt("../data/Q4/q4y.dat",dtype=str) 


# In[43]:


model=GaussianDiscriminantAnalysis()
mu_0,mu_1,sigma=model.fit(X,y,True)
print(y)
y_pred=model.predict(X)
print(y_pred)
print(f"mu_0: {mu_0}")
print(f"mu_1: {mu_1}")
print(f"sigma: {sigma}")



# In[10]:


def plot_data(X,y):
    y = np.where(y == "Canada", 1, 0)
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")  
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada") 

    plt.xlabel("Feature 1 (Normalized Growth in Freshwater)")
    plt.ylabel("Feature 2 (Normalized Growth in Marine Water)")
    plt.title("Salmon Classification: Alaska vs Canada")
    plt.legend()
    plt.grid(True)
    plt.savefig("training_data.png")

plot_data(X,y)


# In[11]:


def normalise(X):

    X_normalized = (X - np.mean(X,axis=0)) / np.std(X,axis=0)
    return X_normalized
def plot_decision_boundary(mu_0, mu_1, sigma,X,y):
    y = np.where(y == "Canada", 1, 0) 
    X=normalise(X)
    plt.figure(figsize=(12, 8))

    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska") 
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")   
   
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

   
    X_grid = np.c_[xx.ravel(), yy.ravel()]

  
    inv_sigma = np.linalg.inv(sigma)
    
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match181-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    diff_mu = mu_1 - mu_0
    
  
    const_term = 0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))
</FONT>

    Z = np.dot(X_grid, np.dot(inv_sigma, diff_mu)) - const_term
    Z = Z.reshape(xx.shape)


    contour = plt.contour(xx, yy, Z, levels=[0], cmap="RdBu", linewidths=2)


 

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('GDA Decision Boundary')
    plt.legend()
  
    plt.savefig("decision_boundary.png")
    

plot_decision_boundary(mu_0,mu_1,sigma,X,y)


# In[52]:


model=GaussianDiscriminantAnalysis()
mu_0,mu_1,sigma_0,sigma_1=model.fit(X,y,False)
print(y)
y_pred=model.predict(X)
print(y_pred)
print(f"mu_0: {mu_0}")
print(f"mu_1: {mu_1}")
print(f"sigma_0: {sigma_0}")
print(f"sigma_1: {sigma_1}")


# In[53]:


def plot_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y):
    y = np.where(y == "Canada", 1, 0)  
    X=normalise(X)
    plt.figure(figsize=(10, 7))
    

  
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

   
    X_grid = np.c_[xx.ravel(), yy.ravel()]

    inv_sigma = np.linalg.inv(sigma)
    
 
    diff_mu = mu_1 - mu_0
    
    const_term = 0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))

  
    Z = np.dot(X_grid, np.dot(inv_sigma, diff_mu)) - const_term
    Z = Z.reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0], cmap="RdBu", linewidths=2)


    term_0 = np.linalg.inv(sigma_0)
    term_1 = np.linalg.inv(sigma_1)
    
    diff_0 = X_grid - mu_0
    diff_1 = X_grid - mu_1
    
    quadratic_0 = np.einsum('ij,jk,ik-&gt;i', diff_0, term_0, diff_0)
    quadratic_1 = np.einsum('ij,jk,ik-&gt;i', diff_1, term_1, diff_1)
    log_det_ratio = np.log(np.linalg.det(sigma_0)) - np.log(np.linalg.det(sigma_1))

   
    Z = quadratic_1 - quadratic_0 + log_det_ratio
    Z = Z.reshape(xx.shape) 
   
   
    plt.contour(xx, yy, Z, levels=[-0.01, 0, 0.01], colors='brown', linewidths=2)
    plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', label="Alaska")
    plt.scatter(X[y==1, 0], X[y==1, 1], color='red', marker='x', label="Canada")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("GDA Decision Boundary")
    plt.legend()


    plt.savefig("quadratic_decision_boundary.png")

plot_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y)


# In[ ]:






</PRE>
</PRE>
</BODY>
</HTML>
