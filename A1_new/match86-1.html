<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_B4G3R.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_Y26GA.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# ### Q1 : Linear Regression Analysis:
# Relevant library imports. I also store in X all the input features and in y all the output values.

# In[2]:


import numpy as np
import pandas as pd
import matplotlib 
import matplotlib.pyplot as plt
import matplotlib.animation as animation
matplotlib.use('TkAgg')
from linear_regression import LinearRegressor
from plot_utils import plot_surface,plot_moving_parameters_in3d,plot_moving_parameters_with_contour,compute_cost


# In[3]:


filename_x = r'../data/Q1/linearX.csv'
filename_y = r'../data/Q1/linearY.csv'
dfx = pd.read_csv(filename_x) 
X = dfx.values 
dfy = pd.read_csv(filename_y)
series_datay = dfy.iloc[:,0]
y = series_datay.values


# The model is an instance of LinearRegressor class, implemented in linear_regression.py, to which learning_rate is given explicitly.
# 
# Final parameters are stored in the variable final_parameters, whereas predictions are stored in y_pred.

# In[13]:


model = LinearRegressor()


# In[14]:


list_parameters = model.fit(X,y,0.1)
final_parameters = list_parameters[-1]
y_pred = model.predict(X)


# In[15]:


print(list_parameters[-1])


# Visualise the training data as well as the hypothesis function.

# In[8]:


plt.figure(figsize=(12, 6))
<A NAME="1"></A><FONT color = #00FF00><A HREF="match86-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X, y, color='dodgerblue', edgecolors='black', s=100, alpha=0.8, label='Training Data')
plt.plot(X, y_pred, color='crimson', linewidth=3, linestyle='-', label='Hypothesis Function')
plt.xlabel('Feature', fontsize=14, fontweight='bold', color='darkslategray')
plt.ylabel('Target', fontsize=14, fontweight='bold', color='darkslategray')
plt.title('Training Data and Learned Hypothesis', fontsize=16, fontweight='bold', color='navy')
plt.legend(frameon=True, shadow=True, fontsize=12, loc='best', facecolor='whitesmoke')
</FONT>plt.grid(True, linestyle='dashed', linewidth=0.7, alpha=0.6)
plt.show()


# Visualise Loss function v/s iteration using the cell below.

# In[9]:


error_values = [compute_cost(X, y, theta[0],theta[1]) for theta in list_parameters]
plt.figure(figsize=(10, 5))
plt.plot(range(len(error_values)), error_values, marker='o', linestyle='-')
plt.xlabel('Iteration')
plt.ylabel('Cost J(θ)')
plt.title('Error Function Convergence Over Training')
plt.grid(True)
plt.show()


# Plot the error surface i.e. 3-D mesh grid.

# In[ ]:


plot_surface(X,y,model,30)


# Plot the gradient-descent trajectory on the 3-D mesh grid.

# In[16]:


plot_moving_parameters_in3d(X,y,model,list_parameters,10,30)


# Plot the gradient-descent trajectory on contours. Contours are essentially locus of points (x1,x2) having same value of cost function for parameters.
# 

# In[ ]:


plot_moving_parameters_with_contour(X,y,model,list_parameters,0.01,30)


# Validation Loss curve on train and test data for optimum learning rate.

# In[5]:


train_ratio = .8
split_idx = int(len(X) * train_ratio)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]


# In[7]:


learning_rates = np.logspace(-4, 0, 10)  

train_errors = []
test_errors = []

for lr in learning_rates:
    model = LinearRegressor()  
    list_parameters = model.fit(X_train, y_train, learning_rate=lr) 
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test) 

    m = len(y_train)
    loss = 0
    for i in range(m):
        loss += (y_train[i] - y_train_pred[i])**2
    loss/=(2*m)
    train_errors.append(loss)

    m = len(y_test)
    loss = 0
    for i in range(m):
        loss += (y_test[i] - y_test_pred[i])**2
    loss/=(2*m)
    test_errors.append(loss)

plt.figure(figsize=(8, 5))
plt.plot(learning_rates, train_errors, label="Training Error", marker='o')
plt.plot(learning_rates, test_errors, label="Validation Error", marker='s')
plt.xscale("log")  
plt.xlabel("Learning Rate")
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Validation Curve: Learning Rate vs Error")
plt.legend()
plt.grid()
plt.show()





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.parameters = None      # Parameters of the model
<A NAME="2"></A><FONT color = #0000FF><A HREF="match86-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.threshold = 1e-6       # Stopping criterion
        
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
</FONT>        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape[0], X.shape[1]   # m : number of samples, n = number of features
        
        X0 = np.empty((m, n + 1))       # X0 : X matrix with a column of 1s
        for i in range(m):
            X0[i, 0] = 1  
            for j in range(n):
                X0[i, j + 1] = X[i, j]  

        self.parameters = np.zeros(n + 1)    
        J_previous , J_current = -1, 0             # J : Loss
        return_list = []                           # Final parameter list to be returned

        while (abs(J_previous - J_current) &gt; self.threshold):
            
            y_hat = np.zeros(m)
            for i in range(m):
                y_hat[i] = np.dot(self.parameters, X0[i])

            J_previous = J_current
            J_current = 0
            del_J = 0                               # Gradient of Loss Function
            
            for i in range(m):
                del_J = del_J + ((y[i] - y_hat[i]) * X0[i])
                J_current = J_current + ((y[i] - y_hat[i])**2)

            self.parameters = self.parameters + learning_rate * (del_J / m)
            J_current /= 2*m
            
            return_list.append(self.parameters.copy())

        return np.array(return_list)       
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape[0], X.shape[1]

        X0 = np.empty((m, n + 1))
        for i in range(m):
            X0[i, 0] = 1  
            for j in range(n):
                X0[i, j + 1] = X[i, j]

        y_pred = np.zeros(m)
        for i in range(m):
            y_hat = np.dot(self.parameters, X0[i])
            y_pred[i] = y_hat

        return y_pred




import numpy as np
import pandas as pd
import matplotlib 
import matplotlib.pyplot as plt
import matplotlib.animation as animation
matplotlib.use('TkAgg')

def compute_cost(X, y, theta_0, theta_1):
    """Compute Mean Squared Error cost function for given parameters."""
    m = len(y)
    y_pred = theta_0 + theta_1 * X
    y_pred = np.ravel(y_pred)
    loss = 0
    for i in range(m):
        loss += (y[i] - y_pred[i])**2
    loss/=(2*m)
    return loss

def plot_surface(X,y,model,intv):
    theta_0_vals = np.linspace(model.parameters[0]-intv,model.parameters[0]+intv, 100)  # Range for theta_0
    theta_1_vals = np.linspace(model.parameters[1]-intv,model.parameters[1]+intv, 100)  # Range for theta_1

    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

    # Compute cost for each (theta_0, theta_1) pair
    for i in range(len(theta_0_vals)):
        for j in range(len(theta_1_vals)):
            J_vals[i, j] = compute_cost(X, y, theta_0_vals[i], theta_1_vals[j])

    # Create meshgrid for plotting
    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

    # Plot the 3D surface
    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(111, projection='3d')

    # Use a color gradient for the error function
    ax.plot_surface(theta_0_vals, theta_1_vals, J_vals.T, cmap='rainbow', edgecolor='none')

    # Labels and title
    ax.set_xlabel('Theta 0 (Intercept)')
    ax.set_ylabel('Theta 1 (Slope)')
    ax.set_zlabel('Cost J(θ)')
    ax.set_title('3D Mesh of Cost Function')

    plt.show()

def plot_moving_parameters_in3d(X,y,model,list_parameters,range0,range1):
    theta_0_vals = np.linspace(model.parameters[0]-range0,model.parameters[0]+range0, 100)
    theta_1_vals = np.linspace(model.parameters[1]-range1,model.parameters[1]+range1, 100)

    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

    # Compute cost function surface
    for i in range(len(theta_0_vals)):
        for j in range(len(theta_1_vals)):
            J_vals[i, j] = compute_cost(X, y, theta_0_vals[i], theta_1_vals[j])

    # Create meshgrid for plotting
    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

    # Convert parameter history to numpy array
    trajectory = np.array(list_parameters)  # Convert list to numpy array
    theta_0_history = trajectory[:, 0]  # Extract θ0 values
    theta_1_history = trajectory[:, 1]  # Extract θ1 values
    J_history = [compute_cost(X, y, theta_0, theta_1) for theta_0, theta_1 in zip(theta_0_history, theta_1_history)]

    # Create figure
    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(111, projection='3d')

    # Plot the cost function surface
    ax.plot_surface(theta_0_vals, theta_1_vals, J_vals.T, cmap='rainbow', edgecolor='none', alpha=0.6)

    # Labels and title
    ax.set_xlabel('Theta 0 (Intercept)')
    ax.set_ylabel('Theta 1 (Slope)')
    ax.set_zlabel('Cost J(θ)')
    ax.set_title('3D Gradient Descent Animation')
    # Initialize scatter plot for trajectory
    scatter = ax.scatter([], [], [], color='red', s=30,alpha=0.9)
    # Update function for animation
    def update(frame):
        scatter._offsets3d = (theta_0_history[:frame], theta_1_history[:frame], J_history[:frame])
        return scatter,
    # Create animation
    ani = animation.FuncAnimation(fig, update, frames=len(theta_0_history), interval=200, blit=False)
    plt.show()

def plot_moving_parameters_with_contour(X, y,model,list_parameters, learning_rate,intv):  # Added learning_rate parameter
    """Plots the 2D contour and animates the dot with trajectory."""

    # 1. Prepare data for the contour plot
    theta0_vals = np.linspace(model.parameters[0] - intv, model.parameters[0] + intv, 100)
    theta1_vals = np.linspace(model.parameters[1] - intv, model.parameters[1] + intv, 100)
    theta_0_vals, theta_1_vals = np.meshgrid(theta0_vals, theta1_vals, indexing='ij')
    J_vals = np.zeros(theta_0_vals.shape)

    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            theta = np.array([theta0_vals[i], theta1_vals[j]])
            cost = compute_cost(X, y, theta[0],theta[1])
            J_vals[i, j] = cost

    # 2. Create the contour plot
    fig, ax = plt.subplots(figsize=(10, 8))
    contour = ax.contour(theta_0_vals, theta_1_vals, J_vals, levels=50, cmap='viridis')
    ax.clabel(contour, inline=True, fontsize=8)
    ax.set_xlabel(r'Theta 0 (Intercept)', fontsize=12)
    ax.set_ylabel(r'Theta 1 (Slope)', fontsize=12)

    # Add title with learning rate
    title = f"Contours for Learning rate(eta) = {learning_rate}"  # Use f-string formatting
    ax.set_title(title, fontsize=15)

    # 3. Create the moving dot and trajectory
    list_parameters = np.array(list_parameters)
    theta0_history = list_parameters[:, 0]
    theta1_history = list_parameters[:, 1]

    moving_dot, = ax.plot(theta0_history[0], theta1_history[0], 'rx', markersize=8, label="Change in Theta")

    trajectory_points, = ax.plot([], [], 'rx', markersize=4)  # Initialize for crosses

    # 4. Define the animation function
    def animate(i):
        if i &lt; len(list_parameters):
            theta = list_parameters[i]
            moving_dot.set_data([theta[0]], [theta[1]])
            trajectory_points.set_data(theta0_history[:i+1], theta1_history[:i+1])
            return moving_dot, trajectory_points
        else:
            return moving_dot, trajectory_points

    # 5. Create the animation
    ani = animation.FuncAnimation(fig, animate, frames=len(list_parameters), interval=200, blit=False, repeat=False)

    # Add figure title (outside the axes)
    fig_title = f"Figure 6: Movement of θ for learning rate = {learning_rate}"
    fig.suptitle(fig_title, fontsize=12)  # Use suptitle for figure title

    ax.legend()
    plt.show()



#!/usr/bin/env python
# coding: utf-8

# ### Q2 : SGD:
# Relevant library imports. I also store in X all the input features and in y all the output values.

# In[ ]:


import numpy as np
import pandas as pd 
import matplotlib 
import matplotlib.pyplot as plt
import matplotlib.animation as animation
matplotlib.use('TkAgg')
from sampling_sgd import StochasticLinearRegressor, generate


# Run the cell below to generate sample data using the **generate** function for given mean and variance.

# In[ ]:


N = 1000000
theta = np.array([3,1,2])
input_mean = np.array([3,4])
input_sigma = np.array([4,4])
noise_sigma = float(2) 
X,y = generate(N,theta,input_mean,input_mean,noise_sigma)


# Split the data generated into training and testing data

# In[ ]:


train_ratio = .8
split_idx = int(len(X) * train_ratio)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]


# Model is an instance of StochasticLinearRegressor() class. Fit the model on the train data using model.fit() method.

# In[ ]:


model = StochasticLinearRegressor()
list_parameters=model.fit(X_train,y_train,0.001)


# Final Parameters learned by the model.

# In[ ]:


# 800000: epoch  10178  epoch loss:  2.0002461107085967  params:  [2.90466467 1.00971195 2.00810725]
print(list_parameters[0])


# Visualise the movement of theta parameters in 3d space, with the axis being theta0, theta1 and theta2.

# In[ ]:


def plot_parameters_movement_in_3d(list_parameter):

    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')

    theta_values = list_parameters
    theta0 = [theta[0] for theta in theta_values]
    theta1 = [theta[1] for theta in theta_values]
    theta2 = [theta[2] for theta in theta_values]

    ax.plot(theta0, theta1, theta2, marker='o', markersize=3)  # Plot the trajectory
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title('Movement of Theta in 3D Parameter Space')
    ax.legend()
    plt.show()

plot_parameters_movement_in_3d(list_parameters[-1])


# ### Code for Closed form solution:

# In[ ]:


m , n = X.shape[0], X.shape[1]
X0 = np.empty((m, n + 1))
for i in range(m):
    X0[i, 0] = 1  
    for j in range(n):
        X0[i, j + 1] = X[i, j]  

closed_parameters = np.linalg.inv(X0.T @ X0) @ X0.T @ y
print(closed_parameters)


# ### Code for train and test error:

# In[ ]:


y_train_pred = model.predict(X_train)
m = len(y_train)
train_error_v = []
for j in range (4):
    train_error=0
    for i in range(m):
        train_error += (y_train[i] - y_train_pred[j][i])**2
    train_error /= (2*m)
    train_error_v.append(train_error)


y_test_pred = model.predict(X_test)
m = len(y_test)
test_error_v = []
for j in range (4):
    test_error = 0
    for i in range(m):
        test_error += (y_test[i] - y_test_pred[j][i])**2
    test_error /= (2*m)
    test_error_v.append(test_error)
print(train_error_v)
print(test_error_v)


# In[ ]:








# Imports - you can add any other permitted libraries
<A NAME="6"></A><FONT color = #00FF00><A HREF="match86-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
</FONT>    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match86-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))  
    n = 2
</FONT>    X0 = np.empty((N, n + 1))       # X0 : has a column of 1s appended to X 
    for i in range(N):
        X0[i, 0] = 1  
        for j in range(n):
            X0[i, j + 1] = X[i, j]  

<A NAME="0"></A><FONT color = #FF0000><A HREF="match86-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(loc=0, scale=noise_sigma, size=N) 
    y = X0 @ theta + noise  
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.parameters = None      
        self.threshold = 0.0000025              #0.0000025 for 0.001 and batch_size=80        #0.0003 non-overlap
</FONT>    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match86-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m, n = X.shape
        X0 = np.hstack((np.ones((m, 1)), X))
</FONT>        if self.parameters is None:
            self.parameters = np.zeros((4,  n + 1))
        batch_settings = {
            1:      {'min_epoch': 0, 'threshold': 1e-7},
            80:     {'min_epoch': 1, 'threshold': 1e-6},
            8000:   {'min_epoch': 100,  'threshold': 1e-7},
            800000: {'min_epoch': 100,     'threshold': 0.000001}
        }
        batch_sizes = [1,80,8000,800000]
        
        params_results = []  
        loss_results = []   
        
        # Loop over each batch size
        for ii in range(4): 
            bs = batch_sizes[ii]
            loss_iter = []
            ite = 0
            settings = batch_settings[bs]
            min_epochs = settings['min_epoch']
            threshold = settings['threshold']
            
            # Initialize parameters (n_features+1,)
            params = np.zeros(n + 1)
            epoch_losses = []
            params_history = []
            
            # Epoch-based training loop
            epoch = 0
            brk = False
            while True:
                indices = np.random.permutation(m)
                X_epoch = X0[indices]
                y_epoch = y[indices]
                total_batches = int(np.ceil(m / bs))
                loss = 0
                for i in range(total_batches):
                    if brk:
                        break
                    start = i * bs
                    end = min(start + bs, m)
                    X_batch = X_epoch[start:end]
                    y_batch = y_epoch[start:end]
                    y_pred = np.matmul(X_batch, params)
                    error = y_batch - y_pred
                    grad = np.matmul(X_batch.T, error) / (end - start)
                    params = params + learning_rate * grad
                    loss = np.sum(np.square((y_pred - y_batch))) / (2 * (end-start+1))
                    loss_iter.append(loss)
                    ite+=1
                    if ite &gt;=40 and epoch &gt;= min_epochs:
                        if abs((loss_iter[-1] - loss_iter[-21])/20) &lt;= threshold:
                            brk=True
                            self.parameters[ii] = params.copy()
                            params_history.append(params.copy())
                if brk:
                    break
                y_pred_epoch = np.matmul(X_epoch, params)
                epoch_loss = np.sum(np.square((y_epoch - y_pred_epoch))) / (2 * m)
                epoch_losses.append(epoch_loss)
                params_history.append(params.copy())
                epoch+=1
                
            params_results.append(np.array(params_history))
            loss_results.append(np.array(epoch_losses))


        return params_results

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape[0], X.shape[1]
        X0 = np.empty((m, n + 1))
        for i in range(m):
            X0[i, 0] = 1  
            for j in range(n):
                X0[i, j + 1] = X[i, j]

        y_pred = np.zeros(m)
        y_pred_all = []
        for j in range(4):
            for i in range(m):
                y_hat = np.dot(self.parameters[j], X0[i])
                y_pred[i] = y_hat
            y_pred_all.append(y_pred)
        

        return y_pred_all

    



#!/usr/bin/env python
# coding: utf-8

# ### Q3: Logistic Regression Analysis:
# Relevant library imports. Store in X all the input features and in y all the output values. 

# In[3]:


import numpy as np
import pandas as pd 
import matplotlib
matplotlib.use('Qt5Agg')  
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

filename_x = '../data/Q3/logisticX.csv'
filename_y = '../data/Q3/logisticY.csv'
dfx = pd.read_csv(filename_x,header=None) 
X = dfx.values 
dfy = pd.read_csv(filename_y,header=None)
series_datay = dfy.iloc[:,0]
y = series_datay.values


# The model is an instance of LogisticRegressor class, implemented in logistic_regression.py, to which learning_rate is given explicitly.
# 
# Final parameters are stored in the variable final_parameters, whereas predictions are stored in y_pred.

# In[4]:


model = LogisticRegressor()
list_parameters = model.fit(X,y,learning_rate=0.01)
final_parameters = list_parameters[-1]
y_pred = model.predict(X)


# In[5]:


print(final_parameters)


# X_norm stores the normalised values of X.

# In[6]:


X_mean, X_std = X.mean(axis=0), X.std(axis=0)
X_norm = (X - X_mean) / X_std


# Visualise the training data (Normalised) as well as the decision boundary on same plot.

# In[7]:


def plot_logistic_boundary(X, y, theta):
    X_class0 = []
    X_class1 = []

    for i in range(len(y)):
        if y[i] == 0:
            X_class0.append(X[i])
        elif y[i] == 1:
            X_class1.append(X[i])

    X_class0 = np.array(X_class0)
    X_class1 = np.array(X_class1)

    plt.figure(figsize=(8, 6))

    if X_class0.size &gt; 0:
        plt.scatter(X_class0[:, 0], X_class0[:, 1], marker='D', color='royalblue', edgecolor='black', s=70, label='Class 0')

    if X_class1.size &gt; 0:
        plt.scatter(X_class1[:, 0], X_class1[:, 1], marker='P', color='darkorange', edgecolor='black', s=70, label='Class 1')

    x1_min = X[:, 0].min() - 1
    x1_max = X[:, 0].max() + 1
    x1_values = np.linspace(x1_min, x1_max, 200)
    x2_values = - (theta[0] + theta[1] * x1_values) / theta[2]

    plt.plot(x1_values, x2_values, color='crimson', linewidth=2, label='Decision Boundary')
    plt.xlabel("Feature 1", fontsize=12)
    plt.ylabel("Feature 2", fontsize=12)
    plt.title("Logistic Regression: Data & Decision Boundary", fontsize=14)
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.8)
    plt.xlim(x1_min, x1_max)
    
    y1_min = X[:, 1].min() - 1
    y1_max = X[:, 1].max() + 1
    plt.ylim(y1_min, y1_max)
    plt.tight_layout()
    plt.show()


# In[8]:


plot_logistic_boundary(X_norm, y, final_parameters)





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    def __init__(self):
        self.parameters = None          
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match86-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.threshold = 1e-7      # stopping criterion
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
</FONT>        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        mu, std = X.mean(axis=0), X.std(axis=0)
        Xn = (X - mu) / std             # Xn : normalised Input Features
        
        m, n = Xn.shape                 # m = number of samples, n = number of features         
        
        X0 = np.empty((m, n + 1))       # X0 : has a column of 1s appended to Xn
        for i in range(m):
            X0[i, 0] = 1  
            for j in range(n):
                X0[i, j + 1] = Xn[i, j]  

        self.parameters = np.zeros(n + 1)                        
        return_list = []                        # Final parameter list to be returned
        parameters_prev = None                  # parameters on previous update (used for experimentation with a different convergence criterion)
        J_previous , J_current = -1, 0          # Loss function values
        
        while len(return_list)&lt;2 or (abs(J_current-J_previous)&gt; self.threshold):
            
            parameters_prev = self.parameters
            
            y_hat = np.zeros(m)
            for i in range(m):
                y_hat[i] = 1.0 / (1.0 + np.exp(-(np.dot(self.parameters, X0[i]))))

            Hessian = np.zeros((n+1, n+1))                        
            for k in range(m):                                                  
                for i in range(n+1):
                    for j in range(n+1):
                        Hessian[i][j] += (-1) * (y_hat[k] * (1-y_hat[k])) * X0[k][i] * X0[k][j]

            gradient = np.zeros(n+1) 
            J_previous = J_current
            J_current = 0                               
            
            for k in range(m):                                 
                gradient += (y[k] - y_hat[k]) * X0[k]
                J_current -= y[k] * np.log(y_hat[k]) + (1-y[k])*np.log(1 - y_hat[k])  

            gradient /= (2*m)
            J_current = J_current/(m)
            
            H_inv = np.linalg.inv(Hessian)  
            del_J = H_inv @ gradient  
            
            self.parameters = self.parameters - del_J     
            return_list.append(self.parameters)                  

        return np.array(return_list)
            
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
       
        mu, std = X.mean(axis=0), X.std(axis=0)
        Xn = (X - mu) / std
        
        m, n = Xn.shape                          
        
        X0 = np.empty((m, n + 1))
        for i in range(m):
            X0[i, 0] = 1  
            for j in range(n):
                X0[i, j + 1] = Xn[i, j]  

        y_hat = np.zeros(m)
        for i in range(m):
            y_hat[i] = 1.0 / (1.0 + np.exp(-(np.dot(self.parameters, X0[i]))))
        
        y_pred = np.zeros(m)
        for i in range(m):
            y_pred[i] = 1 if y_hat[i] &gt; 0.5 else 0

        return y_pred




#!/usr/bin/env python
# coding: utf-8

# ### Q4: Gaussian Discriminant Analysis:
# Relevant Library imports. Store in X all the input features and in y all the output values. Also store in X_norm the normalised values of X.
# 

# In[ ]:


import numpy as np
import pandas as pd 
import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis
X = np.loadtxt('../data/Q4/q4x.dat')
y = pd.read_csv('../data/Q4/q4y.dat', header=None).values
X_mean, X_std = X.mean(axis=0), X.std(axis=0)
X_norm = (X - X_mean) / X_std


# Model is an instance of GaussianDiscriminantAnalysis. Fit the model for two cases, assume_covariance True and False. The parameters are stored in the mu_{}, sigma_{} lists respectively.

# In[ ]:


model = GaussianDiscriminantAnalysis()
mu_T, mu_F, sigma_T, sigma_F, phi = [None,None],[None,None],[None],[None,None], 0
mu_T[0],mu_T[1],sigma_T[0] = model.fit(X,y,assume_same_covariance=True)
mu_F[0],mu_F[1],sigma_F[0],sigma_F[1] = model.fit(X,y,assume_same_covariance=False)
phi = model.phi


# In[ ]:


print("Prior : ",phi)
print("Parameters Learned for covariance same :")
print("mu_0 : ", mu_T[0], " | mu_1 : ", mu_T[1], " | sigma : ", sigma_T[0])
print("\n")
print("Parameters Learned for covariance not same :")
print("mu_0 : ", mu_F[0], " | mu_1 : ", mu_F[1], " | sigma_0 : ", sigma_F[0], " | sigma_1 : ", sigma_F[1])


# In[ ]:


def plot_gda(X, y, case = None, phi = None, mu_T = [None,None], sigma_T = [None], mu_F = [None,None], sigma_F = [None,None]):
    """
    Plots data with different boundary options based on the given case.
    """
    mu_01, mu_11, sigma_01 = mu_T[0], mu_T[1], sigma_T[0]
    mu_02, mu_12, sigma_02, sigma_12 = mu_F[0], mu_F[1], sigma_F[0], sigma_F[1]

    y = np.ravel(y)
    fig, ax = plt.subplots(figsize=(8, 6))
    unique_labels = np.unique(y)
    markers = {'Canada': 'x', 'Alaska' : 'o'}
    for label in unique_labels:
        X_subset = X[y == label]  # Select data points for the current label
        ax.scatter(X_subset[:, 0], X_subset[:, 1], marker=markers[label], edgecolors='black', s=80, label=label)
    ax.set_xlabel("Coordinate 1")
    ax.set_ylabel("Coordinate 2")
    plt.title("Training Data Visualization")
    ax.grid(True, linestyle='--', alpha=0.7)
    
    if case in ['linear', 'both'] and mu_01 is not None and mu_11 is not None and sigma_01 is not None and phi is not None:
        x_start, x_end = X[:, 0].min() - 1, X[:, 0].max() + 1
        x_range = np.linspace(x_start, x_end, 100)
        sigma_inv = np.linalg.pinv(sigma_01)
        y_range = -((np.dot((mu_11 - mu_01).T, sigma_inv)[0] * x_range + (-0.5 * (np.dot(mu_11.T, np.dot(sigma_inv, mu_11)) - np.dot(mu_01.T, np.dot(sigma_inv, mu_01))) - np.log((1 - phi) / phi))) / np.dot((mu_11 - mu_01).T, sigma_inv)[1])
        plt.title("Training Data with GDA Decision Boundary")
        ax.plot(x_range, y_range, color='red', linewidth=2, label='Decision Boundary')
    
    if case in ['quadratic', 'both'] and mu_02 is not None and mu_12 is not None and sigma_02 is not None and sigma_12 is not None and phi is not None:
        a_min, a_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        b_min, b_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        aa, bb = np.meshgrid(np.linspace(a_min, a_max, 200), np.linspace(b_min, b_max, 200))
        W = np.zeros(aa.shape)
        
        sigma_0_inv = np.linalg.pinv(sigma_02)
        sigma_1_inv = np.linalg.pinv(sigma_12)
        
        for m in range(aa.shape[0]):
            for n in range(aa.shape[1]):
                a_point = np.array([aa[m, n], bb[m, n]])
                W[m, n] = 0.5 * np.dot(a_point.T, np.dot(sigma_1_inv - sigma_0_inv, a_point)) - np.dot((mu_12.T @ sigma_1_inv - mu_02.T @ sigma_0_inv), a_point) + 0.5 * (mu_12.T @ sigma_1_inv @ mu_12 - mu_02.T @ sigma_0_inv @ mu_02) + np.log((1 - phi) / phi) + 0.5 * np.log(np.linalg.det(sigma_12) / np.linalg.det(sigma_02))
        
        ax.contour(aa, bb, W, levels=[0], colors='blue', linewidths=2)
        plt.title("Training Data with GDA Decision Boundaries")
        ax.plot([], [], color='blue', label='Quadratic (Different Covariances)')

    ax.legend()
    plt.grid(True)
    plt.show()
    return 


# Visualise (Normalised) Training Data.

# In[ ]:


plot_gda(X, y )


# Visualise Training Data with Linear Separator.

# In[ ]:


plot_gda(X_norm, y, "linear", phi, mu_T, sigma_T)


# Visualise Training Data with Linear Separator and Quadratic Separator.

# In[ ]:


plot_gda(X_norm, y, "both", phi, mu_T, sigma_T, mu_F, sigma_F)


# In[ ]:


y_labels_pred = model.predict(X)
print(y_labels_pred.shape)
print(y_labels_pred)


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.phi = None
        self.mu0 = None
        self.mu1 = None
        self.sigma0 = None
        self.sigma1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # Normalised X values are stored in Xn
        mu, std = X.mean(axis=0), X.std(axis=0)
        Xn = (X - mu) / std
        m, n = Xn.shape         

        y_a = np.zeros(m)
        for i in range(m):
            if y[i] == 'Alaska':
                y_a[i] = 1
        
        self.phi = 0     
        self.mu0 = np.zeros(n)
        self.mu1 = np.zeros(n)
        self.sigma0 = np.zeros((n, n))
        self.sigma1 = np.zeros((n, n))

        count_1 = 0 # number of samples of 1
        for i in range(m):
            self.mu0 += (1 - y_a[i]) * Xn[i]       
            self.mu1 += (y_a[i]) * Xn[i]          
            count_1 += y_a[i]                     
        
        self.phi = count_1 / m                 
        self.mu0 /= (m - count_1)              
        self.mu1 /= count_1                     

        if assume_same_covariance:
            sigma = np.zeros((n, n))           
            for i in range(m):                  
                tmp = Xn[i] - y_a[i] * self.mu1 - (1 - y_a[i]) * self.mu0  
                sigma += np.outer(tmp, tmp)
            sigma /= m
            self.sigma0 = sigma                
            self.sigma1 = sigma
            return (self.mu0, self.mu1, sigma)
        
        for i in range(m):                      
            if y_a[i] == 0:                       
                tmp = Xn[i] - self.mu0           
                self.sigma0 += np.outer(tmp, tmp)  
            elif y_a[i] == 1:
                tmp = Xn[i] - self.mu1
                self.sigma1 += np.outer(tmp, tmp)  

        self.sigma0 /= (m - count_1) 
        self.sigma1 /= count_1       
        return (self.mu0, self.mu1, self.sigma0, self.sigma1)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        mu, std = X.mean(axis=0), X.std(axis=0)
        Xn = (X - mu) / std
        m, n = Xn.shape
        
        y_hat = np.zeros(m)
        for i in range(m):
            x1 = Xn[i] - self.mu0
            x1 = x1.reshape(-1, 1)
            x2 = Xn[i] - self.mu1
            x2 = x2.reshape(-1, 1)
            
            t1 = (x1).T @ np.linalg.inv(self.sigma0) @ (x1)
            t1 /= 2
            A1 = (np.linalg.det(self.sigma0) ** 0.5) * (np.exp((-1) * (t1))) * (1 - self.phi)
            
            t2 = (x2).T @ np.linalg.inv(self.sigma1) @ (x2)
            t2 /= 2
            A2 = (np.linalg.det(self.sigma1) ** 0.5) * (np.exp((-1) * (t2))) * (self.phi)
            
            A = (A1 / A2)
            y_hat[i] = (1 / (1 + A))
        
        y_pred = np.zeros(m)
        for i in range(m):
            y_pred[i] = 1 if y_hat[i] &gt; 0.5 else 0

        y_labels = []
        for i in range(m):
            if (y_pred[i]==1):
                y_labels.append('Alaska')
            else:
                y_labels.append('Canada')

        y_labels_arr = np.array(y_labels)
        return y_labels_arr


</PRE>
</PRE>
</BODY>
</HTML>
