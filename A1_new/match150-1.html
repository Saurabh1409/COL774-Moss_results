<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_D7WS3.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_N90F6.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        theta_list = []
        
    
    def fit(self, X, y,learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        theta = np.zeros(X.shape[1])
        
        theta_list = []
        
        delta_error = 1000
        prev_error = 0
        iteration = 0
        
        while delta_error &gt; 0.00001:
            
            iteration += 1
            y_pred = np.dot(X, theta)
            y_pred = y_pred.reshape(-1,1)
            error = y_pred - y
            squared_error = np.sum(np.square(error)) / (2*X.shape[0])
            delta_error = np.abs(prev_error - squared_error)
            prev_error = squared_error
            
            gradient = np.dot(X.T, error) / X.shape[0]
            gradient = np.sum(gradient,axis=1, keepdims=True).flatten()
            
            
            learning_rate = 0.1
            
            theta -= (learning_rate * gradient)
                        
            theta_list.append(list(theta))
            
        self.theta_list = theta_list
        return np.array(theta_list)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
<A NAME="6"></A><FONT color = #00FF00><A HREF="match150-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        y_pred = np.dot(X, self.theta_list[-1])
</FONT>        
        return y_pred



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv("../data/Q1/linearX.csv")
X = data.values
data = pd.read_csv("../data/Q1/linearY.csv")
Y = data.values
X = np.array(X)
Y = np.array(Y)

train_size = int(X.shape[0])
X_train = X[:train_size]
Y_train = Y[:train_size]
X_test = X[train_size:]
Y_test = Y[train_size:]

from linear_regression import LinearRegressor

model = LinearRegressor()
thetas = model.fit(X_train, Y_train)

theta0_vals = np.linspace(-20, 30, 100)
theta1_vals = np.linspace(0, 50, 100)
theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
J_vals = np.zeros(theta0_vals.shape)

for i in range(theta0_vals.shape[0]):
    for j in range(theta0_vals.shape[1]):
        J_vals[i,j] = np.sum((Y_train - theta0_vals[i,j] - theta1_vals[i,j]*X_train)**2)
        J_vals[i,j] = J_vals[i,j]/(2*Y_train.shape[0])
        
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis', alpha=0.5)

for i in range(1, len(thetas)):
    ax.scatter(thetas[i,0], thetas[i,1], np.sum((Y_train - thetas[i,0] - thetas[i,1]*X_train)**2)/(2*Y_train.shape[0]), c='red', marker='o',s=50)
    plt.pause(0.2)
    

ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Cost Function')
plt.show()

fig = plt.figure()
ax = fig.add_subplot(111)
ax.contour(theta0_vals, theta1_vals, J_vals, cmap='viridis')
for i in range(1, len(thetas)):
    ax.scatter(thetas[i,0], thetas[i,1], c='black', marker='o',s=1)
    plt.pause(0.2)
plt.xlabel('Theta 0')
plt.ylabel('Theta 1')
plt.show()



#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import sys


# In[ ]:


data = pd.read_csv("../data/Q1/linearX.csv")
X = data.values
data = pd.read_csv("../data/Q1/linearY.csv")
Y = data.values
X = np.array(X)
Y = np.array(Y)


# In[7]:


train_size = int(X.shape[0])
X_train = X[:train_size]
Y_train = Y[:train_size]
X_test = X[train_size:]
Y_test = Y[train_size:]



# In[8]:


from linear_regression import LinearRegressor

model = LinearRegressor()
thetas = model.fit(X_train, Y_train)
print(thetas[-1])


# In[9]:


Y_pred_train = model.predict(X_train)
plt.scatter(X_train, Y_train)
plt.plot(X_train, Y_pred_train, color='red')
plt.show()


# In[ ]:


y_pred_test = model.predict(X_test)
plt.scatter(X_test, Y_test)
plt.plot(X_test, y_pred_test, color='red')
plt.show()

y_pred_test = y_pred_test.reshape(-1,1)
print(Y_test.shape, y_pred_test.shape)
mse = np.sum((Y_test - y_pred_test)**2)
mse = mse/(2*Y_test.shape[0])
print(mse)


# In[ ]:



theta0 = np.linspace(-20, 30, 100)
theta1 = np.linspace(0, 60, 100)

theta0, theta1 = np.meshgrid(theta0, theta1)
J = np.zeros(theta0.shape)

for i in range(theta0.shape[0]):
    for j in range(theta0.shape[1]):
        J[i,j] = np.sum((Y_train - theta0[i,j] - theta1[i,j]*X_train)**2)
        J[i,j] = J[i,j]/(2*Y_train.shape[0])
        
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(theta0, theta1, J, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
ax.set_zlabel('J(theta)')
plt.show()





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[2]:


from sampling_sgd import generate

X, y = generate(1000000, np.array([3,1,2]), np.array([3,-1]), np.array([4,4]), np.sqrt(2))


# In[3]:


X_train = X[:800000]
y_train = y[:800000]
X_test = X[800000:]
y_test = y[800000:]


# In[5]:


from sampling_sgd import StochasticLinearRegressor

model = StochasticLinearRegressor()
params = model.fit(X_train, y_train)
print(params[0][-1])
print(params[1][-1])
print(params[2][-1])
print(params[3][-1])


# In[ ]:


X_new = np.hstack([np.ones((X.shape[0], 1)), X])
theta = np.linalg.inv(X_new.T @ X_new) @ X_new.T @ y
print(theta)


# In[ ]:


X_test_new = np.hstack([np.ones((X_test.shape[0], 1)), X_test])
y_pred1 = X_test_new @ model.theta1
y_pred2 = X_test_new @ model.theta2
y_pred3 = X_test_new @ model.theta3
y_pred4 = X_test_new @ model.theta4

print("MSE1: ",0.5 * np.mean((y_test - y_pred1)**2))
print("MSE2: ", 0.5 * np.mean((y_test - y_pred2)**2))
print("MSE3: ", 0.5 * np.mean((y_test - y_pred3)**2))
print("MSE4: ", 0.5 * np.mean((y_test - y_pred4)**2))


# In[ ]:


X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
y_pred1_train = X_train @ model.theta1
y_pred2_train = X_train @ model.theta2
y_pred3_train = X_train @ model.theta3
y_pred4_train = X_train @ model.theta4

print("MSE1_train: ",0.5 * np.mean((y_train - y_pred1_train)**2))
print("MSE2_train: ", 0.5 * np.mean((y_train - y_pred2_train)**2))
print("MSE3_train: ", 0.5 * np.mean((y_train - y_pred3_train)**2))
print("MSE4_train: ", 0.5 * np.mean((y_train - y_pred4_train)**2))


# In[ ]:



fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(params[0][:,0], params[0][:,1], params[0][:,2])
plt.show()


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    X1 = np.random.normal(input_mean[0], input_sigma[0], N)
    X2 = np.random.normal(input_mean[1], input_sigma[1], N)
    X = np.hstack((X1.reshape(-1,1), X2.reshape(-1,1)))
    
    noise = np.random.normal(0, noise_sigma, N)
    
    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))
    
    y = np.dot(X_bias, theta) + noise
    
    return X, y
    

class StochasticLinearRegressor:
    def __init__(self):
        theta1 = None
        theta2 = None
        theta3 = None
        theta4 = None
        
    def fit_helper(self, X, y,batch_size, learning_rate=0.01):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        theta = np.zeros(X.shape[1])
        
        theta_list = []
        
        delta_error = 1000
        prev_epoch = 0
        epoch = 0
        iteration = 0
        
        if batch_size == 1:
            while epoch &lt; 15:
                epoch += 1
                indices = np.random.permutation(X.shape[0])
                X_shuffled = X[indices]
                y_shuffled = y[indices]
        
                
                for i in range(0,X.shape[0],batch_size):
                    iteration += 1
                    
<A NAME="2"></A><FONT color = #0000FF><A HREF="match150-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]
                    
                    y_pred = X_batch @ theta
                    y_pred = y_pred.reshape(-1,1)
</FONT>                    y_batch = y_batch.reshape(-1,1)
                    
                    error = y_pred - y_batch
                    squared_error = np.sum(np.square(error)) / (2*X_batch.shape[0])

                    gradient = X_batch.T @ error / X_batch.shape[0]
                    gradient = np.sum(gradient,axis=1, keepdims=True).flatten()
                    
                    learning_rate = 0.001
                    theta -= learning_rate * gradient
                    
                    theta_list.append(theta.copy())
                    
                epoch_error = np.sum(np.square(X @ theta - y)) / (2*X.shape[0])
                delta_error = abs(epoch_error - prev_epoch)
                prev_epoch = epoch_error
                                
            return np.array(theta_list)
        else:
        
            while delta_error &gt; 0.000001:
                epoch += 1
                indices = np.random.permutation(X.shape[0])
                X_shuffled = X[indices]
                y_shuffled = y[indices]
        
                
                for i in range(0,X.shape[0],batch_size):
                    iteration += 1
                    
<A NAME="5"></A><FONT color = #FF0000><A HREF="match150-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]
                    
                    y_pred = X_batch @ theta
                    y_pred = y_pred.reshape(-1,1)
</FONT>                    y_batch = y_batch.reshape(-1,1)
                    
                    error = y_pred - y_batch
                    squared_error = np.sum(np.square(error)) / (2*X_batch.shape[0])

                    gradient = X_batch.T @ error / X_batch.shape[0]
                    gradient = np.sum(gradient,axis=1, keepdims=True).flatten()
                    
                    learning_rate = 0.001
                    theta -= learning_rate * gradient
                    
                    theta_list.append(theta.copy())
                    
                epoch_error = np.sum(np.square(X @ theta - y)) / (2*X.shape[0])
                delta_error = abs(epoch_error - prev_epoch)
                prev_epoch = epoch_error
                                
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match150-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            return np.array(theta_list)
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Stochastic Gradient Descent.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        thetas1 = self.fit_helper(X, y, 1, learning_rate)
        self.theta1 = thetas1[-1]
        thetas2 = self.fit_helper(X, y, 80, learning_rate)
        self.theta2 = thetas2[-1]
        thetas3 = self.fit_helper(X, y, 8000, learning_rate)
        self.theta3 = thetas3[-1]
        thetas4 = self.fit_helper(X, y, 800000, learning_rate)
        self.theta4 = thetas4[-1]
        
        return [thetas1, thetas2, thetas3, thetas4]
        
        
    def predict_helper(self, X, theta):
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
<A NAME="1"></A><FONT color = #00FF00><A HREF="match150-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred = np.dot(X, theta)
        
        return y_pred
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        y_pred1 = self.predict_helper(X, self.theta1)
        y_pred2 = self.predict_helper(X, self.theta2)
        y_pred3 = self.predict_helper(X, self.theta3)
        y_pred4 = self.predict_helper(X, self.theta4)
        
        return [y_pred1, y_pred2, y_pred3, y_pred4]



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
<A NAME="0"></A><FONT color = #FF0000><A HREF="match150-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
</FONT>        
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        y = y.flatten()
        
        theta = np.zeros(X.shape[1])
        
        params = []
        
        iteration = 0
        
        theta_old = np.ones(X.shape[1])
        
        while np.linalg.norm(theta_old - theta) &gt; 1e-5:
            
            iteration += 1
            prob = 1 / (1 + np.exp(-np.dot(X, theta)))
            
            grad = np.dot(X.T, (y - prob))
            
            R = np.diag(prob * (1 - prob))
            hessian = -np.dot(np.dot(X.T, R), X)
            
            theta_old = theta.copy()
            theta -= np.dot(np.linalg.inv(hessian), grad)
            
            params.append(theta.copy())
        
        self.theta = theta
        return np.array(params)
    
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        y_pred = np.round(1 / (1 + np.exp(-np.dot(X, self.theta))))
        
        return y_pred
        



#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[ ]:


data = pd.read_csv("../data/Q3/logisticX.csv", header=None)
X = data.values
data = pd.read_csv("../data/Q3/logisticY.csv", header=None)
Y = data.values
X = np.array(X)
Y = np.array(Y)


# In[3]:


from logistic_regression import LogisticRegressor

model = LogisticRegressor()
model.fit(X, Y)


# In[ ]:



X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

plt.scatter(X[Y[:, 0] == 0][:, 0], X[Y[:, 0] == 0][:, 1], color='red', label='0')
plt.scatter(X[Y[:, 0] == 1][:, 0], X[Y[:, 0] == 1][:, 1], color='blue', label='1')
plt.legend()

x1 = np.linspace(-3, 3, 100)
x2 = -(model.theta[0] + model.theta[1] * x1) / model.theta[2]
plt.plot(x1, x2, color='black')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Logistic Regression')
plt.show()


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        seperator = None
        
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        y = y.flatten()
        
        phi = np.mean(y)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match150-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        mu_0 = np.mean(X[y==0], axis=0)
        mu_1 = np.mean(X[y==1], axis=0)
</FONT>        
        if assume_same_covariance:
            sigma = np.zeros((X.shape[1], X.shape[1]))
            for i in range(X.shape[0]):
                mu = mu_0 if y[i] == 0 else mu_1
                sigma += np.outer(X[i] - mu, X[i] - mu)
            sigma /= X.shape[0]
            
            self.seperator = self.quadratic_boundary(mu_0, mu_1, sigma, sigma)
            return mu_0, mu_1, sigma
            
            
            
        else:
            sigma_0 = np.zeros((X.shape[1], X.shape[1]))
            sigma_1 = np.zeros((X.shape[1], X.shape[1]))
            for i in range(X.shape[0]):
                if y[i] == 0:
                    sigma_0 += np.outer(X[i] - mu_0, X[i] - mu_0)
                else:
                    sigma_1 += np.outer(X[i] - mu_1, X[i] - mu_1)
            sigma_0 /= np.sum(y==0)
            sigma_1 /= np.sum(y==1)
            
            self.seperator = self.quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1)
            return mu_0, mu_1, sigma_0, sigma_1
            
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        y_pred = np.zeros(X.shape[0])
        for i in range(X.shape[0]):
            y_pred[i] = 1 if self.seperator(X[i][0], X[i][1]) &gt; 0 else 0
            
        return y_pred
    
    def quadratic_boundary(self,mu0, mu1, sigma0, sigma1):
        inv_sigma0 = np.linalg.inv(sigma0)
        inv_sigma1 = np.linalg.inv(sigma1)

        a = inv_sigma0 - inv_sigma1
        b = 2 * (inv_sigma1 @ mu1 - inv_sigma0 @ mu0)
        c = mu0.T @ inv_sigma0 @ mu0 - mu1.T @ inv_sigma1 @ mu1
        c += np.log(np.linalg.det(sigma1) / np.linalg.det(sigma0))

        return lambda x1, x2: (
            a[0, 0] * x1**2 + 2 * a[0, 1] * x1 * x2 + a[1, 1] * x2**2 +
            b[0] * x1 + b[1] * x2 + c
        )



#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[ ]:



X = pd.read_csv("../data/Q4/q4x.dat", delim_whitespace=True, header=None).to_numpy()
Y = pd.read_csv("../data/Q4/q4y.dat", header=None).to_numpy()
Y = (Y == "Alaska").astype(int)


# In[3]:


from gda import GaussianDiscriminantAnalysis

model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = model.fit(X, Y,True)
print("mu_0: ", mu_0)
print("mu_1: ", mu_1)
print("sigma: ", sigma)


# In[4]:


def quadratic_boundary(mu0, mu1, sigma0, sigma1):
        inv_sigma0 = np.linalg.inv(sigma0)
        inv_sigma1 = np.linalg.inv(sigma1)

        a = inv_sigma0 - inv_sigma1
        b = 2 * (inv_sigma1 @ mu1 - inv_sigma0 @ mu0)
        c = mu0.T @ inv_sigma0 @ mu0 - mu1.T @ inv_sigma1 @ mu1
        c += np.log(np.linalg.det(sigma1) / np.linalg.det(sigma0))

        return lambda x1, x2: (
            a[0, 0] * x1**2 + 2 * a[0, 1] * x1 * x2 + a[1, 1] * x2**2 +
            b[0] * x1 + b[1] * x2 + c
        )


# In[ ]:



X_1 = X - np.mean(X, axis=0)
X_1 /= np.std(X_1, axis=0)

Y_1 = Y.flatten()


plt.scatter(X_1[Y_1 == 0][:, 0], X_1[Y_1 == 0][:, 1], color='red', label='Alaska')
plt.scatter(X_1[Y_1 == 1][:, 0], X_1[Y_1 == 1][:, 1], color='blue', label='Canada')

x1 = np.linspace(-2, 2, 100)
x2 = np.linspace(-2, 2, 100)
x1, x2 = np.meshgrid(x1, x2)
boundary = quadratic_boundary(mu_0, mu_1, sigma, sigma)
plt.contour(x1, x2, boundary(x1, x2), levels=[0], colors='black')
plt.scatter(mu_0[0], mu_0[1], color='black', marker='x', label='mu_0')
plt.scatter(mu_1[0], mu_1[1], color='black', marker='x', label='mu_1')
plt.legend()
plt.show()


# In[7]:


model2 = GaussianDiscriminantAnalysis()
mu_02, mu_12, sigma_0, sigma_1 = model2.fit(X, Y,False)

print("mu_0: ", mu_02)
print("mu_1: ", mu_12)
print("sigma_0: ", sigma_0)
print("sigma_1: ", sigma_1)


# In[ ]:



X_2 = X - np.mean(X, axis=0)
X_2 /= np.std(X_2, axis=0)

Y_2 = Y.flatten()

plt.scatter(X_2[Y_2 == 0][:, 0], X_2[Y_2 == 0][:, 1], color='red', label='Alaska')
plt.scatter(X_2[Y_2 == 1][:, 0], X_2[Y_2 == 1][:, 1], color='blue', label='Canada')

x1 = np.linspace(-3, 3, 100)
x2 = np.linspace(-3, 3, 100)
x1, x2 = np.meshgrid(x1, x2)
boundary = quadratic_boundary(mu_02, mu_12, sigma_0, sigma_1)
plt.contour(x1, x2, boundary(x1, x2), levels=[0], colors='black')
boundary2 = quadratic_boundary(mu_0, mu_1, sigma, sigma)
plt.contour(x1, x2, boundary2(x1, x2), levels=[0], colors='black')
plt.scatter(mu_0[0], mu_0[1], color='black', marker='x', label='mu_0')
plt.scatter(mu_1[0], mu_1[1], color='black', marker='x', label='mu_1')
plt.legend()
plt.show()


# In[ ]:






</PRE>
</PRE>
</BODY>
</HTML>
