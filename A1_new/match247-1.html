<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_46U3K.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_L3VEE.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from linear_regression import LinearRegressor

# Load dataset
X = pd.read_csv('../data/Q1/linearX.csv').to_numpy()
y = pd.read_csv('../data/Q1/linearY.csv').to_numpy().reshape(-1, 1)

# Get dimensions
m, n = X.shape

# Generate 2D contour grid for error function J(θ)
theta_1_vals = np.linspace(-50, 50, 100)
theta_2_vals = np.linspace(-50, 50, 100)
Theta_1, Theta_2 = np.meshgrid(theta_1_vals, theta_2_vals)
J_vals = np.zeros(Theta_1.shape)

# Compute J(θ) over the entire mesh grid
for i in range(Theta_1.shape[0]):
    for j in range(Theta_1.shape[1]):
        theta_1 = np.array(Theta_1[i, j]).reshape(1, n)  # Ensure correct shape (1, n)
        theta_2 = Theta_2[i, j] * np.ones((m, 1))  # Broadcast bias term
        predictions = X @ theta_1.T + theta_2  # Compute hypothesis
        J_vals[i, j] = (1 / (2 * m)) * np.sum((predictions - y) ** 2)

# Train model
model = LinearRegressor()
param_history = model.fit(X, y, learning_rate=0.025, tolerance=1e-4, max_iters=10000)
param_history = np.array(param_history)

# Compute loss J(theta) dynamically for each step in param_history
J_history = []
Theta_1_path = []
Theta_2_path = []
for i in range(len(param_history)):
    theta_1 = param_history[i][:-1].reshape(1, n)  # Ensure shape (1, n)
    theta_2 = param_history[i][-1]  # Extract bias term
    predictions = X @ theta_1.T + theta_2 * np.ones((m, 1))  # Compute hypothesis
    J_history.append((1 / (2 * m)) * np.sum((predictions - y) ** 2))
    Theta_1_path.append(theta_1.flatten()[0])  # Store for plotting
    Theta_2_path.append(theta_2)  # Store for plotting
J_history = np.array(J_history)
Theta_1_path = np.array(Theta_1_path)
Theta_2_path = np.array(Theta_2_path)

# Plot 2D contour of error function
fig, ax = plt.subplots()
contour = ax.contourf(Theta_1, Theta_2, J_vals, levels=50, cmap='viridis')
fig.colorbar(contour, ax=ax, shrink=0.5, aspect=5)
ax.set_xlabel("Theta 1")
ax.set_ylabel("Theta 2 (Bias)")
ax.set_title("Contour Plot of Error Function with Gradient Descent Path")

# Overlay gradient descent path
ax.scatter(Theta_1_path, Theta_2_path, color='cyan', marker='.', label='Gradient Descent Path')
plt.legend()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from linear_regression import LinearRegressor

# Load dataset
X = pd.read_csv('../data/Q1/linearX.csv').to_numpy()
y = pd.read_csv('../data/Q1/linearY.csv').to_numpy().reshape(-1, 1)

# Get dimensions
m, n = X.shape

# Train model
model = LinearRegressor()
model.fit(X, y, learning_rate=0.01, tolerance=1e-4, max_iters=10000)

# Generate points uniformly in the range of X for hypothesis function
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
y_preds = model.predict(X_range)  # Predict corresponding y values

# Plot data points and hypothesis function
plt.figure()
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match247-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X, y, color='red', marker='o', label='Data Points')  # Scatter plot for data
plt.plot(X_range, y_preds, color='blue', linestyle='-', label='Hypothesis Function')  # Line plot for hypothesis
plt.xlabel("X")
plt.ylabel("y")
plt.title("Data Points and Hypothesis Function")
</FONT>plt.legend()
plt.show()



import numpy as np
import matplotlib.pyplot as plt

import time
class LinearRegressor:
    def __init__(self):
        self.theta = None  # Row vector (1 x n)
        self.bias = None   # Scalar θ_N+1
        self.called = 0    # Track number of times fit() is called

    def fit(self, X, y, learning_rate=0.01, tolerance=1e-4, max_iters=100000):
        """
        Fit the linear regression model to the data using Gradient Descent until convergence.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
        
        learning_rate : float, default=0.01
            The learning rate for gradient descent.

        tolerance : float, default=1e-4
            Training stops when the absolute change in loss is smaller than this value.

        max_iters : int, default=10000
<A NAME="1"></A><FONT color = #00FF00><A HREF="match247-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            Safety limit for the maximum number of iterations.
        
        Returns
        -------
        param_history : numpy array of shape (actual_iters, n_features + 1)
            The list of parameters obtained at each iteration.
</FONT>        """
        # Reshape y to (m, 1) for matrix operations
        y = y.reshape(-1, 1)  # Ensures y has shape (m, 1)

        # Get dimensions
        m, n = X.shape

        # Initialize parameters only if fit() is called for the first time
        if self.called == 0:
            self.theta = np.zeros((1, n))  # (1 x n)
            self.bias = 0  # Scalar θ_N+1

        # Increment self.called outside the if block to track calls correctly
        self.called += 1

        # Store parameter history dynamically
        param_history = []
        time_history = []

        # Store loss history for plotting
        loss_history = []
        prev_loss = float("inf")  # Start with an infinite previous loss

        # Prevent extra figure logs
        # plt.ioff()  
        # plt.close('all')  # Close any previous figures

        # # Setup live plot
        # plt.ion()  # Interactive mode on
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iterations")
        # ax.set_ylabel("Loss")
        # ax.set_title("Live Loss Plot")
        # line, = ax.plot([], [], 'r-')  # Blue line

        # Gradient Descent
        t_init = time.time()
        for i in range(max_iters):
            # Compute predictions (broadcast bias explicitly)
            y_pred = np.dot(X, self.theta.T) + self.bias * np.ones((m, 1))  # Bias broadcasting

            # Compute loss function
            loss = (1 / (2 * m)) * np.dot((y_pred - y).T, (y_pred - y)).flatten()[0]
            loss_history.append(loss)

            # Check for convergence (Stop if loss change &lt; tolerance)
            if abs(prev_loss - loss) &lt; tolerance:
                print(f"\n[EARLY STOPPING] Converged at iteration {i} with loss = {loss:.6f}")
                break
            prev_loss = loss  # Update previous loss

            # Compute gradients (vectorized)
            grad_theta = (1/m) * np.dot((y_pred - y).T, X)  # (1 x n)
            grad_bias = self.bias + (1/m) * np.sum(y_pred - y)  # Corrected gradient for bias

            # Update parameters using learning_rate
            self.theta -= learning_rate * grad_theta
            self.bias -= learning_rate * grad_bias

            # Store parameters dynamically
            param_history.append(np.hstack((self.theta.flatten(), self.bias)))  # (n_features + 1,)

            # Live update of the loss plot
        #     if i % 10 == 0:  # Update every 10 iterations to avoid flickering
        #         line.set_xdata(range(len(loss_history)))
        #         line.set_ydata(loss_history)
        #         ax.relim()
        #         ax.autoscale_view()
        #         fig.canvas.flush_events()  # Properly update the live plot
        #         plt.pause(0.001)

        #     # Clean logging with '\r' to overwrite output
        #     if i % 10 == 0:  # Print less frequently
        #         print(f"\rIteration {i}: Loss = {loss:.6f}", end="")

        # plt.ioff()  # Turn off interactive mode
        # plt.show()  # Show final plot
        
        return np.array(param_history)  # Convert list to NumPy array and return
        # return np.array(time_history)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples, 1)
            The predicted target values.
        """
        m = X.shape[0]  # Get number of samples
        y_pred = np.dot(X, self.theta.T) + self.bias * np.ones((m, 1))  # Explicit bias broadcasting
        return y_pred



#!/usr/bin/env python
# coding: utf-8

# In[1]:


from linear_regression import LinearRegressor
import pandas as pd
import numpy as np


# In[2]:


X = pd.read_csv('../data/Q1/linearX.csv')
Y = pd.read_csv('../data/Q1/linearY.csv')

X = X.to_numpy()
Y = Y.to_numpy()
Y = Y.reshape(-1)


# In[3]:



model = LinearRegressor()
model.fit(X, Y)


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:








import numpy as np
import pandas as pd
from sampling_sgd import StochasticLinearRegressor  # Import SGD model

# **Load Sampled Data from Part A**
X_train = pd.read_csv('X_train.csv').to_numpy()  # (m, n)
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)  # (m, 1)

# **True Parameters Used for Sampling**
true_theta = np.array([[3], [1], [2]])  # Given in problem

# **Compute Closed-Form Solution for Linear Regression**
X_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))  # Add bias column
closed_form_theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ Y_train  # (X^T X)^(-1) X^T Y

# **Train SGD Model**
sgd_model = StochasticLinearRegressor()
sgd_theta_history = sgd_model.fit(X_train, Y_train, learning_rate=0.001, batch_size=8000, tolerance=1e-5, max_iters=100000)
sgd_theta = sgd_theta_history[-1]  # Extract final parameters from SGD

# **Print Comparison**
print("\nComparison of True Parameters, Closed-Form Solution, and SGD Learned Parameters:")
print("--------------------------------------------------------------------")
print(f"True Parameters: \n{true_theta}")
print(f"Closed-Form Solution: \n{closed_form_theta}")
print(f"SGD Learned Parameters: \n{sgd_theta}")
print("--------------------------------------------------------------------")

# **Analyze Differences**
theta_diff_closed = np.abs(true_theta - closed_form_theta)
theta_diff_sgd = np.abs(true_theta - sgd_theta.reshape(-1, 1))  # Reshape SGD theta for comparison

print(f"Difference (True - Closed-Form): \n{theta_diff_closed}")
print(f"Difference (True - SGD): \n{theta_diff_sgd}")

# **Observations**
print("\nObservations:")
if np.allclose(true_theta, closed_form_theta, atol=1e-5):
    print("- Closed-form solution accurately recovers the true parameters.")
else:
    print("- Closed-form solution slightly deviates due to numerical precision.")

if np.allclose(true_theta, sgd_theta.reshape(-1, 1), atol=1e-2):
    print("- SGD solution is close but might have small deviations due to stochastic updates.")
else:
    print("- SGD has larger deviations, likely due to noise in gradient updates.")

# **Store results for LaTeX report**
results = pd.DataFrame({
    "Method": ["True Parameters", "Closed-Form Solution", "SGD Solution"],
    "Theta_0": [true_theta[0,0], closed_form_theta[0,0], sgd_theta[0]],
    "Theta_1": [true_theta[1,0], closed_form_theta[1,0], sgd_theta[1]],
    "Theta_2": [true_theta[2,0], closed_form_theta[2,0], sgd_theta[2]]
})

# **Save Results as CSV for Report**
results.to_csv("theta_comparison.csv", index=False)



#!/usr/bin/env python
# coding: utf-8

# In[10]:




import numpy as np
import pandas as pd
from sampling_sgd import generate

N = 1000000 # a million samples
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.array([np.sqrt(2)])


# In[11]:




X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)
# we will do a 80-20 split of the data and save it as csv file
# i want to name the headers of features x1 and x2
X = pd.DataFrame(X, columns=['x1', 'x2'])
y = pd.DataFrame(y, columns=['y'])
split = int(0.8 * N)

# Split the data
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Save to csv files
X_train.to_csv("X_train.csv", index=False)
X_test.to_csv("X_test.csv", index=False)
y_train.to_csv("y_train.csv", index=False)
y_test.to_csv("y_test.csv", index=False)
#


# In[ ]:





# In[12]:



X_train = pd.read_csv("X_train.csv")
Y_train = pd.read_csv("y_train.csv")
X_test = pd.read_csv("X_test.csv")
Y_test = pd.read_csv("y_test.csv")


# In[13]:



X_train.shape


# In[14]:



X_train.head


# In[15]:



Y_train


# In[ ]:








import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor

# **Load Training and Test Data**
X_train = pd.read_csv('X_train.csv').to_numpy()
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)  # Ensure column vector

X_test = pd.read_csv('X_test.csv').to_numpy()
Y_test = pd.read_csv('Y_test.csv').to_numpy().reshape(-1, 1)  # Ensure column vector

# **Train SGD Model**
sgd_model = StochasticLinearRegressor()
sgd_theta_history = sgd_model.fit(X_train, Y_train, learning_rate=0.001, batch_size=8000, tolerance=1e-5, max_iters=100000)
sgd_theta = sgd_theta_history[-1]  # Extract final parameters from SGD

# **Function to Compute Mean Squared Error (MSE)**
# **Function to Compute Mean Squared Error (MSE)**
def compute_mse(X, Y, theta):
    """Compute Mean Squared Error (MSE) correctly"""
    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))  # Add bias column
    Y_pred = np.dot(X_bias, theta)  # Predictions
    mse = np.mean((Y_pred - Y) ** 2)  # Corrected MSE formula
    return mse

# **Compute Training and Test MSE**
train_mse = compute_mse(X_train, Y_train, sgd_theta)
test_mse = compute_mse(X_test, Y_test, sgd_theta)

# **Print and Compare Errors**
print("\nTraining vs. Test MSE Comparison:")
print("--------------------------------------")
print(f"Training MSE: {train_mse:.6f}")
print(f"Test MSE: {test_mse:.6f}")
print("--------------------------------------")

# **Plot Training vs Test Error**
plt.figure(figsize=(6, 5))
plt.bar(["Training MSE", "Test MSE"], [train_mse, test_mse], color=['blue', 'red'])
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Training vs Test MSE Comparison")
plt.show()




import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor

# Load training data
X_train = pd.read_csv('X_train.csv').to_numpy()
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)

# Batch sizes to test
batch_sizes = [8000, 1]
learning_rate = 0.001
colors = ['b', 'r']  # Different color for each batch size

# Create a 3D figure
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')

# Store results for analysis
results = []

# Iterate over different batch sizes
for idx, batch_size in enumerate(batch_sizes):
    print(f"\nTraining with batch size r = {batch_size}...")
    
    # Initialize model
    model = StochasticLinearRegressor()
    
    # Train model and get parameter history
    start_time = time.time()
    param_history = model.fit(X_train, Y_train, 
                            learning_rate=learning_rate,
                            batch_size=batch_size, 
                            tolerance=1e-4,
                            max_iters=100000)
    end_time = time.time()
    
    # Convert parameter history to numpy array for easier plotting
    param_history = np.array(param_history)
    
    # Plot the parameter trajectory in 3D
    ax.plot(param_history[:, 0],  # theta0
            param_history[:, 1],  # theta1
            param_history[:, 2],  # theta2
            color=colors[idx],
            label=f'Batch Size = {batch_size}',
            linewidth=2,
            alpha=0.7)
    
    # Add markers for start and end points
    ax.scatter(param_history[0, 0],  # theta0 start
              param_history[0, 1],  # theta1 start
              param_history[0, 2],  # theta2 start
              color=colors[idx],
              marker='o',
              s=100,
              label=f'Start (r={batch_size})')
    
    ax.scatter(param_history[-1, 0],  # theta0 end
              param_history[-1, 1],  # theta1 end
              param_history[-1, 2],  # theta2 end
              color=colors[idx],
              marker='*',
              s=200,
              label=f'End (r={batch_size})')
    
    # Calculate final loss
    final_theta = param_history[-1]
    final_loss = (1 / (2 * len(Y_train))) * np.sum((model.predict(X_train) - Y_train) ** 2)
    elapsed_time = end_time - start_time
    
    # Store results
    results.append((batch_size, final_theta, final_loss, elapsed_time))
    print(f"Final θ values for batch size {batch_size}:")
    print(f"θ₀: {final_theta[0]:.6f}")
    print(f"θ₁: {final_theta[1]:.6f}")
    print(f"θ₂: {final_theta[2]:.6f}")
    print(f"Final loss: {final_loss:.6f}, Time taken: {elapsed_time:.2f} seconds")

# Customize the 3D plot
ax.set_xlabel('θ₀')
ax.set_ylabel('θ₁')
ax.set_zlabel('θ₂')
ax.set_title('Parameter Movement in 3D Space (θ₀, θ₁, θ₂) for Different Batch Sizes')

# Add legend
ax.legend(bbox_to_anchor=(1.15, 1), loc='upper right')

# Adjust the view angle for better visualization
ax.view_init(elev=20, azim=45)

# Add grid
ax.grid(True)

# Adjust layout to prevent label cutoff
plt.tight_layout()
plt.show()

# Create DataFrame for additional analysis
df_results = pd.DataFrame(results, columns=["Batch Size", "Final Theta", "Final Loss", "Time Taken"])

# Plot convergence metrics



#!/usr/bin/env python
# coding: utf-8

# In[ ]:



import numpy as np
import pandas as pd


# In[ ]:


X_train = pd.read_csv('X_train.csv')
Y_train = pd.read_csv('Y_train.csv')






import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor as LinearRegressor

# Define learning rates to test
# learning_rates = [10, 1, 0.1, 0.01, 0.001]
learning_rates = [10,1 ,0.1, 0.01, 0.001, 0.0005, 0.00001]

n_iters_list = []
final_losses = []
time_taken_list = []

X = pd.read_csv('../data/Q1/linearX.csv')
Y = pd.read_csv('../data/Q1/linearY.csv')

X = X.to_numpy()
Y = Y.to_numpy()
y = Y.reshape(-1)

# Run experiments for each learning rate
for eta in learning_rates:
    print(f"Running gradient descent with learning rate {eta}...")
    model = LinearRegressor()
    start_time = time.time()
    param_history = model.fit(X, y, learning_rate=eta, tolerance=1e-4, max_iters=10000)
    # Extract the final parameters
    final_params = param_history[-1]  # Last row contains the final parameters
    theta_values = final_params[:-1]  # All but last value are theta
    bias_value = final_params[-1]  # Last value is bias
    print(f"Final parameters for learning rate {eta}:")
    print(f"Theta (θ): {theta_values}")
    print(f"Bias (θ₀): {bias_value}\n")
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # Track results
    n_iters = param_history.shape[0]  # Number of iterations taken to stop
    final_loss = (1 / (2 * len(y))) * np.sum((model.predict(X) - y.reshape(-1, 1)) ** 2)
    
    n_iters_list.append(n_iters)
    final_losses.append(final_loss)
    time_taken_list.append(elapsed_time)
    print(f"Learning rate {eta}: Converged in {n_iters} iterations with final loss {final_loss:.6f} in {elapsed_time:.2f} seconds\n")

# Convert learning rates to log scale
log_learning_rates = np.log10(learning_rates)

# Plot 1: n_iters vs. learning rate
plt.figure()
plt.plot(learning_rates, n_iters_list, marker='o', linestyle='-')
plt.xlabel("Learning Rate")
plt.ylabel("Iterations Until Convergence")
plt.xscale("log")
plt.title("Iterations Until Convergence vs. Learning Rate")
plt.grid(True)
plt.show()

# Plot 2: Final loss vs. learning rate
plt.figure()
plt.plot(learning_rates, final_losses, marker='o', linestyle='-')
plt.xlabel("Learning Rate")
plt.ylabel("Final Loss at Convergence")
plt.xscale("log")
plt.title("Final Loss vs. Learning Rate")
plt.grid(True)
plt.show()

# Plot 3: n_iters and final loss vs. log(learning rate)
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel("Log(Learning Rate)")
ax1.set_ylabel("Iterations Until Convergence", color=color)
ax1.plot(log_learning_rates, n_iters_list, marker='o', linestyle='-', color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.grid(True)

ax2 = ax1.twinx()  # Create a secondary y-axis
color = 'tab:blue'
ax2.set_ylabel("Final Loss at Convergence", color=color)
ax2.plot(log_learning_rates, final_losses, marker='s', linestyle='--', color=color)
ax2.tick_params(axis='y', labelcolor=color)

fig.suptitle("Iterations & Final Loss vs. Log(η)")
plt.show()

# 3D Plot: Learning Rate (η) vs. Time Taken vs. Final Loss
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(learning_rates, time_taken_list, final_losses, marker='o', color='b')
ax.set_xlabel("η (Learning Rate)")
ax.set_ylabel("Time Taken to Converge (s)")
ax.set_zlabel("Final Loss at Convergence")
ax.set_xscale("log")
ax.set_title("3D Plot: Learning Rate vs. Time vs. Final Loss")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import time
import psutil

import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    Note that we have 2 input features.

    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match247-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    # Ensure inputs are numpy arrays (for safety)
    theta = np.array(theta)
    input_mean = np.array(input_mean)
</FONT>    input_sigma = np.array(input_sigma)

    # Generate input fea
    # ures x1 and x2 from normal distributions
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    
    # Generate Gaussian noise epsilon
    epsilon = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute y using y = theta_0 + theta_1*x1 + theta_2*x2 + epsilon
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + epsilon

    # Compute mean and standard deviation of y
    y_mean = np.mean(y)
    y_std = np.std(y)

    # Logging shapes and statistics for debugging
    print(f"Generated Data Shapes:")
    print(f"  X shape: {X.shape}  (should be {N, 2})")
    print(f"  y shape: {y.shape}  (should be {N,})")
    print(f"  epsilon shape: {epsilon.shape}  (should be {N,})")
    print(f"\nStatistics of y:")
    print(f"  Mean of y: {y_mean:.4f}")
    print(f"  Standard deviation of y: {y_std:.4f}")

    return X, y



class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None  # Parameter vector (n+1, 1)
        self.called = 0  # Track number of calls to fit()

    def fit(self, X, y, learning_rate=0.01, batch_size=32, tolerance=1e-4, max_iters=100000):
        """
        Fit the linear regression model using Stochastic Gradient Descent (SGD)
        with live batch loss plotting.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input feature matrix (without bias column).
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float, default=0.01
            Learning rate for gradient descent.
        batch_size : int, default=32
            Batch size for mini-batch updates.
        tolerance : float, default=1e-4
            Stopping criterion based on epoch loss change.
        max_iters : int, default=100000
            Maximum number of iterations.

        Returns
        -------
        param_history : numpy array of shape (n_iter, n+1)
            The list of parameter values at each iteration.
        """
        # Get dimensions
        m, n = X.shape

        # Prepend a column of ones to X for bias term
        X = np.hstack((np.ones((m, 1)), X))  # X is now (m, n+1)
        y = y.reshape(-1, 1)  # Ensure y is (m, 1)

        # Initialize parameters
        if self.called == 0:
            self.theta = np.zeros((n + 1, 1))  # (n+1, 1)

        self.called += 1  # Track number of calls

        num_batches = m // batch_size + int(m % batch_size != 0)  # Ensure last batch is counted
        prev_epoch_loss = 0  # **Updated Fix: Start from 0**
        
        param_history = []  # Store parameter history
        batch_loss_history = []  # Store batch loss for live plotting

        # **Initialize Live Plot**
        # plt.ion()  # Enable interactive mode
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iteration")
        # ax.set_ylabel("Batch Loss")
        # ax.set_title("Live Batch Loss Plot")
        # line, = ax.plot([], [], 'r-', label="Batch Loss")  # Line for updating loss
        # plt.legend()
        
        iter_count = 0  # Track number of iterations

        # **Training Loop Over Epochs**
        for epoch in range(max_iters // num_batches):
            epoch_loss = 0  # Initialize loss for this epoch

            # **Iterate Over Mini-Batches**
            for batch_no in range(num_batches):
                batch_start = batch_no * batch_size
                batch_end = min((batch_no + 1) * batch_size, m)
                X_b = X[batch_start:batch_end]
                y_b = y[batch_start:batch_end]
                actual_batch_size = X_b.shape[0]

                # Compute Predictions
                y_pred = np.dot(X_b, self.theta)  # (batch_size, 1)

                # Compute Batch Loss
                batch_loss = (1 / (2 * actual_batch_size)) * np.sum((y_pred - y_b) ** 2)
                epoch_loss += actual_batch_size * batch_loss  # Accumulate batch loss weighted by size

                # Compute Gradient and Update Parameters
                grad_theta = (1 / actual_batch_size) * np.dot(X_b.T, (y_pred - y_b))  # (n+1, 1)
                self.theta -= learning_rate * grad_theta  # Update parameters

                # Store parameter history at this iteration
                param_history.append(self.theta.flatten().copy())  # Ensure independent storage

                # **Update Live Plot**
                batch_loss_history.append(batch_loss)
                # line.set_xdata(range(len(batch_loss_history)))
                # line.set_ydata(batch_loss_history)
                # ax.relim()
                # ax.autoscale_view()
                # fig.canvas.flush_events()  # Refresh the plot

                iter_count += 1  # Increment iteration count

            # **Normalize Epoch Loss**
            epoch_loss /= m

            print(f"Epoch {epoch + 1}: Loss = {epoch_loss:.6f}, self.theta = {self.theta }" , end = '\r')
            # **Check Convergence**
            if abs(epoch_loss - prev_epoch_loss) &lt; tolerance:
                print(f"[EARLY STOPPING] Converged at epoch {epoch} with loss = {epoch_loss:.6f}")
                break

            prev_epoch_loss = epoch_loss  # Update for next epoch tracking

        # plt.ioff()  # Turn off interactive mode
        # plt.show()  # Display final plot

        return np.array(param_history)  # Convert list to numpy array (n_iter, n+1)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input feature matrix.

        Returns
        -------
        y_pred : numpy array of shape (n_samples, 1)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))  # Add bias column
        return np.dot(X, self.theta)  # Compute predictions



import numpy as np
import matplotlib.pyplot as plt
import time

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.called = 0  # Track number of times fit() is called
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, max_iters=100, tolerance=1e-4):
        """
        Fit the logistic regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        max_iters : int, default=100
            Maximum number of iterations for Newton's Method.
        
        tolerance : float, default=1e-4
            The stopping criterion based on function value change.
        
        Returns
        -------
        param_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Add intercept term (convert X from m x n to m x (n+1))
        m, n = X.shape
        X = np.hstack((X, np.ones((m, 1))))  # Append a column of ones
        
        # Initialize parameters only if fit() is called for the first time
        if self.called == 0:
            self.theta = np.zeros((n + 1, 1))  # (n+1) x 1
        self.called += 1  # Increment call count
        
        y = y.reshape(-1, 1)  # Ensure y is a column vector
        param_history = []
        prev_log_likelihood = -np.inf
        loss_history = []
        
        # Setup live plot
        # plt.ion()
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iterations")
        # ax.set_ylabel("Log-Likelihood")
        # ax.set_title("Live Loss Plot")
        # line, = ax.plot([], [], 'r-')
        
        for i in range(max_iters):
            # Compute predictions
            h = self.sigmoid(X @ self.theta)
            
            # Compute log-likelihood
            log_likelihood = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
            loss_history.append(log_likelihood)
            
            # Compute gradient
            gradient = X.T @ (y - h)  # (n+1) x 1
            
            # Compute Hessian
            R = np.diagflat(h * (1 - h))  # m x m
            H = -(X.T @ R @ X)  # (n+1) x (n+1)
            
            # Newton's update step
            theta_update = np.linalg.inv(H) @ gradient  # (n+1) x 1
            self.theta -= theta_update
            param_history.append(self.theta.flatten())
            
            # Update live loss plot
            # line.set_xdata(range(len(loss_history)))
            # line.set_ydata(loss_history)
            # ax.relim()
            # ax.autoscale_view()
            # fig.canvas.flush_events()
            # plt.pause(0.001)
            
            # Check stopping criterion (change in function value)
            if abs(log_likelihood - prev_log_likelihood) &lt; tolerance:
                print(f"Converged after {i+1} iterations.")
                break
            prev_log_likelihood = log_likelihood
        
        # plt.ioff() plt.show()
        
        return np.array(param_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Add intercept term
        m, _ = X.shape
        X = np.hstack((X, np.ones((m, 1))))
        
        # Compute probabilities
        probs = self.sigmoid(X @ self.theta)
        
        # Convert to binary predictions
        return (probs &gt;= 0.5).astype(int).flatten()



#!/usr/bin/env python
# coding: utf-8

# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[2]:



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[6]:



X = pd.read_csv('../data/Q3/logisticX.csv', header=None).to_numpy()
Y = pd.read_csv('../data/Q3/logisticY.csv', header=None).to_numpy().reshape(-1,1)


# In[7]:



X.shape
Y.shape


# 

# In[ ]:



X_mean = np.mean(X, axis= 0)
X_std = np.std(X, axis= 0)
print(X_mean.shape)
print(X_std.shape)


# In[ ]:


X_norm = (X - X_mean)/X_std
print(X_norm)


# In[23]:



from logistic_regression import LogisticRegressor

logistic_regressor = LogisticRegressor()
params = logistic_regressor.fit(X_norm, Y)


# In[ ]:








import numpy as np

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = True
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        assume_same_covariance : bool, default=False
            Whether to assume a shared covariance matrix across both classes.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays (mu_0, mu_1, sigma) 
            If assume_same_covariance = False - 4-tuple of numpy arrays (mu_0, mu_1, sigma_0, sigma_1)
            The parameters learned by the model.
        """
        # Store assumption
        self.assume_same_covariance = assume_same_covariance
        
        # Compute class priors
        self.phi = np.mean(y)  # P(y=1)
        
        # Compute class means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        if assume_same_covariance:
            # Compute shared covariance matrix
            m = X.shape[0]
            self.sigma = (1/m) * np.sum([(X[i] - (self.mu_1 if y[i] else self.mu_0)).reshape(-1, 1) @
                                         (X[i] - (self.mu_1 if y[i] else self.mu_0)).reshape(1, -1)
                                         for i in range(m)], axis=0)
            return self.mu_0, self.mu_1, self.sigma
        else:
            # Compute class-specific covariance matrices
            self.sigma_0 = np.cov(X[y == 0].T, bias=True)
            self.sigma_1 = np.cov(X[y == 1].T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data using Bayes' theorem.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        def gaussian_pdf(x, mean, cov):
            d = x.shape[0]
            det_cov = np.linalg.det(cov)
            inv_cov = np.linalg.inv(cov)
            norm_factor = 1 / (np.sqrt((2 * np.pi) ** d * det_cov))
            exp_term = np.exp(-0.5 * (x - mean).T @ inv_cov @ (x - mean))
            return norm_factor * exp_term
        
        # Compute likelihoods using Gaussian PDF
        p_x_given_y0 = np.array([gaussian_pdf(x, self.mu_0, self.sigma if self.assume_same_covariance else self.sigma_0) for x in X])
        p_x_given_y1 = np.array([gaussian_pdf(x, self.mu_1, self.sigma if self.assume_same_covariance else self.sigma_1) for x in X])
        
        # Compute posterior probabilities using Bayes' theorem
        p_y1_given_x = (self.phi * p_x_given_y1) / (self.phi * p_x_given_y1 + (1 - self.phi) * p_x_given_y0)
        
        # Assign class labels based on probability threshold
        return (p_y1_given_x &gt;= 0.5).astype(int)




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train GDA with shared covariance (Linear Boundary)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match247-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)

# Compute posterior probabilities for Linear Boundary
sigma_inv = np.linalg.inv(sigma)
w = np.dot(sigma_inv, (mu_1 - mu_0))  # Weight vector
</FONT>b = -0.5 * np.dot(mu_1.T, np.dot(sigma_inv, mu_1)) + 0.5 * np.dot(mu_0.T, np.dot(sigma_inv, mu_0))

linear_decision = np.dot(X_norm, w) + b
y_pred_linear = (linear_decision &gt; 0).astype(int)  # Convert to class labels

# Train GDA with separate covariance (Quadratic Boundary)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match247-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

mu_0_q, mu_1_q, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)

# Compute posterior probabilities for Quadratic Boundary
sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)

A = 0.5 * (sigma_0_inv - sigma_1_inv)
B = np.dot(sigma_1_inv, mu_1_q) - np.dot(sigma_0_inv, mu_0_q)
</FONT>C = (
    -0.5 * np.dot(mu_1_q.T, np.dot(sigma_1_inv, mu_1_q))
    + 0.5 * np.dot(mu_0_q.T, np.dot(sigma_0_inv, mu_0_q))
    + 0.5 * np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
)

quadratic_decision = np.einsum("ij,jk,ik-&gt;i", X_norm, A, X_norm) + np.dot(X_norm, B) + C
y_pred_quadratic = (quadratic_decision &gt; 0).astype(int)  # Convert to class labels

# Compute Misclassification Metrics
misclassified_linear = (y_pred_linear != y)
misclassified_quadratic = (y_pred_quadratic != y)

num_misclass_linear = np.sum(misclassified_linear)
num_misclass_quadratic = np.sum(misclassified_quadratic)

misclass_rate_linear = num_misclass_linear / len(y)
misclass_rate_quadratic = num_misclass_quadratic / len(y)

# Output the misclassification statistics
print(f"Misclassification Rate (Linear Boundary): {misclass_rate_linear:.4f} ({num_misclass_linear} points misclassified)")
print(f"Misclassification Rate (Quadratic Boundary): {misclass_rate_quadratic:.4f} ({num_misclass_quadratic} points misclassified)")

# Unnormalize for plotting
X1_unnorm = X[:, 0]
X2_unnorm = X[:, 1]

# Plot the data with misclassified points highlighted
plt.figure(figsize=(8, 6))
plt.scatter(X1_unnorm[y == 0], X2_unnorm[y == 0], color='red', label="Alaska (Class 0)", alpha=0.6)
plt.scatter(X1_unnorm[y == 1], X2_unnorm[y == 1], color='blue', label="Canada (Class 1)", alpha=0.6)

# Mark misclassified points separately for each model
plt.scatter(X1_unnorm[misclassified_linear], X2_unnorm[misclassified_linear], edgecolors='black', facecolors='none', s=100, label="Misclassified (Linear)")
plt.scatter(X1_unnorm[misclassified_quadratic], X2_unnorm[misclassified_quadratic], edgecolors='green', facecolors='none', s=100, label="Misclassified (Quadratic)")

# Labels and Legends
plt.xlabel("Freshwater Growth Ring Diameter (X1)")
plt.ylabel("Marine Water Growth Ring Diameter (X2)")
plt.title("Misclassified Points in GDA")
plt.legend()
plt.grid()
plt.savefig("../fig/gda_misclassification.png")
plt.show()




import pandas as pd
import numpy as np
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# Print computed parameters
print(f"μ_0 (Mean for Class 0 - Alaska):\n{mu_0}")
print(f"μ_1 (Mean for Class 1 - Canada):\n{mu_1}")
print(f"Σ (Shared Covariance Matrix):\n{sigma}")



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# **Plot Training Data**
plt.figure(figsize=(8, 6))

# Scatter plot for each class
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska (Class 0)")
<A NAME="5"></A><FONT color = #FF0000><A HREF="match247-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada (Class 1)")

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (x1)")
plt.ylabel("Marine Water Growth Ring Diameter (x2)")
plt.title("Scatter Plot of Training Data")
plt.legend()
plt.grid()
</FONT>plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# Compute Decision Boundary
sigma_inv = np.linalg.inv(sigma)  # Compute Σ⁻¹
w = np.dot(sigma_inv, (mu_1 - mu_0))  # Compute w
b = -0.5 * np.dot((mu_1 + mu_0).T, np.dot(sigma_inv, (mu_1 - mu_0)))  # Compute b

# Convert w and b back to unnormalized space
w_unnorm = w / X_std  # Adjust w using standard deviation
b_unnorm = b - np.dot(w_unnorm, X_mean)  # Adjust b using mean

# Generate decision boundary (Unnormalized)
x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)  # Unnormalized x1 values
x2_vals = -(w_unnorm[0] / w_unnorm[1]) * x1_vals - (b_unnorm / w_unnorm[1])  # Unnormalized decision boundary

# Plot GDA Decision Boundary
plt.figure(figsize=(8, 6))

# Scatter plot of original (unnormalized) data
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska (Class 0)")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada (Class 1)")

# Corrected decision boundary using unnormalized values
plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (x1)")
plt.ylabel("Marine Water Growth Ring Diameter (x2)")
plt.title("GDA Decision Boundary (Fixed)")
plt.legend()
plt.grid()

# Save & Show the Figure
plt.savefig("../fig/gda_decision_boundary_fixed.png")
plt.show()




import numpy as np
import pandas as pd
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model with separate covariances
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)  # Compute separate covariance matrices

# Print computed parameters
print(f"μ_0 (Mean for Class 0 - Alaska):\n{mu_0}")
print(f"μ_1 (Mean for Class 1 - Canada):\n{mu_1}")
print(f"Σ_0 (Covariance Matrix for Alaska):\n{sigma_0}")
print(f"Σ_1 (Covariance Matrix for Canada):\n{sigma_1}")




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train GDA Model (for both linear and quadratic cases)
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Linear boundary
mu_0_q, mu_1_q, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)  # Quadratic boundary

# Unnormalize means for plotting
mu_0 = mu_0 * X_std + X_mean
mu_1 = mu_1 * X_std + X_mean

# Compute Linear Decision Boundary
sigma_inv = np.linalg.inv(sigma)
theta = sigma_inv @ (mu_1 - mu_0)
theta_0 = -0.5 * (mu_1.T @ sigma_inv @ mu_1) + 0.5 * (mu_0.T @ sigma_inv @ mu_0)

# Define range for plotting
x1_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_linear = -(theta[0] / theta[1]) * x1_range - (theta_0 / theta[1])

# Compute Quadratic Decision Boundary
sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)
A = 0.5 * (sigma_0_inv - sigma_1_inv)
B = (sigma_1_inv @ mu_1 - sigma_0_inv @ mu_0).T
C = (
    -0.5 * (mu_1.T @ sigma_1_inv @ mu_1)
    + 0.5 * (mu_0.T @ sigma_0_inv @ mu_0)
    - 0.5 * np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
)

# Create meshgrid for quadratic boundary
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
x2_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 200)
X1, X2 = np.meshgrid(x1_vals, x2_vals)
Z = np.zeros_like(X1)

for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        x_vec = np.array([[X1[i, j]], [X2[i, j]]])
        Z[i, j] = x_vec.T @ A @ x_vec + B @ x_vec + C  # Quadratic boundary equation

# Plot data points
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Alaska (Class 0)')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Canada (Class 1)')

# Plot decision boundaries
plt.plot(x1_range, x2_linear, color="black", label="Linear Boundary")
plt.contour(X1, X2, Z, levels=[0], colors="green", linewidths=2)  # Quadratic Boundary

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (X1)")
plt.ylabel("Marine Water Growth Ring Diameter (X2)")
plt.title("GDA Decision Boundaries (Linear and Quadratic)")
plt.legend()
plt.grid()
plt.savefig("../fig/gda_decision_boundary.png")
plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
