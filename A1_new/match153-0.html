<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_HQ61X.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_HQ61X.py<p><PRE>


import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

class LinearRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 20000

    def error_function(self, X, y):
        m = X.shape[0]
        predictions = X @ self.parameters
        errors = y - predictions
        return (errors @ errors) / (2 * m)
        
    def stopping_condition_1(self, old_error, new_error):
        return abs(new_error - old_error) &lt; 1e-7
    
    def fit(self, X_input, y, learning_rate=0.01):
     
        X = np.c_[np.ones(X_input.shape[0]), X_input]
        
        m, n = X.shape
        self.parameters = np.zeros(n)
        old_error = self.error_function(X, y)

        answer_list = [self.parameters.copy()]
        for iteration in range(self.max_iteration):
            predictions = X @ self.parameters
            errors = y - predictions

            gradients = -(X.T @ errors) / m
  
            self.parameters -= learning_rate * gradients
            answer_list.append(self.parameters.copy())

            new_error = self.error_function(X, y)

            if self.stopping_condition_1(old_error, new_error):
                # print(f"Converged at iteration: {iteration}")
                break
            old_error = new_error

        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result

    def predict(self, X1):
    
        X = np.c_[np.ones(X1.shape[0]), X1]
        predictions = X @ self.parameters
        result = np.column_stack((np.arange(len(predictions)), predictions))
        return result
    
    def hypothesis_plot_Q2(self,X,y):
        plt.scatter(X, y, color='blue', label='Data Points')
        x_values = np.linspace(min(X), max(X), 100)
        d = self.fit(X, y)
        y_values = d[-1][1] + d[-1][2] * x_values
        plt.plot(x_values, y_values, color='red', label='Hypothesis Function')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Data Points and Hypothesis Function')
        plt.legend()
        plt.show()
        
    def mesh_3d_Q3_a(self,X,y):
        theta0_vals = np.linspace(-60, 60, 1000)
        theta1_vals = np.linspace(-60, 60, 1000)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        X_with_intercept = np.c_[np.ones(X.shape[0]), X]
   
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                t = np.array([theta0, theta1])
                m = X_with_intercept.shape[0]
                predictions = X_with_intercept @ t
                errors = y - predictions
                J_vals[i, j] = (errors @ errors) / (2 * m)


        # Plot the surface
        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis')

        plt.xlabel('theta0')
        plt.ylabel('theta1')
        ax.set_zlabel('Error')
        plt.title('Error function')
        plt.show()
        
    def mesh_3d_Q3_b(self, X, y):
    
        theta0_vals = np.linspace(-20, 40, 100)
        theta1_vals = np.linspace(-20, 40, 100)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        X_with_intercept = np.c_[np.ones(X.shape[0]), X]
 
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                t = np.array([theta0, theta1])
                m = X_with_intercept.shape[0]
                predictions = X_with_intercept @ t
                errors = y - predictions
                J_vals[i, j] = (errors @ errors) / (2 * m)


        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis', alpha=0.6)
   
        d = self.fit(X, y)
        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_zlabel('Error')
        plt.title('Error Function with Gradient Descent Path')
        

        path_points = [] 
        errors_list = []

        for frame in range(len(d)):
            params = d[frame]
            theta0_val, theta1_val = params[1], params[2]
  
            t = np.array([theta0_val, theta1_val])
            predictions = X_with_intercept @ t
            errors = y - predictions
            error_val = (errors @ errors) / (2 * X_with_intercept.shape[0])

            path_points.append([theta0_val, theta1_val])
            errors_list.append(error_val)
    
            path_points_arr = np.array(path_points)
    
            ax.plot(path_points_arr[:, 0], path_points_arr[:, 1], errors_list, color='red', marker='o', markersize=2)
            
            plt.pause(0.2)

        plt.show()
        

    def contour_4(self,X,y):
        theta0_vals = np.linspace(-20, 40, 1000)
        theta1_vals = np.linspace(-20, 40, 1000)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        X_with_intercept = np.c_[np.ones(X.shape[0]), X]
        fig, ax = plt.subplots(figsize=(12, 10))

        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                t = np.array([theta0, theta1])
                m = X_with_intercept.shape[0]
                predictions = X_with_intercept @ t
                errors = y - predictions
                J_vals[i, j] = (errors @ errors) / (2 * m)
  
        d = self.fit(X, y,0.001)
        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        ax.contour(theta0_vals, theta1_vals, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='viridis')
        path_points = [] 

        for frame in range(len(d)):
            params = d[frame]
            theta0_val, theta1_val = params[1], params[2]
            path_points.append([theta0_val, theta1_val])
            path_points_arr = np.array(path_points)
            
            ax.plot(path_points_arr[:, 0], path_points_arr[:, 1], 'r-o', markersize=5, linewidth=1)
            
            plt.pause(0.01)

        plt.xlabel('theta0')
        plt.ylabel('theta1')
        plt.title('Error Function Contours with Gradient Descent Path')
        plt.show()

def read_csv_to_arrays(file_x, file_y):
    x_data = np.loadtxt(file_x, delimiter=',').reshape(-1, 1)
    y_data = np.loadtxt(file_y, delimiter=',')
    return x_data, y_data


def file_read():
    X, y = read_csv_to_arrays('../data/Q1/linearX.csv', '../data/Q1/linearY.csv')
    return X,y

def train_test_split_basic(X, y, test_size=0.2):
 
    num_samples = X.shape[0]

    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    
    split_idx = int(num_samples * (1 - test_size))
 
    train_indices, test_indices = indices[:split_idx], indices[split_idx:]
    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y[train_indices], y[test_indices]
    
    return X_train, X_test, y_train, y_test

def best_learning_rate_finder():
    a = LinearRegressor()
    error = np.inf
    learn = 0.0001
    X, y = file_read()
    X_train, X_test, y_train, y_test = train_test_split_basic(X, y, test_size=0.3)
    while learn &lt; 1:
        w  = a.fit(X_train, y_train, learning_rate=learn)
        d = w[-1]
        Y = np.c_[np.ones(X_test.shape[0]), X_test]
        error2 = a.error_function(Y, y_test)
        if(error2&lt;error):
            error = error2
            best_learn = learn
            print("Final parameters:", d[1],d[2],learn,error2,w.shape[0])
            error = error2
        learn+=0.0001
    print("Best Learning Rate:", best_learn)
    return best_learn
    
def final_parameter():
    a = LinearRegressor()
    X,y = file_read()
    learn = best_learning_rate_finder()
    d = a.fit(X, y,learn)[-1]
    print("Final parameters:", d[1],d[2])
    return d
    
a = LinearRegressor()
X,y = file_read()

a.fit(X, y,0.005)
a.contour_4(X,y)
# a = LinearRegressor()
# a.contour_4(*file_read())



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LinearRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 10000

    def error_function(self, X, y):
        m = X.shape[0]
        predictions = X @ self.parameters
        errors = y - predictions
        return (errors @ errors) / (2 * m)
        
    def stopping_condition_1(self, old_error, new_error):
        return abs(new_error - old_error) &lt; 1e-7
    
    def fit(self, X_input, y, learning_rate=0.005):
        X = np.c_[np.ones(X_input.shape[0]), X_input]
        
        m, n = X.shape
        self.parameters = np.zeros(n)
        old_error = self.error_function(X, y)
        answer_list = [self.parameters.copy()]
        for iteration in range(self.max_iteration):
            predictions = X @ self.parameters
            errors = y - predictions
            
  
            gradients = -(X.T @ errors) / m
            
        
            self.parameters -= learning_rate * gradients
            answer_list.append(self.parameters.copy())
       
            new_error = self.error_function(X, y)
        
            
            if self.stopping_condition_1(old_error, new_error):
                #print(iteration)
                break
            old_error = new_error

        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result

    def predict(self, X1):
        X = np.c_[np.ones(X1.shape[0]), X1]
        predictions = X @ self.parameters
        result = np.column_stack((np.arange(len(predictions)), predictions))
        return result
    




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt



def splitter(X,y):
    train_size = int(0.8 * X.shape[0])  
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_test = X[train_size:]
    y_test = y[train_size:]
    return X_train, y_train, X_test, y_test

def generate(N, theta, input_mean, input_sigma, noise_sigma):
  

    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match153-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
  
    X = np.column_stack((x1, x2))
  
    noise = np.random.normal(0, noise_sigma, N)
   
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    
    return X, y
</FONT>

class StochasticLinearRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 10000
        
    def error_function(self, X, y):
        m = X.shape[0]
        predictions = X @ self.parameters
        errors = y - predictions
        return (errors @ errors) / (2 * m)
        
    def stopping_condition_1(self, old_error, new_error):
        return abs(new_error - old_error) &lt; 1e-6
    
    def fit(self, X_input, y, learning_rate=0.01):
        
      
        X = np.c_[np.ones(X_input.shape[0]), X_input]
        m, n = X.shape
        self.parameters = np.zeros(n)
        old_error = self.error_function(X, y)
  
        self.parameters = None
        answer_list = [self.parameters.copy()]
        final_values = self.parameters.copy()
        for iteration in range(self.max_iteration):
            print("iteration_done",iteration)
            indices = np.random.permutation(X.shape[0])
            X = X[indices]
            y = y[indices]
            for i in range(m):
                predictions = X[i] @ self.parameters
                errors = y[i] - predictions
            
                gradients = -(errors)*(X[i].T)
               
                self.parameters -= learning_rate * gradients
                
                
                answer_list.append(self.parameters.copy())       
            
            
            new_error = self.error_function(X, y)
            if(new_error&lt;old_error):
                final_values = self.parameters.copy()
                print(iteration)
            print(self.parameters)
           
            print(new_error)
            
            if abs(new_error - old_error) &lt; 1e-7:
                print(f"Converged at iteration: {iteration}")
                final_values = self.parameters.copy()
                break
            old_error = new_error
        self.parameters = final_values
        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result
    
    def fit_with_batch(self, X, y, learning_rate=0.001, batch_size=1):
        if batch_size==1:
            self.max_iteration=100
        n_samples, n_features = X.shape
        self.parameters = np.zeros(n_features + 1) 
        
    
        X_bias = np.column_stack((np.ones(n_samples), X))
        prev_cost = np.inf
        answer_list = []
        final_value = self.parameters.copy()
        lowest_cost = np.inf
        for iteration in range(self.max_iteration):
            print("iteration_done", iteration)
          
            indices = np.random.permutation(n_samples)
            X_shuffled = X_bias[indices]
            y_shuffled = y[indices]
            
            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i + batch_size]
                y_batch = y_shuffled[i:i + batch_size]
             
                predictions = X_batch @ self.parameters
             
                errors = predictions - y_batch
                gradients = (X_batch.T @ errors) / batch_size

                self.parameters -= learning_rate * gradients
            answer_list.append(self.parameters.copy()) 
            
            cost = self.error_function(X_shuffled, y_shuffled)
            # if(cost&lt;lowest_cost):
            #     final_value = self.parameters.copy()
            #     lowest_cost = cost
            #     print(iteration)
            # print("Current parameters:", self.parameters)
            # print("Cost difference:", abs(prev_cost - cost),cost)
            
          
            if abs(prev_cost - cost) &lt; 1e-6:
                print(f"Converged at iteration {iteration}")
                final_value = self.parameters.copy()
                break
                
            prev_cost = cost
            
        # self.parameters = final_value
        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result

    
    def predict(self, X):
        # Add intercept column to X
        X = np.c_[np.ones(X.shape[0]), X]
        predictions = X @ self.parameters
        result = np.column_stack((np.arange(len(predictions)), predictions))
        return result
    
    def mean_square_error_comparison(self,batch):
        X_train, y_train, X_test, y_test = splitter(*generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), np.sqrt(2)))
        self.fit_with_batch(X_train, y_train,batch_size=batch)
        X_bias = np.column_stack((np.ones(X_test.shape[0]), X_test))
        predictions = X_bias @ self.parameters
        errors = y_test - predictions
        return (errors @ errors) / (2 * X_bias.shape[0])
    
    def plot(self,X,y):


        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        
        
        
        
        history = self.fit_with_batch(X, y, batch_size=8000)
       
        
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_zlabel('θ3')
        for i in range(len(history)):
            ax.plot(history[:i+1, 1], history[:i+1, 2], history[:i+1, 3],color = 'red')
            # plt.pause(0.01)
        
        
        
        ax.legend()
        plt.show()
        # Generate data for plotting
        

def read_csv_to_arrays(file_x, file_y):
    x_data = np.loadtxt(file_x, delimiter=',').reshape(-1, 1)
    y_data = np.loadtxt(file_y, delimiter=',')
    return x_data, y_data



# X, y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), np.sqrt(2))
# a = StochasticLinearRegressor()
# d = a.fit_with_batch(X, y,batch_size=1)
# print("Final parameters:", d)


def closed_form(X,y):

    X_bias = np.c_[np.ones(X.shape[0]), X]
    theta_closed_form = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y
    return theta_closed_form


X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), np.sqrt(2))
a = StochasticLinearRegressor()
a.plot(X,y)
# X_train, y_train, X_test, y_test = splitter(X,y)
# a = StochasticLinearRegressor()
# w = a.fit_with_batch(X_train,y_train,batch_size=8000)[-1]
# print(a.error_function(np.column_stack((np.ones(X_train.shape[0]), X_train)),y_train))
# print(a.error_function(np.column_stack((np.ones(X_test.shape[0]), X_test)),y_test))

# b = StochasticLinearRegressor()
# w = b.fit_with_batch(X_train,y_train,batch_size=80)[-1]
# print(b.error_function(np.column_stack((np.ones(X_train.shape[0]), X_train)),y_train))
# print(b.error_function(np.column_stack((np.ones(X_test.shape[0]), X_test)),y_test))

# c = StochasticLinearRegressor()
# w = c.fit_with_batch(X_train,y_train,batch_size=800000)[-1]
# print(c.error_function(np.column_stack((np.ones(X_train.shape[0]), X_train)),y_train))
# print(c.error_function(np.column_stack((np.ones(X_test.shape[0]), X_test)),y_test))

# d = StochasticLinearRegressor()
# w = d.fit_with_batch(X_train,y_train,batch_size=8000)[-1]
# print(a.error_function(np.column_stack((np.ones(X_train.shape[0]), X_train)),y_train))
# print(a.error_function(np.column_stack((np.ones(X_test.shape[0]), X_test)),y_test))



# Imports - you can add any other permitted libraries
import numpy as np



def generate(N, theta, input_mean, input_sigma, noise_sigma):

    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match153-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2 = np.random.normal(input_mean[1], input_sigma[1], N)

    X = np.column_stack((x1, x2))

    noise = np.random.normal(0, noise_sigma, N)
    
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    
    return X, y
</FONT>

class StochasticLinearRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 1000
        self.final_parameters = []
        
    def error_function(self, X, y):
        m = X.shape[0]
        predictions = X @ self.parameters
        errors = y - predictions
        return (errors @ errors) / (2 * m)
        
    def stopping_condition_1(self, old_error, new_error):
        return abs(new_error - old_error) &lt; 1e-6
   
    
    def fit(self, X, y, learning_rate=0.001):
        self.final_parameters = []
        n_samples, n_features = X.shape
        answer_to_be_returned = []
        for i in range(4):
            if (i==0):
                batch_size = 1
                self.max_iteration = 150
            elif (i==1):
                batch_size = 80
                self.max_iteration = 10000
            elif (i==2):
                batch_size = 8000
                self.max_iteration = 10000
            else:
                batch_size = 800000
                self.max_iteration = 20000
                
            self.parameters = np.zeros(n_features + 1) 
            X_bias = np.column_stack((np.ones(n_samples), X))
            prev_cost = np.inf
            answer_list = []
            final_value = self.parameters.copy()
            lowest_cost = np.inf
            for iteration in range(self.max_iteration):
                indices = np.random.permutation(n_samples)
                X_shuffled = X_bias[indices]
                y_shuffled = y[indices]    
                for i in range(0, n_samples, batch_size):
                    X_batch = X_shuffled[i:i + batch_size]
                    y_batch = y_shuffled[i:i + batch_size]

                    predictions = X_batch @ self.parameters

                    errors = predictions - y_batch
                    gradients = (X_batch.T @ errors) / batch_size

                    self.parameters -= learning_rate * gradients
                    
                answer_list.append(self.parameters.copy()) 

                cost = self.error_function(X_shuffled, y_shuffled)
                
                    # print(iteration)
                # print("Current parameters:", self.parameters)
                # print("Cost difference:", abs(prev_cost - cost),cost)
                
        
                if abs(prev_cost - cost) &lt; 1e-6:
                    # print(iteration")
                    # final_value = self.parameters.copy()
                    break
                    
                prev_cost = cost
                
            # self.parameters = final_value
            self.final_parameters.append(final_value)
            result = np.column_stack((np.arange(len(answer_list)), answer_list))
            answer_to_be_returned.append(result)
        return answer_to_be_returned


    
    def predict(self, X1):
        result_to_be_returned = []
        for i in range(4):
            self.parameters = self.final_parameters[i]
            X = np.c_[np.ones(X1.shape[0]), X1]
            predictions = X @ self.parameters
            result = np.column_stack((np.arange(len(predictions)), predictions))
            result_to_be_returned.append(result)
        return result_to_be_returned
 



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.





class LogisticRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 20000

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y, learning_rate=0.001):
       
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
    
        n_samples, n_features = X.shape
        X_bias = np.c_[np.ones(n_samples), X]  
        self.parameters = np.zeros(n_features + 1)  
        answer_list = [self.parameters.copy()]
        for iteration in range(self.max_iteration):
           
            predictions = self.sigmoid(X_bias @ self.parameters)

          
            errors = predictions - y
            gradient = X_bias.T @ errors

            W = np.diag(predictions * (1 - predictions))
            hessian = X_bias.T @ W @ X_bias

           
            while(True):
                try:
                    delta_theta = np.linalg.inv(hessian) @ gradient
                    break
                except np.linalg.LinAlgError:
                    hessian += 1e-10 * np.identity(hessian.shape[0])
                    print("Hessian is singular. Stopping iteration.")
                    # break

            self.parameters -= delta_theta
            answer_list.append(self.parameters.copy()) 
           
            if np.linalg.norm(delta_theta) &lt; 1e-10:
                print(f"Converged at iteration {iteration}")
                break
        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result
        # return self.parameters

    def predict(self, X):
       
   
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
       
        n_samples = X.shape[0]
        X_bias = np.c_[np.ones(n_samples), X]
        

        probabilities = self.sigmoid(X_bias @ self.parameters)
        return (probabilities &gt; 0.5).astype(int)
    def plot_decision_boundary(self, X, y):
       
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

     
        plt.figure(figsize=(8, 6))
        plt.scatter(X_norm[y == 0][:, 0], X_norm[y == 0][:, 1], marker='o', color='red', label='Class 0')
        plt.scatter(X_norm[y == 1][:, 0], X_norm[y == 1][:, 1], marker='x', color='blue', label='Class 1')

        x_values = np.array([np.min(X_norm[:, 0]) - 1, np.max(X_norm[:, 0]) + 1])
       
        y_values = -(self.parameters[0] + self.parameters[1] * x_values) / self.parameters[2]

   
        plt.xlabel('Feature 1 (x1)')
        plt.ylabel('Feature 2 (x2)')
        plt.legend()
        plt.title('Training Data ')
        plt.show()

import numpy as np

def read_csv_to_arrays(X_file: str, Y_file: str):
  
    X = np.loadtxt(X_file, delimiter=',')  
    y = np.loadtxt(Y_file, delimiter=',') 
    
  
    y = y.ravel()
    
    return X, y


def file_read():
   
    X, y = read_csv_to_arrays('../data/Q3/logisticX.csv', '../data/Q3/logisticY.csv')
    return X, y

a = LogisticRegressor()
w = a.fit(*file_read())
print(a.parameters)
a.plot_decision_boundary(*file_read())
# print(b[-1])






import numpy as np
import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
        self.parameters = None
        self.max_iteration = 20000

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y,learning_rate=0.001):
       
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
      
        n_samples, n_features = X.shape
        X_bias = np.c_[np.ones(n_samples), X]  
        self.parameters = np.zeros(n_features + 1)  
        answer_list = [self.parameters.copy()]
        for iteration in range(self.max_iteration):
          
            predictions = self.sigmoid(X_bias @ self.parameters)

            
            errors = predictions - y
            gradient = X_bias.T @ errors

            W = np.diag(predictions * (1 - predictions))
            hessian = X_bias.T @ W @ X_bias
            while(True):
                try:
                    delta_theta = np.linalg.inv(hessian) @ gradient
                    break
                except np.linalg.LinAlgError:
                    hessian += 1e-10 * np.identity(hessian.shape[0])
                    # print("Hessian is singular. Stopping iteration.")
                    # break

            self.parameters -= delta_theta
            answer_list.append(self.parameters.copy()) 
          
            if np.linalg.norm(delta_theta) &lt; 1e-10:
                # print(iteration")
                break
        result = np.column_stack((np.arange(len(answer_list)), answer_list))
        return result
        # return self.parameters

    def predict(self, X1):


        X = (X1 - np.mean(X1, axis=0)) / np.std(X1, axis=0)
        

        n_samples = X.shape[0]
        X_bias = np.c_[np.ones(n_samples), X]
        

        probabilities = self.sigmoid(X_bias @ self.parameters)
        return (probabilities &gt; 0.5).astype(int)
    



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None

    def fit(self, X1, y, assume_same_covariance=False):

        X = (X1 - np.mean(X1, axis=0)) / np.std(X1, axis=0)
        

        X0 = X[y == 0]
        X1 = X[y == 1]
        
       
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match153-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)
        
      
        self.phi = len(X1) / len(X)
</FONT>        
        if assume_same_covariance:
        
            sigma = np.cov(X.T, bias=True)
            self.sigma = sigma
            return self.mu_0, self.mu_1, self.sigma
        else:
    
            self.sigma_0 = np.cov(X0.T, bias=True)
            self.sigma_1 = np.cov(X1.T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        if self.sigma_0 is not None:
            p0 = self.compute_likelihood(X, self.mu_0, self.sigma_0)
            p1 = self.compute_likelihood(X, self.mu_1, self.sigma_1)
        else:
            p0 = self.compute_likelihood(X, self.mu_0, self.sigma)
            p1 = self.compute_likelihood(X, self.mu_1, self.sigma)
            
        return (p1 &gt; p0).astype(int)

    def compute_likelihood(self, X, mu, sigma):
  
        n = X.shape[1]
        det_sigma = np.linalg.det(sigma)
        inv_sigma = np.linalg.inv(sigma)
        
        diff = X - mu

        exponent = -0.5 * np.sum(diff @ inv_sigma * diff, axis=1)
        coefficient = 1 / np.sqrt((2 * np.pi) ** n * det_sigma)
        return coefficient * np.exp(exponent)

    def plot_decision_boundary(self, X, y):
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        plt.figure(figsize=(8, 6))
        plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada')

        
        x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000)
        x2_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000)
        X1, X2 = np.meshgrid(x1_vals, x2_vals)
        grid_points = np.c_[X1.ravel(), X2.ravel()]
        predictions = self.predict(grid_points).reshape(X1.shape)
        plt.contour(X1, X2, predictions, levels=[0.5], colors='black', linewidths=1.5)
        
<A NAME="2"></A><FONT color = #0000FF><A HREF="match153-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.xlabel('x1 (normalized)')
        plt.ylabel('x2 (normalized)')
        plt.legend()
        plt.title('Training data')
        plt.show()


# model = GaussianDiscriminantAnalysis()
# mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)
# print("Mu0:", mu_0, "Mu1:", mu_1, "Sigma:", sigma)
# model.plot_decision_boundary(X, y)


X = np.loadtxt("../data/Q4/q4x.dat")
y = np.loadtxt("../data/Q4/q4y.dat", dtype=str)
y = np.where(y == "Alaska", 0, 1)
</FONT>
# Example usage
model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma1 = model.fit(X, y, assume_same_covariance=True)
print("Mu0:", mu_0, "Mu1:", mu_1, "Sigma1:", sigma1)
# print(model.sigma[0][0])
# print(model.predict(X))
model.plot_decision_boundary(X, y)




# Imports - you can add any other permitted libraries
import numpy as np



import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None

    def fit(self, X, y, assume_same_covariance=False):
      
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
       
        X0 = X[y == 0]
        X1 = X[y == 1]
        
      
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match153-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)
        
   
        self.phi = len(X1) / len(X)
</FONT>        
        if assume_same_covariance:
     
            sigma = np.cov(X.T, bias=True)
            self.sigma = sigma
            return self.mu_0, self.mu_1, self.sigma
        else:
           
            self.sigma_0 = np.cov(X0.T, bias=True)
            self.sigma_1 = np.cov(X1.T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X1):
  
        X = (X1 - np.mean(X1, axis=0)) / np.std(X1, axis=0)

        if self.sigma_0 is not None:
            p0 = self.compute_likelihood(X, self.mu_0, self.sigma_0)
            p1 = self.compute_likelihood(X, self.mu_1, self.sigma_1)
        else:
            p0 = self.compute_likelihood(X, self.mu_0, self.sigma)
            p1 = self.compute_likelihood(X, self.mu_1, self.sigma)
            
        return (p1 &gt; p0).astype(int)

    def compute_likelihood(self, X, mu, sigma):

        n = X.shape[1]
        det_sigma = np.linalg.det(sigma)
        inv_sigma = np.linalg.inv(sigma)
        
        diff = X - mu

        exponent = -0.5 * np.sum(diff @ inv_sigma * diff, axis=1)
        coefficient = 1 / np.sqrt((2 * np.pi) ** n * det_sigma)
        return coefficient * np.exp(exponent)

    

</PRE>
</PRE>
</BODY>
</HTML>
