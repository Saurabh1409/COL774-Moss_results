<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_46U3K.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_QPDTS.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def load_data(x_data_file, y_data_file):
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match115-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X = pd.read_csv(x_data_file, header = None).values
    y = pd.read_csv(y_data_file, header = None).values.flatten()
    return X, y
</FONT>


class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X = np.hstack([X, np.ones((X.shape[0], 1))])
        num_samples = X.shape[0]
        n_features = X.shape[1]
        self.theta = np.zeros(shape = (n_features,1))
        n_iter = 5000
        # parameters_history = np.zeros((n_iter, n_features))
        parameters_history = []
        y = y.reshape(-1,1)

        old_cost = float('inf')
        print(old_cost)
        for i in range(n_iter):
            h_theta = X.dot(self.theta)
            delta = (h_theta-y)
            delta_j_theta = delta.T.dot(X)

            new_cost = (np.sum(delta**2))/(2*num_samples)
            print(new_cost)

            self.theta = self.theta - learning_rate*delta_j_theta.T/(num_samples)
            # parameters_history[i] = self.theta.flatten()
            parameters_history.append(self.theta.flatten())
            print(self.theta)

            if(old_cost - new_cost &lt; 1e-6):
                print(f"Converged at iteration {i+1}")
                break

            old_cost = new_cost
        
        parameters_history = np.array(parameters_history)
    
        return parameters_history

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.hstack([X, np.ones((X.shape[0], 1))])
        y_pred = X.dot(self.theta)
        y_pred = y_pred.flatten()
        return y_pred
    
    def plot_hypothesis(self, X, y):
        plt.figure(figsize=(10,8))
        plt.scatter(X, y, color='blue', label="Training Data", alpha = 0.6)
        X_augmented = np.hstack([X, np.ones((X.shape[0],1))])
        y_pred = X_augmented.dot(self.theta)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match115-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.plot(X,y_pred, color='red', linewidth = 2, label='Hypothesis function')
        plt.xlabel("Input Feature X")
        plt.ylabel("Predicted Variable Y")
        plt.title("Data Points and Learned Hypothesis Function")
        plt.legend()
        plt.grid(True)
        plt.savefig("hypothesis_plot.png", format="png", dpi=300)
</FONT>        plt.show()

    def plot_error_function(self, X, y):
        X_augmented = np.hstack([X, np.ones((X.shape[0],1))])
        y_reshaped = y.reshape(-1,1)
        num_samples = len(y)
        theta0_min = self.theta[1] - 3*abs(self.theta[1])
        theta0_max = self.theta[1] + 3*abs(self.theta[1])
        theta1_min = self.theta[0] - 1*abs(self.theta[0])
        theta1_max = self.theta[0] + 1*abs(self.theta[0])

        step = 0.1
        theta0_vals = np.arange(theta0_min, theta0_max, step)
        theta1_vals = np.arange(theta1_min, theta1_max, step)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

        error_function_values = np.zeros_like(Theta0)

        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([[Theta1[i,j]],[Theta0[i,j]]])
                h_theta = X_augmented.dot(theta_temp)
                delta = (h_theta-y_reshaped)
                error_function = (np.sum(delta**2))/(2*num_samples)
                error_function_values[i,j] = error_function

        fig = plt.figure(figsize=(10,7))
        ax = fig.add_subplot(111, projection='3d')

        surf = ax.plot_surface(Theta0, Theta1, error_function_values, cmap='viridis', alpha = 0.8)

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title("3D Visualization of Error Function")

        # Color bar for reference
        fig.colorbar(surf, shrink=0.5, aspect=5)
        
        plt.savefig("error_function_3D.png", format="png", dpi=300)

        plt.show()

    def plot_error_function_with_gradient_descent(self, X, y, parameters_history):
        X_augmented = np.hstack([X, np.ones((X.shape[0],1))])
        y_reshaped = y.reshape(-1,1)
        num_samples = len(y)
        theta0_min = self.theta[1] - 3*abs(self.theta[1])
        theta0_max = self.theta[1] + 3*abs(self.theta[1])
        theta1_min = self.theta[0] - 1*abs(self.theta[0])
        theta1_max = self.theta[0] + 1*abs(self.theta[0])

        step = 0.1
        theta0_vals = np.arange(theta0_min, theta0_max, step)
        theta1_vals = np.arange(theta1_min, theta1_max, step)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

        error_function_values = np.zeros_like(Theta0)

        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([[Theta1[i,j]],[Theta0[i,j]]])
                h_theta = X_augmented.dot(theta_temp)
                delta = (h_theta-y_reshaped)
                error_function = (np.sum(delta**2))/(2*num_samples)
                error_function_values[i,j] = error_function

        fig = plt.figure(figsize=(10,7))
        ax = fig.add_subplot(111, projection='3d')

        surf = ax.plot_surface(Theta0, Theta1, error_function_values, cmap='viridis', alpha = 0.8)

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title("Gradient Descent Progress on Cost Function")
        
        gd_path, = ax.plot([], [], [], 'ro-', markersize=5, label="Gradient Descent Path")

        # Show legend
        ax.legend()

        x_vals, y_vals, z_vals = [], [], []
        # Update in real time for each iteration

        k=1
        for i in range(len(parameters_history)):
            theta_1, theta_0 = parameters_history[i]

            # Compute current cost value
            h_theta = X_augmented.dot(np.array([[theta_1], [theta_0]]))
            delta = (h_theta-y_reshaped)

            J_theta = (np.sum(delta**2)) / (2 * num_samples)

            print(J_theta)

            # Update the gradient descent path
            x_vals.append(theta_0)
            y_vals.append(theta_1)
            z_vals.append(J_theta)

            gd_path.set_data(x_vals, y_vals)  # Update X and Y
            gd_path.set_3d_properties(z_vals)  # Update Z

            plt.draw()
            plt.pause(0.2)
            
            print(k)
            k+=1
        plt.show()
        
    def plot_error_function_contours(self, X, y, parameters_history):
        X_augmented = np.hstack([X, np.ones((X.shape[0],1))])
        y_reshaped = y.reshape(-1,1)
        num_samples = len(y)
        theta0_min = self.theta[1] - 3*abs(self.theta[1])
        theta0_max = self.theta[1] + 3*abs(self.theta[1])
        theta1_min = self.theta[0] - 1*abs(self.theta[0])
        theta1_max = self.theta[0] + 1*abs(self.theta[0])

        step = 0.1
        theta0_vals = np.arange(theta0_min, theta0_max, step)
        theta1_vals = np.arange(theta1_min, theta1_max, step)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

        error_function_values = np.zeros_like(Theta0)

        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([[Theta1[i,j]],[Theta0[i,j]]])
                h_theta = X_augmented.dot(theta_temp)
                delta = (h_theta-y_reshaped)
                error_function = (np.sum(delta**2))/(2*num_samples)
                error_function_values[i,j] = error_function

        fig, ax = plt.subplots(figsize=(8, 6))
        contour = ax.contourf(Theta0, Theta1, error_function_values, cmap='viridis', levels=50, alpha=0.8) 
        plt.colorbar(contour, ax=ax)

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_title(" Error Function Contours")

        gd_path, = ax.plot([], [], 'ro-', markersize=5, label="Gradient Descent Path")

        ax.legend()

        x_vals, y_vals = [], []       

        k=0
        for i in range(len(parameters_history)):
            theta_1, theta_0 = parameters_history[i]

            # Append to lists
            x_vals.append(theta_0)
            y_vals.append(theta_1)

            # Update the gradient descent path dynamically
            gd_path.set_data(x_vals, y_vals)

            plt.draw()
            plt.pause(0.2)  # Pause for visibility
            print(k)
            k+=1

        plt.show()
        






from linear_regression import *


x_data_file = "./data/Q1/linearX.csv"
y_data_file = "./data/Q1/linearY.csv"
X,y = load_data(x_data_file, y_data_file)



linear_regressor = LinearRegressor()
p_history = linear_regressor.fit(X, y)
linear_regressor.predict(X)
linear_regressor.plot_hypothesis(X,y)
# linear_regressor.plot_error_function(X,y)
# linear_regressor.plot_error_function_with_gradient_descent(X, y, p_history)
# linear_regressor.plot_error_function_contours(X,y,p_history)



from sampling_sgd import * 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


X, y = generate(1000000, np.array([3,1,2]), np.array([3,-1]), np.array([2,2]), np.sqrt(2))
train_ratio = 0.8
test_ratio = 1 - train_ratio
train_size = int(train_ratio * 1000000)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]
model = StochasticLinearRegressor()
parameters_history = model.fit(X_train, y_train)

y_predicted_test = model.predict(X_test)
test_error = np.sum((y_predicted_test-y_test)**2)/ (test_ratio*1000000)
print(test_error)

y_predicted_train = model.predict(X_train)
train_error = np.sum((y_predicted_train-y_train)**2) / (train_ratio*1000000)
print(train_error)


fig = plt.figure(figsize=(16, 12))
ax = fig.add_subplot(111, projection='3d')
<A NAME="1"></A><FONT color = #00FF00><A HREF="match115-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

ax.plot(parameters_history[:, 0], parameters_history[:, 1], parameters_history[:, 2], marker='o', markersize=2, label=f'Batch size = {800000}')
ax.set_title(f'Batch size = {800000}')
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')
</FONT>ax.legend()
plt.show()




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    mean_1 = input_mean[0]
    mean_2 = input_mean[1]
    sigma_1 = input_sigma[0]
    sigma_2 = input_sigma[1]
    x1 = np.random.normal(mean_1, sigma_1, N)
    x2 = np.random.normal(mean_2, sigma_2, N)
    X = np.column_stack((x1,x2))
    epsilon = np.random.normal(0, noise_sigma, N)
    X_aug = np.hstack((np.ones((N, 1)), X))
    y = np.dot(X_aug, theta) + epsilon
    return X, y


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_normal_equations = None
    
    def fit(self, X, y, learning_rate= 0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        num_samples = X.shape[0]
        n_features = X.shape[1]
        X_aug = np.hstack((np.ones((num_samples, 1)), X))
        self.theta = np.zeros(shape = (n_features + 1, 1))
        parameters_history = []
        r = 800000
        num_batches = int(np.ceil(num_samples / r))
        epochs = 10000
        batch_loss_history = []
        tol = 1e-6
        
        converged = False
        sample_indices = np.arange(num_samples)
        for epoch in range(epochs):
            np.random.shuffle(sample_indices)
            X_shuffled = X_aug[sample_indices]
            y_shuffled = y[sample_indices]
            print("Epoch ---- ",epoch)
            for b in range(1,num_batches+1):
                i_1 = (b-1)*r+1
                i_r = min((b-1)*r+r, num_samples)
                X_batch = X_shuffled[i_1-1 : i_r]
                y_batch = y_shuffled[i_1-1 : i_r]
                h_theta = X_batch.dot(self.theta)
                delta = (h_theta-y_batch.reshape(-1,1))
                delta_j_theta = delta.T.dot(X_batch)

                self.theta = self.theta - learning_rate*delta_j_theta.T/(i_r-i_1+1)

                j_theta = (np.sum(delta**2))/(2*(i_r-i_1+1))
                batch_loss_history.append(j_theta)
                parameters_history.append(self.theta.copy())
                if len(batch_loss_history) &gt;= 10:
                    prev_window_avg = np.mean(batch_loss_history[-10:-5])
                    current_window_avg = np.mean(batch_loss_history[-5:])
                    improvement = abs(prev_window_avg - current_window_avg) / prev_window_avg
                    if improvement &lt; tol:
                        print(f"Convergence reached at epoch {epoch+1}, batch {b+1} with relative improvement {improvement:.2e}")
                        converged = True
                        break
            if converged:
                break

        y_reshaped = y.reshape(-1,1)
        self.theta_normal_equations = np.linalg.inv((X_aug.T)@X_aug)@X_aug.T@(y_reshaped)
        print(self.theta)
        print(self.theta_normal_equations)
        return np.array(parameters_history)



    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        num_samples = X.shape[0]

        X_aug = np.hstack((np.ones((num_samples, 1)), X))
        y_pred = X_aug.dot(self.theta)
        y_pred = y_pred.flatten()
        return y_pred
        





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def load_data(x_data_file, y_data_file):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match115-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X = pd.read_csv(x_data_file, header = None).values
    y = pd.read_csv(y_data_file, header = None).values.flatten()
    return X, y
</FONT>

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None

    def sigmoid_function(self, x):
        return 1/(1+np.exp(-x))

    def gradient(self, X, y, h_theta):
        h_theta = h_theta.reshape(-1,1)
        grad = X.T@(h_theta-y)
        return grad

    def hessian(self, X, D):
        hessian_matrix = (X.T)@D@X
        return hessian_matrix

    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        mean = np.mean(X, axis=0)
        standard_deviation = np.std(X, axis=0)

        X_normalized = (X - mean)/standard_deviation
        # X_augmented = np.hstack([X_normalized, np.ones((X_normalized.shape[0], 1))])
        X_augmented = np.hstack([np.ones((X_normalized.shape[0], 1)), X_normalized])
        y_reshaped = y.reshape(-1,1)

        n_features = X_augmented.shape[1]
        self.theta = np.zeros(n_features).reshape(-1,1)
        old_theta = []

        n_iter = 10
        
        for i in range(n_iter):
            h_theta = (self.sigmoid_function(X_augmented@self.theta))
            grad = self.gradient(X_augmented, y_reshaped, h_theta)
            diagonal_matrix = np.diagflat(h_theta*(1-h_theta))
            hessian_matrix = self.hessian(X_augmented, diagonal_matrix)            
            self.theta = self.theta - (np.linalg.inv(hessian_matrix))@grad         
            old_theta.append(self.theta.flatten())
            print(self.theta)
        
        print(self.theta)
        old_theta = np.array(old_theta)
        return old_theta

        
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        mean = np.mean(X, axis=0)
        standard_deviation = np.std(X, axis=0)

        X_normalized = (X - mean)/standard_deviation
        X_augmented = np.hstack([np.ones((X_normalized.shape[0], 1)), X_normalized])
        h_theta = (self.sigmoid_function(X_augmented@self.theta))
        # y_pred = (h_theta&gt;=0.5).astype(int)
        y_pred = np.round(h_theta).astype(int)
        y_pred = y_pred.reshape(-1,)
        return y_pred








from logistic_regression import *
import matplotlib.pyplot as plt

x_data_file = "./data/Q3/logisticX.csv"
y_data_file = "./data/Q3/logisticY.csv"
X,y = load_data(x_data_file, y_data_file)
mean_X = np.mean(X, axis=0)
std_X = np.std(X, axis=0)


lr = LogisticRegressor()
parameter_history = lr.fit(X,y)
final_parameter = parameter_history[-1]


# Plot training data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='red', label="Class 0")
<A NAME="0"></A><FONT color = #FF0000><A HREF="match115-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='blue', label="Class 1")

x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)  # Range of x1
</FONT>x2_vals = - (final_parameter[0] + final_parameter[1] * (x1_vals)) / (final_parameter[2])  # Solve for x2

plt.plot(x1_vals, x2_vals, color='black', label="Decision Boundary")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.savefig("logistic_regression.png", format="png", dpi=300)

plt.show()




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi_0 = None
        self.phi_1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        mean = np.mean(X, axis=0)
        standard_deviation = np.std(X, axis=0)
        X_normalized = (X - mean)/standard_deviation

        n = len(y)
        X_class0 = []
        X_class1 = []
        y_class0 = 0
        y_class1 = 0

        for i in range(n):
            if y[i]==0:
                X_class0.append(X_normalized[i])
                y_class0+=1
            else:
                X_class1.append(X_normalized[i])
                y_class1+=1
        
        X_class0 = np.array(X_class0)
        X_class1 = np.array(X_class1)

        self.mu_0 = sum(X_class0) / y_class0
        self.mu_1 = sum(X_class1) / y_class1

        self.phi_0 = y_class0 / n
        self.phi_1 = y_class1 / n

        if (assume_same_covariance):
            d = X_normalized.shape[1]
            self.sigma =    np.zeros((d,d))
            X_temp = X_normalized.copy()
            X_temp = X_temp.reshape(-1,1)
            mu_0_temp = self.mu_0.reshape(-1,1)
            mu_1_temp = self.mu_1.reshape(-1,1)
            for i in range(n):
                if y[i] == 0:
                    self.sigma += (X_temp[i] - mu_0_temp)@(X_temp[i] - mu_0_temp).T
                else:
                    self.sigma += (X_temp[i] - mu_1_temp)@(X_temp[i] - mu_1_temp).T

            self.sigma = self.sigma/n
            return self.mu_0, self.mu_1, self.sigma
        else:
            # self.sigma0 = (X_class0 - self.mu_0) @ (X_class0 - self.mu_0).T 
            # print(self.sigma0.shape)
            # self.sigma1 = (X_class1 - self.mu_1) @ (X_class1 - self.mu_1).T 
            # return self.mu_0, self.mu_1, self.sigma_0, self.sigma1
            d = X_normalized.shape[1]
            self.sigma_0 =    np.zeros((d,d))
            self.sigma_1 =    np.zeros((d,d))

            X_temp = X_normalized.copy()
            X_temp = X_temp.reshape(-1,1)
            mu_0_temp = self.mu_0.reshape(-1,1)
            mu_1_temp = self.mu_1.reshape(-1,1)
            for i in range(n):
                if y[i] == 0:
                    self.sigma_0 += (X_temp[i] - mu_0_temp)@(X_temp[i] - mu_0_temp).T
                else:
                    self.sigma_1 += (X_temp[i] - mu_1_temp)@(X_temp[i] - mu_1_temp).T

            self.sigma_0 = (self.sigma_0)/y_class0
            self.sigma_1 = (self.sigma_1)/y_class1


            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        mean = np.mean(X, axis=0)
        standard_deviation = np.std(X, axis=0)
        X_normalized = (X - mean)/standard_deviation

        if (self.sigma is not None):
            sigma_inverse = np.linalg.inv(self.sigma)
            print(sigma_inverse)
            mu_0 = self.mu_0.reshape(-1,1)
            mu_1 = self.mu_1.reshape(-1,1)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match115-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            theta_1 = sigma_inverse@(mu_1-mu_0)
            theta_0 = 0.5 * (mu_0.T@sigma_inverse@mu_0 - mu_1.T@sigma_inverse@mu_1) + np.log(self.phi_1 / self.phi_0)
</FONT>
            y_pred = np.dot(X_normalized, theta_1) + theta_0
            y_pred = np.where(y_pred &gt;= 0, 'Alaska', 'Canada')
            print(y_pred)
        else:
            sigma0_inv = np.linalg.inv(self.sigma_0)
            sigma1_inv = np.linalg.inv(self.sigma_1)
            det_sigma0 = np.linalg.det(self.sigma_0)
            det_sigma1 = np.linalg.det(self.sigma_1)
            predictions = []
            for x in X_normalized:
                D = (x - self.mu_0).T @ sigma0_inv @ (x - self.mu_0) - (x - self.mu_1).T @ sigma1_inv @ (x - self.mu_1) + np.log(det_sigma0 / det_sigma1) - 2 * np.log(self.phi_1 / self.phi_0)
                # If D(x) &lt;= 0 predict 'Canada'; otherwise 'Alaska'
                if D &lt;= 0:
                    predictions.append('Canada')
                else:
                    predictions.append('Alaska')
            print(predictions)
            return np.array(predictions)


# X = np.loadtxt('data/Q4/q4x.dat')
# with open('data/Q4/q4y.dat', 'r') as f:
#     y = np.array([1 if label.strip() == 'Alaska' else 0 for label in f.readlines()])
# gda = GaussianDiscriminantAnalysis()
# gda.fit(X,y)
# gda.predict(X)



from gda import *
import matplotlib.pyplot as plt


X = np.loadtxt('data/Q4/q4x.dat')
with open('data/Q4/q4y.dat', 'r') as f:
    y = np.array([1 if label.strip() == 'Alaska' else 0 for label in f.readlines()])

# Assuming that the covaraince matrices are same for both
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X,y,True)

sigma_inverse = np.linalg.inv(sigma)
mu_0 = mu_0.reshape(-1,1)
mu_1 = mu_1.reshape(-1,1)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match115-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta_1 = sigma_inverse@(mu_1-mu_0)
theta_0 = 0.5 * (mu_0.T@sigma_inverse@mu_0 - mu_1.T@sigma_inverse@mu_1) + np.log(gda.phi_1 / gda.phi_0)
</FONT>print("When Sigma1 = Sigma0")
print(mu_0, mu_1, sigma)


mean = np.mean(X, axis=0)
standard_deviation = np.std(X, axis=0)

x1_range = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
x2_range = -((theta_1[0] * (x1_range - mean[0])/standard_deviation[0] + theta_0) / theta_1[1])*standard_deviation[1] + mean[1]
x2_range = x2_range.flatten()


# Assuming that the covariance matrices are not same
gda_2 = GaussianDiscriminantAnalysis()
mu_0_part2, mu_1_part2, sigma_0, sigma_1 = gda_2.fit(X,y)

print("When Sigma1 != Sigma0")
print(mu_0_part2, mu_1_part2, sigma_0, sigma_1)

sigma_0_inverse = np.linalg.inv(sigma_0)
sigma_1_inverse = np.linalg.inv(sigma_1)

term_1 = np.log(np.linalg.det(sigma_0) / np.linalg.det(sigma_1))
x1_vals, x2_vals = np.meshgrid(np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100), np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100))

final_values = np.zeros_like(x1_vals)

for i in range(x1_vals.shape[0]):
    for j in range(x1_vals.shape[1]):
        x = np.array([(x1_vals[i, j]-mean[0])/standard_deviation[0], (x2_vals[i, j]-mean[1])/standard_deviation[1]])
        # print(x)
        term_2 = (x - mu_0_part2).T @ sigma_0_inverse @ (x - mu_0_part2)
        term_3 = (x - mu_1_part2).T @ sigma_1_inverse @ (x - mu_1_part2)
        final_values[i, j] = term_2 - term_3 + term_1 - 2 * np.log(gda_2.phi_1 / (gda_2.phi_0))
        

# plt.contour(x1_vals, x2_vals, final_values, levels=[0], colors='green', linestyles='dashed', label="Quadratic Boundary")
quad_contour = plt.contour(x1_vals, x2_vals, final_values, levels=[0], colors='green', linestyles='dashed')
plt.clabel(quad_contour, inline=True, fontsize=10, fmt="Quadratic Boundary")




plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Alaska', color='blue')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Canada', color='red')
plt.plot(x1_range, x2_range, 'k-', label='Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.title('Training Data Plot')
plt.savefig("gda.png", format="png", dpi=300)

plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
