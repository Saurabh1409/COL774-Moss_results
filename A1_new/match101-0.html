<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_1I4TJ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_1I4TJ.py<p><PRE>


"""
        FIRST PART
"""
import numpy as np

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []

    def compute_cost(self, X, y, theta):
        """
        Compute the least squares cost function J(θ).
        """
        m = len(y)
        predictions = X.dot(theta)
        error = predictions - y
        return (1/(2*m)) * np.sum(error**2)

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model using batch gradient descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data
        y : numpy array of shape (n_samples,)
            The target values
        learning_rate : float
            The learning rate for gradient descent
        tol : float
            Tolerance for stopping criteria based on change in cost
        max_iter : int
            Maximum number of iterations

        Returns
        -------
        theta_history : numpy array of shape (n_iter, n_features+1)
            History of parameter values during training
        """
        # Add bias term to X
        m = X.shape[0]
        X_b = np.c_[np.ones((m, 1)), X]
        n_features = X_b.shape[1]

        # Initialize parameters
        self.theta = np.zeros(n_features)
        self.theta_history = [self.theta.copy()]

        # Initialize variables for tracking convergence
        prev_cost = float('inf')
        num_iterations = 0

        # Gradient descent
        for i in range(max_iterations):
            # Compute predictions
            predictions = X_b.dot(self.theta)

            # Compute gradients
            gradients = (1/m) * X_b.T.dot(predictions - y)

            # Update parameters
            self.theta = self.theta - learning_rate * gradients
            self.theta_history.append(self.theta.copy())

            # Compute current cost
            current_cost = self.compute_cost(X_b, y, self.theta)
            num_iterations += 1

            # Check convergence
            if abs(prev_cost - current_cost) &lt; tolerance:
                print(f"Converged after {num_iterations} iterations.")
                break

            prev_cost = current_cost

        if num_iterations == max_iterations:
          print(f"Reached maximum iterations ({max_iterations}) without full convergence.")

        return self.theta_history

    def predict(self, X):
        """
        Predict target values for input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            Predicted target values
        """
        if self.theta is None:
            raise ValueError("Model has not been fitted yet!")

        # Add bias term to X
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        return X_b.dot(self.theta)

# Load the data
X = np.loadtxt('linearX.csv')
y = np.loadtxt('linearY.csv')

# Create and train the model
model = LinearRegressor()

# Hyperparameters
learning_rate = 0.01
tolerance = 1e-11
max_iterations = 10000

# Train the model
theta_history = model.fit(X, y, learning_rate=learning_rate)

# Get final parameters
final_parameters = model.theta

print("Training Results:")
print("-----------------")
print(f"Learning Rate: {learning_rate}")
print(f"Stopping Criteria:")
print(f"- Tolerance (change in cost): {tolerance}")
print(f"- Maximum iterations: {max_iterations}")
print("\nFinal Parameters:")
print(f"θ₀ (intercept): {final_parameters[0]:.6f}")
print(f"θ₁ (coefficient): {final_parameters[1]:.6f}")

# Calculate final cost
X_b = np.c_[np.ones((X.shape[0], 1)), X]
final_cost = model.compute_cost(X_b, y, final_parameters)
print(f"\nFinal Cost (J(θ)): {final_cost:.6f}")

"""
        SECOND PART
"""
import matplotlib.pyplot as plt

def plot_regression( X, y):
    """
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match101-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    Plot the data points and regression line.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data
</FONT>    y : numpy array of shape (n_samples,)
        The target values
    """
    if model.theta is None:
        raise ValueError("Model has not been fitted yet!")

    # Create the figure and axis
    plt.figure(figsize=(10, 6))

    # Plot the data points
    plt.scatter(X, y, color='blue', alpha=0.5, label='Data points')

    # Create points for the regression line
    X_line = np.array([X.min(), X.max()]).reshape(-1, 1)
    y_line = model.predict(X_line)

    # Plot the regression line
<A NAME="6"></A><FONT color = #00FF00><A HREF="match101-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot(X_line, y_line, color='red', linewidth=2, label='Regression line')

    # Add labels and title
    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Wine Density vs Acidity: Linear Regression')
    plt.legend()
    plt.grid(True, alpha=0.3)
</FONT>
    # Show the plot
    plt.tight_layout()
    plt.show()

plot_regression(X, y)

"""
        THIRD PART
"""
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation

def plot_error_surface_and_animated_trajectory(X, y, theta_history):
    """
    Creates a 3D visualization of the error surface and animates the gradient descent trajectory.
    """
    # Convert theta_history to a NumPy array
    theta_values = np.array(theta_history)
    padding = 0.5

    # Define parameter space
    theta1_min = np.min(theta_values[:, 0]) - padding
    theta1_max = np.max(theta_values[:, 0]) + padding
    theta0_min = np.min(theta_values[:, 1]) - padding
    theta0_max = np.max(theta_values[:, 1]) + padding

    # Create parameter grid
    grid_points = 25
    param1_space = np.linspace(theta1_min, theta1_max, grid_points)
    param0_space = np.linspace(theta0_min, theta0_max, grid_points)
    param_grid1, param_grid0 = np.meshgrid(param1_space, param0_space)

    # Prepare design matrix
    X_design = np.column_stack([np.ones_like(X), X])

    # Calculate error surface
    error_surface = np.zeros((grid_points, grid_points))
    for i in range(grid_points):
        for j in range(grid_points):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match101-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            current_theta = np.array([param_grid1[i, j], param_grid0[i, j]])
            predictions = X_design.dot(current_theta)
            errors = predictions - y
            error_surface[i, j] = np.sum(errors ** 2) / (2 * len(y))
</FONT>
    # Create a figure for the error surface
    fig1 = plt.figure(figsize=(10, 7))
    ax1 = fig1.add_subplot(111, projection='3d')

    # Plot error surface
    ax1.plot_surface(param_grid1, param_grid0, error_surface, cmap='viridis', alpha=0.7)
    ax1.set_ylabel('Parameter θ₁')
    ax1.set_xlabel('Parameter θ₀')
    ax1.set_zlabel('Error J(θ)')
    ax1.set_title('Error Surface')
    ax1.view_init(elev=25, azim=60)  # Adjust view
    plt.show()  # Show error surface first

    # Create figure for animation
    fig2 = plt.figure(figsize=(10, 7))
    ax2 = fig2.add_subplot(111, projection='3d')

    # Plot error surface again for animation
    ax2.plot_surface(param_grid1, param_grid0, error_surface, cmap='viridis', alpha=0.7)

    # Extract trajectory data
    theta1_path = [theta[0] for theta in theta_history]
    theta0_path = [theta[1] for theta in theta_history]
    error_values = []
    for theta in theta_history:
        predictions = X_design.dot(theta)
        errors = predictions - y
        error_values.append(np.sum(errors ** 2) / (2 * len(y)))
    
    # Initialize empty trajectory line
    trajectory, = ax2.plot([], [], [], 'r.-', markersize=8, label='Gradient descent path')
    start_point = ax2.scatter([], [], [], color='green', s=100, label='Start')
    end_point = ax2.scatter([], [], [], color='red', s=100, label='End')
    iteration_text = ax2.text2D(0.05, 0.95, '', transform=ax2.transAxes, fontsize=12, color='black')
    
    ax2.set_ylabel('Parameter θ₁')
    ax2.set_xlabel('Parameter θ₀')
    ax2.set_zlabel('Error J(θ)')
    ax2.set_title('Gradient Descent Trajectory (Animated)')
    ax2.legend(loc="upper center", bbox_to_anchor=(0.5, 1.15), ncol=3)
    
    # Animation function
    def update(frame):
        trajectory.set_data(theta1_path[:frame], theta0_path[:frame])
        trajectory.set_3d_properties(error_values[:frame])
        start_point._offsets3d = ([theta1_path[0]], [theta0_path[0]], [error_values[0]])
        if frame == len(theta_history) - 1:
            end_point._offsets3d = ([theta1_path[-1]], [theta0_path[-1]], [error_values[-1]])
        iteration_text.set_text(f'Iteration: {frame + 1}/{len(theta_history)}')
        return trajectory, start_point, end_point, iteration_text

    # Create animation
    ani = FuncAnimation(fig2, update, frames=len(theta_history), interval=200, blit=False)
    ax2.view_init(elev=25, azim=60)
    plt.show()  # Show the animated plot

# Call the function
plot_error_surface_and_animated_trajectory(X, y, model.theta_history)


"""
        FOURTH PART
"""
def animate_error_contours(X, y, theta_history):
    """
    Creates an animated contour plot showing the gradient descent path
    and then displays the final plot after convergence.
    """
    # Convert theta_history to NumPy array
    theta_values = np.array(theta_history)
    padding = 0.5

    # Define parameter space
    theta0_min = np.min(theta_values[:, 0]) - padding
    theta0_max = np.max(theta_values[:, 0]) + padding
    theta1_min = np.min(theta_values[:, 1]) - padding
    theta1_max = np.max(theta_values[:, 1]) + padding

    # Create parameter grid
    grid_points = 100  # High resolution for smooth contours
    param0_space = np.linspace(theta0_min, theta0_max, grid_points)
    param1_space = np.linspace(theta1_min, theta1_max, grid_points)
    param_grid0, param_grid1 = np.meshgrid(param0_space, param1_space)

    # Prepare design matrix
    X_design = np.column_stack([np.ones_like(X), X])

    # Calculate error surface
    error_surface = np.zeros((grid_points, grid_points))
    for i in range(grid_points):
        for j in range(grid_points):
<A NAME="2"></A><FONT color = #0000FF><A HREF="match101-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            current_theta = np.array([param_grid0[i, j], param_grid1[i, j]])
            predictions = X_design.dot(current_theta)
            errors = predictions - y
            error_surface[i, j] = np.sum(errors ** 2) / (2 * len(y))
</FONT>
    # Create figure and axis for animation
    fig, ax = plt.subplots(figsize=(10, 8))

    # Create contour plot
    levels = np.logspace(np.log10(np.min(error_surface)),
                         np.log10(np.max(error_surface)), 20)
    contour = ax.contour(param_grid0, param_grid1, error_surface,
                         levels=levels, cmap='viridis')

    # Extract gradient descent path
    theta0_path = [theta[0] for theta in theta_history]
    theta1_path = [theta[1] for theta in theta_history]

    # Initialize empty trajectory line
    trajectory, = ax.plot([], [], 'r.-', linewidth=2, markersize=8, label='Gradient descent path')
    start_point, = ax.plot([], [], 'go', markersize=10, label='Start')
    end_point, = ax.plot([], [], 'ro', markersize=10, label='End')
    iteration_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, color='black')

    ax.set_xlabel('Parameter θ₀')
    ax.set_ylabel('Parameter θ₁')
    ax.set_title('Gradient Descent Animation on Error Contours')
    ax.legend(loc="upper center", bbox_to_anchor=(0.5, 1.15), ncol=3)
    ax.grid(True)

    # Animation function
    def update(frame):
        trajectory.set_data(theta0_path[:frame], theta1_path[:frame])
        start_point.set_data([theta0_path[0]], [theta1_path[0]])
        if frame == len(theta_history) - 1:
            end_point.set_data([theta0_path[-1]], [theta1_path[-1]])
        iteration_text.set_text(f'Iteration: {frame + 1}/{len(theta_history)}')
        return trajectory, start_point, end_point, iteration_text

    # Create animation
    ani = FuncAnimation(fig, update, frames=len(theta_history), interval=200, blit=False)
    plt.show()

    # Display final static plot
    plt.figure(figsize=(10, 8))
    contour = plt.contour(param_grid0, param_grid1, error_surface,
                           levels=levels, cmap='viridis')
    plt.plot(theta0_path, theta1_path, 'r.-', linewidth=2, markersize=8, label='Gradient descent path')
    plt.plot(theta0_path[0], theta1_path[0], 'go', markersize=10, label='Start')
    plt.plot(theta0_path[-1], theta1_path[-1], 'ro', markersize=10, label='End')
    plt.xlabel('Parameter θ₀')
    plt.ylabel('Parameter θ₁')
    plt.title('Final Error Contours with Gradient Descent Path')
    plt.legend(loc="upper center", bbox_to_anchor=(0.5, 1.15), ncol=3)
    plt.grid(True)
    plt.show()

# Call the function
animate_error_contours(X, y, model.theta_history)





# -*- coding: utf-8 -*-
"""Assignment1part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jluUGstZBSj3Z2d13ygFNzBlV-6IOpUk
"""

# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.

    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.

    input_mean : numpy array of shape (2,)
        The mean of the input data.

    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.

    noise_sigma : float
        The standard deviation of the Gaussian noise.

    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.

    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise

    return np.column_stack((x1, x2)), y

def split_train_test(N, X, y, train_ratio=0.8):
    data = pd.DataFrame({'x1': X[:,0], 'x2': X[:,1], 'y': y})

    data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index

    train_size = int(train_ratio * N)

    train_data = data.iloc[:train_size]
    test_data = data.iloc[train_size:]

    return (train_data[['x1', 'x2']].to_numpy(), train_data['y'].to_numpy()), (test_data[['x1', 'x2']].to_numpy(), test_data['y'].to_numpy())

class StochasticLinearRegressor:
    def __init__(self):
        self.r = 1
        self.theta = None
        self.maxInterations = 5000

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        total_iterations = 0

        for epoch in range(self.maxInterations):
            indices = np.random.permutation(m)
            X_shuffled = XWithIntercept[indices]
            y_shuffled = y[indices]

            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch

                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)

                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                total_iterations += 1

                if np.linalg.norm(oldTheta - self.theta) &lt; 1e-5:
                    print(f"\nConverged after {total_iterations} iterations")
                    return self.theta

        print(f"\nDid not converge after {total_iterations} iterations")

        return self.theta

    def initialize_theta(self, n_features):
        self.theta = np.zeros(n_features + 1)

    def fit_with_batch_size(self, X, y, learning_rate=0.001, batch_size=1):
        self.r = batch_size
        return self.fit(X, y, learning_rate)

    def fit_get_all_thetas(self, X, y, learning_rate=0.001):
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        all_thetas = []

        for iter in range(self.maxInterations):
            indices = np.random.permutation(m)
            X_shuffled = XWithIntercept[indices]
            y_shuffled = y[indices]

            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch

                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)

                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                all_thetas.append(self.theta.copy())
                if np.linalg.norm(oldTheta - self.theta) &lt; 1e-5:
                    return np.array(all_thetas)
        return np.array(all_thetas)

    def draw_plot_3d_theta_movement(self, X, y, learning_rate=0.001, max_iterations=1000):
        batch_sizes = [1, 80, 8000, 800000]
        colors = {'1': 'red', '80': 'green', '8000': 'blue', '800000': 'yellow'}

        # Create a figure with 2x2 subplots
        fig = plt.figure(figsize=(20, 20))

        for idx, batch_size in enumerate(batch_sizes, 1):
            self.r = batch_size
            current_max_iterations = max_iterations if batch_size == 800000 else self.maxInterations

            # Initialize parameters
            self.initialize_theta(X.shape[1])
            XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
            m = XWithIntercept.shape[0]
<A NAME="5"></A><FONT color = #FF0000><A HREF="match101-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            all_thetas = []

            # Main optimization loop with early stopping
            for iter in range(current_max_iterations):
                indices = np.random.permutation(m)
                X_shuffled = XWithIntercept[indices]
                y_shuffled = y[indices]

                for i in range(0, m, self.r):
</FONT>                    X_batch = X_shuffled[i:i+self.r]
                    y_batch = y_shuffled[i:i+self.r]

                    y_pred = X_batch.dot(self.theta)
                    error = y_pred - y_batch
                    gradient = (1 / len(y_batch)) * X_batch.T.dot(error)

                    oldTheta = self.theta.copy()
                    self.theta -= learning_rate * gradient

                    all_thetas.append(self.theta.copy())

                    if np.linalg.norm(oldTheta - self.theta) &lt; 1e-5:
                        break

                if len(all_thetas) &gt; 1 and np.linalg.norm(all_thetas[-1] - all_thetas[-2]) &lt; 1e-5:
                    break

            print(f"\nBatch size {batch_size} - Total iterations:", len(all_thetas))

            all_thetas = np.array(all_thetas)  # Use all points
            print(f"Total points plotted:", len(all_thetas))

            theta_0 = all_thetas[:, 0]
            theta_1 = all_thetas[:, 1]
            theta_2 = all_thetas[:, 2]

            ax = fig.add_subplot(2, 2, idx, projection='3d')
            ax.plot(theta_0, theta_1, theta_2,
                    c=colors[str(batch_size)],
                    marker='o',
                    markersize=2,
                    label='Parameter trajectory')

            ax.scatter(self.theta[0], self.theta[1], self.theta[2],
                      c=colors[str(batch_size)],
                      marker='x',
                      s=100,
                      label='Final value')

            ax.scatter(3, 1, 2,
                      c='black',
                      marker='*',
                      s=100,
                      label='True value')

            ax.set_xlabel(r'$\theta_0$')
            ax.set_ylabel(r'$\theta_1$')
            ax.set_zlabel(r'$\theta_2$')
            ax.set_title(f'Parameter Updates for Batch Size = {batch_size}')
            ax.legend()
            ax.grid(True)

            ax.set_xlim([min(theta_0)-0.5, max(theta_0)+0.5])
            ax.set_ylim([min(theta_1)-0.5, max(theta_1)+0.5])
            ax.set_zlim([min(theta_2)-0.5, max(theta_2)+0.5])

        plt.suptitle('3D Parameter Trajectories for Different Batch Sizes', fontsize=16)
        plt.tight_layout()
        plt.show()


    def fit_with_closed_form(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        self.theta = np.linalg.inv(XWithIntercept.T.dot(XWithIntercept)).dot(XWithIntercept.T).dot(y)
        return self.theta

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return y_pred
    def test(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return np.mean((y_pred - y)**2)

X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), np.sqrt(2))
train, test = split_train_test(1000000, X, y)
slr = StochasticLinearRegressor()

import time

print("\nClosed form solution")
theta_with_closed_form = slr.fit_with_closed_form(X, y)
print("Closed form parameters:", theta_with_closed_form)

for batch_size in [1, 80, 8000, 800000]:
    print(f"\n\nBatch size {batch_size}:")
    slr = StochasticLinearRegressor()  # Reset for each batch size

    # Start timing
    start_time = time.time()

    theta_with_slr = slr.fit_with_batch_size(train[0], train[1], learning_rate=0.01, batch_size=batch_size)

    # End timing and calculate duration
    end_time = time.time()
    duration = end_time - start_time

    # Calculate MSE for both train and test sets
    train_mse = slr.test(train[0], train[1])
    test_mse = slr.test(test[0], test[1])

    print("SGD parameters:", theta_with_slr)
    print(f"Time taken: {duration:.2f} seconds")
    print(f"Training MSE: {train_mse:.6f}")
    print(f"Test MSE: {test_mse:.6f}")

# Part 5: Visualization
print("\nGenerating 3D parameter trajectory plots...")
slr.draw_plot_3d_theta_movement(train[0], train[1])



# -*- coding: utf-8 -*-
"""Assignment1part3 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cr8KyNFPLR5uyDc7pugunpep5jFH-nb-
"""

import numpy as np
import pandas as pd

class LogisticRegressor:
    def __init__(self):
        self.theta = None

    def _normalize_features(self, X):
        """Normalize features by subtracting mean and dividing by standard deviation"""
        self.means = np.mean(X, axis=0)
        self.stds = np.std(X, axis=0)
        return (X - self.means) / self.stds

    def _sigmoid(self, z):
        """Compute sigmoid function"""
        return 1 / (1 + np.exp(-z))

    def _compute_hessian(self, X, h):
        """Compute the Hessian matrix for logistic regression"""
        m = X.shape[0]
        W = np.diag(h * (1 - h))
        return -(1/m) * X.T @ W @ X

    def _compute_gradient(self, X, y, h):
        """Compute the gradient of the log-likelihood"""
        m = X.shape[0]
        return (1/m) * X.T @ (y - h)

    def _log_likelihood(self, X, y, theta):
        """Compute the log-likelihood"""
        h = self._sigmoid(X @ theta)
        return np.mean(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit logistic regression using Newton's Method.
        Learning rate parameter is included to match the required interface but not used.
        """
        # Normalize features
        X_normalized = self._normalize_features(X)

        # Add intercept term
        X_with_intercept = np.column_stack([np.ones(X_normalized.shape[0]), X_normalized])

        # Initialize parameters
        self.theta = np.zeros(X_with_intercept.shape[1])
        theta_history = [self.theta.copy()]

        # Newton's Method iterations
        for i in range(max_iter):
            # Compute hypothesis
            z = X_with_intercept @ self.theta
            h = self._sigmoid(z)

            # Compute gradient and Hessian
            gradient = self._compute_gradient(X_with_intercept, y, h)
            hessian = self._compute_hessian(X_with_intercept, h)

            # Update parameters
            delta = np.linalg.solve(hessian, gradient)
            theta_new = self.theta - delta

            # Store parameters
            theta_history.append(theta_new.copy())

            # Check convergence
            if np.all(np.abs(theta_new - self.theta) &lt; tol):
                break

            self.theta = theta_new

        return np.array(theta_history)

    def predict(self, X):
        """Predict class labels for samples in X."""
        if self.theta is None:
            raise ValueError("Model has not been fitted yet!")

        # Normalize features
        X_normalized = (X - self.means) / self.stds

        # Add intercept term
        X_with_intercept = np.column_stack([np.ones(X_normalized.shape[0]), X_normalized])

        # Compute probabilities
        probs = self._sigmoid(X_with_intercept @ self.theta)

        # Return class predictions
        return (probs &gt;= 0.5).astype(int)

# Load data
X = pd.read_csv('/logisticX.csv', header=None).values
y = pd.read_csv('/logisticY.csv', header=None).values.ravel()

max_iter=100
tol=1e-6

# Create and fit model
model = LogisticRegressor()
theta_history = model.fit(X, y)

# Get final coefficients
final_theta = theta_history[-1]

# Print results
print("Fitted coefficients (theta):")
print(f"Intercept: {final_theta[0]:.6f}")
print(f"Coefficient 1: {final_theta[1]:.6f}")
print(f"Coefficient 2: {final_theta[2]:.6f}")

import matplotlib.pyplot as plt

def plot_decision_boundary(X, y, model):
    """
    Plot the training data and decision boundary from logistic regression.

    Parameters:
        X: numpy array of shape (n_samples, 2), the training features
        y: numpy array of shape (n_samples,), the binary labels (0 or 1)
        model: trained LogisticRegressor instance
    """
    # Create a meshgrid to plot the decision boundary
    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),
                          np.linspace(x2_min, x2_max, 100))

    # Create the figure
    plt.figure(figsize=(10, 8))

    # Plot training points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='o', label='Class 0', alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='x', label='Class 1', alpha=0.7)

    # Get the coefficients from the model
    theta = model.theta

    # Calculate decision boundary line
    # For the decision boundary, h(x) = 0.5 which means theta0 + theta1*x1 + theta2*x2 = 0
    # Rearranging for x2: x2 = -(theta0 + theta1*x1)/theta2
    x1_boundary = np.array([x1_min, x1_max])
    x2_boundary = -(theta[0] + theta[1] * x1_boundary) / theta[2]

    # Plot decision boundary
    plt.plot(x1_boundary, x2_boundary, 'g-', label='Decision Boundary')

    # Set labels and title
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Logistic Regression Decision Boundary')
    plt.legend()

    # Set axis limits
    plt.xlim([x1_min, x1_max])
    plt.ylim([x2_min, x2_max])

    # Add grid
    plt.grid(True, linestyle='--', alpha=0.7)

    plt.show()

plot_decision_boundary(X, y, model)



# -*- coding: utf-8 -*-
"""Assignment1part4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/137OAI_GpaqVibGYy5z44g_dJJzMrhaiI
"""

import numpy as np

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.phi = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        """
        # Normalize X
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Convert y labels to binary (0 for Alaska, 1 for Canada)
        y = np.array([0 if label == "Alaska" else 1 for label in y])

        # Separate data by class
        X_0 = X[y == 0]
        X_1 = X[y == 1]

        # Compute mean vectors
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        # Compute covariance matrices
        n_0 = X_0.shape[0]
        n_1 = X_1.shape[0]

        sigma_0 = np.cov(X_0.T, bias=True)
        sigma_1 = np.cov(X_1.T, bias=True)

        if assume_same_covariance:
            self.sigma = (n_0 * sigma_0 + n_1 * sigma_1) / (n_0 + n_1)

        # Compute prior probability
        self.phi = np.mean(y)

        if assume_same_covariance:
            return self.mu_0, self.mu_1, self.sigma
        else:
            return self.mu_0, self.mu_1, sigma_0, sigma_1

    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        # Normalize X
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Compute discriminant functions
        delta_0 = -0.5 * np.sum(np.dot(X - self.mu_0, np.linalg.inv(self.sigma)) * (X - self.mu_0), axis=1)
        delta_1 = -0.5 * np.sum(np.dot(X - self.mu_1, np.linalg.inv(self.sigma)) * (X - self.mu_1), axis=1)

        # Predict based on which class has a higher discriminant score
        return np.where(delta_1 &gt; delta_0, 1, 0)

# Load data
X = np.loadtxt('/q4x.dat')
y = np.loadtxt('/q4y.dat', dtype=str)

# Normalize X
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match101-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
</FONT>
print("Mean for Alaska (µ0):", mu_0)
print("Mean for Canada (µ1):", mu_1)
print("Shared Covariance Matrix (Σ):", sigma)

import matplotlib.pyplot as plt

def plot_normalized_data(X, y):
    """
    Plot normalized salmon growth ring data for Alaska and Canada.

    Parameters:
    -----------
    X : ndarray
        Normalized feature matrix.
    y : ndarray
        Array of class labels.

    Returns:
    --------
    None
    """
    plt.figure(figsize=(8, 6))

    # Plot normalized data points for Alaska and Canada
    plt.scatter(X[y == "Alaska"][:, 0], X[y == "Alaska"][:, 1],
                c='blue', marker='o', label='Alaska', alpha=0.6)
    plt.scatter(X[y == "Canada"][:, 0], X[y == "Canada"][:, 1],
                c='red', marker='^', label='Canada', alpha=0.6)

    # Set labels and title
    plt.xlabel('Growth Ring Diameter (Fresh Water)')
    plt.ylabel('Growth Ring Diameter (Marine Water)')
    plt.title('Salmon Growth Ring Data: Alaska vs Canada')

    # Show legend and grid
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Display the plot
    plt.show()

# Call the function
plot_normalized_data(X, y)

def plot_decision_boundary(X, y, mu_0, mu_1, Sigma):
    """Plot data points and decision boundary"""
    plt.figure(figsize=(10, 6))

    # Plot data points
    alaska_points = X[y == "Alaska"]
    canada_points = X[y == "Canada"]
    plt.scatter(alaska_points[:, 0], alaska_points[:, 1],
               c='blue', marker='o', label='Alaska', alpha=0.6)
    plt.scatter(canada_points[:, 0], canada_points[:, 1],
               c='red', marker='^', label='Canada', alpha=0.6)

    # Calculate decision boundary
    # Get the inverse of Sigma
    Sigma_inv = np.linalg.inv(Sigma)

    # Calculate the coefficients of the linear boundary
    # From the equation: x^T Σ⁻¹(μ₁ - μ₀) = 1/2(μ₁ + μ₀)^T Σ⁻¹(μ₁ - μ₀)
    diff = mu_1 - mu_0
    mean = (mu_1 + mu_0) / 2

    # Calculate points for plotting the decision boundary
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5

    # From the linear equation ax + by + c = 0
    a = Sigma_inv[0, 0] * diff[0] + Sigma_inv[0, 1] * diff[1]
    b = Sigma_inv[1, 0] * diff[0] + Sigma_inv[1, 1] * diff[1]
    c = -diff.T @ Sigma_inv @ mean

    # Calculate y values for the decision boundary
    x = np.linspace(x_min, x_max, 100)
    y = (-a * x - c) / b

    # Plot decision boundary
    plt.plot(x, y, 'g--', label='Decision Boundary')

    plt.xlabel('Normalized Fresh Water Growth Ring Diameter')
    plt.ylabel('Normalized Marine Water Growth Ring Diameter')
    plt.title('GDA Classification with Decision Boundary')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

plot_decision_boundary(X, y, mu_0, mu_1, sigma)

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels.

        Returns
        -------
        If assume_same_covariance = True - 3-tuple of numpy arrays (mu_0, mu_1, sigma)
        If assume_same_covariance = False - 4-tuple of numpy arrays (mu_0, mu_1, sigma_0, sigma_1)
        """

        # Compute means
<A NAME="0"></A><FONT color = #FF0000><A HREF="match101-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu_0 = np.mean(X[y == "Alaska"], axis=0)
        self.mu_1 = np.mean(X[y == "Canada"], axis=0)

        # Compute covariance matrices
        X0_centered = X[y == "Alaska"] - self.mu_0
</FONT>        n0 = np.sum(y == "Alaska")
        self.sigma_0 = (X0_centered.T @ X0_centered) / n0

        X1_centered = X[y == "Canada"] - self.mu_1
        n1 = np.sum(y == "Canada")
        self.sigma_1 = (X1_centered.T @ X1_centered) / n1

        if assume_same_covariance:
            sigma = (n0 * self.sigma_0 + n1 * self.sigma_1) / (n0 + n1)
            return self.mu_0, self.mu_1, sigma
        else:
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

# Example usage
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)

# Print the results
print("\nMean for Alaska (µ0):")
print(mu_0)
print("\nMean for Canada (µ1):")
print(mu_1)
print("\nCovariance Matrix for Alaska (Σ0):")
print(sigma_0)
print("\nCovariance Matrix for Canada (Σ1):")
print(sigma_1)

def plot_decision_boundaries(X, y, mu_0, mu_1, Sigma, Sigma_0, Sigma_1):
    """
    Plot data points with both linear and quadratic decision boundaries.

    Parameters:
    -----------
    X : normalized feature matrix
    y : class labels
    mu_0, mu_1 : class means
    Sigma : shared covariance matrix
    Sigma_0, Sigma_1 : class-specific covariance matrices
    """
    plt.figure(figsize=(12, 8))

    # Plot data points
    plt.scatter(X[y == "Alaska"][:, 0], X[y == "Alaska"][:, 1],
               c='blue', marker='o', label='Alaska', alpha=0.6)
    plt.scatter(X[y == "Canada"][:, 0], X[y == "Canada"][:, 1],
               c='red', marker='^', label='Canada', alpha=0.6)

    # Create grid for decision boundaries
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]

    # Linear boundary (shared covariance)
    Sigma_inv = np.linalg.inv(Sigma)
    diff = mu_1 - mu_0
    mean = (mu_1 + mu_0) / 2
    linear_scores = np.array([x.T @ Sigma_inv @ diff - mean.T @ Sigma_inv @ diff for x in grid_points])

    # Quadratic boundary (separate covariances)
    Sigma_0_inv = np.linalg.inv(Sigma_0)
    Sigma_1_inv = np.linalg.inv(Sigma_1)
    quad_scores = np.array([
        (-0.5 * (x - mu_0).T @ Sigma_0_inv @ (x - mu_0) +
         0.5 * (x - mu_1).T @ Sigma_1_inv @ (x - mu_1) +
         0.5 * (np.log(np.linalg.det(Sigma_1)) - np.log(np.linalg.det(Sigma_0))))
        for x in grid_points
    ])

    # Reshape scores and plot contours
    linear_scores = linear_scores.reshape(xx.shape)
    quad_scores = quad_scores.reshape(xx.shape)

    # Plot decision boundaries
    plt.contour(xx, yy, linear_scores, levels=[0], colors='g', linestyles='--')
    plt.contour(xx, yy, quad_scores, levels=[0], colors='purple', linestyles='-')

    # Add manual legend for decision boundaries
    plt.plot([], [], 'g--', label='Linear Boundary')
    plt.plot([], [], 'purple', label='Quadratic Boundary')

    plt.xlabel('Fresh Water Growth Ring Diameter')
    plt.ylabel('Marine Water Growth Ring Diameter')
    plt.title('GDA Classification with Linear and Quadratic Decision Boundaries')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# Using previously calculated parameters
plot_decision_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1)

</PRE>
</PRE>
</BODY>
</HTML>
