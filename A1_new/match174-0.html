<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_6ATXW.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_6ATXW.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
<A NAME="0"></A><FONT color = #FF0000><A HREF="match174-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from mpl_toolkits.mplot3d import Axes3D



# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
        self.loss_history = []
    
    def fit(self, X, y, learning_rate=0.01):
</FONT>        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        # Add intercept term (column of ones)
        # Create column of ones and append it to FRONT of X
        ones = np.ones((X.shape[0],1)) 
        X = np.append(ones, X, axis=1)

        m,n = X.shape
        self.theta = np.zeros(n)
        self.theta_history = [self.theta.copy()]
        self.loss_history = [self.mse_loss(X, y, self.theta)]


        # Convergence Parameters
        epsilon = 1.5e-5
        max_iter = 10000
        converged = False
        check = 0

        while not converged and check &lt; max_iter:
            # Compute gradient
            h = X.dot(self.theta)
            error = h - y.ravel()
            gradient = X.T.dot(error) / m
            
            # Update parameters
            self.theta = self.theta - (learning_rate * gradient)
            
            # Track history
            self.theta_history.append(self.theta.copy())

            # Calculate loss
            loss = self.mse_loss(X, y, self.theta)
            self.loss_history.append(loss)
            
            # Check convergence
            loss_diff = abs(self.loss_history[-2] - self.loss_history[-1])
            converged = loss_diff &lt; epsilon
            
            # Next iteration
            check += 1
        return np.array(self.theta_history)



    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        ones = np.ones((X.shape[0], 1))
        X = np.append(ones, X, axis=1)
        return X.dot(self.theta)
    

    def mse_loss(self, X, y, theta):
            """
            Compute Mean Squared Error (MSE) loss.
            """
            h = X.dot(theta)
            error = h - y.ravel()
            return np.sum(error ** 2) / (2 * X.shape[0])
    
    
def load_data():
    """
    Load training data from CSV files.
    """
    X = pd.read_csv("../data/Q1/linearX.csv", header=None).values
    y = pd.read_csv("../data/Q1/linearY.csv", header=None).values
    return X, y
    
def plot_cost_convergence(model):
    """
    Plot cost function convergence over iterations.
    """
    plt.figure(figsize=(10, 6))
    plt.plot(model.loss_history, label="Cost J(θ)")
    plt.xlabel("Iteration")
    plt.ylabel("Cost J(θ)")
    plt.title("Convergence Plot")
    plt.legend()
    plt.savefig("../data/Q1/learning_curve.png")
    
        


def plot_hypothesis(X, y, model):
    """
    Plot training data and learned hypothesis function.
    """
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 0], y, color="blue", label="Training Data", alpha=1)

    x_range = np.array([X[:, 0].min(), X[:, 0].max()])
    X_plot = np.c_[np.ones(2), x_range]
    y_plot = X_plot.dot(model.theta)
    plt.plot(x_range, y_plot, color="red", label="Hypothesis Line")

    plt.xlabel("Acidity")
    plt.ylabel("Density")
    plt.title("Wine Density vs Acidity with Linear Regression Fit")
    plt.legend()
    plt.savefig("../data/Q1/hypothesis_plot.png")

def plot_3D_surface(X, y, model, theta_history):
    """
<A NAME="2"></A><FONT color = #0000FF><A HREF="match174-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    Plot 3D surface of the cost function with gradient descent path.
    """
    # First we create augmented X with added intercept for computing the cost surface
    X_aug = np.append(np.ones((X.shape[0], 1)), X, axis=1)
</FONT>    
    theta0_vals = np.linspace(-10, 20, 100)
    theta1_vals = np.linspace(-10, 50, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

    J_vals = np.zeros_like(theta0_mesh)
    # Loop over grid and compute the cost.
    for i in range(len(theta0_vals)):
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match174-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(len(theta1_vals)):
            theta = np.array([theta0_vals[i], theta1_vals[j]])
            J_vals[j, i] = model.mse_loss(X_aug, y, theta)
</FONT>
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111, projection="3d")
    ax.view_init(elev=10, azim=10)
    
    surf = ax.plot_surface(theta0_mesh, theta1_mesh, J_vals, cmap="viridis", alpha=0.7)

    # Extract gradient path from theta_history
    theta_0_vals = theta_history[:, 0]
    theta_1_vals = theta_history[:, 1]
    # Compute cost along the trajectory using the augmented X
    J_vals_trajectory = [model.mse_loss(X_aug, y, t) for t in theta_history]

    # Animate the gradient descent path
    for i in range(1, len(theta_0_vals)):
        ax.scatter(theta_0_vals[i-1:i+1], theta_1_vals[i-1:i+1],
                   J_vals_trajectory[i-1:i+1], color='red', s=7)
        ax.plot(theta_0_vals[i-1:i+1], theta_1_vals[i-1:i+1], 
                J_vals_trajectory[i-1:i+1], color='black', linewidth=2)
        plt.pause(0.2)

    ax.set_xlabel("θ₀")
    ax.set_ylabel("θ₁")
    ax.set_zlabel("J(θ)")
    ax.set_title("Cost Function with Gradient Descent Path")
    plt.savefig("../data/Q1/3D_plot.png")


def plot_contour(X, y, model, theta_history):
    """
    Plot contour of cost function with gradient descent path.
    """
    # Augmented X with a column of ones
    X_aug = np.append(np.ones((X.shape[0], 1)), X, axis=1)
    
    theta0_vals = np.linspace(-10, 20, 100)
    theta1_vals = np.linspace(-10, 50, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

    J_vals = np.zeros_like(theta0_mesh)

    for i in range(len(theta0_vals)):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match174-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(len(theta1_vals)):
            theta = np.array([theta0_vals[i], theta1_vals[j]])
            J_vals[j, i] = model.mse_loss(X_aug, y, theta)
</FONT>
    plt.figure(figsize=(10, 6))
    contour = plt.contour(theta0_mesh, theta1_mesh, J_vals,
                          levels=np.logspace(0.5, 3, 20), cmap="viridis")
    plt.clabel(contour, inline=True, fontsize=8)

    theta_0_vals = theta_history[:, 0]
    theta_1_vals = theta_history[:, 1]

    # Animate the gradient descent path on the contour plot
    for i in range(1, len(theta_0_vals)):
        plt.plot(theta_0_vals[:i+1], theta_1_vals[:i+1], 'r-o', markersize=2)
        plt.pause(0.2)

    plt.xlabel("θ₀")
    plt.ylabel("θ₁")
    plt.title("Contour Plot with Gradient Descent Path")
    plt.legend(['Gradient Descent Path'])
    plt.savefig("../data/Q1/contour_plot.png")

def compare(X, y):
    """
    Train model with different learning rates and plot results.
    """
    learning_rates = [0.001, 0.025, 0.1]
    for eta in learning_rates:
        model = LinearRegressor()
        theta_history = model.fit(X, y, learning_rate=eta)
        print(f"Learning Rate: {eta}")
        print(f"Final θ: {model.theta}")
        print(f"Final Cost: {model.loss_history[-1]:.6f}")
        print(f"Total Iterations: {len(model.loss_history) - 1}\n")


def main():
    """
    Main function to load data, train the model, and generate plots.
    """
    X, y = load_data()
    compare(X, y)

    '''
    This is testing code and has been commented out after testing the working of the script.

    # Train model with eta=0.1
    learning_rate = 0.1
    model = LinearRegressor()
    theta_history = model.fit(X, y, learning_rate=learning_rate)
    
    # Print results
    print(f"Learning Rate: {learning_rate}")
    print(f"Final θ: {model.theta}")
    print(f"Final Cost: {model.loss_history[-1]:.6f}")
    print(f"Total Iterations: {len(model.loss_history) - 1}\n")
    
    # Generate required plots
    plot_hypothesis(X, y, model)  # Hypothesis and data points graph
    plot_cost_convergence(model)  # Learning curve graph
    plot_3D_surface(X, y, model, theta_history)  # 3D surface plot
    plot_contour(X, y, model, theta_history)  # Contour plot
    
    '''


if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
<A NAME="1"></A><FONT color = #00FF00><A HREF="match174-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    np.random.seed(42)
    
    # Generate features with correct variance (std=2 since variance=4)
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
</FONT>    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    
    # Create design matrix with intercept
    X = np.column_stack([np.ones(N), x1, x2])
    y = X.dot(theta) + noise
    
    return X[:, 1:], y  # Return without intercept column since X : numpy array of shape (N, 2) is needed

class StochasticLinearRegressor:
    def __init__(self):
        self.theta_hist = []
        self.closed_theta = None
        self.true_theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        # Store true theta from generation
        self.true_theta = np.array([3.0, 1.0, 2.0])  # From problem statement
        
        # Add intercept column
        X = np.column_stack([np.ones(len(X)), X])
        
        # Compute closed-form solution
        self.closed_form(X, y)
        
        # Run SGD for all batch sizes
        batch_sizes = [1, 80, 8000, 800000]
        self.theta_hist = []
        
        for r in batch_sizes:
            hist = self.sgd_optim(X, y, r, learning_rate)
            self.theta_hist.append(hist)
            
            # Generate plots and analysis
            if __name__ == "__main__":
                self.gen_analys(X, y, hist, r)
        
        return self.theta_hist
    
    
    def sgd_optim(self, X, y, batch_size, eta, tol=1e-6, max_epochs=10000):
        """
        Core SGD implementation with convergence tracking
        """
        m, n = X.shape
        theta = np.zeros(n)
        history = [theta.copy()]
        prev_theta = theta.copy()
        epoch = 0
        
        while epoch &lt; max_epochs:
            # Process all batches per epoch
            for b in range(m // batch_size):
                start = b * batch_size
                end = start + batch_size
                X_batch = X[start:end]
                y_batch = y[start:end]
                
                # Gradient calculation
                error = np.dot(X_batch, theta) - y_batch
                gradient = np.dot(X_batch.T, error) / len(X_batch)
                theta = theta - eta * gradient
                history.append(theta.copy())
            
            # Convergence check
            if np.sqrt(np.sum((theta - prev_theta)**2)) &lt; tol:
                break
            prev_theta = theta.copy()
            epoch += 1
            
        return np.array(history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.column_stack([np.ones(len(X)), X])
        return np.dot(X, self.theta_hist[-1][-1])
    
    
    def closed_form(self, X, y):
        """
        Closed-form solution implementation
        """
        XtX = np.dot(X.T, X)
        XtX_inv = np.linalg.inv(XtX)
        self.closed_theta = np.dot(np.dot(XtX_inv, X.T), y)
    
    def gen_analys(self, X, y, history, batch_size):
        """
        Handles all plotting and error calculations
        """
        # Split data
        split_idx = int(0.8 * len(X))
        X_train, y_train = X[:split_idx], y[:split_idx]
        X_test, y_test = X[split_idx:], y[split_idx:]
        
        # Calculate errors
        train_err = self.mse(X_train, y_train, history[-1])
        test_err = self.mse(X_test, y_test, history[-1])
        
        # Print results
        print(f"\nBatch Size: {batch_size}")
        print(f"Final Params: {history[-1]}")
        print(f"Train Error: {train_err:.5f}, Test Error: {test_err:.5f}")
        
        # Generate 3D plot
        self.plot_path(history, batch_size)
    
    def mse(self, X, y, theta):
        """
        Computes MSE for given parameters
        """
        return np.mean((np.dot(X, theta) - y)**2)
    
    def plot_path(self, history, batch_size):
        """
        Generates 3D parameter space visualization
        """
        fig = plt.figure(figsize=(15,10))
        ax = fig.add_subplot(111, projection='3d')
        
        # Plot trajectory
        ax.plot(history[:,0], history[:,1], history[:,2], c='red', alpha=0.7, linewidth=0.8, label='SGD Path')
        
        # Plot true parameters
        ax.scatter(self.true_theta[0], self.true_theta[1], self.true_theta[2], c='green', s=100, marker='*', label='True θ')
        
        # Plot final estimate
        ax.scatter(history[-1,0], history[-1,1], history[-1,2], c='blue', s=100, marker='X', label='Final θ')
        
        ax.set_xlabel('θ₀', fontsize=10)
        ax.set_ylabel('θ₁', fontsize=10)
        ax.set_zlabel('θ₂', fontsize=10)
        ax.set_title(f'Batch Size: {batch_size}', fontsize=12)
        ax.legend()
        plt.savefig(f'q2_batch_{batch_size}.png')
        plt.close()

if __name__ == "__main__":
    # This part is for testing purpose of the script
    
    # Generate data using problem parameters
    theta_true = np.array([3.0, 1.0, 2.0])
    X, y = generate(1000000, theta_true, 
                   input_mean=np.array([3.0, -1.0]),
                   input_sigma=np.array([2.0, 2.0]),
                   noise_sigma=np.sqrt(2))
    
    # Train model (triggers analysis plots)
    model = StochasticLinearRegressor()
    histories = model.fit(X, y)
    
    # Closed-form comparison
    print("\nClosed-Form vs True Parameters:")
    print(f"True θ:       {theta_true}")
    print(f"Closed-Form:  {model.closed_theta}")
    print(f"Distance:     {np.linalg.norm(model.closed_theta - theta_true):.5f}")



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None 
        self.mu = None 
        self.sigma = None 
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        # Normalize features
        X_norm, self.mu, self.sigma = self.normalize(X)
        
        # Add intercept term
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])
        
        # Initialize parameters
        m, n = X_norm.shape
        self.theta = np.zeros((n, 1))
        
        # Reshape y to be a column vector
        y = y.reshape(-1, 1)
        
        # Newton's Method
        max_iter = 100
        tol = 1e-6
        i = 0
        theta_hist = []  # To store parameters at each iteration
        
        while i &lt; max_iter:
            # Predictions
            h = self.sigmoid(np.dot(X_norm, self.theta))
            
            # Gradient
            grdn = np.dot(X_norm.T, (y - h))
            
            # Hessian
            S = np.diagflat(h * (1 - h))
            H = -np.dot(np.dot(X_norm.T, S), X_norm)
            
            # Update rule (directly using matrix inverse)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match174-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            delta = np.dot(np.linalg.inv(H), grdn)  # Solve H^-1 * gradient
            self.theta -= delta
            
            # Store current theta for history tracking
            theta_hist.append(self.theta.flatten())
            
            # Convergence check
            if np.linalg.norm(delta) &lt; tol:
                break
</FONT>            
            i += 1
        
        return np.array(theta_hist)

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        # Normalize features using stored mean and std from training data
        X_norm = (X - self.mu) / self.sigma
        
        # Add intercept term
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])
        
        # Compute predictions using sigmoid function and learned parameters
        pb = self.sigmoid(np.dot(X_norm, self.theta))
        
        # Convert probabilities to binary predictions (threshold at 0.5)
        return (pb &gt;= 0.5).astype(int).flatten()

    def sigmoid(self, z):
        """
        Sigmoid function
        """
        return 1 / (1 + np.exp(-z))
       
    def normalize(self, X):
        """
        Normalize features using z-score normalization.
        """
        mu = np.mean(X, axis=0)
        sigma = np.std(X, axis=0)
        return (X - mu) / sigma, mu, sigma
    
def plot_divide(X, y, model):
    plt.figure(figsize=(8, 6))
        
    # Scatter plot of data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0', marker='o')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1', marker='x')
       
    # Decision boundary: theta[0] + theta[1]*x1 + theta[2]*x2 = 0
    x_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    y_vals = -(model.theta[0] + model.theta[1] * x_vals) / model.theta[2]
        
    plt.plot(x_vals, y_vals, color='red', label='Decision Boundary')
        
    plt.xlabel('Feature x₁')
    plt.ylabel('Feature x₂')
    plt.legend()
    plt.title('Logistic Regression Decision Boundary')
    plt.grid(True)
    plt.savefig("../data/Q3/boundary_plot.png")
    

# This code is for testing purpose of this script and the defined functions.  
if __name__ == "__main__":
    # Load data
    X = pd.read_csv("../data/Q3/logisticX.csv", header=None).values
    y = pd.read_csv("../data/Q3/logisticY.csv", header=None).values

    # Split into train and test sets (80%-20%)
    k = int(0.8 * len(X))
    X_train, X_test = X[:k], X[k:]
    y_train, y_test = y[:k], y[k:]
    
    # Initialize and train model
    model = LogisticRegressor()
    theta_history = model.fit(X_train, y_train)
    
    print("Learned Parameters:", model.theta.ravel())
    
    # Predict on test set and calculate accuracy
    y_pred_test = model.predict(X_test)
    accuracy_test = np.mean(y_pred_test == y_test) * 100
    
    print(f"Test Accuracy: {accuracy_test:.2f}%")
    

    
    plot_divide(X_train[:, :2], y_train.flatten(), model)




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
        self.phi = None
        self.assume_same_covariance = None
    
    def normalize_data(self, X):
        """
        Function to normalize features
        """
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        return (X - mean) / std
    
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        self.assume_same_covariance = assume_same_covariance
        
        # Normalize the input data
        X = self.normalize_data(X)
        
        n_samples, n_features = X.shape
        
        # Calculate phi (probability of y=1)
        self.phi = np.mean(y)
        
        # Calculate means
        self.mu0 = np.mean(X[y == 0], axis=0)
        self.mu1 = np.mean(X[y == 1], axis=0)
        
        if assume_same_covariance:
            # Calculate shared covariance matrix
            self.sigma = np.zeros((n_features, n_features))
            for i in range(n_samples):
                mu = self.mu1 if y[i] == 1 else self.mu0
                diff = (X[i] - mu).reshape(-1, 1)
                self.sigma += diff.dot(diff.T)
            self.sigma /= n_samples
            return self.mu0, self.mu1, self.sigma
        else:
            # Calculate separate covariance matrices
            n0 = np.sum(y == 0)
            n1 = np.sum(y == 1)
            
            self.sigma0 = np.zeros((n_features, n_features))
            self.sigma1 = np.zeros((n_features, n_features))
            
            for i in range(n_samples):
                if y[i] == 0:
                    diff = (X[i] - self.mu0).reshape(-1, 1)
                    self.sigma0 += diff.dot(diff.T)
                else:
                    diff = (X[i] - self.mu1).reshape(-1, 1)
                    self.sigma1 += diff.dot(diff.T)
            
            self.sigma0 /= n0
            self.sigma1 /= n1
            return self.mu0, self.mu1, self.sigma0, self.sigma1
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        # Normalize the input data
        X = self.normalize_data(X)
        
        if self.assume_same_covariance:
            # Linear decision boundary
            sigma_inv = np.linalg.inv(self.sigma)
            w = sigma_inv.dot(self.mu1 - self.mu0)
            b = (-0.5 * self.mu1.T.dot(sigma_inv).dot(self.mu1) + 
                  0.5 * self.mu0.T.dot(sigma_inv).dot(self.mu0) +
                  np.log(self.phi / (1 - self.phi)))
            scores = X.dot(w) + b
        else:
            # Quadratic decision boundary
            scores = np.zeros(len(X))
            for i, x in enumerate(X):
                term0 = -0.5 * np.log(np.linalg.det(self.sigma0))
                term1 = -0.5 * np.log(np.linalg.det(self.sigma1))
                diff0 = x - self.mu0
                diff1 = x - self.mu1
                term2 = -0.5 * diff0.dot(np.linalg.inv(self.sigma0)).dot(diff0)
                term3 = -0.5 * diff1.dot(np.linalg.inv(self.sigma1)).dot(diff1)
                scores[i] = (term1 + term3) - (term0 + term2) + np.log((1 - self.phi)/self.phi)

        
        return (scores &gt; 0).astype(int)
    
    def plot_decision_boundary(self, X, y):
        """
        Helper function to plot decision boundaries
        """
<A NAME="6"></A><FONT color = #00FF00><A HREF="match174-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = self.normalize_data(X)
        
        plt.figure(figsize=(10, 6))
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Alaska', marker='o')
</FONT>        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Canada', marker='x')
        
        # Create grid for decision boundary
        x1, x2 = np.meshgrid(
            np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
            np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        )
        grid = np.c_[x1.ravel(), x2.ravel()]
        
        # Get predictions for grid points
        predictions = self.predict(grid)
        
        # Plot decision boundary
        plt.contour(x1, x2, predictions.reshape(x1.shape), levels=[0.5], colors='g')
        
        plt.xlabel('Growth ring diameter in freshwater')
        plt.ylabel('Growth ring diameter in marine water')
        plt.legend()
        plt.title('GDA Decision Boundary')
        plt.savefig("../data/Q4/GDA_boundary_plot.png")

# This code is for testing the working of the functions defined in this file
def test_gda():
    # Load data
    X = np.loadtxt('../data/Q4/q4x.dat')
    y_str = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
    
    # Convert labels to binary (0 for Alaska, 1 for Canada)
    y = (y_str == 'Canada').astype(int)
    
    # Initialize and fit GDA model with shared covariance
    print("Testing GDA with shared covariance...")
    gda_shared = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda_shared.fit(X, y, assume_same_covariance=True)
    
    print("\nParameters for shared covariance:")
    print(f"mu0: {mu0}")
    print(f"mu1: {mu1}")
    print(f"Sigma:\n{sigma}")
    
    # Make predictions
    y_pred = gda_shared.predict(X)
    accuracy = np.mean(y_pred == y)
    print(f"\nAccuracy with shared covariance: {accuracy:.4f}")
    
    # Plot decision boundary
    print("\nPlotting linear decision boundary...")
    gda_shared.plot_decision_boundary(X, y)
    
    # Test separate covariance
    print("\nTesting GDA with separate covariance...")
    gda_separate = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma0, sigma1 = gda_separate.fit(X, y, assume_same_covariance=False)
    
    print("\nParameters for separate covariance:")
    print(f"mu0: {mu0}")
    print(f"mu1: {mu1}")
    print(f"Sigma0:\n{sigma0}")
    print(f"Sigma1:\n{sigma1}")
    
    # Make predictions
    y_pred = gda_separate.predict(X)
    accuracy = np.mean(y_pred == y)
    print(f"\nAccuracy with separate covariance: {accuracy:.4f}")
    
    # Plot decision boundary
    print("\nPlotting quadratic decision boundary...")
    gda_separate.plot_decision_boundary(X, y)

if __name__ == "__main__":
    test_gda()


</PRE>
</PRE>
</BODY>
</HTML>
