<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_71W6W.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_71W6W.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time



class LinearRegressor:
    # Question 1
    def __init__(self, max_iter = 50000):
        self.weights = None
        self.max_iter = max_iter
        self.costs_history = []
        self.weights_history = []
        self.bias_history = []


    def compute_cost(self, X, y):
        m = X.shape[0]
        y_pred = self.predict(X)
        return ((1 / (2 * m)) * np.sum((y_pred - y) ** 2))
    
    def compute_cost_with_theta(self, X, y, theta):
        m = X.shape[0]  # Number of training examples
        y_pred = X @ theta  # Matrix multiplication to compute predictions
        cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)  # MSE cost function
        return cost

    
    
    def fit(self, X, y ,learning_rate = 0.01 ):
        m, n = X.shape
        self.weights = np.zeros(n)
        # print(np.dot(X, self.weights))

        prev_cost = float('inf')

        result = []
        result.append(self.weights)
        
        for i in range(self.max_iter):
            y_pred = self.predict(X)
            cost = self.compute_cost(X, y)

            self.costs_history.append(cost)
            self.weights_history.append(self.weights[0])
            self.bias_history.append(self.weights[1])   

            # if i % 10 == 0:  # Print MSE every 100 iterations
                # print(f"Iteration {i}, Cost: {cost:.6f}")
                # print(f"Weights: {self.weights}")

            if abs(prev_cost - cost) &lt; 1e-6:
                # print(f"Converged at iteration {i}")
                # print(f"Final Weights: {self.weights}")
                break
            # print("Divv - ", X.T.shape , (y_pred - y).shape)
            grad = (1 / m) * np.dot(X.T, (y_pred - y))
            # print(self.weights.shape, grad.shape)   
            self.weights = self.weights - (learning_rate * grad)
            prev_cost = cost
            result.append(self.weights)
        
        
        # print(f"Final Cost: {self.compute_cost(X, y):.6f}")
        # print(f"Final Weights: {self.weights}")

        return result



    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match242-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
    
        return  np.dot(X, self.weights)
</FONT>    
    # Question 2
    def plot(self, X, y):
        plt.scatter(X[:, 0], y, color='red')
        plt.plot(X[:, 0], self.predict(X), color='blue')
        plt.show()

    # # Question 3
    # def plot3d(self):
    #     cost_history = self.costs_history 
    #     bias_history = self.bias_history   
    #     weight_history = self.weights_history  

    #     fig = plt.figure(figsize=(12, 8))
    #     ax = fig.add_subplot(111, projection='3d')

    #     ax.set_xlabel('Bias (θ₀)', fontsize=12)
    #     ax.set_ylabel('Weight (θ₁)', fontsize=12)
    #     ax.set_zlabel('Cost (J(θ))', fontsize=12)
    #     ax.set_title('3D Visualization of Gradient Descent', fontsize=16)

  
    #     ax.view_init(elev=25, azim=45)

    #     scat = ax.scatter([], [], [], c='r', marker='o')
    #     text = ax.text2D(0.05, 0.95, '', transform=ax.transAxes)

    #     ax.set_xlim(min(bias_history)-0.5, max(bias_history)+0.5)
    #     ax.set_ylim(min(weight_history)-0.5, max(weight_history)+0.5)
    #     ax.set_zlim(0, max(cost_history)+5)

    #     # k = 1
    #     for i in range(len(cost_history)):
    #         scat._offsets3d = (bias_history[:i+1], weight_history[:i+1], cost_history[:i+1])

    #         if i &gt; 0:
    #             ax.plot(bias_history[i-1:i+1], 
    #                     weight_history[i-1:i+1], 
    #                     cost_history[i-1:i+1], 
    #                     'b--', alpha=0.5)
            
    #         text.set_text(f'Iteration: {i}\nCost: {cost_history[i]:.6f}')
    #         # k = k+5

    #         plt.pause(0.1)

    #     plt.show()

    # Question 3
    def plot3d(self, X, y):
        stored_cost = np.array(self.costs_history)
        theta_0_values = self.weights_history
        theta_1_values = self.bias_history

        m = X.shape[0]
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)


        theta_0_vals = np.linspace(min(theta_0_values) - 5, max(theta_0_values) + 10, 100)
        theta_1_vals = np.linspace(min(theta_1_values) - 5, max(theta_1_values) + 30, 100)

        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
        Z = np.zeros_like(T0)
        for i in range(len(theta_0_vals)):
            for j in range(len(theta_1_vals)):
                temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
                Z[i, j] = self.compute_cost_with_theta(X, y , temp_theta)


        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(T0, T1, Z, cmap='viridis', alpha=0.5)
        ax.set_xlabel("Theta 0")
        ax.set_ylabel("Theta 1")
        ax.set_zlabel("Cost J(Theta)")
        ax.set_title("3D Visualization")

        text = ax.text2D(0.05, 0.95, '', transform=ax.transAxes)

        for i in range(1, len(theta_0_values) , 4):
            ax.scatter(theta_0_values[i], theta_1_values[i], stored_cost[i],
                       color='r', marker='o')
            
            text.set_text(f'Iteration: {i+1}\nCost: {stored_cost[i]:.6f}\nTheta 0: {theta_0_values[i]:.6f}\nTheta 1: {theta_1_values[i]:.6f}')
            plt.draw()
            plt.pause(0.2)

        plt.show()
        plt.close()


    # Question 4
    def plot_contour(self):
        bias_history = self.bias_history
        theta_history = self.weights_history
        cost_history = self.costs_history

        optimal_bias = self.weights[1]
        optimal_theta = self.weights[0]

        bias_vals = np.linspace(0, optimal_bias + 2, 100)
        theta_vals = np.linspace(0, optimal_theta + 2, 100)
        bias_grid, theta_grid = np.meshgrid(bias_vals, theta_vals)

        def cost_function(bias, theta):
            return (bias - optimal_bias)**2 + (theta - optimal_theta)**2

        cost_grid = cost_function(bias_grid, theta_grid)

        fig, ax = plt.subplots(figsize=(12, 8))
        contours = ax.contour(bias_grid, theta_grid, cost_grid, levels=20, cmap="viridis")
        plt.clabel(contours, inline=True, fontsize=8)
        ax.set_xlabel("Bias (θ₀)")
        ax.set_ylabel("Weight (θ₁)")
        ax.set_title("Gradient Descent Contour Plot")

        ax.scatter(optimal_bias, optimal_theta, color='green', s=100, label='Optimal Point')

        text_box = ax.text(0.02, 0.98, '', transform=ax.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        # Animate the gradient descent path
        line, = ax.plot([], [], 'r--', label='Gradient Descent Path')
        point, = ax.plot([], [], 'ro', markersize=8)

        for i in range(0,len(bias_history),10):
            line.set_data(bias_history[:i+1], theta_history[:i+1])
   
            point.set_data(bias_history[i], theta_history[i])

            text_box.set_text(f'Iteration: {i}\nBias: {bias_history[i]:.6f}\nWeight: {theta_history[i]:.6f}\nCost: {cost_history[i]:.6f}')
            
            plt.pause(0.5) 

        ax.legend()
        ax.grid(True)
        plt.show()


def load_data(input_data_file , output_data_file):
    x = pd.read_csv(input_data_file ,header= None)
    y = pd.read_csv(output_data_file ,header= None)
    X = np.array(x)
    Y = np.array(y)
    Y = Y.reshape((Y.shape[0],))
    return X,Y

if __name__ == "__main__":
    X, Y = load_data("../data/Q1/linearX.csv", "../data/Q1/linearY.csv")
    X = np.hstack((X , np.ones((X.shape[0], 1))))
    print(X.shape, Y.shape)
    print(X[:5], Y[:5])

    linear_regressor = LinearRegressor()
    res = linear_regressor.fit(X, Y)  # Quesiton 1
    # print(f"Result - {len(res)} , {len(res[0])}")
    linear_regressor.plot(X, Y) # Quesiton 2
    linear_regressor.plot3d(X ,Y) # Quesiton 3
    linear_regressor.plot_contour() # Quesiton 4






# Imports - you can add any other permitted libraries
import numpy as np
<A NAME="1"></A><FONT color = #00FF00><A HREF="match242-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
</FONT>    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 3)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    np.random.seed(42)

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match242-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X0 = np.random.normal(input_mean[0], input_sigma[0], N)
    X1 = np.random.normal(input_mean[1], input_sigma[1], N)

    X = np.column_stack((np.ones(N), X0, X1))
</FONT>    y = np.dot(X, theta) + np.random.normal(0, noise_sigma, N)

    return X, y
    

class StochasticLinearRegressor:
    def __init__(self , epochs = 50000):
        self.epoch = epochs
        self.weights = None
        self.theta_0 = None
        self.theta_1 = None
        self.theta_2 = None
        self.batch_size = [1,80,8000,800000]

    def compute_cost(self, X, y):
        m = X.shape[0]
        y_pred = self.predict(X)
        return ((1 / (2 * m)) * np.sum((y_pred - y) ** 2))
    
    def fit(self, X, y ,learning_rate = 0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        
        # print(np.dot(X, self.weights))
        self.theta_0 = []
        self.theta_1 = []
        self.theta_2 = []

        batch_size = self.batch_size

        result = []

        for b in batch_size:
            res = []
            self.weights = np.zeros(n)
            teta1 = [self.weights[0]]
            teta2 = [self.weights[1]]
            teta3 = [self.weights[2]]
            
            start_time = time.time()

            prev_cost = float('inf')
            num_batches = m // b

            for i in range(self.epoch):
                avg_cost = 0
                for j in range(num_batches):
                    X_batch = X[j*b : (j+1)*b]
                    y_batch = y[j*b : (j+1)*b]
                    y_pred = self.predict(X_batch)
                    cost = self.compute_cost(X_batch, y_batch)
                    avg_cost += cost

                    # Convergence Condition
                    # if i % 10 == 0:  # Print MSE every 100 iterations
                    #     print(f"Iteration {i}, Cost: {cost:.6f}")
                    #     print(f"Weights: {self.weights}")

                    # if abs(prev_cost - cost) &lt; 1e-6:
                    #     print(f"Converged at iteration {i}")
                    #     print(f"Final Weights: {self.weights}")
                    #     break

                    grad = (1 / b) * np.dot(X_batch.T, (y_pred - y_batch))
                    self.weights = self.weights - (learning_rate * grad)
                    teta1.append(self.weights[0])
                    teta2.append(self.weights[1])
                    teta3.append(self.weights[2])

                


                cost = self.compute_cost(X , y)
                # print(f"Epochs {i} , Cost: {cost} , weights: {self.weights}")
                avg_cost = avg_cost / num_batches
                if abs(avg_cost - prev_cost) &lt; 1e-8:
                    # print(f"Converged at Iteration {i}")
                    # print(f"Final Weights: {self.weights}")
                    break

                prev_cost = avg_cost

            end_time = time.time()

            execution_time = end_time - start_time
            
            self.theta_0.append(teta1)
            self.theta_1.append(teta2)
            self.theta_2.append(teta3)
            # print("--------------------------------------------------------")
            # print(f"Batch Size: {b}")
            # print(f"Execution time: {execution_time:.2f} seconds")
            # print(f"Final Cost: {self.compute_cost(X, y):.6f}")
            # print(f"Final Weights: {self.weights}")

            # print("--------------------------------------------------------")

        result.append(self.theta_0)
        result.append(self.theta_1)
        result.append(self.theta_2)

        return result

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.dot(X, self.weights)
    
    def testing(self , x, y):
        y_pred = self.predict(x)
        # print(x.shape)
        # print(self.weights.shape)
        # print(y_pred.shape)
        # print(y.shape)
        mse = np.mean((y_pred - y) ** 2)
        return mse
        # print("Mean Squared Error on test set:", mse)

    # Question 5

    def plot_for_batches(self):
        for i in range(len(self.batch_size)):
            theta_0_vals = self.theta_0[i]
            theta_1_vals = self.theta_1[i]
            theta_2_vals = self.theta_2[i]

            # Create a 3D plot
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match242-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')

            # Plot the trajectory
            ax.plot(theta_0_vals, theta_1_vals, theta_2_vals, marker='o', color='b')

            # Label axes
            ax.set_xlabel(r'$\theta_0$')
</FONT>            ax.set_ylabel(r'$\theta_1$')
            ax.set_zlabel(r'$\theta_2$')
            ax.set_title(f"Parameter Trajectory ,Batch Size: {self.batch_size[i]}")

            plt.show()
    

    # def plot(self, batch_size):  
    #     theta_0_vals = self.theta_0
    #     theta_1_vals = self.theta_1
    #     theta_2_vals = self.theta_2

    #     # Create a 3D plot
    #     fig = plt.figure()
    #     ax = fig.add_subplot(111, projection='3d')

    #     # Plot the trajectory
    #     ax.plot(theta_0_vals, theta_1_vals, theta_2_vals, marker='o', color='b')

    #     # Label axes
    #     ax.set_xlabel(r'$\theta_0$')
    #     ax.set_ylabel(r'$\theta_1$')
    #     ax.set_zlabel(r'$\theta_2$')
    #     ax.set_title(f"Parameter Trajectory ,Batch Size: {batch_size}")

    #     plt.show()


    

def LinearRegressionClosedForm(X , y):
    theta_closed_form = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

    m = X.shape[0]
    y_pred = np.dot(X, theta_closed_form)
    cost = ((1 / (2 * m)) * np.sum((y_pred - y) ** 2))
    # print("LinearRegressionClosedForm")
    # print(f"Final Cost: {cost}")
    # print(f"Final Weights: {theta_closed_form}")
    return theta_closed_form



if __name__ == "__main__":
    # Question 1
    X , y = generate(1000000 , [3,1,2], [3,-1] , [2,2] , np.sqrt(2))
    split_index = int(0.8 * X.shape[0])
    X_train = X[:split_index]
    X_test = X[split_index:]
    y_train = y[:split_index]
    y_test = y[split_index:]
    # print(X.shape , y.shape)
    # print(X_test.shape)
    # print(y_test.shape)

    # Question 2
    sgd_linear_regressor = StochasticLinearRegressor()

    sgd_linear_regressor.fit(X_train , y_train)

    # # Question 3
    # print("--------------------------------------------------------")

    # batch_sizes = [1, 80, 8000]
    # learning_rate = 0.001
    # epochs = 1000

    # for batch_size in batch_sizes:
    #     print(f"Batch size: {batch_size}")
    #     start_time = time.time()
    #     sgd = StochasticLinearRegressor(batch_size)
    #     sgd.fit(X_train , y_train)
        
    #     end_time = time.time()
    #     execution_time = end_time - start_time
        
    #     print(f"Execution time: {execution_time:.2f} seconds")

    #     sgd.plot(batch_size)

    #     print("-------------------------------------------------------------------")




    theta_closed_form = LinearRegressionClosedForm(X_train , y_train)

    # Question 4
    # print()
    # print("--------------------------------------------------------")
    training_error = sgd_linear_regressor.testing(X_train , y_train)
    testing_error = sgd_linear_regressor.testing(X_test , y_test)

    # print(f"Training Error: {training_error}")
    # print(f"Testing Error: {testing_error}")
    
    # print("--------------------------------------------------------")
    # print()

    #Question 5

    sgd_linear_regressor.plot_for_batches()





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self , max_iter = 1000):
        self.theta = None
        self.max_iter = max_iter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def log_likelihood(self , theta, X, y):
        h = self.sigmoid(X.dot(theta))
        return np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    
    def hessian(self, theta, X):
        h = self.sigmoid(X.dot(theta))
        ans = X.T.dot(np.diag(h * (1 - h))).dot(X)
        # print(ans)
        return ans
    

    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        m, n = X.shape
        self.theta = np.zeros(n)
        prev_ll = self.log_likelihood(self.theta, X, y)
        result = []
        result.append(self.theta)

        for i in range(self.max_iter):
            h = self.sigmoid(X.dot(self.theta))
            grad = X.T.dot(h - y)

            H = self.hessian(self.theta, X)
            H_inv = np.linalg.inv(H)  
            delta = H_inv.dot(grad)

            self.theta -= learning_rate * delta  
            
            current_ll = self.log_likelihood(self.theta, X, y)
            if abs(current_ll - prev_ll) &lt; 1e-6:
                print(f"Converged after {i+1} iterations")
                break
            
            prev_ll = current_ll
            result.append(self.theta)

        print(f"{self.theta}")
        return result
        
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # print(self.theta.shape)
        # print(X.shape)
        z = np.dot(self.theta, X.T)
        probability = self.sigmoid(z)
        # print(f"Probability: {probability}")
        binary_prediction = []
        for i in probability:
            if i &gt;= 0.5:
                binary_prediction.append(1)
            else:
                binary_prediction.append(0)

        binary_prediction = np.array(binary_prediction)
        # print(binary_prediction)
        return binary_prediction
    
    def plot(self, X, y):
        X_class_0 = X[y == 0]
        X_class_1 = X[y == 1]

        plt.figure(figsize=(10, 8))

        plt.scatter(X_class_0[:, 0], X_class_0[:, 1], c='blue', marker='o', label='Class 0')
        plt.scatter(X_class_1[:, 0], X_class_1[:, 1], c='red', marker='x', label='Class 1')

        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.01), np.arange(x2_min, x2_max, 0.01))
        Z = self.theta[0] * xx1 + self.theta[1] * xx2 + self.theta[2] 
        Z = 1 / (1 + np.exp(-Z))
        plt.contour(xx1, xx2, Z, levels=[0.5], colors='green', linestyles='dashed')

        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.title('Logistic Regression Decision Boundary')
        plt.legend()
        plt.grid(True)
        plt.show()
        
def normalize_features(path):
    X = pd.read_csv(path ,header= None).values
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_normalized = (X - mean) / std
    return np.array(X_normalized)

def load_data(input_data_file , output_data_file):
    x = pd.read_csv(input_data_file ,header= None)
    y = pd.read_csv(output_data_file ,header= None)
    X = np.array(x)
    Y = np.array(y)
    Y = Y.reshape((Y.shape[0],))
    return X,Y

if __name__ == "__main__":
    X, y = load_data("../data/Q3/logisticX.csv", "../data/Q3/logisticY.csv")
    X_normalized = normalize_features("../data/Q3/logisticX.csv")
    X_normalized = np.hstack((X_normalized , np.ones((X_normalized.shape[0], 1))))
    
    # print(X.shape)
    # print(X_normalized.shape)    

    LogisticRegressor = LogisticRegressor()
    # Question 1 
    LogisticRegressor.fit(X_normalized, y)
    # print(f"Final Weights: {LogisticRegressor.theta}")

    # Question 2
    LogisticRegressor.plot(X_normalized, y)

    # LogisticRegressor.predict(X_normalized)



    





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal


class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.phi = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
    
    def fit(self, X, y ,assume_same_covariance = False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="0"></A><FONT color = #FF0000><A HREF="match242-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X_0 = X[y == 0]
        X_1 = X[y == 1] 

        self.phi = np.mean(y)
</FONT>
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        if assume_same_covariance == False:
            self.individual_covariance(X, y)
            return (self.mu_0 , self.mu_1 , self.sigma_0 , self.sigma_1)

            

        n = X.shape[0]

        self.sigma = np.zeros((X.shape[1], X.shape[1]))

        for i in range(n):
            mu = self.mu_1 if y[i] == 1 else self.mu_0
            diff = X[i] - mu
            self.sigma += np.outer(diff, diff)

        self.sigma = self.sigma / n

        # print("Parameters of Gaussian Discriminant Analysis:")
        # print(f"phi (P(y=1)): {self.phi}")
        # print(f"mu_0: {self.mu_0}")
        # print(f"mu_1: {self.mu_1}")
        # print(f"Shared covariance matrix Sigma:\n{self.sigma}")

        return (self.mu_0 , self.mu_1 , self.sigma)



    def individual_covariance(self, X, y):
        X0 = X[y == 0]
        X1 = X[y == 1]

        m0 = X0.shape[0]
        m1 = X1.shape[0]

        self.sigma_0 = np.dot((X0 - self.mu_0).T, (X0 - self.mu_0)) / m0
        self.sigma_1 = np.dot((X1 - self.mu_1).T, (X1 - self.mu_1)) / m1

        # print(f"mu_0: {self.mu_0}")
        # print(f"mu_1: {self.mu_1}")
        # print(f"Sigma_0:\n{self.sigma_0}")
        # print(f"Sigma_1:\n{self.sigma_1}")



    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
    
        p_x_given_0 = multivariate_normal.pdf(X, mean=self.mu_0, cov=self.sigma)
        p_x_given_1 = multivariate_normal.pdf(X, mean=self.mu_1, cov=self.sigma)
        
        p_0_given_x = p_x_given_0 * (1 - self.phi)
        p_1_given_x = p_x_given_1 * self.phi
        
        y_pred = np.where(p_1_given_x &gt; p_0_given_x, 1, 0)
        return y_pred
            
    def plot_gda_decision_boundary1(self, X, y):
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Canada', color='blue')
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Alaska', color='orange')

        sigma_inv = np.linalg.inv(self.sigma)

        w = np.dot(sigma_inv, self.mu_1 - self.mu_0) 
        b = -0.5 * (np.dot(self.mu_1.T, np.dot(sigma_inv, self.mu_1)) - np.dot(self.mu_0.T, np.dot(sigma_inv, self.mu_0)))

        x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match242-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_vals = (-w[0] * x_vals - b) / w[1]  

        plt.plot(x_vals, y_vals, color='green', label='Decision Boundary')

        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.legend()
        plt.title('GDA Decision Boundary with Data Points')
        plt.show()
</FONT>

    def plot_gda_decision_boundary(self, X, y):
        plt.figure(figsize=(10, 8))
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0 (Canada)', color='blue', alpha=0.6)
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1 (Alaska)', color='orange', alpha=0.6)

        zoom_out_factor = 2.5
        # x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        # y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        x_range = (X[:, 0].max() - X[:, 0].min()) * zoom_out_factor
        y_range = (X[:, 1].max() - X[:, 1].min()) * zoom_out_factor

        x_min, x_max = X[:, 0].mean() - x_range / 2, X[:, 0].mean() + x_range / 2
        y_min, y_max = X[:, 1].mean() - y_range / 2, X[:, 1].mean() + y_range / 2       
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),
                            np.linspace(y_min, y_max, 500))

        sigma_inv = np.linalg.inv(self.sigma)
        w_linear = np.dot(sigma_inv, self.mu_1 - self.mu_0) 
        b_linear = -0.5 * (np.dot(self.mu_1.T, np.dot(sigma_inv, self.mu_1)) - np.dot(self.mu_0.T, np.dot(sigma_inv, self.mu_0)))

        Z_linear = w_linear[0] * xx + w_linear[1] * yy + b_linear

        sigma_0_inv = np.linalg.inv(self.sigma_0)
        sigma_1_inv = np.linalg.inv(self.sigma_1)

        def quad_boundary(x):
            diff_0 = x - self.mu_0
            diff_1 = x - self.mu_1
            return (np.dot(diff_0.T, np.dot(sigma_0_inv, diff_0)) -
                    np.dot(diff_1.T, np.dot(sigma_1_inv, diff_1)) -
                    2 * np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0)))

        Z_quadratic = np.array([quad_boundary(np.array([x_, y_])) for x_, y_ in zip(np.ravel(xx), np.ravel(yy))])
        Z_quadratic = Z_quadratic.reshape(xx.shape)

        plt.contour(xx, yy, Z_linear, levels=[0], colors='red', linestyles='solid', linewidths=2,
                    label='Linear Decision Boundary (LDA)')

        plt.contour(xx, yy, Z_quadratic, levels=[0], colors='green', linestyles='dashed', linewidths=2,
                    label='Quadratic Decision Boundary (QDA)')
        
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('GDA: Linear and Quadratic Decision Boundaries')
        plt.legend()
        plt.grid(True)
        plt.show()


def normalize_features(path):
    X = pd.read_csv(path ,header= None ,sep='\\s+').values
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_normalized = (X - mean) / std
    return np.array(X_normalized)


def load_data(input_data_file , output_data_file):
    x = pd.read_csv(input_data_file ,sep='\\s+', header=None)
    y = pd.read_csv(output_data_file ,sep='\\s+', header=None)
    X = np.array(x)
    y = np.array(y)
    Y = np.where(y == 'Alaska', 1, 0)
    Y = Y.reshape((Y.shape[0],))
    return X ,Y

if __name__ == "__main__":
    X, y = load_data("../data/Q4/q4x.dat", "../data/Q4/q4y.dat")
    X_normalized = normalize_features("../data/Q4/q4x.dat")
    # print(X_normalized.shape)
    # print(X.shape)
    # print(y.shape)

    gda = GaussianDiscriminantAnalysis()
    r1 = gda.fit(X_normalized, y, True)
    r2 = gda.fit(X_normalized, y, False)

    gda.plot_gda_decision_boundary(X_normalized, y)

    







</PRE>
</PRE>
</BODY>
</HTML>
