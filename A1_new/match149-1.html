<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_EQGVX.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_YAQMB.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib
# matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.interpolate import griddata


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.tol = None
        self.max_iters = None
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        """
        m = X.shape[0]
        # print("m = " , m , " Type = ", type(m))
        n = 1
        X = np.c_[np.ones(m), X]  # Add intercept term
        # print(X.shape)
        self.theta = np.zeros(n + 1)  # Initialize theta with zeros
        theta_history = []
        cost_history = []

        for _ in range(self.max_iters):
            predictions = X.dot(self.theta)
            errors = predictions - y
            gradient = (1 / m) * X.T.dot(errors)
            new_theta = self.theta - learning_rate * gradient
            
            theta_history.append(new_theta.copy())
            cost_history.append(self.compute_cost(X, y, new_theta).item())
            
            if np.linalg.norm(new_theta - self.theta, ord=2) &lt; self.tol:
                break
            
            self.theta = new_theta
        
        return np.array(theta_history), cost_history
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        if self.theta is None:
            raise ValueError("Model is not trained yet. Call fit() first.")
        
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term
        return X.dot(self.theta)
    
    def compute_cost(self, X, y, theta):
        """
        Compute the cost function for linear regression.
        """
        # m = X.shape[0]
        # print("m = " , m , " Type = ", type(m))
        # n = 1
        # print("I am in compute cost function -&gt; ")

        m = len(y)
        # print(m)

        # print("shape of X -&gt;", x.shape)
        # print("shape of Y -&gt;", y.shape)
        predictions = X.dot(theta)
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost
    
    def plot_data_and_fit(self, X, y):
        """
        Plot the data points and the fitted line.
        """
        X = X.flatten()  # Ensure X is 1D
        plt.scatter(X, y, color='red', marker='x', label='Training Data')
        plt.plot(X, self.predict(X.reshape(-1, 1)), color='blue', label='Linear Regression')
        plt.xlabel('Feature')
        plt.ylabel('Target')
        plt.legend()
        plt.title('Linear Regression Fit')
        plt.savefig("plot1.png")
        plt.show()
    
    def plot_error_value(self, x, y, z):
        # print("X -&gt; ", x)
        # print("Y -&gt; ", y)
        # # z axis contains the error values for different theta
        # print("Z -&gt; ", z)

        # Create a colormap based on z values
        colors = plt.cm.viridis((z - min(z)) / (max(z) - min(z)))  # Normalize z values to [0,1]

        # Create a 3D figure
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        sc = ax.scatter(x, y, z, c=colors, marker='o')

        # Add a color bar
        cbar = plt.colorbar(sc)
        cbar.set_label('Z Value')


        # Labels
        ax.set_xlabel('X Label(theta 0)')
        ax.set_ylabel('Y Label(theta 1)')
        ax.set_zlabel('Z Label(error)')

        # Show the plot
        plt.savefig("plot2.png")
        plt.show()


    def plot_one_by_one(self, x, y, z):
        """
        Real-time update of a single plot with fixed axis limits and different colors for each point.
        """
        plt.ion()
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_zlabel('Cost')
        ax.set_title('Gradient Descent Progress')
        colors = plt.cm.viridis((z - 0) / (max(z) - min(z)))  # Normalize z values to [0,1]

        ax.set_xlim(min(x), max(x))
        ax.set_ylim(min(y), max(y))
        ax.set_zlim(min(z), max(z))

        for i in range(len(z)):
            print(i)
            ax.scatter(x[i], y[i], z[i], color=colors[i], marker='o')
            plt.pause(0.2)
        
        plt.ioff()
        plt.show()




    
    def plot_contour(self, theta_history, cost_history):
        """
        Contour plot of cost function with gradient descent steps.
        """
        theta0_vals, theta1_vals = np.linspace(-1, 1, 100), np.linspace(-1, 1, 100)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
        for i, t0 in enumerate(theta0_vals):
            for j, t1 in enumerate(theta1_vals):
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(theta0_vals)), theta0_vals], theta1_vals, [t0, t1])
        
        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
        plt.contour(T0, T1, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='jet')
        plt.xlabel('Theta 0')
        plt.ylabel('Theta 1')
        plt.title('Contour Plot of Cost Function')
        
        for i, theta in enumerate(theta_history):
            plt.plot(theta[0], theta[1], marker='o', color='red')
            plt.pause(0.2)
        # plt.show()


    
    def compare_learning_rates(self, X, y, rates):
        """
        Compare different learning rates and plot the cost reduction.
        """
        for alpha in rates:
            self.theta = np.zeros(X.ndim + 1)
            _, cost_history = self.fit(X, y, alpha)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match149-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            plt.plot(cost_history, label=f'alpha = {alpha}')
        plt.xlabel('Iterations')
        plt.ylabel('Cost')
        plt.legend()
        plt.title('Cost Reduction Over Iterations')
        plt.show()


x = np.loadtxt("./../data/Q1/linearX.csv")

y = np.loadtxt("./../data/Q1/linearY.csv")


# Question 1
model = LinearRegressor()
model.tol = 1e-6
</FONT>model.max_iters = 1000
theta_history, cost_history = model.fit(x, y)


#Question 2
# print(theta_history)
theta0 = theta_history[:,0]
theta1 = theta_history[:,1]
cost = np.array(cost_history)


# print("theta 0 -&gt; ", theta0)
# print("theta 1 -&gt; ", theta1)
# print("theta 0 size -&gt; ", theta0.shape)
# print("theta 1 size -&gt; ", theta1.shape)
# print("cost shape -&gt; ", cost.shape)


# Print final results
print("\n========== Required Outputs ==========")
print(f"Final Parameters (Î¸): {model.theta}")
print(f"Learning Rate: 0.01")
print(f"Stopping Criteria (Tolerance): 1e-6")
print(f"Final Cost Value: {cost_history[-1]}")
print("======================================\n")

model.plot_data_and_fit(x, y)
model.plot_error_value(theta0, theta1, cost)
model.plot_one_by_one(theta0, theta1, cost)
model.plot_contour(theta_history, cost_history)
model.compare_learning_rates(x, y, [0.001, 0.025, 0.1])







# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_sizes = None
        self.max_epochs = None
        self.tol = 1e-6
        
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model using Stochastic Gradient Descent.
        """
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))  # Add intercept term
        self.results = {}
        
        for r in self.batch_sizes:
            print("batch size -&gt; ", r)
            self.max_epochs = min(r*10, 1000)
            if self.max_epochs&lt;50:
                self.max_epochs = 50
            theta = np.zeros(n + 1)
            prev_loss = float('inf')
            losses = []
            
            for epoch in range(self.max_epochs):
                print("epoch -&gt; ", epoch)
                indices = np.random.permutation(m)
                X_shuffled, y_shuffled = X[indices], y[indices]
                
                for b in range(0, m, r):
                    X_batch = X_shuffled[b:b + r]
                    y_batch = y_shuffled[b:b + r]
                    
                    predictions = X_batch.dot(theta)
                    gradient = -np.dot(X_batch.T, (y_batch - predictions)) / r
                    theta -= learning_rate * gradient
                
                loss = np.mean((y - X.dot(theta))**2) / 2
                losses.append(loss)
                if abs(prev_loss - loss) &lt; self.tol:
                    break
                prev_loss = loss
            
            self.results[r] = (theta, losses)
        
    def predict(self, X):
        """
        Predict target values for input data.
        """
        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term
        return X.dot(self.theta)

# Generate Data
N = 1000000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train-Test Split
train_size = int(0.8 * N)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]

# Train Model
regressor = StochasticLinearRegressor()
regressor.batch_sizes = [1, 80, 8000, 800000]
regressor.max_epochs = 1000
regressor.tol = 1e-6
regressor.fit(X_train, y_train)

# Closed-Form Solution
X_train_b = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
X_test_b = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
closed_form_theta = np.linalg.inv(X_train_b.T @ X_train_b) @ X_train_b.T @ y_train

# Compute Errors
train_errors = {}
test_errors = {}

for r, (theta, _) in regressor.results.items():
    train_pred = X_train_b.dot(theta)
    test_pred = X_test_b.dot(theta)
    train_errors[r] = np.mean((y_train - train_pred) ** 2)
    test_errors[r] = np.mean((y_test - test_pred) ** 2)

# 3D Plot of Theta Movement
# fig = plt.figure(figsize=(12, 8))
# ax = fig.add_subplot(111, projection='3d')
# for r, theta_path in regressor.theta_updates.items():
#     ax.plot(theta_path[:, 0], theta_path[:, 1], theta_path[:, 2], label=f'Batch {r}')
# ax.set_xlabel("Theta 0")
# ax.set_ylabel("Theta 1")
# ax.set_zlabel("Theta 2")
# ax.set_title("Trajectory of Theta Updates in 3D")
# ax.legend()
# plt.savefig("theta_movement.png")
# plt.show()

# Print Results
print("True Theta:", theta_true)
print("Closed-Form Theta:", closed_form_theta)
for r, (theta, _) in regressor.results.items():
    print(f"SGD Theta for batch size {r}:", theta)
    print(f"Train Error: {train_errors[r]}, Test Error: {test_errors[r]}")

# Plot convergence
plt.figure(figsize=(10, 6))
for r, (_, losses) in regressor.results.items():
    plt.plot(losses, label=f'Batch size {r}')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.title('SGD Convergence for Different Batch Sizes')
plt.savefig("plot.png")
plt.show()







import numpy as np
import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mean = None
<A NAME="2"></A><FONT color = #0000FF><A HREF="match149-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.std = None
        self.max_iter = None
        self.epsilon = None
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def compute_gradient_hessian(self, theta, X, y):
</FONT>        h = self.sigmoid(X @ theta)
        gradient = X.T @ (h - y)
        S = np.diag(h * (1 - h))
        Hessian = X.T @ S @ X
        return gradient, Hessian

    def fit(self, X, y, learning_rate = 0.01):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        self.epsilon = 1e-6
        X = (X - self.mean) / self.std
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))
        self.theta = np.zeros(n + 1)
        iter = 0
        for _ in range(self.max_iter):
            # print("iter -&gt; ", iter )
            iter += 1
            grad, H = self.compute_gradient_hessian(self.theta, X, y)
            self.theta -= learning_rate*np.linalg.inv(H) @ grad
            if np.linalg.norm(grad) &lt; self.epsilon:  # Gradient is small
                break
        
        return self.theta

    def predict(self, X):
        X = (X - self.mean) / self.std
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)

# Load data
X = np.loadtxt('./../data/Q3/logisticX.csv', delimiter=',')
y = np.loadtxt('./../data/Q3/logisticY.csv', delimiter=',')

# Train model
model = LogisticRegressor()
model.max_iter = 10000
theta = model.fit(X, y)
print("Optimized Theta:", theta)

# Normalize X for plotting (use stored mean and std from model)
X_norm = (X - model.mean) / model.std

# Plot data
plt.figure(figsize=(8, 6))
plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', label='y=0', color='red')
<A NAME="0"></A><FONT color = #FF0000><A HREF="match149-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', label='y=1', color='blue')

# Decision boundary (in normalized space)
x1_vals = np.linspace(X_norm[:, 0].min(), X_norm[:, 0].max(), 100)
x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]
plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')

plt.xlabel('x1 (normalized)')
plt.ylabel('x2 (normalized)')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
</FONT>plt.savefig("plot.png")
plt.show()




import numpy as np
import matplotlib.pyplot as plt


def quadratic_boundary(x1, x2, cov_0, cov_1):
    inv_cov_0 = np.linalg.inv(cov_0)
    inv_cov_1 = np.linalg.inv(cov_1)
    
    term1 = x1 * (inv_cov_1[0, 0] - inv_cov_0[0, 0]) * x1
    term2 = x2 * (inv_cov_1[1, 1] - inv_cov_0[1, 1]) * x2
    term3 = 2 * x1 * x2 * (inv_cov_1[0, 1] - inv_cov_0[0, 1])
    
    term4 = 2 * (inv_cov_1[0, 0] * mu_1[0] - inv_cov_0[0, 0] * mu_0[0]) * x1
    term5 = 2 * (inv_cov_1[1, 1] * mu_1[1] - inv_cov_0[1, 1] * mu_0[1]) * x2
    term6 = 2 * (inv_cov_1[0, 1] * mu_1[1] - inv_cov_0[0, 1] * mu_0[1]) * x1
    term7 = 2 * (inv_cov_1[1, 0] * mu_1[0] - inv_cov_0[1, 0] * mu_0[0]) * x2
    
    term8 = np.log(np.linalg.det(cov_1) / np.linalg.det(cov_0))
    term9 = mu_0.T @ inv_cov_0 @ mu_0 - mu_1.T @ inv_cov_1 @ mu_1
    
    return term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.phi = None
        self.cov_0 = None
        self.cov_1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        """
        # Normalize features
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std

        # Compute class priors
        self.phi = np.mean(y)  # P(y=1), so P(y=0) = 1 - phi
        
        # Compute class means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)

        if assume_same_covariance:        
            # Compute shared covariance matrix
            self.sigma = np.cov(X, rowvar=False, bias=True)
            
            return self.mu_0, self.mu_1, self.sigma
        else:
            # Compute class-specific covariance matrices
            self.cov_0 = np.cov(X[y == 0], rowvar=False, bias=True)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match149-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.cov_1 = np.cov(X[y == 1], rowvar=False, bias=True)
            return self.mu_0, self.mu_1, self.cov_0, self.cov_1

    def predict(self, X):
        """
        Predict the target values for the input data.
</FONT>        """
        inv_sigma = np.linalg.inv(self.sigma)
        w = inv_sigma @ (self.mu_1 - self.mu_0)
        b = -0.5 * (self.mu_1.T @ inv_sigma @ self.mu_1 - self.mu_0.T @ inv_sigma @ self.mu_0)
        
        linear_scores = X @ w + b
        return (linear_scores &gt;= 0).astype(int)
    
    def plot_data(self,X,y):
        plt.figure()
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Canada", marker="o")
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Alaska", marker="x")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.legend()
        plt.savefig("plot1.png")
        plt.show()
    
    def plot_decision_boundary(self, X, y):
        """
        Plot the data points and decision boundary.
        """
        # Normalize features
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std

        inv_sigma = np.linalg.inv(self.sigma)
        w = inv_sigma @ (self.mu_1 - self.mu_0)
        b = -0.5 * (self.mu_1.T @ inv_sigma @ self.mu_1 - self.mu_0.T @ inv_sigma @ self.mu_0)
        
        plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Alaska", marker='o')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Canada", marker='x')
        
        # Plot linear decision boundary
        x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
        y_vals = (-w[0] * x_vals - b) / w[1]
        plt.plot(x_vals, y_vals, 'blue', label='Linear Decision Boundary')
        
        plt.xlabel("x1 (fresh water growth)")
        plt.ylabel("x2 (marine water growth)")
        plt.legend()
        plt.title("Linear Decision Boundary - GDA with Shared Covariance Matrix")
        plt.savefig("plot2.png")
        plt.show()

    # Decision boundary (quadratic equation)

    
    def plot_quadratic_boundary(self, X, y):
        # Plot data points

        # Normalize features
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std

        
        plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Alaska", marker='o')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Canada", marker='x')

        # Plot decision boundary
        x1, x2 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
        inv_cov_0 = np.linalg.inv(self.cov_0)
        inv_cov_1 = np.linalg.inv(self.cov_1)
        
        term1 = x1 * (inv_cov_1[0, 0] - inv_cov_0[0, 0]) * x1
        term2 = x2 * (inv_cov_1[1, 1] - inv_cov_0[1, 1]) * x2
        term3 = 2 * x1 * x2 * (inv_cov_1[0, 1] - inv_cov_0[0, 1])
        
        term4 = 2 * (inv_cov_1[0, 0] * mu_1[0] - inv_cov_0[0, 0] * mu_0[0]) * x1
        term5 = 2 * (inv_cov_1[1, 1] * mu_1[1] - inv_cov_0[1, 1] * mu_0[1]) * x2
        term6 = 2 * (inv_cov_1[0, 1] * mu_1[1] - inv_cov_0[0, 1] * mu_0[1]) * x1
        term7 = 2 * (inv_cov_1[1, 0] * mu_1[0] - inv_cov_0[1, 0] * mu_0[0]) * x2
        
        term8 = np.log(np.linalg.det(self.cov_1) / np.linalg.det(self.cov_0))
        term9 = mu_0.T @ inv_cov_0 @ mu_0 - mu_1.T @ inv_cov_1 @ mu_1
        
        Z = term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9


        # Z = gda.quadratic_boundary(grid_x1, grid_x2)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match149-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.contour(x1, x2, Z, levels=[0], colors='red')

        plt.xlabel("x1 (fresh water growth)")
        plt.ylabel("x2 (marine water growth)")
        plt.legend()
        plt.title("Quadratic Decision Boundary (GDA)")
</FONT>        plt.savefig("plot3.png")
        plt.show()
# Load data
X = np.loadtxt("./../data/Q4/q4x.dat")  # Feature values (x1, x2)
y = np.loadtxt("./../data/Q4/q4y.dat", dtype=str)  # Labels (Alaska, Canada)

# Convert labels to binary (Alaska -&gt; 0, Canada -&gt; 1)
y = np.where(y == "Alaska", 0, 1)

# Initialize and train model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X, y, True)

# Print parameters
print("Class prior (phi):", gda.phi)
print("Mean (mu_0):", mu_0)
print("Mean (mu_1):", mu_1)
print("Shared Covariance Matrix (Sigma):\n", sigma)

# Report generation
report = f"""
Gaussian Discriminant Analysis (GDA) Report
------------------------------------------
Class Prior:
  P(y=1) = {gda.phi}
  P(y=0) = {1 - gda.phi}

Class Means:
  mu_0 = {mu_0}
  mu_1 = {mu_1}

Shared Covariance Matrix:
  Sigma = 
{sigma}

Linear Decision Boundary Equation:
  Given by w^T x + b = 0, where:
  w = Sigma^(-1) (mu_1 - mu_0)
  b = -0.5 * (mu_1^T Sigma^(-1) mu_1 - mu_0^T Sigma^(-1) mu_0)
"""

print(report)

gda.plot_data(X,y)
# Plot decision boundary
gda.plot_decision_boundary(X, y)

mu_0,mu_1,cov_0, cov_1 = gda.fit(X,y, False)
# Print parameters
print("Class prior (phi):", gda.phi)
print("Mean (mu_0):", mu_0)
print("Mean (mu_1):", mu_1)
print("Covariance matrix (Sigma_0):\n", cov_0)
print("Covariance matrix (Sigma_1):\n", cov_1)


# Report generation
report = f"""
Gaussian Discriminant Analysis (GDA) Report
------------------------------------------
Class Prior:
  P(y=1) = {gda.phi}
  P(y=0) = {1 - gda.phi}

Class Means:
  mu_0 = {mu_0}
  mu_1 = {mu_1}

Class Covariances:
  Sigma_0 = 
{cov_0}
  Sigma_1 = 
{cov_1}

Quadratic Decision Boundary Equation:
  (x - mu_0)^T Sigma_0^(-1) (x - mu_0) - (x - mu_1)^T Sigma_1^(-1) (x - mu_1) = log(|Sigma_1| / |Sigma_0|)
  Expanding this results in a quadratic equation:
  x^T (Sigma_1^(-1) - Sigma_0^(-1)) x + 2 (mu_0^T Sigma_0^(-1) - mu_1^T Sigma_1^(-1)) x + const = 0
"""

print(report)

gda.plot_quadratic_boundary(X, y)

</PRE>
</PRE>
</BODY>
</HTML>
