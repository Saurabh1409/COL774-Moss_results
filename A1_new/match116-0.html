<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_O1KGG.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_O1KGG.py<p><PRE>


# Imports - you can add any other permitted libraries
from matplotlib import pyplot as plt
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        X = np.c_[np.ones(m), X]            # Adding intercept term
        self.theta = np.zeros(n + 1)        # Initializing parameters as 0
        threshold=1e-5
        mse_old = 0
        theta_after_iteration = []

        while True:
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match116-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            hQ = X.dot(self.theta)
            diff = hQ - y
            gradient = (1/m) * X.T.dot(diff)
            self.theta -= learning_rate * gradient
</FONT>            mse_new = (1/(2*m)) * np.sum(diff**2)
            theta_after_iteration.append(self.theta.copy())
            if(abs(mse_new - mse_old) &lt; threshold):
                break

            mse_old = mse_new
        
        return np.array(theta_after_iteration)
            
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.c_[np.ones(m), X]  # Adding intercept term
        return X.dot(self.theta)
    
    def compute_mse(self, X, y,theta):
        """Compute J(θ)"""
        m = len(y)
        hQ = X.dot(theta)
        diff = hQ - y
        mse = (1 / (2 * m)) * np.sum(diff ** 2)
        return mse
        

#reading the data
X = np.loadtxt('../data/Q1/linearX.csv', delimiter=',').reshape(-1, 1)
y = np.loadtxt('../data/Q1/linearY.csv', delimiter=',')

#creating instance of LinearRegressor
lin_reg = LinearRegressor()

#calling fit() to train the data
final_theta = lin_reg.fit(X, y, learning_rate=0.01)

print(final_theta)
print(len(final_theta))
print('Final parameters:',final_theta[-1])

#(Part 2) Plot data and hypothesis function 
plt.scatter(X, y, color='blue', label='Training Data')  # Scatter plot of the data
plt.plot(X, lin_reg.predict(X), color='red', label='Hypothesis Function')  # Hypothesis function
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()


#(Part 3a)
theta0 = final_theta[:, 0]  # Intercept terms
theta1 = final_theta[:, 1]  # Slope terms

# Computing mse values for corresponding thetas
J_vals = [lin_reg.compute_mse(np.c_[np.ones(X.shape[0]), X], y, theta) for theta in final_theta]
# print(J_vals)
# Create a 3D Plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plotting the gradient descent path
ax.plot(theta0, theta1, J_vals, color='red', marker='o', label='Gradient Descent Path')

ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('J(θ)')
ax.set_title('Gradient Descent Path in 3D Space')

plt.legend()
<A NAME="2"></A><FONT color = #0000FF><A HREF="match116-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()

# (Part 3b)
# Creating another 3d plot
fig2 = plt.figure()
f = fig2.add_subplot(111, projection='3d')
f.set_xlabel('Theta 0')
f.set_ylabel('Theta 1')
f.set_zlabel('J(θ)')
f.set_title('Gradient Descent Path in 3D Space')
</FONT>
# Animated Gradient Descent Path
for i in range(len(theta0)):
    f.scatter(theta0[i], theta1[i], J_vals[i], color='red', marker='o')
    plt.pause(0.2)
plt.show()

# (Part 4)  Plot the contour
fig3, g = plt.subplots()
T0, T1 = np.meshgrid(theta0, theta1)
JQ = np.zeros((len(theta0), len(theta1)))
th0 = (theta0,)
th1 = (theta1,)
for i, th0 in enumerate(th0):
    for j, th1 in enumerate(th1):
        temp_theta = np.array([th0[i], th1[j]])
        JQ[i, j] = lin_reg.compute_mse(np.c_[np.ones(X.shape[0]), X], y,temp_theta)

CS = g.contour(T0, T1, JQ.T, cmap='viridis')
g.set_xlabel('Theta 0')
g.set_ylabel('Theta 1')
g.set_title('Contour Plot of Cost Function with Gradient Descent Path')

# Add gradient descent path dynamically
for i in range(len(theta0)):
    # Plot the gradient descent path up to the current iteration
    g.plot(theta0[:i+1], theta1[:i+1], color='red', marker='o', label='Gradient Descent Path' if i == 0 else "")
    
    # Highlight the current point
    g.scatter(theta0[i], theta1[i], color='blue', marker='o', s=50)
    
    # Pause for animation effect
    plt.pause(0.2)

plt.legend()
plt.show()

# (Part 5)
# Different learning rates
learning_rates = [0.001, 0.025, 0.1]
for lr in learning_rates:
    #creating instance of LinearRegressor
    linear_reg = LinearRegressor()
    final_thet = linear_reg.fit(X, y, lr)
    thet0 = final_thet[:, 0]
    thet1 = final_thet[:, 1]
    fig4, g1 = plt.subplots()
    T_0, T_1 = np.meshgrid(thet0, thet1)
    J_Q = np.zeros((len(thet0), len(thet1)))
    the0 = (thet0,)
    the1 = (thet1,)
    for i, the0 in enumerate(the0):
        for j, the1 in enumerate(the1):
            temp_thet = np.array([the0[i], the1[j]])
            J_Q[i, j] = linear_reg.compute_mse(np.c_[np.ones(X.shape[0]), X], y,temp_thet)

    C_S = g1.contour(T_0, T_1, J_Q.T, cmap='viridis')
    g1.set_xlabel('Theta 0')
    g1.set_ylabel('Theta 1')
    g1.set_title(f'Contours and Gradient Descent Path (Learning Rate: {lr})')

    # Add gradient descent path dynamically
    for i in range(len(thet0)):
        # Plot the gradient descent path up to the current iteration
        g1.plot(thet0[:i+1], thet1[:i+1], color='red', marker='o', label='Gradient Descent Path' if i == 0 else "")
        
        # Highlight the current point
        g1.scatter(thet0[i], thet1[i], color='blue', marker='o')
        
        # Pause for animation effect
        plt.pause(0.2)

    plt.legend()
    plt.show()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    # Generate random inputs x1 and x2
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    
    # Create the input matrix X (with a column of ones for the intercept term)
    X = np.vstack((np.ones(N), x1, x2)).T  # Shape (N, 3), where the first column is 1s (intercept)
    
    # Calculate the target values y based on the model y = theta_0 + theta_1 * x1 + theta_2 * x2 + noise
    noise = np.random.normal(0, noise_sigma, N)
    y = X @ theta + noise  # Matrix multiplication to get y
    
    return X[:, 1:], y  # Return X with only the features (excluding intercept column) and y

def closed_form_solution(X, y):
        """
        Compute the closed-form solution for Linear Regression.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data (without the intercept term).
        
        y : numpy array of shape (n_samples,)
            The target values.
        
        Returns
        -------
        theta : numpy array of shape (n_features+1,)
            The parameters computed using the closed-form solution.
        """
        # Add intercept term to X
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        # Compute closed-form solution
        theta = np.linalg.inv(X.T @ X) @ X.T @ y
        return theta

# Function to calculate Mean Squared Error (MSE)
def compute_mse(y_true, y_pred):
    """
    Compute Mean Squared Error (MSE).
    
    Parameters
    ----------
    y_true : numpy array of shape (n_samples,)
        The true target values.
        
    y_pred : numpy array of shape (n_samples,)
        The predicted target values.
    
    Returns
    -------
    mse : float
        The Mean Squared Error.
    """
    mse = np.sum((y_true - y_pred) ** 2) / len(y_true)
    return mse

def plot_theta_movement(theta_histories, batch_sizes):
    """
    Plots the movement of θ in 3D parameter space for varying batch sizes.
    
    Parameters
    ----------
    theta_histories : list of numpy arrays
        A list where each element is an array of θ updates for a specific batch size
        (shape: n_iter x n_features+1).
        
    batch_sizes : list of int
        The batch sizes corresponding to each θ update history.
    """
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    colors = ['r', 'g', 'b', 'y']  # Different colors for each batch size

    for i, (theta_history, batch_size) in enumerate(zip(theta_histories, batch_sizes)):
        theta_0 = theta_history[:, 0]
        theta_1 = theta_history[:, 1]
        theta_2 = theta_history[:, 2]
        
        # Plot the trajectory in 3D space
        ax.plot(theta_0, theta_1, theta_2, color=colors[i], label=f"Batch Size {batch_size}")
        ax.scatter(theta_0[-1], theta_1[-1], theta_2[-1], color=colors[i], s=50, label=f"Convergence (BS {batch_size})")

    ax.set_xlabel("θ0")
    ax.set_ylabel("θ1")
    ax.set_zlabel("θ2")
    ax.set_title("Movement of θ in 3D Parameter Space")
    ax.legend()
    plt.show()

def plot_theta_movement_separate(theta_histories, batch_sizes):
    """
    Plots the movement of θ in 3D parameter space for varying batch sizes, each in a separate plot.
    
    Parameters
    ----------
    theta_histories : list of numpy arrays
        A list where each element is an array of θ updates for a specific batch size
        (shape: n_iter x n_features+1).
        
    batch_sizes : list of int
        The batch sizes corresponding to each θ update history.
    """
    fig = plt.figure(figsize=(15, 15))

    # Loop through each batch size and create a subplot
    for i, (theta_history, batch_size) in enumerate(zip(theta_histories, batch_sizes)):
        ax = fig.add_subplot(2, 2, i+1, projection='3d')
        
        theta_0 = theta_history[:, 0]
        theta_1 = theta_history[:, 1]
        theta_2 = theta_history[:, 2]
        
        # Plot the trajectory in 3D space
        ax.plot(theta_0, theta_1, theta_2, label=f"Batch Size {batch_size}")
        ax.scatter(theta_0[-1], theta_1[-1], theta_2[-1], color='r', s=50, label=f"Convergence (BS {batch_size})")
        
        # Label the axes
        ax.set_xlabel("θ0")
        ax.set_ylabel("θ1")
        ax.set_zlabel("θ2")
        ax.set_title(f"Batch Size {batch_size}")
        ax.legend()
    
    plt.tight_layout()
    plt.show()


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of numpy arrays: list of 4 numpy arrays of shape (n_iter, n_features+1)
            The list of parameter histories for each batch size.
        """
        n_samples, n_features = X.shape
        X = np.hstack((np.ones((n_samples, 1)), X))  # Add intercept term to X

        # Define the batch sizes to use
        batch_sizes = [1, 80, 8000, 800000]
        max_epochs = 1000  # Maximum number of epochs
        moving_average_window = 5  # Window size for moving average
        convergence_criteria = 1e-4  # Convergence threshold for moving average

        theta_histories = []  # To store the history of theta for each batch size

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match116-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for batch_size in batch_sizes:
            self.theta = np.zeros(n_features + 1)  # Reinitialize theta for each batch size
            theta_history = []  # History for current batch size
            losses = []  # To store the moving average of loss

            for epoch in range(max_epochs):
                # Shuffle the data
                indices = np.arange(n_samples)
</FONT>                np.random.shuffle(indices)
                X_shuffled = X[indices]
                y_shuffled = y[indices]

                for batch_start in range(0, n_samples, batch_size):
                    # Select the batch
<A NAME="5"></A><FONT color = #FF0000><A HREF="match116-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    X_batch = X_shuffled[batch_start:batch_start + batch_size]
                    y_batch = y_shuffled[batch_start:batch_start + batch_size]

                    # Compute predictions
                    # y_pred = X_batch @ self.theta
                    y_pred = self.predict(X_batch)

                    # Compute gradient
                    gradient = -(1 / batch_size) * (X_batch.T @ (y_batch - y_pred))
</FONT>
                    # Update theta
                    self.theta -= learning_rate * gradient

                    # Save current theta to history
                    theta_history.append(self.theta.copy())

                # Compute loss over the entire dataset
                loss = np.mean((y - X @ self.theta) ** 2) / 2
                losses.append(loss)

                # Check for convergence using moving average
                if len(losses) &gt;= moving_average_window:
                    moving_avg = np.mean(losses[-moving_average_window:])
                    if len(losses) &gt; moving_average_window:
                        prev_moving_avg = np.mean(losses[-moving_average_window - 1: -1])
                        if abs(moving_avg - prev_moving_avg) &lt; convergence_criteria:
                            break

            theta_histories.append(np.array(theta_history))  # Save history for the current batch size

        return theta_histories
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        # X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term to X
        return X @ self.theta
    
    
# Define parameters for data generation
N = 1000000  # Number of samples
theta = np.array([3, 1, 2])  # parameters
input_mean = np.array([3, -1])  # Mean of x1 and x2
input_sigma = np.array([2, 2])  # Standard deviation of x1 and x2
noise_sigma = np.sqrt(2)  # Standard deviation of the noise

# Generate the dataset
X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

# Now X and y contain the generated features and target values
print(f"Generated X shape: {X.shape}")
print(f"Generated y shape: {y.shape}")
# Split the data into training and test sets 
train_size = int(0.8 * N)  # 80% for training
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Print the shapes of the generated data and the split sets
print(f"Generated X shape: {X.shape}")
print(f"Generated y shape: {y.shape}")
print(f"Training data X shape: {X_train.shape}, y shape: {y_train.shape}")
print(f"Test data X shape: {X_test.shape}, y shape: {y_test.shape}")

#(Part 2)
# Initialize the regressor
reg = StochasticLinearRegressor()

# Run fit and get parameter histories
final_thetas = reg.fit(X_train, y_train, learning_rate=0.001)

# Print the final learned θ for each batch size
batch_sizes = [1, 80, 8000, 800000]
for i, batch_size in enumerate(batch_sizes):
    print(f"\nFinal learned θ for batch size {batch_size}: {final_thetas[i][-1]}")
    print(f"Number of iterations: {len(final_thetas[i])}")


#(Part 3) Closed-form solution
theta_closed_form = closed_form_solution(X_train, y_train)

# Print results
print("True Parameters: ", theta)
print("Closed-Form Parameters: ", theta_closed_form)
theta_sgd_final = [history[-1] for history in final_thetas]  # Final parameters for each batch size
for i, batch_size in enumerate([1, 80, 8000, 800000]):
    print(f"SGD Parameters (Batch Size {batch_size}): ", theta_sgd_final[i])

# (Part 4) Compute Training and Test Errors
train_errors = []
test_errors = []

for theta in theta_sgd_final:
    # Add intercept term to X_train and X_test
<A NAME="0"></A><FONT color = #FF0000><A HREF="match116-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X_train_with_intercept = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
    X_test_with_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
    
    # Predict on training and test sets
    y_train_pred = X_train_with_intercept @ theta
</FONT>    y_test_pred = X_test_with_intercept @ theta
    
    # Compute MSE
    train_mse = compute_mse(y_train, y_train_pred)
    test_mse = compute_mse(y_test, y_test_pred)
    
    train_errors.append(train_mse)
    test_errors.append(test_mse)

# Print Errors
for i, batch_size in enumerate([1, 80, 8000, 800000]):
    print(f"Batch Size {batch_size}:")
    print(f"  Training Error (MSE): {train_errors[i]:.6f}")
    print(f"  Test Error (MSE): {test_errors[i]:.6f}\n")

#(Part 5) 
# plot_theta_movement(final_thetas, batch_sizes)
plot_theta_movement_separate(final_thetas, batch_sizes)




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None

    def _sigmoid(self, z):
        """Compute the sigmoid function."""
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Normalize the features
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Add intercept term
        X = np.hstack([np.ones((X.shape[0], 1)), X])

        n_samples, n_features = X.shape
        threshold = 1e-1

        # Initialize parameters to zeros
        self.theta = np.zeros(n_features)
        parameters_list = []

        for iteration in range(100):
            # Compute predictions
<A NAME="1"></A><FONT color = #00FF00><A HREF="match116-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            h_theta = self._sigmoid(X @ self.theta)

            # Compute the gradient
            gradient = X.T @ (h_theta - y)

            # Compute the Hessian matrix
            S = np.diag(h_theta * (1 - h_theta))  # Diagonal matrix of weights
            Hessian = X.T @ S @ X
</FONT>
            # Update parameters using Newton's method
            delta_theta = np.linalg.inv(Hessian) @ gradient
            self.theta -= delta_theta

            # Store the parameters for each iteration
            parameters_list.append(self.theta.copy())

            # Check for convergence
            if np.linalg.norm(delta_theta, ord=1) &lt; threshold:
                break

        return np.array(parameters_list)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize the features
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Add intercept term
        X = np.hstack([np.ones((X.shape[0], 1)), X])

        # Compute probabilities
        probabilities = self._sigmoid(X @ self.theta)

        # Convert probabilities to binary predictions
        return (probabilities &gt;= 0.5).astype(int)

X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

# Fit the model
model = LogisticRegressor()
parameter_history = model.fit(X, y)

# Output the learned parameters
print("Learned coefficients (theta):", model.theta)

# (Part 2)
# Plot the training data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='o', label='Label 0')
<A NAME="6"></A><FONT color = #00FF00><A HREF="match116-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='x', label='Label 1')

# Plot the decision boundary
x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
x2_vals = -(model.theta[0] + model.theta[1] * x1_vals) / model.theta[2]
</FONT>plt.plot(x1_vals, x2_vals, color='green', label='Decision Boundary')
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        # Separating the data by class
        X0 = X[y == 0]  # Class 0 (Alaska)
        X1 = X[y == 1]  # Class 1 (Canada)

        # Calculate the means for each class
        mu_0 = np.mean(X0, axis=0)
        mu_1 = np.mean(X1, axis=0)

        if assume_same_covariance:
            # Calculate the shared covariance matrix
            #weighted covariance matrix of class 0 (Alaska) samples
            sigma = np.cov(X0, rowvar=False) * len(X0)  

            # Add the weighted covariance matrix of class 1 to weighted covariance matrix of class 0
            sigma += np.cov(X1, rowvar=False) * len(X1) 
            sigma /= len(X)  # Normalize by total samples
            return mu_0, mu_1, sigma
        else:
            # Calculate separate covariance matrices for each class
            sigma_0 = np.cov(X0, rowvar=False) #for class 0
            sigma_1 = np.cov(X1, rowvar=False) #for class 1
            return mu_0, mu_1, sigma_0, sigma_1

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        pass

# Calculate the quadratic decision boundary
def quadratic_boundary(x1, x2, mu_0, mu_1, sigma_0, sigma_1):
        sigma_0_inv = np.linalg.inv(sigma_0)
        sigma_1_inv = np.linalg.inv(sigma_1)

        term1 = -0.5 * ((x1 - mu_0) @ sigma_0_inv @ (x1 - mu_0).T)
        term2 = 0.5 * ((x1 - mu_1) @ sigma_1_inv @ (x1 - mu_1).T)
        return term1 + term2

# Load data
X = np.loadtxt("../data/Q4/q4x.dat")  
y_raw = np.loadtxt("../data/Q4/q4y.dat", dtype=str)  

# Map labels to binary values
y = np.where(y_raw == "Alaska", 0, 1)
# Normalize the data
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Instantiate the model
gda = GaussianDiscriminantAnalysis()

# Fit the model assuming same covariance
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)

# printing the parameters
print("Mean for class 0 (Alaska):", mu_0)
print("Mean for class 1 (Canada):", mu_1)
print("Shared Covariance Matrix:\n", sigma)

# (Part 2)
# Plot the training data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label='Canada')

# Calculate the decision boundary
sigma_inv = np.linalg.inv(sigma)
diff_mu = mu_1 - mu_0
intercept = -0.5 * (mu_0.T @ sigma_inv @ mu_0 - mu_1.T @ sigma_inv @ mu_1)
slope = sigma_inv @ diff_mu

# Plot the decision boundary
x_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
y_vals = -(slope[0] * x_vals + intercept) / slope[1]
plt.plot(x_vals, y_vals, color='green', label='Decision Boundary')

plt.title('Training Data with decision boundary')
plt.xlabel('Normalized Growth Ring Diameter - Fresh Water')
plt.ylabel('Normalized Growth Ring Diameter - Marine Water')
plt.legend()
plt.grid(True)
plt.show()

# (Part 4) Fit the model without assuming same covariance
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)

print("Mean for class 0 (Alaska):", mu_0)
print("Mean for class 1 (Canada):", mu_1)
print("Covariance Matrix for class 0 (Alaska):\n", sigma_0)
print("Covariance Matrix for class 1 (Canada):\n", sigma_1)

# Plot the training data with class-specific covariance
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label='Canada')

x1_vals, x2_vals = np.meshgrid(
np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
np.linspace(X[:, 1].min(), X[:, 1].max(), 100),)
boundary_vals = np.zeros_like(x1_vals)

for i in range(x1_vals.shape[0]):
        for j in range(x1_vals.shape[1]):
            x_point = np.array([x1_vals[i, j], x2_vals[i, j]])
            boundary_vals[i, j] = quadratic_boundary(x_point, x_point, mu_0, mu_1, sigma_0, sigma_1)

plt.contour(x1_vals, x2_vals, boundary_vals, levels=[0], colors='black', label='Quadratic Boundary')
plt.plot(x_vals, y_vals, color='green')

plt.title('Training Data with Class-Specific Covariance and Quadratic Decision Boundary')
plt.xlabel('Normalized Growth Ring Diameter - Fresh Water')
plt.ylabel('Normalized Growth Ring Diameter - Marine Water')
plt.legend()
plt.grid(True)
plt.show()


</PRE>
</PRE>
</BODY>
</HTML>
