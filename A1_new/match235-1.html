<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_6ATXW.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_83PSN.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        pass


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features+1)

        X = np.c_[np.ones(n_samples), X]

        self.parameters = []
        self.parameters.append(self.theta.copy())

        prev_cost = float('inf')
        finished = False

        tolerance = 1e-9

        while(finished == False):
            
            y_pred = X.dot(self.theta)
            cost = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)
            gradient = (1 / n_samples) * X.T.dot(y_pred - y)
            self.theta -= learning_rate * gradient
            self.parameters.append(self.theta.copy())
            
            if abs(prev_cost - cost) &lt; tolerance:
                finished = True
            
            prev_cost = cost
            
            

        return self.parameters
    

    # Only purpose of this function is to take tolerance as a parameter for the purpose of hyperparameter tuning
    def test_fit(self, X, y, learning_rate, tolerance):
        
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features+1)

        X = np.c_[np.ones(n_samples), X]

        self.parameters = []
        self.parameters.append(self.theta.copy())

       
        prev_cost = float('inf')
        finished = False

        while(finished == False):
            
            y_pred = X.dot(self.theta)
            
            cost = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)
            gradient = (1 / n_samples) * X.T.dot(y_pred - y)
            self.theta -= learning_rate * gradient
            self.parameters.append(self.theta.copy())
            
            if abs(prev_cost - cost) &lt; tolerance:
                finished = True
            
            prev_cost = cost
            
        return self.parameters
    

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        n_samples = X.shape[0]

        X = np.c_[np.ones(n_samples), X]

        y_pred = X.dot(self.parameters[-1])

        return y_pred
    



#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_line_magic('pip', 'install pandas')
get_ipython().run_line_magic('pip', 'install numpy')
get_ipython().run_line_magic('pip', 'install matplotlib')


# In[2]:


import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import pandas as pd
from itertools import product
import time
from mpl_toolkits.mplot3d import Axes3D
from IPython.display import display, clear_output


# ### Part 2: Plotting data and hypothesis function
# 

# In[3]:


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

X = X.reshape(-1, 1) 

model = LinearRegressor()
parameters = model.fit(X, y, learning_rate = 0.1)


plt.scatter(X, y, color='blue', label='Data Points', s=10)
        
X_with_intercept = np.c_[np.ones(X.shape[0]), X]
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match235-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_pred = X_with_intercept @ parameters[-1]

plt.plot(X, y_pred, color='red', label='Regression Line')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.legend()
plt.title('Linear Regression Fit')
plt.show()


# ## Part 1: Hyperparameter Tuning

# In[4]:


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
</FONT>y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

X = X.reshape(-1, 1) 

learning_rates = [0.0001, 0.001, 0.01, 0.1]
tolerances = [1e-3, 1e-6, 1e-9]

n_samples = len(y)

def compute_cost(X, y, theta):
    X_bias = np.c_[np.ones(n_samples), X]
    predictions = X_bias.dot(theta)
    cost = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2)
    return cost

results = []
plt.figure(figsize=(10, 6))

for lr, tol in product(learning_rates, tolerances):
    model = LinearRegressor()
    parameters = model.test_fit(X, y, learning_rate=lr, tolerance=tol)
    cost_history = [compute_cost(X, y, theta) for theta in parameters]
    # Plot the first 4000 iterations
    if(len(cost_history) &gt; 1000):
        plt.plot(range(1000), cost_history[:1000], label=f"LR = {lr}, Tol = {tol}")
    else:
        plt.plot(range(len(cost_history)), cost_history, label=f"LR = {lr}, Tol = {tol}")
    results.append({
        "Learning Rate": lr,
        "Tolerance": tol,
        "Final Theta": parameters[-1],  
        "Iterations": len(cost_history),
        "Final Cost": cost_history[-1] 
    })


plt.xlabel("Iterations")
plt.ylabel("Cost (J(θ))")
plt.title("Gradient Descent Convergence for Different Learning Rates & Tolerances")
plt.legend()
plt.grid(True)
plt.show()

# Display results as a DataFrame
df_results = pd.DataFrame(results)
print(df_results)

# Choose the best combination based on lowest final cost
best_params = df_results.sort_values(by="Final Cost").iloc[0]
print(f"Best Learning Rate: {best_params['Learning Rate']}, Best Tolerance: {best_params['Tolerance']}")


# ### Final Parameters

# In[5]:


# Report the final learning rate, stopping criterion, and set of parameters
print(f"Final Learning Rate: {best_params['Learning Rate']}")
print(f"Stopping Criterion: {best_params['Tolerance']}")
print(f"Final Parameters: {best_params['Final Theta']}")


# ## Part 3: Error Function Plot

# In[6]:


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

X = X.reshape(-1, 1)
n_samples = len(y)

theta0_vals = np.linspace(0, 15, 50)  
theta1_vals = np.linspace(0, 35, 50)  

J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))


for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        theta = np.array([theta0, theta1])
        J_vals[i, j] = compute_cost(X, y, theta)

# Train model and get gradient descent updates
model = LinearRegressor()
parameters = model.fit(X, y, learning_rate=0.1)

theta_updates = np.array(parameters)


fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Create meshgrid for 3D surface
theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

# Plot surface with color gradient
ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap="coolwarm", alpha=0.7)

ax.set_xlabel("θ0")
ax.set_ylabel("θ1")
ax.set_zlabel("Cost J(θ)")
ax.set_title("3D Mesh of Cost Function with Gradient Descent Path")

red_dot = None

for i in range(len(theta_updates)):
    theta0, theta1 = theta_updates[i]
    J_theta = compute_cost(X, y, [theta0, theta1])
    
    # Plot the updated dot
    red_dot = ax.scatter(theta0, theta1, J_theta, color='red', s=20)

    # Clear previous output and re-display the updated plot
    clear_output(wait=True)
    display(fig)

    time.sleep(0.2)

clear_output(wait=True)


# ## Part 4: Error function contours

# In[7]:


theta0_vals = np.linspace(0, 15, 50)  
<A NAME="1"></A><FONT color = #00FF00><A HREF="match235-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta1_vals = np.linspace(0, 35, 50) 

theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

J_vals = np.zeros_like(theta0_grid)

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        theta = np.array([theta0_grid[j, i], theta1_grid[j, i]])
</FONT>        J_vals[j, i] = compute_cost(X, y, theta)

# Train model and get gradient descent updates
model = LinearRegressor()
parameters = model.fit(X, y, learning_rate=0.1)

theta_updates = np.array(parameters)

fig, ax = plt.subplots(figsize=(8, 6))

contour = ax.contourf(theta0_grid, theta1_grid, J_vals, cmap="coolwarm", levels=30)
plt.colorbar(contour)

ax.set_xlabel("θ0")
ax.set_ylabel("θ1")
ax.set_title("Contour Plot of Cost Function with Gradient Descent Path")

red_dot = None

for i in range(len(theta_updates)):
    theta0, theta1 = theta_updates[i]
    
    red_dot = ax.scatter(theta0, theta1, color='red', s=40, edgecolors='black')

    clear_output(wait=True)
    display(fig)


clear_output(wait=True)


# ## Part 5: Contours with varying learning rates

# In[12]:


theta0_vals = np.linspace(0, 15, 50)  
<A NAME="2"></A><FONT color = #0000FF><A HREF="match235-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta1_vals = np.linspace(0, 35, 50)  

theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
J_vals = np.zeros_like(theta0_grid)

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        theta = np.array([theta0_grid[j, i], theta1_grid[j, i]])
</FONT>        J_vals[j, i] = compute_cost(X, y, theta)

learning_rates = [0.001, 0.025, 0.1]

# Create separate figures for each learning rate
figs, axes = [], []
for eta in learning_rates:
    fig, ax = plt.subplots(figsize=(8, 6))
    figs.append(fig)
    axes.append(ax)

# Iterate over learning rates and plot in separate figures
for fig, ax, eta in zip(figs, axes, learning_rates):
    model = LinearRegressor()
    parameters = model.fit(X, y, learning_rate=eta)

    theta_updates = np.array(parameters)

    contour = ax.contourf(theta0_grid, theta1_grid, J_vals, cmap="coolwarm", levels=30)
    fig.colorbar(contour)

    ax.set_xlabel("θ0")
    ax.set_ylabel("θ1")
    ax.set_title(f"Gradient Descent Path (η = {eta})")

    ax.plot(theta_updates[:, 0], theta_updates[:, 1], 'ro-', markersize=3, linewidth=1, alpha=0.6)
    display(fig)        
    

clear_output(wait=True)





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor


# In[3]:


X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

X = X.reshape(-1, 1)  # Ensure X is 2D

model = LinearRegressor()
model.fit(X, y)
model.plot_regression(X, y)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_line_magic('pip', 'install pandas')
get_ipython().run_line_magic('pip', 'install numpy')
get_ipython().run_line_magic('pip', 'install matplotlib')


# In[2]:


from sampling_sgd import *
import numpy as np
import matplotlib.pyplot as plt


# ## Part 1: Sampling data points

# In[3]:


np.random.seed(42)

#parameters
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)

#generate data
n = 1000000
x1 = np.random.normal(input_mean[0], input_sigma[0], n)
x2 = np.random.normal(input_mean[1], input_sigma[1], n)
X = np.column_stack((x1, x2))
noise = np.random.normal(0, noise_sigma, n)
y = theta_true[0] + theta_true[1]*x1 + theta_true[2]*x2 + noise

train_size = int(0.8*n)
X_train = X[:train_size]
y_train = y[:train_size]
X_test = X[train_size:]
y_test = y[train_size:]

print(f"Generated {train_size} training samples and {n - train_size} test samples.")


# ## Part 2: Training Model on varying batch sizes

# In[4]:


model = StochasticLinearRegressor()

all_parameters = model.fit(X_train, y_train, learning_rate = 0.001)
batch_sizes = [1, 80, 8000, 800000]

for i in range(len(batch_sizes)):
    final_theta = all_parameters[i][-1]
    print(f"Final theta for batch size {batch_sizes[i]}: {final_theta}")


# In[5]:


# Print first and last cost for each batch size using the last theta
n_samples = len(y_train)

def compute_cost(X, y, theta):
    X_bias = np.c_[np.ones(n_samples), X]
    predictions = X_bias.dot(theta)
    cost = (1 / (2 * n_samples)) * np.sum((predictions - y) ** 2)
    return cost

for i in range(len(batch_sizes)):
    theta = all_parameters[i][-1]
    cost_first = compute_cost(X_train, y_train, all_parameters[i][0])
    cost_last = compute_cost(X_train, y_train, theta)
    print(f"Cost for batch size {batch_sizes[i]}: {cost_first} -&gt; {cost_last}")


# ## Part 3: Closed-form comparison

# ### 3b: Closed form calculation

# In[6]:


X = np.c_[np.ones(n), X]

theta_closed_form = np.linalg.inv(X.T @ X) @ X.T @ y
print(len(theta_closed_form))
print("Closed-Form Theta:", theta_closed_form)

sgd_theta = [params[-1] for params in all_parameters]

batch_sizes = [1, 80, 8000, 800000]
for i, batch_size in enumerate(batch_sizes):
    print(f"SGD Theta (Batch Size {batch_size}):", sgd_theta[i])

plt.figure(figsize=(8, 5))
batch_labels = [str(bs) for bs in batch_sizes]
th0 = [theta[0] for theta in sgd_theta]
th1 = [theta[1] for theta in sgd_theta]
th2 = [theta[2] for theta in sgd_theta]

plt.plot(batch_labels, th0, 'o-', label='SGD θ0')
plt.plot(batch_labels, th1, 's-', label='SGD θ1')
plt.plot(batch_labels, th2, 'd-', label='SGD θ2')
plt.axhline(y=theta_closed_form[0], color='r', linestyle='--', label='Closed-Form θ0')
plt.axhline(y=theta_closed_form[1], color='g', linestyle='--', label='Closed-Form θ1')
plt.axhline(y=theta_closed_form[2], color='b', linestyle='--', label='Closed-Form θ2')

plt.xlabel('Batch Size')
plt.ylabel('Theta Values')
plt.title('SGD vs. Closed-Form Solution')
plt.legend()
plt.grid(True)
plt.show()


# ### 3c: True Parameter Comparison

# In[7]:


# Compare true parameters, to parameters learned in closed form, to parameters learned by SGD
theta_true_round = np.round(theta_true, 3)
theta_closed_form_round = np.round(theta_closed_form, 3)
sgd_theta_round = np.round(sgd_theta, 3)

print("True Theta:", theta_true_round) 
print("Closed-Form Theta:", theta_closed_form_round)
print("SGD Theta (Batch Size 1):", sgd_theta_round[0])
print("SGD Theta (Batch Size 80):", sgd_theta_round[1])
print("SGD Theta (Batch Size 8000):", sgd_theta_round[2])
print("SGD Theta (Batch Size 800000):", sgd_theta_round[3])


# ## Part 4: MSE Calculations

# In[8]:


def mean_squared_error(y_true, y_pred):
    """Compute Mean Squared Error (MSE)."""
    return np.mean((y_true - y_pred) ** 2)

# Get predictions for each batch size
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Calculate and print training and test error for each batch size
batch_sizes = [1, 80, 8000, 800000]
for i, batch_size in enumerate(batch_sizes):
    train_mse = mean_squared_error(y_train, train_predictions[i])
    test_mse = mean_squared_error(y_test, test_predictions[i])

    print(f"Batch Size {batch_size}:")
    print(f"  Training Error (MSE): {train_mse}")
    print(f"  Test Error (MSE): {test_mse}\n")


# ## Part 5: Plotting parameter iterations

# In[11]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
get_ipython().run_line_magic('matplotlib', 'inline')

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

batch_sizes = [1, 80, 8000, 800000]
colors = ['r', 'g', 'b', 'purple']

for i, batch_size in enumerate(batch_sizes):
    params = np.array(all_parameters[i])
    theta0 = params[:, 0]
    theta1 = params[:, 1]
    theta2 = params[:, 2]

    ax.plot(theta0, theta1, theta2, label=f'Batch size {batch_size}', color=colors[i])
    ax.scatter(theta0[-1], theta1[-1], theta2[-1], color=colors[i], marker='x', s=100, label=f'Final θ (Batch {batch_size})')

ax.set_xlabel('θ₀')
ax.set_ylabel('θ₁')
ax.set_zlabel('θ₂')
ax.set_title('3D Movement of θ During SGD for Different Batch Sizes')
ax.legend()

plt.show()


# ### Plotting paths on 4 different graphs

# In[12]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
get_ipython().run_line_magic('matplotlib', 'inline')

batch_sizes = [1, 80, 8000, 800000]
colors = ['r', 'g', 'b', 'purple']

for i, batch_size in enumerate(batch_sizes):
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    
    params = np.array(all_parameters[i])
    theta0 = params[:, 0]
    theta1 = params[:, 1]
    theta2 = params[:, 2]

    ax.plot(theta0, theta1, theta2, label=f'Batch size {batch_size}', color=colors[i])
    ax.scatter(theta0[-1], theta1[-1], theta2[-1], color=colors[i], marker='x', s=100, label='Final θ')

    ax.set_xlabel('θ₀')
    ax.set_ylabel('θ₁')
    ax.set_zlabel('θ₂')
    ax.set_title(f'3D Movement of θ for Batch Size {batch_size}')
    ax.legend()

    plt.show()





# Imports - you can add any other permitted libraries
import numpy as np
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))

    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise

    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        
        X = np.c_[np.ones(n_samples), X]

        batch_sizes = [1, 80, 8000, 800000]
        self.all_parameters = []
        
        for batch_size in batch_sizes:
            start_time = time.time()
            if batch_size == 1:
                tolerance = 1e-4
            else:
                tolerance = 1e-6
            epoch_count = 0
            parameters = []
            
            self.theta = np.zeros(n_features+1)
            parameters.append(self.theta.copy())

            prev_avg = float('inf')
            finished = False
            batches_per_epoch = n_samples // batch_size

            while(finished == False):
                random_indices = np.random.permutation(n_samples)
                X_shuffled = X[random_indices]
                y_shuffled = y[random_indices]
                epoch_sum = 0

                for i in range(0, n_samples, batch_size):
                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]

                    y_pred = X_batch.dot(self.theta)
                    gradient = (1 / batch_size) * X_batch.T.dot(y_pred - y_batch)
                    self.theta -= learning_rate * gradient

                    batch_cost = np.mean((y_pred - y_batch) ** 2) / 2

                    epoch_sum += batch_cost
                    parameters.append(self.theta.copy())
                    
                
                # Check convergence condition after epoch finishes
                epoch_count += 1
                avg_loss = epoch_sum / batches_per_epoch
                if abs(avg_loss - prev_avg) &lt; tolerance:
                    finished = True
                    break
                prev_avg = avg_loss

            end_time = time.time()
            elapsed_time = end_time - start_time
            print(f"Batch Size: {batch_size} completed with {epoch_count} epochs, in {elapsed_time} seconds")

            # print how much time it took to converge


            self.all_parameters.append(parameters)

        return self.all_parameters
    
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        X = np.c_[np.ones(X.shape[0]), X]

        predictions = [X.dot(params[-1]) for params in self.all_parameters]

        return np.array(predictions)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass

    def normalize(self, X, training=True):
        if training:
            self.mean = np.mean(X, axis=0)
            self.std = np.std(X, axis=0)
        X = (X - self.mean) / self.std
        X = np.c_[np.ones(X.shape[0]), X]  
        return X

    def getMean(self):
        return self.mean
    
    def getSd(self):
        return self.std
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def hessian(self, X, theta):
        n_samples = X.shape[0]
        h = self.sigmoid(X.dot(theta))
        D = np.diag(h * (1 - h))
        hessian = X.T.dot(D).dot(X)*(1/n_samples)
        return hessian
    
    def gradient(self, X, y, theta):
        n_samples = X.shape[0]
        h = self.sigmoid(X.dot(theta))
        gradient = (1 / n_samples) * X.T.dot(h - y)
        return gradient
    
    def log_likelihood(self, X, y, theta):
        h = self.sigmoid(X.dot(theta))
        return -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        X = self.normalize(X, training=True) 
        n_samples, n_features = X.shape 
        self.theta = np.zeros(n_features)
        self.parameters = []
        self.parameters.append(self.theta.copy())
        prev_log_likelihood = self.log_likelihood(X, y, self.theta)
        finished = False
        tolerance = 1e-6
        while(finished == False):
            hessian = self.hessian(X, self.theta)
            gradient = self.gradient(X, y, self.theta)
            update = np.linalg.inv(hessian + 1e-9 * np.eye(hessian.shape[0])) @ gradient
            self.theta -= update
            log_likelihood = self.log_likelihood(X, y, self.theta)
            self.parameters.append(self.theta.copy())
            if abs(prev_log_likelihood - log_likelihood) &lt; tolerance:
                finished = True
                break
            prev_log_likelihood = log_likelihood
        return self.parameters

    # Only purpose of this function is to take tolerance as a parameter for the purpose of hyperparameter tuning
    def test_fit(self, X, y, tolerance, learning_rate=0.01):
        
        X = self.normalize(X, training=True) 
        n_samples, n_features = X.shape 
        self.theta = np.zeros(n_features)
        self.parameters = []
        self.parameters.append(self.theta.copy())
        prev_log_likelihood = self.log_likelihood(X, y, self.theta)
        finished = False
        while(finished == False):
            hessian = self.hessian(X, self.theta)
            gradient = self.gradient(X, y, self.theta)
            update = np.linalg.inv(hessian + 1e-9 * np.eye(hessian.shape[0])) @ gradient
            self.theta -= update
            log_likelihood = self.log_likelihood(X, y, self.theta)
            self.parameters.append(self.theta.copy())
            if abs(prev_log_likelihood - log_likelihood) &lt; tolerance:
                finished = True
                break
            prev_log_likelihood = log_likelihood
        return self.parameters
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        X = self.normalize(X, training=False)
        z = X.dot(self.parameters[-1])
        y_pred_prob = self.sigmoid(z)
        y_pred = np.where(y_pred_prob &gt;= 0.5, 1, 0)
         

        return y_pred



#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_line_magic('pip', 'install pandas')
get_ipython().run_line_magic('pip', 'install numpy')
get_ipython().run_line_magic('pip', 'install matplotlib')


# In[2]:


from logistic_regression import LogisticRegressor
import numpy as np
import matplotlib.pyplot as plt


# ## Loading Data and Creating Test-Train splits

# In[3]:


X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

n = X.shape[0]

# set seed
np.random.seed(42)

random_indices = np.random.permutation(n)
X_shuffled = X[random_indices]
y_shuffled = y[random_indices]

train_size = int(0.8*n)
X_train = X_shuffled[:train_size]
y_train = y_shuffled[:train_size]
X_test = X_shuffled[train_size:]
y_test = y_shuffled[train_size:]


# ## Part 1: Training Model

# In[4]:


model = LogisticRegressor()
parameters = model.fit(X_train, y_train, learning_rate=0.01)
print(f"Final parameters: {parameters[-1]}")


# ### Tolerance Testing

# In[5]:


## train and test the model for varying tolerances

tolerances = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-15]

for tolerance in tolerances:
    model = LogisticRegressor()
    parameter_history = model.test_fit(X_train, y_train, tolerance=tolerance, learning_rate=0.001)
    print(f"Converged after {len(parameter_history)} iterations for tolerance {tolerance}")
    ### Print the last set of parameters
    print(f"Parameters: {parameter_history[-1]}")
    y_pred = model.predict(X_test)
    accuracy = np.mean(y_pred == y_test)
    train_accuracy = np.mean(model.predict(X_train) == y_train)
    print(f"Train accuracy for tolerance {tolerance}: {train_accuracy}")
    print(f"Test accuracy for tolerance {tolerance}: {accuracy}")
    print("\n")


# ## Part 2: Plotting Training and Testing Data

# ### 2.1 Training Accuracy and Plot

# In[6]:


## Plot training data and decision boundary 
## Use a different symbol for each point based on the label
mean = model.getMean()
std = model.getSd()
X_train_norm = (X_train - mean) / std

plt.scatter(X_train_norm[y_train == 0][:, 0], X_train_norm[y_train == 0][:, 1], c='r', marker='x', label='Class 0')
plt.scatter(X_train_norm[y_train == 1][:, 0], X_train_norm[y_train == 1][:, 1], c='b', marker='o', label='Class 1')


theta = parameters[-1]

# Plot decision boundary
x_values = np.linspace(X_train_norm[:, 0].min(), X_train_norm[:, 0].max(), 100)
y_values = -(theta[0] + theta[1] * x_values) / theta[2]
plt.plot(x_values, y_values, 'k-', label='Decision Boundary')

# Add labels and legend
plt.xlabel('x1 (Normalized)')
plt.ylabel('x2 (Normalized)')
plt.legend()
plt.grid(True)
plt.title('Logistic Regression Decision Boundary (Normalized)')
plt.show()


# In[7]:


y_pred = model.predict(X_train)
train_accuracy = np.mean(y_pred == y_train)
print(f"Train accuracy: {train_accuracy}")


# ## 2.2 Testing Accuracy and Plot

# In[8]:


theta = parameters[-1]
mean = model.getMean()
std = model.getSd()

X_test_norm = (X_test - mean) / std

# Plot original data
plt.scatter(X_test_norm[y_test == 0][:, 0], X_test_norm[y_test == 0][:, 1], c='r', marker='x', label='Class 0')
<A NAME="0"></A><FONT color = #FF0000><A HREF="match235-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X_test_norm[y_test == 1][:, 0], X_test_norm[y_test == 1][:, 1], c='b', marker='o', label='Class 1')

# Plot decision boundary
x_values = np.linspace(X_test_norm[:, 0].min(), X_test_norm[:, 0].max(), 100)
y_values = -(theta[0] + theta[1] * x_values) / theta[2]
</FONT>plt.plot(x_values, y_values, 'k-', label='Decision Boundary')

plt.xlabel('x1 (Original Scale)')
plt.ylabel('x2 (Original Scale)')
plt.legend()
plt.grid(True)
plt.title('Logistic Regression Decision Boundary (Original Scale)')
plt.show()


# In[9]:


y_pred = model.predict(X_test)
test_accuracy = np.mean(y_pred == y_test)
print(f"Test accuracy: {test_accuracy}")





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass


    def normalize(self, X, training=True):
        if training:
            self.mean = np.mean(X, axis=0)
            self.std = np.std(X, axis=0)
        X = (X - self.mean) / self.std 
        return X
    
    def getMean(self):
        return self.mean
    
    def getSd(self):
        return self.std
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match235-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.assume_same = assume_same_covariance
        self.phi = np.mean(y)
</FONT>        X = self.normalize(X, training=True)
        X_0 = X[y == 0]
        X_1 = X[y == 1]

        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        if assume_same_covariance:
            self.sigma = ((X_0 - self.mu_0).T.dot(X_0 - self.mu_0) + (X_1 - self.mu_1).T.dot(X_1 - self.mu_1)) / X.shape[0]
            return self.mu_0, self.mu_1, self.sigma, self.phi
        else:
            self.sigma_0 = sum([(x - self.mu_0).reshape(-1, 1) @ (x - self.mu_0).reshape(1, -1) for x in X_0]) / len(X_0)
            self.sigma_1 = sum([(x - self.mu_1).reshape(-1, 1) @ (x - self.mu_1).reshape(1, -1) for x in X_1]) / len(X_1)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1, self.phi


    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X, training=False)

        def discriminant(x, mu, inv_sigma, phi_log=0):
            return -0.5 * (x - mu).T @ inv_sigma @ (x - mu) + phi_log

        if self.assume_same:
            inv_sigma = np.linalg.inv(self.sigma)
            predictions = [
                1 if discriminant(x, self.mu_1, inv_sigma, np.log(self.phi)) &gt; 
                discriminant(x, self.mu_0, inv_sigma, np.log(1 - self.phi)) 
                else 0 
                for x in X
            ]
        else:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            predictions = [
                1 if discriminant(x, self.mu_1, inv_sigma_1, np.log(self.phi)) &gt; 
                discriminant(x, self.mu_0, inv_sigma_0, np.log(1 - self.phi)) 
                else 0 
                for x in X
            ]
        return np.array(predictions)




#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_line_magic('pip', 'install pandas')
get_ipython().run_line_magic('pip', 'install numpy')
get_ipython().run_line_magic('pip', 'install matplotlib')


# In[2]:


from gda import GaussianDiscriminantAnalysis
import numpy as np
import matplotlib.pyplot as plt


# ## Load the data

# In[3]:


X = np.loadtxt("../data/Q4/q4x.dat", delimiter=None)

# Load y data and map "Alaska" to 0 and "Canada" to 1
y = np.loadtxt("../data/Q4/q4y.dat", dtype=str)
y = np.where(y == "Alaska", 0, 1)
n = X.shape[0]

np.random.seed(42)

random_indices = np.random.permutation(n)
X_shuffled = X[random_indices]
y_shuffled = y[random_indices]

# Split data into training and test sets
train_size = int(0.8*n)
X_train = X_shuffled[:train_size]
y_train = y_shuffled[:train_size]
X_test = X_shuffled[train_size:]
y_test = y_shuffled[train_size:]


# ## Part 1: Training Model

# In[4]:


model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma, phi = model.fit(X, y, assume_same_covariance=True)

print("phi: \n", phi)
print("mu_0: \n", mu_0)
print("mu_1: \n", mu_1)
print("sigma: \n", sigma)


# In[5]:


model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma, phi = model.fit(X_train, y_train, assume_same_covariance=True)

def compute_linear_boundary(mu0, mu1, cov):
    # Compute inverse of covariance matrix
    inv_cov = np.linalg.inv(cov)

    # Compute weights w and bias b
    w = inv_cov @ (mu1 - mu0)
    b = -0.5 * (mu1.T @ inv_cov @ mu1 - mu0.T @ inv_cov @ mu0)

    return w, b

X_train_norm = (X_train - model.getMean()) / model.getSd()

# report the values for mu_0, mu_1, and sigma
print("phi: \n", phi)
print("mu_0: \n", mu_0)
print("mu_1: \n", mu_1)
print("sigma: \n", sigma)


# ## Part 2: Plot Training Data

# In[6]:


plt.scatter(X_train_norm[y_train == 0][:, 0], X_train_norm[y_train == 0][:, 1], c='r', marker='x', label='Alaska')
plt.scatter(X_train_norm[y_train == 1][:, 0], X_train_norm[y_train == 1][:, 1], c='b', marker='o', label='Canada')


plt.xlabel('Fresh Water Growth Ring Diameter')
plt.ylabel('Marine Water Growth Ring Diameter')
plt.legend()
plt.grid(True)
plt.title('Alaskan and Canadian Salmon Training Data (Normalized)')
plt.show()


# ## Part 3: Training Model and Plotting Decision Boundary

# In[7]:


plt.scatter(X_train_norm[y_train == 0][:, 0], X_train_norm[y_train == 0][:, 1], c='r', marker='x', label='Alaska')
plt.scatter(X_train_norm[y_train == 1][:, 0], X_train_norm[y_train == 1][:, 1], c='b', marker='o', label='Canada')

# Plot decision boundary
w, b = compute_linear_boundary(mu_0, mu_1, sigma)
x_vals = np.linspace(X_train_norm[:, 0].min(), X_train_norm[:, 0].max(), 100)
y_vals = -(w[0] * x_vals + b) / w[1]
plt.plot(x_vals, y_vals, label="Linear Boundary", color="black")

plt.xlabel('Fresh Water Growth Ring Diameter')
plt.ylabel('Marine Water Growth Ring Diameter')
plt.legend()
plt.grid(True)
plt.title('Alaskan and Canadian Salmon Training Data (Normalized)')
plt.show()


# ### Training Accuracy

# In[8]:


y_pred = model.predict(X_train)
train_accuracy = np.mean(y_pred == y_train)
print(f"Train accuracy: {train_accuracy}")


# ### Testing Accuracy

# In[9]:


# Normalize Data
X_test_norm = (X_test - model.getMean()) / model.getSd()


plt.scatter(X_test_norm[y_test == 0][:, 0], X_test_norm[y_test == 0][:, 1], c='r', marker='x', label='Alaska')
plt.scatter(X_test_norm[y_test == 1][:, 0], X_test_norm[y_test == 1][:, 1], c='b', marker='o', label='Canada')


# Plot decision boundary
x_vals = np.linspace(X_test_norm[:, 0].min(), X_test_norm[:, 0].max(), 100)
y_vals = -(w[0] * x_vals + b) / w[1]
plt.plot(x_vals, y_vals, label="Linear Boundary", color="black")


plt.xlabel('Fresh Water Growth Ring Diameter')
plt.ylabel('Marine Water Growth Ring Diameter')
plt.legend()
plt.grid(True)
plt.title('Alaskan and Canadian Salmon Testing Data (Normalized)')
plt.show()


# In[10]:


y_pred = model.predict(X_test)
test_accuracy = np.mean(y_pred == y_test)
print(f"Test accuracy: {test_accuracy}")


# ## Part 4: General GDA

# In[11]:


def compute_quadratic_boundary(mu0, mu1, cov0, cov1):
    inv_cov0 = np.linalg.inv(cov0)
    inv_cov1 = np.linalg.inv(cov1)

    v = 0.5 * (inv_cov0 - inv_cov1)
    w = inv_cov1 @ mu1 - inv_cov0 @ mu0
    b = -0.5 * (mu1.T @ inv_cov1 @ mu1 - mu0.T @ inv_cov0 @ mu0)         + 0.5 * np.log(np.linalg.det(cov1) / np.linalg.det(cov0))

    return w, v, b


# ### Parameters for Whole Data

# In[12]:


model_quadratic = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1, phi = model_quadratic.fit(X, y, assume_same_covariance=False)

# Normalize data
X_norm = (X - model_quadratic.getMean()) / model_quadratic.getSd()

print("phi: \n", phi)
print("mu_0: \n", mu_0)
print("mu_1: \n", mu_1)
print("sigma_0: \n", sigma_0)
print("sigma_1: \n", sigma_1)


# ### Parameters for Training Data

# In[13]:


model_quadratic = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1, phi = model_quadratic.fit(X_train, y_train, assume_same_covariance=False)


# Normalize data
X_norm = (X - model_quadratic.getMean()) / model_quadratic.getSd()

print("phi: \n", phi)
print("mu_0: \n", mu_0)
print("mu_1: \n", mu_1)
print("sigma_0: \n", sigma_0)
print("sigma_1: \n", sigma_1)


# ## Parts 5 and 6: Quadratic Decision Boundary vs Linear Decision Boundary

# ### Plotting Decision Boundary for Training Data

# In[14]:


# Plot training data
plt.scatter(X_train_norm[y_train == 0][:, 0], X_train_norm[y_train == 0][:, 1], c='r', marker='x', label='Alaska')
plt.scatter(X_train_norm[y_train == 1][:, 0], X_train_norm[y_train == 1][:, 1], c='b', marker='o', label='Canada')

# Plot decision boundary
w, v, b = compute_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1)

# Meshgrid for plotting
x_vals = np.linspace(X_train_norm[:, 0].min(), X_train_norm[:, 0].max(), 300)
y_vals = np.linspace(X_train_norm[:, 1].min(), X_train_norm[:, 1].max(), 300)
x, y = np.meshgrid(x_vals, y_vals)

# Compute quadratic form for the decision boundary
V_xx = v[0, 0]
V_xy = v[0, 1]
V_yy = v[1, 1]
w_x = w[0]
w_y = w[1]

z = (V_xx * x**2) + (2 * V_xy * x * y) + (V_yy * y**2) + (w_x * x) + (w_y * y) + b
contour = plt.contour(x, y, z, levels=[0], colors='black', linestyles='--')
for i, seg in enumerate(contour.allsegs):
    if i == 0:
        plt.plot(seg[0][:, 0], seg[0][:, 1], label='Quadratic Boundary', color='black', linestyle='--')


model_linear = GaussianDiscriminantAnalysis()
mu_0_lin, mu_1_lin, sigma_lin, phi_lin = model_linear.fit(X_train, y_train, assume_same_covariance=True)

# Plot linear decision boundary
w_lin, b_lin = compute_linear_boundary(mu_0_lin, mu_1_lin, sigma_lin)
x_vals_lin = np.linspace(X_train_norm[:, 0].min(), X_train_norm[:, 0].max(), 100)
y_vals_lin = -(w_lin[0] * x_vals_lin + b_lin) / w_lin[1]
plt.plot(x_vals_lin, y_vals_lin, label="Linear Boundary", color="black")


plt.xlabel('Fresh Water Growth Ring Diameter')
plt.ylabel('Marine Water Growth Ring Diameter')
plt.legend()
plt.grid(True)
plt.title('Alaskan and Canadian Salmon Training Data (Normalized)')
plt.show()


# In[15]:


# Output quadratic training accuracy 
y_pred = model_quadratic.predict(X_train)
train_accuracy = np.mean(y_pred == y_train)
print(f"Train accuracy for quadratic boundary: {train_accuracy}")

# Output linear training accuracy 
y_pred = model_linear.predict(X_train)
train_accuracy = np.mean(y_pred == y_train)
print(f"Train accuracy for linear boundary: {train_accuracy}")


# ### Plotting Decision Boundary for Testing Data

# In[16]:


# Normalize Data
X_test_norm = (X_test - model_quadratic.getMean()) / model_quadratic.getSd()

plt.scatter(X_test_norm[y_test == 0][:, 0], X_test_norm[y_test == 0][:, 1], c='r', marker='x', label='Alaska')
plt.scatter(X_test_norm[y_test == 1][:, 0], X_test_norm[y_test == 1][:, 1], c='b', marker='o', label='Canada')


w, v, b = compute_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1)

# Meshgrid for plotting
x_vals = np.linspace(X_test_norm[:, 0].min()-0.5, X_test_norm[:, 0].max()+0.5, 300)
y_vals = np.linspace(X_test_norm[:, 1].min()-0.5, X_test_norm[:, 1].max()+0.5, 300)
x, y = np.meshgrid(x_vals, y_vals)

# Compute quadratic form for the decision boundary
V_xx = v[0, 0]
V_xy = v[0, 1]
V_yy = v[1, 1]
w_x = w[0]
w_y = w[1]

z = (V_xx * x**2) + (2 * V_xy * x * y) + (V_yy * y**2) + (w_x * x) + (w_y * y) + b
contour = plt.contour(x, y, z, levels=[0], colors='black', linestyles='--')
for i, seg in enumerate(contour.allsegs):
    if i == 0:
        plt.plot(seg[0][:, 0], seg[0][:, 1], label='Quadratic Boundary', color='black', linestyle='--')
    

# Plot linear decision boundary
w_lin, b_lin = compute_linear_boundary(mu_0_lin, mu_1_lin, sigma_lin)
x_vals_lin = np.linspace(X_test_norm[:, 0].min()-0.5, X_test_norm[:, 0].max()+0.5, 100)
y_vals_lin = -(w_lin[0] * x_vals_lin + b_lin) / w_lin[1]
plt.plot(x_vals_lin, y_vals_lin, label="Linear Boundary", color="black")


plt.xlabel('Fresh Water Growth Ring Diameter')
plt.ylabel('Marine Water Growth Ring Diameter')
plt.legend()
plt.grid(True)
plt.title('Alaskan and Canadian Salmon Testing Data (Normalized)')
plt.show()


# In[17]:


# Output quadratic teating accuracy 
y_pred = model_quadratic.predict(X_test)
test_accuracy = np.mean(y_pred == y_test)
print(f"Train accuracy for quadratic boundary: {test_accuracy}")

# Output linear testing accuracy 
y_pred = model_linear.predict(X_test)
test_accuracy = np.mean(y_pred == y_test)
print(f"Train accuracy for linear boundary: {test_accuracy}")



</PRE>
</PRE>
</BODY>
</HTML>
