<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_KQ3JZ.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_KQ3JZ.py<p><PRE>



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time





class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.theta_history = []

    def compute_cost(self, X, y, theta):
        """Compute the cost function J(θ)."""
        m = len(y)
        error = X.dot(theta) - y
<A NAME="5"></A><FONT color = #FF0000><A HREF="match62-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        """Fit the linear regression model using Batch Gradient Descent."""
</FONT>        epsilon=1e-6
        max_iterations=100000
        m, n = X.shape
        X = np.c_[np.ones(m), X]  
        self.theta = np.zeros(n + 1)  
        
        for i in range(max_iterations):
            h = X.dot(self.theta)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match62-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            gradient = (1/m) * X.T.dot(h - y)
            self.theta -= learning_rate * gradient
            
            cost = self.compute_cost(X, y, self.theta)
            self.cost_history.append(cost)
            self.theta_history.append(self.theta.copy())
</FONT>            print(cost)
            
            if i &gt; 0 and abs(self.cost_history[-1] - self.cost_history[-2]) &lt; epsilon:
                print(f"Convergence reached at iteration {i}.")
                break
        return self.theta_history
    
    def predict(self, X):
        """Predict the target values for the input data."""
        X = np.c_[np.ones(X.shape[0]), X]  
        print(X.dot(self.theta).shape)
        return X.dot(self.theta)
    
    def plot_data_and_hypothesis(self, X, y):
        """Plot the data and the learned hypothesis function."""
        plt.figure(figsize=(10, 6))
        plt.scatter(X, y, color='b', label='Data points')
        plt.plot(X, self.predict(X), color='r', label='Hypothesis function')
        plt.xlabel('Acidity')
        plt.ylabel('Density')
        plt.title('Linear Regression: Data and Hypothesis Function')
        plt.legend()
        plt.show()
    
    def plot_cost_3d(self, X, y):
        """Plot the 3D cost function with a color gradient and a color bar."""
        theta0_vals = np.linspace(-100, 100, 200)
        theta1_vals = np.linspace(-100, 100, 200)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
        
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
        
        
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

        plt.show()


    def plot_cost_3d_animated(self, X, y, speed_factor=1):
        """Plot the 3D cost function and animate gradient descent steps."""
        theta_history = self.theta_history
        cost_history = self.cost_history

        
        theta0_vals = np.linspace(-10, 30, 40)
        theta1_vals = np.linspace(0, 40, 40)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

        
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')

        
        surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")

        
        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Mesh Plot of Cost Function with Gradient Descent Path')

        
        scatter = ax.scatter([], [], [], color='red', s=50, label="Current Position")
        path, = ax.plot([], [], [], color='red', linestyle='-', linewidth=2, label="Gradient Descent Path")
        persistent_scatter = ax.scatter([], [], [], color='black', s=30)  

        
        theta_0_vals = []
        theta_1_vals = []
        cost_vals = []

        
        for i in range(0, len(theta_history), speed_factor):
            theta_0, theta_1 = theta_history[i]
            cost = cost_history[i]

            
            theta_0_vals.append(theta_0)
            theta_1_vals.append(theta_1)
            cost_vals.append(cost)

            
            scatter._offsets3d = (np.array([theta_0]), np.array([theta_1]), np.array([cost]))

            
            path.set_data(theta_0_vals, theta_1_vals)
            path.set_3d_properties(cost_vals)

            
            persistent_scatter._offsets3d = (np.array(theta_0_vals), np.array(theta_1_vals), np.array(cost_vals))

            plt.legend()
            plt.pause(0.01)  

        
        plt.close(fig)




    def plot_contours(self, X, y):
        """Plot the contours of the cost function and animate gradient descent updates."""
        theta0_vals = np.linspace(-100, 100, 200)
        theta1_vals = np.linspace(-100, 100, 200)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        
        plt.figure(figsize=(10, 6))
        
        
        plt.contour(theta0_vals, theta1_vals, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='jet')
        plt.xlabel(r'$\theta_0$')
        plt.ylabel(r'$\theta_1$')
        plt.title('Contour Plot of Cost Function')

        
        theta_history = np.array(self.theta_history)
        path, = plt.plot([], [], 'r-x', label='Gradient Descent Path')
        
        plt.legend()

        
        for i in range(len(theta_history)):
            path.set_data(theta_history[:i+1, 0], theta_history[:i+1, 1])  
            plt.pause(0.2)  

        plt.show()


    def plot_contours_multiple_lrs(self, X, y, learning_rates):
        """Plot the contours of the cost function and compare different learning rates."""
        theta0_vals = np.linspace(-2, 8, 200)   
        theta1_vals = np.linspace(-2, 30, 200)  
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))  
        axes = axes.flatten()  

        for idx, eta in enumerate(learning_rates):
            
            self.theta = np.zeros(2)  
            self.theta_history = []  

            
            self.fit(X, y, eta)  
            theta_history = np.array(self.theta_history)

            ax = axes[idx]
            CS = ax.contour(theta0_vals, theta1_vals, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='viridis')
            ax.set_xlabel(r'$\theta_0$ (Intercept)')
            ax.set_ylabel(r'$\theta_1$ (Slope)')
            ax.set_title(f'Contour Plot with Gradient Descent Path (η={eta})')
            fig.colorbar(CS, ax=ax, label=r'Cost $J(\theta)$')

            
            ax.scatter(theta_history[:, 0], theta_history[:, 1], color='red', s=40)  
            ax.plot(theta_history[:, 0], theta_history[:, 1], 'r-', linewidth=2)  

        plt.tight_layout()
        plt.show()


    
    def compare_learning_rates(self, X, y, learning_rates=[0.001, 0.025, 0.1]):
        """Compare contour plots for different learning rates."""
        for alpha in learning_rates:
            print(f"Learning rate: {alpha}")
            self.fit(X, y, learning_rate=alpha)
            self.plot_contours_multiple_lrs(X, y, [0.001, 0.025, 0.1])


X = np.loadtxt('data/Q1/linearX.csv', delimiter=',')
y = np.loadtxt('data/Q1/linearY.csv', delimiter=',')
X=X.reshape(-1,1)

model = LinearRegressor()
model.fit(X, y)
model.predict(X)









model.compare_learning_rates(X, y, learning_rates=[0.1, 0.025, 0.001])





import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.theta_history = []

    def compute_cost(self, X, y, theta): # Just compute cost here
        """Compute the cost function J(θ)."""
        m = len(y)
        error = X.dot(theta) - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        """Fit the linear regression model using Batch Gradient Descent."""
        epsilon=1e-20 #Try 200 or 500
        max_iterations=1000000 #Can change
        m, n = X.shape
        X = np.c_[np.ones(m), X]  #here too add one
        self.theta = np.zeros(n + 1)  
        
        for i in range(max_iterations):
            h = X.dot(self.theta)
            gradient = (1/m) * X.T.dot(h - y)
            self.theta -= learning_rate * gradient
            
            cost = self.compute_cost(X, y, self.theta)
            self.cost_history.append(cost)
            self.theta_history.append(self.theta.copy())
            print(cost)
            
            if i &gt; 0 and abs(self.cost_history[-1] - self.cost_history[-2]) &lt; epsilon:
                print(f"Convergence reached at iteration {i}.")
                break
        return np.array(self.theta_history)
    
    def predict(self, X):
        """Predict the target values for the input data."""
        X = np.c_[np.ones(X.shape[0]), X]  #remeber add one col
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match62-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return X.dot(self.theta)
    




import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
</FONT>    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    np.random.seed(0)  
    theta_true = theta  
    num_samples = N
    x1 = np.random.normal(input_mean[0], input_sigma[0], num_samples)
    x2 = np.random.normal(input_mean[1], input_sigma[1], num_samples)
    epsilon = np.random.normal(0, noise_sigma, num_samples)
    y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon
    X = np.column_stack((x1, x2))
    return X,y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X = np.column_stack((np.ones(X.shape[0]),X))
        m, n = X.shape
        self.theta = np.zeros(n)
        self.theta_history = [] #empty lis otherwise one extra
        indices = np.random.permutation(m)
        X, y = X[indices], y[indices]
        batch_size=64; max_epochs=1000; tol=1e-6

        for _ in range(max_epochs):
            prev_theta = self.theta.copy()

            for i in range(0, m, batch_size):
                X_batch = X[i:i+batch_size]
                y_batch = y[i:i+batch_size]

<A NAME="6"></A><FONT color = #00FF00><A HREF="match62-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                gradients = -X_batch.T @ (y_batch - X_batch @ self.theta) / batch_size
                self.theta -= learning_rate * gradients

            self.theta_history.append(self.theta.copy())

            
            if np.linalg.norm(self.theta - prev_theta) &lt; tol:
</FONT>                break
        return np.array(self.theta_history) #check dim for sure
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.column_stack((np.ones(X.shape[0]),X)) #add one 
        return X @ self.theta
    





#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:



plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:



def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match62-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
</FONT>plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
plt.show()





import numpy as np
class LogisticRegressor:
    
    def __init__(self):
        self.theta=None
    
<A NAME="0"></A><FONT color = #FF0000><A HREF="match62-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
</FONT>        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.column_stack((np.ones(X.shape[0]),X))
        m, n = X.shape
        tol=1e-20;max_iter=500 #check epochs by increasing or dec
        self.theta = np.zeros(n)  
        theta_history = []  

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match62-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for _ in range(max_iter):
            
            h = self.sigmoid(X @ self.theta)
            gradient = X.T @ (h - y)        
            S = np.diag(h * (1 - h))  
</FONT>            hessian = X.T @ S @ X
            delta_theta = np.linalg.inv(hessian) @ gradient
            self.theta -= delta_theta
            theta_history.append(self.theta.copy())           
            if np.linalg.norm(gradient) &lt; tol:  
                break
        return np.array(theta_history) #check dim
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.column_stack((np.ones(X.shape[0]),X))
        probabilities = self.sigmoid(X @ self.theta)
        return (probabilities &gt;= 0.5).astype(int) #convert remember




#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:



plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:



def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
<A NAME="7"></A><FONT color = #0000FF><A HREF="match62-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()






import numpy as np

class GaussianDiscriminantAnalysis:
    
    def __init__(self):
        self.mu_0 = None  
        self.mu_1 = None  
        self.sigma = None  
</FONT>        self.sigma_0 = None  
        self.sigma_1 = None  
        self.phi = None  
        self.assume_same_covariance = None#check all values are corrct
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        assume_same_covariance : bool
            Whether to assume same covariance matrix for both classes.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
        """
        self.assume_same_covariance = assume_same_covariance
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_0 = X[y == 0]
        X_1 = X[y == 1]
        self.phi = np.mean(y)
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)
        
        if assume_same_covariance:
            n_0 = X_0.T.shape[0]
            n_1 = X_1.T.shape[0]
            n = n_0 + n_1
            sigma_0 = np.cov(X_0.T, bias=True) * (n_0 - 1)
            sigma_1 = np.cov(X_1.T, bias=True) * (n_1 - 1)#check that without formula is correct
            self.sigma = (sigma_0 + sigma_1) / (n - 2)
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.cov(X_0.T, bias=True)
            self.sigma_1 = np.cov(X_1.T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1 #above and below sig should match
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
            score_0 = (-0.5 * ((X - self.mu_0) @ sigma_inv @ (X - self.mu_0).T).diagonal() + 
                      np.log(1 - self.phi))
            score_1 = (-0.5 * ((X - self.mu_1) @ sigma_inv @ (X - self.mu_1).T).diagonal() + 
                      np.log(self.phi))
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            score_0 = (-0.5 * np.log(np.linalg.det(self.sigma_0)) 
                      -0.5 * ((X - self.mu_0) @ sigma_0_inv @ (X - self.mu_0).T).diagonal() + 
                      np.log(1 - self.phi))
            score_1 = (-0.5 * np.log(np.linalg.det(self.sigma_1)) 
                      -0.5 * ((X - self.mu_1) @ sigma_1_inv @ (X - self.mu_1).T).diagonal() + 
                      np.log(self.phi))
        return (score_1 &gt; score_0).astype(int)
    



#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:



plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:



def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
plt.show()



</PRE>
</PRE>
</BODY>
</HTML>
