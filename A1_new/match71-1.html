<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_1L6WT.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_33WKP.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

class LinearRegressor:
    def __init__(self):
        self.theta = None  \

    def fit(self, X, y, learning_rate=0.01):

        tolerance=1e-6
        max_iters=10000

        print(f"X ka shape is -&gt; {X.shape} & y ka shape is -&gt; {y.shape}")

        m = X.shape[0]  # Get number of samples and features
        n = 1

        X = X.reshape(-1, 1)  # Convert to shape (m, 1)

        X = np.hstack((np.ones((m, 1)), X))  # Add column of ones for theta_0

        self.theta = np.zeros((n + 1,))  # (n_features + 1,) shape
        theta_list = [] 

        for i in range(max_iters):

            y_pred = X.dot(self.theta) # X(m,2) * theta(2,) = y_pred(m,)
            # print("y_pred shape")
            # print(y_pred.shape)
            # print(y.shape)

            # y is of shape (1000,1) converting it to (1000,)
            flatted_y = y.reshape(-1)
            
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match71-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            error = y_pred - flatted_y # (m,) - (m,)
            # print("error shape")
            # print(error.shape)

            gradient = (1 / m) * X.T.dot(error) # XT(2,m) * (m,) = gradient(2,)
            # gradient = X.T.dot(error) # XT(2,m) * (m,) = gradient(2,)

            # gradient = gradient.flatten()

            new_theta = self.theta - learning_rate * gradient
            
            theta_list.append(new_theta.copy())
</FONT>
            if np.linalg.norm(new_theta - self.theta, ord=2) &lt; tolerance:
                print(f"Converged at iteration {i}")
                break

            self.theta = new_theta
        
        return np.array(theta_list)

    def predict(self, X):
  
        m = X.shape[0]
        # print(X.shape)

        X = X.reshape(-1, 1)  # Convert to shape (m, 1)

        X = np.hstack((np.ones((m, 1)), X)) # X has dim -&gt; (m,2)

        # print("theta shape -")
        # print(self.theta.shape)

        return X.dot(self.theta) # X(m,2) * theta(2,)
    
    def calc_J_theta(self, y, y_pred):

        m = y.shape[0]
        flatted_y = y.reshape(-1)
        intmed = flatted_y - y_pred
        j = 0.0
        for var in intmed:
            j = j + (1/(2*m))*var*var

<A NAME="5"></A><FONT color = #FF0000><A HREF="match71-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return j

    

# X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")  # Feature matrix
# y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")  # Target values

# if y.ndim == 1:
#     y = y.reshape(-1, 1)

# model = LinearRegressor()
# learning_rate = 0.1
# tolerance = 1e-9
# theta_history = model.fit(X, y, learning_rate, tolerance)
# print(f"theta hist ka shape -&gt; {theta_history.shape}")

# print("Final Parameters:", model.theta)

# y_pred = model.predict(X) # y_pred(m,)
# print("Predictions:", y_pred[:5])  # Show first 5 predictions

# calculated_J_theta = model.calc_J_theta(y, y_pred)
# print(f"value of J_theta is -&gt; {calculated_J_theta} with learning_rate = {learning_rate} and tolerance = {tolerance}")


#1.2 ka graph plotter #####################################################################################

# plt.scatter(X, y, color='red', label="Actual Data")  # Scatter plot for actual data

# plt.plot(X, y_pred, color='blue', label="Predicted Values")  # Line plot for predictions

# plt.xlabel("X (Feature)")
# plt.ylabel("Y (Target / Predictions)")
# plt.title("Linear Regression Fit")
# plt.legend()

# plt.show()

#############################################################################################################

#1.3 3D graph plotter wala ##################################################################################

# theta_0_vals = theta_history[:, 0]  # Intercept values
# theta_1_vals = theta_history[:, 1]  # Slope values

# #for 1.3 ka graph
# # theta_0_range = np.linspace(min(theta_0_vals)-1, max(theta_0_vals)+1, 100)
# # theta_1_range = np.linspace(min(theta_1_vals)-1, max(theta_1_vals)+1, 100)

# #for contour i.e. 1.4 ka graph
# theta_0_range = np.linspace(-10, 20, 100)
# theta_1_range = np.linspace(-4, 60, 100)

# Theta_0, Theta_1 = np.meshgrid(theta_0_range, theta_1_range)

# J_vals = np.zeros_like(Theta_0)
# m = len(y)

# for i in range(Theta_0.shape[0]):
#     for j in range(Theta_0.shape[1]):
#         theta = np.array([Theta_0[i, j], Theta_1[i, j]])
#         predictions = np.hstack((np.ones((m, 1)), X.reshape(-1, 1))).dot(theta)
#         J_vals[i, j] = (1 / (2 * m)) * np.sum((predictions - y.flatten()) ** 2)

#3d plot ka start ; above data also used fo 1.4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# fig = plt.figure(figsize=(10, 7))
# ax = fig.add_subplot(111, projection='3d')
# ax.plot_surface(Theta_0, Theta_1, J_vals, cmap='coolwarm', alpha=0.7)

# for i in range(len(theta_history)):
#     theta_0 = theta_history[i, 0]
#     theta_1 = theta_history[i, 1]
    
#     y_pred = np.hstack((np.ones((m, 1)), X.reshape(-1, 1))).dot([theta_0, theta_1])
#     J_theta = (1 / (2 * m)) * np.sum((y_pred - y.flatten()) ** 2)
    
#     ax.scatter(theta_0, theta_1, J_theta, color='black', marker='o', s=20)
#     plt.draw()
#     time.sleep(0.2)  # Pause for 0.2 seconds to animate
#     plt.pause(0.2)

# ax.set_xlabel(r'$\theta_0$ (Intercept)')
# ax.set_ylabel(r'$\theta_1$ (Slope)')
# ax.set_zlabel(r'$J(\theta)$ (Cost)')
# ax.set_title('3D Visualization of J(θ) with Gradient Descent Path')

# plt.show()

#############################################################################################################

#1.4 contour graph plotter ##################################################################################

# plt.figure(figsize=(6, 5))
# plt.contourf(Theta_0, Theta_1, J_vals, cmap='coolwarm', levels=50)
# plt.xlabel(r'$\theta_0$')
# plt.ylabel(r'$\theta_1$')
# plt.title("Contour Plot of Cost Function")

# for i in range(len(theta_0_vals)):
#     plt.scatter(theta_0_vals[i], theta_1_vals[i], color='red', s=15)
#     plt.pause(0.2)  # Pause for visualization

# plt.show()

#############################################################################################################



import numpy as np
import matplotlib.pyplot as plt

def generate(N, theta, input_mean, input_sigma, noise_sigma):

    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=(N, 1))  # Shape (N, 1)
</FONT>    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=(N, 1))  # Shape (N, 1)

    X = np.hstack((x1,x2)) # my shape is (N,2)

    noise = np.random.normal(loc=0, scale=noise_sigma, size=(N, 1))  # its Shape (N, 1)

    y = (theta[0] + theta[1]*x1 + theta[2]*x2 + noise).reshape(N,)

    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = {}
        self.cvgd_epoch = []
    
    def fit(self, X, y, learning_rate=0.001):

        max_epochs = {1 : 100, 80 : 100000, 8000 : 100000, 800000 : 100000}
        # max_epochs = {1 : 100, 80 : 100000, 80000 : 50000, 800000 : 50000}

        self.cvgd_epoch = []
        eps = 1e-4
        res_arr = []  

        for r in [1, 80, 8000, 800000]:
        # for r in [80000, 800000]:
        

            m, n = X.shape
            r = min(r, m) 

            X_run = np.hstack((np.ones((m, 1)), X))  # Shape: (m, n+1)

            self.theta[r] = np.zeros((n + 1,))
            theta_list = []  

            for epoch in range(max_epochs[r]):

                # print(epoch)

                indices = np.random.permutation(m)  

                for i in range(0, m, r): 

                    batch_indices = indices[i : i + r]
                    X_batch = X_run[batch_indices]
                    y_batch = y[batch_indices]

                    y_pred = X_batch @ self.theta[r]  # Shape: (r,)

                    error = y_pred - y_batch  # Shape: (r,)
                    gradient = (1 / r) * X_batch.T @ error  # Shape: (n+1,)

                    new_theta = self.theta[r] - learning_rate * gradient

                    theta_list.append(new_theta.copy())


                if np.linalg.norm(new_theta - self.theta[r], ord=2) &lt; eps:

                    print(new_theta)
                    print(self.theta[r])

                    self.theta[r] = new_theta

                    self.cvgd_epoch.append(r)
                    self.cvgd_epoch.append(epoch)

                    print(f"Converged for r={r} at epoch {epoch}")
                    break

                self.theta[r] = new_theta 

            

            res_arr.append(np.array(theta_list))  

        return res_arr  
    
    def fit_closed_form(self, X, y):

        m, n = X.shape
        X_bias = np.hstack((np.ones((m, 1)), X))  

        theta_closed = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y

        self.theta['closed_form'] = theta_closed
        return theta_closed


    
    def predict(self, X):

        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))
        # return X @ self.theta[max(self.theta.keys())]  

        return [X @ self.theta[k] for k in self.theta.keys()]
    

    def calc_J_theta(self, y, y_pred):

            m = y.shape[0]
            flatted_y = y.reshape(-1)
            intmed = flatted_y - y_pred
            j = 0.0
            for var in intmed:
                j = j + (1/(2*m))*var*var

            return j
    
    def plot_theta_trajectory(self, theta_history):
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        batch_sizes = [1, 80, 8000, 800000] 
        colors = ["red", "blue", "green", "purple"]

        for idx, theta_vals in enumerate(theta_history):
            if theta_vals.shape[0] &gt; 0:  

                downsample_factor = max(1, len(theta_vals) // 1000)  
                theta_vals = theta_vals[::downsample_factor]

                ax.plot(theta_vals[:, 0], theta_vals[:, 1], theta_vals[:, 2], 
                        label=f"Batch size: {batch_sizes[idx]}", color=colors[idx], alpha=0.8)

                ax.scatter(theta_vals[0, 0], theta_vals[0, 1], theta_vals[0, 2], 
                            color=colors[idx], marker="o", s=50, label=f"Start r={batch_sizes[idx]}")
                ax.scatter(theta_vals[-1, 0], theta_vals[-1, 1], theta_vals[-1, 2], 
                            color=colors[idx], marker="x", s=100, label=f"End r={batch_sizes[idx]}")

        ax.set_xlabel(r"$\theta_0$")
        ax.set_ylabel(r"$\theta_1$")
        ax.set_zlabel(r"$\theta_2$")
        ax.set_title("SGD Parameter Movement in 3D Space")
        ax.legend()
        plt.show()      





# N = 1_000_000  
# theta = np.array([3, 1, 2])  
# input_mean = np.array([3, -1])
# input_sigma = np.array([2, 2])  
# noise_sigma = np.sqrt(2) 

# X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

# train_size = int(0.8 * N)  # 800,000 samples for training
# X_train, y_train = X[:train_size], y[:train_size]
# X_test, y_test = X[train_size:], y[train_size:]

# print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
# print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

# model = StochasticLinearRegressor()
# # out = model.fit_closed_form(X_train, y_train)

# # print(out)

# list_theta_batch = model.fit(X_train, y_train)

# print(f"ye rhae thetas -&gt; {model.theta}")

# print(f"here is model ke iterations -&gt; {model.cvgd_epoch}")

# list_y_pred_test = model.predict(X_test)
# list_y_pred_train = model.predict(X_train)

# for x in range(len(list_y_pred_test)):
#     print(f"here is testing loss for {x} is {model.calc_J_theta(y_test, list_y_pred_test[x])}")

# for x in range(len(list_y_pred_train)):
#     print(f"here is testing loss for {x} is {model.calc_J_theta(y_train, list_y_pred_train[x])}")

# model.plot_theta_trajectory(list_theta_batch)



#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    # Generate normally distributed input features x1 and x2
    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=(N, 1))  # Shape (N, 1)
    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=(N, 1))  # Shape (N, 1)

    X = np.hstack((x1,x2)) #shape is (N,2)

    # Add Gaussian noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=(N, 1))  # Shape (N, 1)

    # Compute target values using the hypothesis function
    y = (theta[0] + theta[1]*x1 + theta[2]*x2 + noise).reshape(N,)

    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = {}
        self.cvgd_epoch = []
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: return a list of numpy arrays in the same order for the batch sizes, numpy array of shape (n_iter, n_features+1,)
        The list of parameters obtained after each iteration of Gradient Descent.
        
        """

        max_epochs = {1 : 100, 80 : 100000, 8000 : 100000, 800000 : 100000}
        # max_epochs = {1 : 100, 80 : 100000, 80000 : 50000, 800000 : 50000}

        self.cvgd_epoch = []
        eps = 1e-4
        res_arr = []  # Store results for each batch size

        for r in [1, 80, 8000, 800000]:
        # for r in [80000, 800000]:
        

            m, n = X.shape
            r = min(r, m)  # Ensure batch size is not greater than dataset size

            # Add intercept term (bias)
            X_run = np.hstack((np.ones((m, 1)), X))  # Shape: (m, n+1)

            # Initialize parameters to zeros
            self.theta[r] = np.zeros((n + 1,))
            theta_list = []  # Store theta values after each update

            for epoch in range(max_epochs[r]):

                # print(epoch)

                indices = np.random.permutation(m)  # Shuffle indices

                for i in range(0, m, r):  # Process in mini-batches of size `r`

                    batch_indices = indices[i : i + r]
                    X_batch = X_run[batch_indices]
                    y_batch = y[batch_indices]

                    # Compute predictions
                    y_pred = X_batch @ self.theta[r]  # Shape: (r,)

                    # Compute gradient using the batch
                    error = y_pred - y_batch  # Shape: (r,)
                    gradient = (1 / r) * X_batch.T @ error  # Shape: (n+1,)

                    # Update parameters
                    new_theta = self.theta[r] - learning_rate * gradient

                    theta_list.append(new_theta.copy())


                    # Check for convergence
                if np.linalg.norm(new_theta - self.theta[r], ord=2) &lt; eps:

                    print(new_theta)
                    print(self.theta[r])

                    self.theta[r] = new_theta

                    self.cvgd_epoch.append(r)
                    self.cvgd_epoch.append(epoch)

                    print(f"Converged for r={r} at epoch {epoch}, batch {i // r}")
                    break

                self.theta[r] = new_theta  # Update theta

                # else:
                #     continue  # Continue outer loop if inner loop does not break

                # break  # Break outer loop if convergence occurred

            res_arr.append(np.array(theta_list))  # Store thetas for this batch size

        return res_arr  # List of theta values for each r

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))  # Add bias term
        # return X @ self.theta[max(self.theta.keys())]  # Use the last trained theta

        return [X @ self.theta[k] for k in self.theta.keys()]
    

    def calc_J_theta(self, y, y_pred):

            m = y.shape[0]
            flatted_y = y.reshape(-1)
            intmed = flatted_y - y_pred
            j = 0.0
            for var in intmed:
                j = j + (1/(2*m))*var*var

            return j
    
    def plot_theta_trajectory(self, theta_history):
        """
        Plot the movement of theta in 3D space for different batch sizes.
        
        Parameters
        ----------
        theta_history : dict
            Dictionary where keys are batch sizes, and values are numpy arrays
            of shape (n_iter, n_features+1,) containing the parameter updates.
        """

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        colors = {1: "red", 80: "blue", 8000: "green", 800000: "purple"}

        for r, theta_vals in theta_history.items():
            if len(theta_vals) &gt; 0:
                ax.plot(theta_vals[:, 0], theta_vals[:, 1], theta_vals[:, 2], 
                        label=f"Batch size: {r}", color=colors[r], alpha=0.8)

                # Mark start and end points
                ax.scatter(theta_vals[0, 0], theta_vals[0, 1], theta_vals[0, 2], 
                            color=colors[r], marker="o", s=50, label=f"Start r={r}")
                ax.scatter(theta_vals[-1, 0], theta_vals[-1, 1], theta_vals[-1, 2], 
                            color=colors[r], marker="x", s=100, label=f"End r={r}")

        ax.set_xlabel(r"$\theta_0$")
        ax.set_ylabel(r"$\theta_1$")
        ax.set_zlabel(r"$\theta_2$")
        ax.set_title("SGD Parameter Movement in 3D Space")
        ax.legend()
        plt.show()


# In[ ]:


N = 1_000_000  # 1 million samples
theta = np.array([3, 1, 2])  # Given theta values
input_mean = np.array([3, -1])  # Given means
input_sigma = np.array([2, 2])  # Given standard deviations
noise_sigma = np.sqrt(2)  # Given noise variance (σ^2 = 2 → σ = sqrt(2))

# Generate dataset
X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

# Split into train (80%) and test (20%) sets
train_size = int(0.8 * N)  # 800,000 samples for training
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]

# Print dataset shapes for verification
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

model = StochasticLinearRegressor()
list_theta_batch = model.fit(X_train, y_train)

print(f"ye rhae thetas -&gt; {model.theta}")

print(f"here is model ke iterations -&gt; {model.cvgd_epoch}")

list_y_pred_test = model.predict(X_test)
list_y_pred_train = model.predict(X_train)

for x in range(len(list_y_pred_test)):
    print(f"here is testing loss for {x} is {model.calc_J_theta(y_test, list_y_pred_test[x])}")

for x in range(len(list_y_pred_train)):
    print(f"here is testing loss for {x} is {model.calc_J_theta(y_train, list_y_pred_train[x])}")


# In[ ]:


model.plot_theta_trajectory(list_theta_batch)





import numpy as np
<A NAME="0"></A><FONT color = #FF0000><A HREF="match71-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt

class LogisticRegressor:

    def __init__(self):
        self.theta = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def fit(self, X, y, learning_rate=0.01):
</FONT>        
        max_iter = 10000

        # X = self.normalize(X)
        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X))  # (m, n+1)
        
        self.theta = np.zeros(n + 1)
        
        theta_list = []  # i will be store theta after each iteration
        
<A NAME="1"></A><FONT color = #00FF00><A HREF="match71-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for _ in range(max_iter):
            h = self.sigmoid(X @ self.theta)  # (m,)
            
            gradient = X.T @ (y - h)  # (n+1,)
            
            D = np.diag(h * (1 - h))  # (m, m)
</FONT>            H = -(X.T @ D @ X)  # (n+1, n+1)
            
            H_inv = np.linalg.inv(H)
            self.theta -= H_inv @ gradient 
            
            theta_list.append(self.theta.copy())
        
        return np.array(theta_list)
    
    def predict(self, X):
        # X = self.normalize(X)
        
        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))  # (m, n+1)
        
        y_prob = self.sigmoid(X @ self.theta)  # (m,)
        return (y_prob &gt;= 0.5).astype(int)  
    

# X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
# y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

# X_mean = np.mean(X, axis=0)
# X_std = np.std(X, axis=0)
# X = (X - X_mean) / X_std

# model = LogisticRegressor()
# theta_list = model.fit(X, y)
# theta_final = theta_list[-1]

# print("Final theta:", theta_final)

# plt.figure(figsize=(8, 6))
# plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label="Class 0", marker="o")
# plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label="Class 1", marker="x")

# x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
# x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]

# plt.plot(x1_vals, x2_vals, 'k-', label="Decision Boundary")

# plt.xlabel("x1 (Normalized)")
# plt.ylabel("x2 (Normalized)")
# plt.legend()
# plt.title("Logistic Regression Decision Boundary")
# plt.show()

# # y_pred = model.predict(X)




import numpy as np
import matplotlib.pyplot as plt

X = np.loadtxt("../data/Q4/q4x.dat")

with open("../data/Q4/q4y.dat", "r") as f:
    y = np.array([1 if label.strip() == "Canada" else 0 for label in f])

plt.figure(figsize=(8, 6))

plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")

plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")

plt.xlabel("Growth ring diameter in Freshwater (x1)")
plt.ylabel("Growth ring diameter in Marine water (x2)")
plt.title("Training Data: Alaska vs. Canada Salmon")
plt.legend()
plt.grid(True)

plt.show()




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

X = np.loadtxt("../data/Q4/q4x.dat")

with open("../data/Q4/q4y.dat", "r") as f:
    y = np.array([1 if label.strip() == "Canada" else 0 for label in f])

X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X = (X - X_mean) / X_std  

<A NAME="2"></A><FONT color = #0000FF><A HREF="match71-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

mu_0 = np.mean(X[y == 0], axis=0) 
mu_1 = np.mean(X[y == 1], axis=0)  

cov_matrix = np.cov(X.T)  # (2 x 2) cov mat.
</FONT>
cov_inv = np.linalg.inv(cov_matrix)

<A NAME="6"></A><FONT color = #00FF00><A HREF="match71-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

w = cov_inv @ (mu_1 - mu_0)
b = -0.5 * (mu_1.T @ cov_inv @ mu_1 - mu_0.T @ cov_inv @ mu_0) 

plt.figure(figsize=(8, 6))
</FONT>plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")

x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100) 
x2_vals = -(w[0] * x1_vals + b) / w[1]  

plt.plot(x1_vals, x2_vals, 'g-', label="Decision Boundary")  

plt.xlabel("Growth ring diameter in Freshwater (x1)")
plt.ylabel("Growth ring diameter in Marine water (x2)")
plt.title("GDA Decision Boundary")
plt.legend()
plt.grid(True)

plt.show()




import numpy as np

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.phi = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.mean = None
        self.std = None

    def normalize(self, X, is_training=True):
        if is_training:
            self.mean = np.mean(X, axis=0)
            self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std
    
    def fit(self, X, y, assume_same_covariance=False):

        X = self.normalize(X, is_training=True)

        self.phi = np.mean(y)

        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)

        if assume_same_covariance:
            m = X.shape[0]
            self.sigma = (1 / m) * sum(
                np.outer(X[i] - (self.mu_1 if y[i] else self.mu_0), 
                         X[i] - (self.mu_1 if y[i] else self.mu_0))
                for i in range(m)
            )
            return self.mu_0, self.mu_1, self.sigma
        
        else:

            m0 = np.sum(y == 0)  
            m1 = np.sum(y == 1) 

            epsilon = 1e-6

            self.sigma_0 = sum(
                np.outer(X[i] - self.mu_0, X[i] - self.mu_0) 
                for i in range(len(y)) if y[i] == 0
            ) / m0
            self.sigma_0 += epsilon * np.eye(X.shape[1]) 

            self.sigma_1 = sum(
                np.outer(X[i] - self.mu_1, X[i] - self.mu_1) 
                for i in range(len(y)) if y[i] == 1
            ) / m1
            self.sigma_1 += epsilon * np.eye(X.shape[1])

            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
        
    
    def predict(self, X):

        X = self.normalize(X, is_training=False)

        sigma_inv = np.linalg.inv(self.sigma)
        term = (X @ sigma_inv @ self.mu_1) - (X @ sigma_inv @ self.mu_0)
        constant = -0.5 * (self.mu_1 @ sigma_inv @ self.mu_1 - self.mu_0 @ sigma_inv @ self.mu_0)
        decision_boundary = term + constant + np.log(self.phi / (1 - self.phi))
        
        return (decision_boundary &gt; 0).astype(int)
    





import numpy as np
from gda import GaussianDiscriminantAnalysis
import matplotlib.pyplot as plt

def normalize(X, is_training=True):
        if is_training:
            mean = np.mean(X, axis=0)
            std = np.std(X, axis=0)
        return (X - mean) / std

X = np.loadtxt("../data/Q4/q4x.dat")

with open("../data/Q4/q4y.dat", "r") as f:
    y = np.array([1 if label.strip() == "Canada" else 0 for label in f])

print(X.shape)
print(y.shape)

N = len(y)
print(N)
    
# train_size = int(0.5 * N)  # 800,000 samples for training
# X_train, y_train = X[0::2], y[0::2]
# X_test, y_test = X[1::2], y[1::2]

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
print("mu_0:", mu_0)
print("mu_1:", mu_1)
print("Sigma:", sigma)

# y_pred = gda.predict(X_test)
# print(y_pred)
# print(y_test)

cov_inv = np.linalg.inv(sigma)

w = cov_inv @ (mu_1 - mu_0)  
b = -0.5 * (mu_1.T @ cov_inv @ mu_1 - mu_0.T @ cov_inv @ mu_0)
print(f"here is w -&gt; {w} and here is b -&gt; {b}")

X = normalize(X, is_training=True)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")

x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)  
x2_vals = -(w[0] * x1_vals + b) / w[1] 

plt.plot(x1_vals, x2_vals, 'g-', label="Decision Boundary")

plt.xlabel("Growth ring diameter in Freshwater (x1)")
plt.ylabel("Growth ring diameter in Marine water (x2)")
plt.title("GDA Decision Boundary")
plt.legend()
plt.grid(True)

mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)
print("mu_0:", mu_0)
print("mu_1:", mu_1)
print("Sigma_0:", sigma_0)
print("Sigma_1:", sigma_1)

Sigma_0_inv = np.linalg.inv(gda.sigma_0)
Sigma_1_inv = np.linalg.inv(gda.sigma_1)
A = Sigma_0_inv - Sigma_1_inv
B = 2 * (gda.mu_1.T @ Sigma_1_inv - gda.mu_0.T @ Sigma_0_inv)
C = (gda.mu_0.T @ Sigma_0_inv @ gda.mu_0) - (gda.mu_1.T @ Sigma_1_inv @ gda.mu_1) - np.log(np.linalg.det(gda.sigma_0) / np.linalg.det(gda.sigma_1))

x1_vals = np.linspace(min(X[:, 0])-1, max(X[:, 0])+1, 100)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match71-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x2_vals = np.linspace(min(X[:, 1])-1, max(X[:, 1])+1, 100)
X1, X2 = np.meshgrid(x1_vals, x2_vals)
Z = np.zeros_like(X1)

for i in range(X1.shape[0]):
</FONT>    for j in range(X1.shape[1]):
        x = np.array([X1[i, j], X2[i, j]])
        Z[i, j] = x.T @ A @ x + B @ x + C

plt.contour(X1, X2, Z, levels=[0], colors='g')
plt.show()




import numpy as np
from gda import GaussianDiscriminantAnalysis
import matplotlib.pyplot as plt

def normalize(X, is_training=True):
        if is_training:
            mean = np.mean(X, axis=0)
            std = np.std(X, axis=0)
        return (X - mean) / std

X = np.loadtxt("../data/Q4/q4x.dat")

with open("../data/Q4/q4y.dat", "r") as f:
    y = np.array([1 if label.strip() == "Canada" else 0 for label in f])

print(X.shape)
print(y.shape)

N = len(y)
print(N)
    
# train_size = int(0.5 * N)  # 800,000 samples for training
# X_train, y_train = X[0::2], y[0::2]
# X_test, y_test = X[1::2], y[1::2]

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)
print("mu_0:", mu_0)
print("mu_1:", mu_1)
print("Sigma_0:", sigma_0)
print("Sigma_1:", sigma_1)


X = normalize(X, is_training=True)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")


Sigma_0_inv = np.linalg.inv(gda.sigma_0)
Sigma_1_inv = np.linalg.inv(gda.sigma_1)
A = Sigma_0_inv - Sigma_1_inv
B = 2 * (gda.mu_1.T @ Sigma_1_inv - gda.mu_0.T @ Sigma_0_inv)
C = (gda.mu_0.T @ Sigma_0_inv @ gda.mu_0) - (gda.mu_1.T @ Sigma_1_inv @ gda.mu_1) - np.log(np.linalg.det(gda.sigma_0) / np.linalg.det(gda.sigma_1))

x1_vals = np.linspace(min(X[:, 0])-1, max(X[:, 0])+1, 100)
x2_vals = np.linspace(min(X[:, 1])-1, max(X[:, 1])+1, 100)
X1, X2 = np.meshgrid(x1_vals, x2_vals)
Z = np.zeros_like(X1)

for a in range(X1.shape[0]):
    for b in range(X1.shape[1]):
        x = np.array([X1[a, b], X2[a, b]])
        Z[a, b] = x.T @ A @ x + B @ x + C

plt.contour(X1, X2, Z, levels=[0], colors='g')

plt.xlabel("Growth ring diameter in Freshwater (x1)")
plt.ylabel("Growth ring diameter in Marine water (x2)")
plt.title("GDA Decision Boundary")
plt.legend()
plt.grid(True)

plt.show()


</PRE>
</PRE>
</BODY>
</HTML>
