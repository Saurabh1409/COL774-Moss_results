<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_EU2KU.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_EU2KU.py<p><PRE>



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  
import time  

class LinearRegressor:
    def __init__(self):
        """
        Initialize the linear regressor model.
        """
        self.theta = None 
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Batch Gradient Descent.
        """
        m, n = X.shape  
        X = np.hstack([np.ones((m, 1)), X]) 
        tol=1e-6
        self.theta = np.zeros(n + 1) 
        
        def compute_cost(X, y, theta):
            predictions = X.dot(theta)
            errors = predictions - y
            return (1 / (2 * m)) * np.sum(errors ** 2)
        
        param_history = [self.theta.copy()]
        prev_cost = compute_cost(X, y, self.theta)

        while True:
<A NAME="1"></A><FONT color = #00FF00><A HREF="match141-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            predictions = X.dot(self.theta)
            errors = predictions - y
            gradients = (1 / m) * X.T.dot(errors)
            self.theta -= learning_rate * gradients 
            
            current_cost = compute_cost(X, y, self.theta)
</FONT>            param_history.append(self.theta.copy())

            if abs(prev_cost - current_cost) &lt; tol:  
                break
            
            prev_cost = current_cost
        
        return np.array(param_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        if self.theta is None:
            raise ValueError("Model is not yet trained. Call fit first.")
        
        X = np.hstack([np.ones((X.shape[0], 1)), X]) 
        return X.dot(self.theta)






from linear_regression import LinearRegressor
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D 

def main(): 
    X_raw = np.loadtxt("data/Q1/linearX.csv", delimiter=",")
    y = np.loadtxt("data/Q1/linearY.csv", delimiter=",")
    if X_raw.ndim == 1:
        X_raw = X_raw.reshape(-1, 1) 
    
    regressor = LinearRegressor()
    
  
    learning_rate = 0.01
    tol = 1e-6
    
    
    param_history = regressor.fit(X_raw, y, learning_rate=learning_rate)
    
    final_theta = param_history[-1]
    
    print("Learning Rate:", learning_rate)
    print("Stopping Tolerance:", tol)
    
    print("Final Parameters (theta0, theta1):", final_theta)
    
  
    plt.figure(figsize=(8, 5))
    plt.scatter(X_raw, y, label="Training Data")
    
    x_line = np.linspace(X_raw.min(), X_raw.max(), 100)
    X_line = x_line.reshape(-1, 1)
    y_line = regressor.predict(X_line)
    
    plt.plot(x_line, y_line, 'r-', label="Learned Hypothesis")
    plt.xlabel("Acidity (X)")
    plt.ylabel("Density (Y)")
    plt.title("Training Data and Learned Linear Regression Fit")
    plt.legend()
    plt.show()

   
    def compute_cost(X, y, theta):
        m = X.shape[0]
        predictions = X.dot(theta)
        errors = predictions - y
        return (1/(2*m)) * np.sum(errors**2)
    
   
    m = X_raw.shape[0]
    X = np.hstack([np.ones((m, 1)), X_raw])  
    

    theta0_range = 2 
    theta1_range = 10
    theta0_vals = np.linspace(final_theta[0] - theta0_range, final_theta[0] + theta0_range, 50)
    theta1_vals = np.linspace(final_theta[1] - theta1_range, final_theta[1] + theta1_range, 100)
    
    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
    J_vals = np.zeros_like(T0)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            t = np.array([T0[i, j], T1[i, j]])
            J_vals[i, j] = compute_cost(X, y, t)


    max_cost_value = np.max(J_vals)

  
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.8)
    ax.set_xlabel('theta0 (Intercept)')
    ax.set_ylabel('theta1 (Slope)')
    ax.set_zlabel('Cost J(theta)')
    ax.set_title("3D Surface of Cost Function J(theta)")

    ax.set_zlim(0, max_cost_value * 1.2) 
    param_history_costs = [compute_cost(X, y, t) for t in param_history]

    for i in range(len(param_history)):
        t0 = param_history[i][0]
        t1 = param_history[i][1]
        c = param_history_costs[i]

      
        ax.scatter(t0, t1, c, color='r', s=25)  

        plt.pause(0.2) 

    plt.show()

   

    fig, ax = plt.subplots(figsize=(8, 6))

   
    theta0_range = 50
    theta1_range = 30

    theta0_vals = np.linspace(final_theta[0] - theta0_range, final_theta[0] + theta0_range, 50)
    theta1_vals = np.linspace(final_theta[1] - theta1_range, final_theta[1] + theta1_range, 50)

    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
    J_vals = np.zeros_like(T0)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            t = np.array([T0[i, j], T1[i, j]])
            J_vals[i, j] = compute_cost(X, y, t)

    contour = ax.contour(T0, T1, J_vals, levels=50, cmap='coolwarm', alpha=0.75)

    ax.set_xlabel("theta0 (Intercept)", fontsize=12)
    ax.set_ylabel("theta1 (Slope)", fontsize=12)
    ax.set_title("Contour Plot of Cost Function with Gradient Descent Path", fontsize=14)

   
    param_history_costs = [compute_cost(X, y, t) for t in param_history]

    for i in range(len(param_history)):
        t0 = param_history[i][0]
        t1 = param_history[i][1]

       
        ax.plot(t0, t1, 'ro', markersize=4) 

       
        plt.pause(0.2)

    plt.show()



if __name__ == "__main__":
    main()



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  
from sampling_sgd import generate, StochasticLinearRegressor

def main():
    np.random.seed(42)


    N = 1000000
    true_theta = np.array([3.0, 1.0, 2.0])
    input_mean = np.array([3.0, -1.0])
    input_sigma = np.array([2.0, 2.0])
    noise_sigma = np.sqrt(2.0)

    X, y = generate(N, true_theta, input_mean, input_sigma, noise_sigma)

  
    cutoff = int(0.8 * N)
    X_train, y_train = X[:cutoff], y[:cutoff]
    X_test,  y_test  = X[cutoff:], y[cutoff:]

   
    model = StochasticLinearRegressor()
    model.max_epochs = 100000
    model.tolerance  = 1e-6

    param_histories_list = model.fit(X_train, y_train, learning_rate=0.001)

   
    batch_sizes = [1, 80, 8000, 800000]
    learned_thetas = []
    for i, r in enumerate(batch_sizes):
        phist = param_histories_list[i]
        final_theta = phist[-1, :] 
        learned_thetas.append(final_theta)

   
    print(" Learned Parameters with different batch sizes")
    for r, theta_ in zip(batch_sizes, learned_thetas):
        print(f"Batch size = {r}: {theta_}")

    X_train_aug = np.column_stack([np.ones(X_train.shape[0]), X_train])
    theta_closed_form = np.linalg.inv(X_train_aug.T @ X_train_aug) @ (X_train_aug.T @ y_train)
    print("\n Closed-form solution")
    print("Closed-form theta =", theta_closed_form)

  
    print("\n Comparison of True vs. Closed-form vs. SGD ")
    print("True theta:        ", true_theta)
    print("Closed-form theta: ", theta_closed_form)
    for r, theta_ in zip(batch_sizes, learned_thetas):
        print(f"SGD (r={r}) theta: ", theta_)

   
    def mse(theta_vals, X_data, y_data):
      
        X_aug = np.column_stack([np.ones(X_data.shape[0]), X_data])
        y_pred = X_aug @ theta_vals
        return np.mean((y_pred - y_data)**2)

    print("\n Training and Test Errors (MSE)")
    for r, theta_ in zip(batch_sizes, learned_thetas):
        train_error = mse(theta_, X_train, y_train)
        test_error  = mse(theta_, X_test,  y_test)
        print(f"Batch size {r}: Training MSE={train_error:.4f}, Test MSE={test_error:.4f}")

    
    train_error_cf = mse(theta_closed_form, X_train, y_train)
    test_error_cf  = mse(theta_closed_form, X_test,  y_test)
    print("\nClosed-form: Training MSE={:.4f}, Test MSE={:.4f}".format(train_error_cf, test_error_cf))

    fig = plt.figure(figsize=(12, 10))

    for idx, r in enumerate(batch_sizes, start=1):
        ax = fig.add_subplot(2, 2, idx, projection='3d')
        phist = param_histories_list[idx - 1] 
        ax.plot(phist[:, 0], phist[:, 1], phist[:, 2], marker='o')
        ax.set_title(f"SGD Trajectory (batch_size={r})")
        ax.set_xlabel(r"$\theta_0$")
        ax.set_ylabel(r"$\theta_1$")
        ax.set_zlabel(r"$\theta_2$")

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()




import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    We have 2 input features: x1, x2.
    For each sample i:
        x1[i] ~ N(input_mean[0], input_sigma[0]^2),
        x2[i] ~ N(input_mean[1], input_sigma[1]^2),
        y[i]  = theta[0] + theta[1]*x1[i] + theta[2]*x2[i] + epsilon
    where epsilon ~ N(0, noise_sigma^2).
    
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model, [theta0, theta1, theta2].
        
    input_mean : numpy array of shape (2,)
        The mean of the input data [mean_x1, mean_x2].
        
    input_sigma : numpy array of shape (2,)
        The *standard deviations* of the input data [std_x1, std_x2].
        
    noise_sigma : float
        The standard deviation of the Gaussian noise in y.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data (two features per sample).
        
    y : numpy array of shape (N,)
        The target values.
    """
   
    mu1, mu2 = input_mean
    std1, std2 = input_sigma

   
    x1 = np.random.normal(mu1, std1, N)
    x2 = np.random.normal(mu2, std2, N)

    epsilon = np.random.normal(0.0, noise_sigma, N)

  
    y = theta[0] + theta[1]*x1 + theta[2]*x2 + epsilon

    X = np.vstack((x1, x2)).T  
    return X, y


class StochasticLinearRegressor:
    def __init__(self):
        """
        You can define default hyperparameters here, if desired.
        For example, batch_size, max_epochs, tolerance, etc.
        """
        self.theta = None            
        self.batch_size = 1          
        self.max_epochs = 100      
        self.tolerance = 1e-6       
        self.param_history = []      
    
    def _augment_X(self, X):
        """
        Internal helper function:
        Given X of shape (n_samples, n_features),
        return an augmented copy with a leading column of 1's.
        This way, we incorporate the intercept (theta0) into the parameter vector.
        """
        n_samples = X.shape[0]
        ones_col = np.ones((n_samples, 1))
        return np.hstack((ones_col, X))  
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model (SGD) for the four fixed batch sizes:
            [1, 80, 8000, 800000].
        
        For each batch size 'r':
        1) Shuffle the data
        2) Initialize self.theta = 0
        3) Run mini-batch SGD up to self.max_epochs (or until convergence)
        4) Record a 'local' param_history after each mini-batch update
        5) Store that array (local_param_history) in a master list
        
        Finally, return the list of 4 param_history arrays, in the order:
            [r=1, r=80, r=8000, r=800000].
        
        Each array has shape (n_updates, n_features+1), where n_updates can differ
        depending on how many epochs/batches happened before convergence.
        """
       
        batch_sizes = [1, 80, 8000, 800000]

       
        param_histories_list = []
        iterations_list = [] 

      
        X_aug = self._augment_X(X)  
        n_samples, n_features = X_aug.shape
        
        for r in batch_sizes:
           
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_shuffled = X_aug[indices]
            y_shuffled = y[indices]

           
            self.theta = np.zeros(n_features) 

        
            local_param_history = []
            local_param_history.append(self.theta.copy())

           
            num_batches = n_samples // r
            if n_samples % r != 0:
                num_batches += 1

            num_iterations = 0
           
            for epoch in range(self.max_epochs):
                old_theta = self.theta.copy()

                for b in range(num_batches):
                    num_iterations += 1 
                    start = b * r
                    end   = min(start + r, n_samples)
                    X_batch = X_shuffled[start:end, :]
                    y_batch = y_shuffled[start:end]

                   
                    y_pred = X_batch @ self.theta
                    errors = y_pred - y_batch

                    
                    current_batch_size = end - start  
                    grad = (X_batch.T @ errors) / current_batch_size

                  
                    self.theta = self.theta - learning_rate * grad

                   
                    local_param_history.append(self.theta.copy())

               
                if np.linalg.norm(self.theta - old_theta) &lt; self.tolerance:
                    break
                    
            iterations_list.append(num_iterations)
           
            local_param_history = np.array(local_param_history)
            param_histories_list.append(local_param_history)

            print(f"Batch size {r}: Converged in {num_iterations} iterations")

        # Return 4 arrays, in order of batch_sizes = [1, 80, 8000, 800000]
        return param_histories_list

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data (without intercept).
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_aug = self._augment_X(X)  
      
        y_pred = X_aug @ self.theta
        return y_pred




import numpy as np
import pandas as pd
<A NAME="0"></A><FONT color = #FF0000><A HREF="match141-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
        self.theta = None
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def compute_gradient(self, X, y, h):
</FONT>        return X.T @ (h - y)
    
    def compute_hessian(self, X, h):
        m, n = X.shape
        S = np.diag((h * (1 - h)).flatten())
        return X.T @ S @ X
    
    def fit(self, X, y,  learning_rate=0.01 ):
        X = self.normalize(X)
        max_iters=10
        tol=1e-6
<A NAME="2"></A><FONT color = #0000FF><A HREF="match141-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        m, n = X.shape
        X = np.hstack((np.ones((m, 1)), X)) 
        self.theta = np.zeros((n + 1, 1))
</FONT>        y = y.reshape(-1, 1)
        
        for i in range(max_iters):
            z = X @ self.theta
            h = self.sigmoid(z)
            grad = self.compute_gradient(X, y, h)
            H = self.compute_hessian(X, h)
            
            theta_update = np.linalg.inv(H) @ grad
            self.theta -= theta_update
            
            print(f"Iteration {i+1}: \nTheta: {self.theta.flatten()}\n")
            
            if np.linalg.norm(theta_update) &lt; tol:
                break
    
    def predict(self, X):
        X = self.normalize(X)
        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X)) 
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)
    
    def plot_decision_boundary(self, X, y):
        plt.figure(figsize=(8, 6))
        X_norm = self.normalize(X)
        
        plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', label='Class 0', edgecolors='k')
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match141-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', label='Class 1', edgecolors='k')
        
        x_values = np.linspace(np.min(X_norm[:, 0]), np.max(X_norm[:, 0]), 100)
</FONT>        y_values = -(self.theta[0] + self.theta[1] * x_values) / self.theta[2]
        
        plt.plot(x_values, y_values, 'r-', label='Decision Boundary')
        plt.xlabel('X1')
        plt.ylabel('X2')
        plt.legend()
        plt.title('Logistic Regression Decision Boundary')
        plt.show()







import numpy as np
from logistic_regression import LogisticRegressor

def main():
   
    X_raw = np.loadtxt("data/Q3/logisticX.csv", delimiter=",")
    y = np.loadtxt("data/Q3/logisticY.csv", delimiter=",")
   
    model = LogisticRegressor()
    model.fit(X_raw, y, learning_rate=0.01)
    
   
    print("Final Coefficients (Theta):", model.theta.flatten())

   
    model.plot_decision_boundary(X_raw, y)

if __name__ == "__main__":
    main()




import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
    
    def normalize(self, X):
        """
        Normalize the dataset X.
        """
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match141-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def fit(self, X, y, assume_same_covariance=False):
</FONT>        """
        Fit the Gaussian Discriminant Analysis model to the data.
        """
       
        y_binary = (y == 'Canada').astype(int)
        self.phi = np.mean(y_binary)  
        
      
        self.mu_0 = np.mean(X[y_binary == 0], axis=0)
        self.mu_1 = np.mean(X[y_binary == 1], axis=0)
        
        if assume_same_covariance:
           
            m = X.shape[0]
            sigma = np.zeros((X.shape[1], X.shape[1]))
            for i in range(m):
                x_i = X[i].reshape(-1, 1)
                mu = self.mu_1 if y_binary[i] == 1 else self.mu_0
                mu = mu.reshape(-1, 1)
                sigma += (x_i - mu) @ (x_i - mu).T
            self.sigma = sigma / m
            return self.mu_0, self.mu_1, self.sigma
        else:
           
            self.sigma_0 = np.cov(X[y_binary == 0], rowvar=False, bias=True)
            self.sigma_1 = np.cov(X[y_binary == 1], rowvar=False, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        (For the shared-covariance case.)
        """
        inv_sigma = np.linalg.inv(self.sigma)
        
       
        term_1 = -0.5 * np.dot(np.dot(self.mu_1.T, inv_sigma), self.mu_1)
        term_2 = 0.5 * np.dot(np.dot(self.mu_0.T, inv_sigma), self.mu_0)
        term_3 = np.dot(np.dot(X, inv_sigma), (self.mu_1 - self.mu_0))
        term_4 = np.log(self.phi / (1 - self.phi))
   
        logits = term_1 + term_2 + term_3 + term_4
        y_pred = (logits &gt;= 0).astype(int)
        
        return y_pred


   
    



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis


def main():
    X = np.loadtxt("data/Q4/q4x.dat")
    
    y = np.loadtxt("data/Q4/q4y.dat", dtype=str)
    
    gda = GaussianDiscriminantAnalysis()
    X = gda.normalize(X) 
 
    mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
    print("Case 1: Shared Covariance")
    print("Mean for Alaska (Class 0):", mu_0)
    print("Mean for Canada (Class 1):", mu_1)
    print("Shared Covariance Matrix:")
    print(sigma)
    
    
    y_binary = (y == 'Canada').astype(int)
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y_binary == 0, 0], X[y_binary == 0, 1], 
                marker='o', label='Alaska')
    plt.scatter(X[y_binary == 1, 0], X[y_binary == 1, 1], 
                marker='x', label='Canada')
    
   
    inv_sigma = np.linalg.inv(sigma)
    w = inv_sigma @ (mu_1 - mu_0)
   
    b = (-0.5 * (mu_1.T @ inv_sigma @ mu_1)
         + 0.5 * (mu_0.T @ inv_sigma @ mu_0)
         + np.log(gda.phi / (1 - gda.phi)))
    
    x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
    y_vals = -(w[0] * x_vals + b) / w[1]
    
    plt.plot(x_vals, y_vals, label='Linear Boundary', color='red')
    
   
    mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)
    print("\nCase 2: Different Covariance Matrices")
    print("Mean for Alaska (Class 0):", mu_0)
    print("Mean for Canada (Class 1):", mu_1)
    print("Covariance Matrix for Alaska (Class 0):")
    print(sigma_0)
    print("Covariance Matrix for Canada (Class 1):")
    print(sigma_1)
    
    inv_sigma_0 = np.linalg.inv(sigma_0)
    inv_sigma_1 = np.linalg.inv(sigma_1)
    
    
    xx, yy = np.meshgrid(
        np.linspace(np.min(X[:,0]) - 1, np.max(X[:,0]) + 1, 200),
        np.linspace(np.min(X[:,1]) - 1, np.max(X[:,1]) + 1, 200)
    )
<A NAME="5"></A><FONT color = #FF0000><A HREF="match141-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    grid = np.c_[xx.ravel(), yy.ravel()]
    
   
    
    z = []
    for point in grid:
        x = point.reshape(-1, 1)  
</FONT>        
       
        diff1 = x - mu_1.reshape(-1, 1)
        quad1 = diff1.T @ inv_sigma_1 @ diff1
       
        diff0 = x - mu_0.reshape(-1, 1)
        quad0 = diff0.T @ inv_sigma_0 @ diff0
     
        val = -0.5 * (quad1 - quad0)
        val += -0.5 * np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
        val += np.log(gda.phi / (1 - gda.phi))
        
        z.append(val[0,0])  

    z = np.array(z).reshape(xx.shape)
    
  
    plt.contour(xx, yy, z, levels=[0], colors='blue', linewidths=2)
    plt.plot([], [], label='Quadratic Boundary', color='blue')
    
 
    plt.xlabel("Feature x1 (Growth ring diameter in fresh water)")
    plt.ylabel("Feature x2 (Growth ring diameter in marine water)")
    plt.title("Alaska vs Canada Salmon with Linear and Quadratic Boundaries")
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
