<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_2VT93.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_2VT93.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt 
class LinearRegressor:
    def __init__(self):
        self.parameters = None
        self.history=[]

    def cost(self, X, Y,theta):
        m = X.shape[0]
        predictions = X @ theta   
        errors = Y - predictions
        cost_value = np.sum(errors ** 2) / (2 * m)
        return cost_value

    def fit(self, X, Y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        parameters = np.zeros(n + 1)
        self.stopping_criteria = 1e-9
        cost_function = 1e15

        X_bias = np.zeros((m, X.shape[1] + 1))  
        for i in range(m):
            X_bias[i][0] = 1
        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]

        it = 0
        while True:
            it += 1
            
            # Hypothesis: h(x) = X_bias @ parameters
            h_x = X_bias @ parameters
            errors = Y - h_x
            new_cost_function = (errors @ errors) / (2 * m)
        #    print(parameters,new_cost_function,"hi")
            if abs(new_cost_function - cost_function) &lt; self.stopping_criteria:
           #     print(f"Converged after {it - 1} iterations.")
                break

            cost_function = new_cost_function
            self.history.append(parameters.copy())
            # Compute gradient: (-1/m) * (X_bias.T @ errors)
            gradient = -X_bias.T @ errors / m
            learning_rate=0.08
            # Update parameters
            parameters =parameters- learning_rate * gradient

        self.parameters = parameters.tolist()
    #    print("Final Parameters:", self.parameters)
        return np.array(parameters)

    def predict(self, X):
        m,n =X.shape 
        X_bias = np.zeros((m, X.shape[1] + 1))  
        for i in range(m):
            X_bias[i][0] = 1
        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]
        pred = X_bias@self.parameters
        return pred 




import numpy as np 
import matplotlib.pyplot as plt 
import linear_regression
from linear_regression import LinearRegressor

def plot_cost_function(X, Y,history):
  #  print(history)
    m,n =X.shape 
    
    X_bias = np.zeros((m, X.shape[1] + 1)) 

    for i in range(m):
        X_bias[i][0] = 1
    for i in range(m):
        for j in range(X.shape[1]):
            X_bias[i][j + 1] = X[i][j]

    theta_0_vals = np.linspace(-10, 10, 100)
    theta_1_vals = np.linspace(-10, 30, 100)
    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

    for i, theta_0 in enumerate(theta_0_vals):
        for j, theta_1 in enumerate(theta_1_vals):
            theta = np.array([theta_0, theta_1])
            J_vals[i, j] = LinearRegressor().cost(X_bias, Y, theta)
      #      if (theta_0&gt;=6 and theta_0&lt;=7)and (theta_1&gt;=29 and theta_1&lt;=30):
       #         print(theta,J_vals[i,j])
    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta_0_vals, theta_1_vals, J_vals, cmap='viridis', edgecolor='none')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$J(\theta)$')
    ax.set_title('Cost Function Surface')
    for theta in history:
        cost_value = LinearRegressor().cost(X_bias, Y, theta)
        ax.scatter(theta[0], theta[1], cost_value, color='red', s=40, label=None)
        plt.legend()
        plt.pause(0.2)
    plt.show()
def plot_contours_with_gradient_descent(X, Y, history):
    m,n =X.shape 
    X_bias = np.zeros((m, X.shape[1] + 1))  

    for i in range(m):
        X_bias[i][0] = 1

    for i in range(m):
        for j in range(X.shape[1]):
            X_bias[i][j + 1] = X[i][j]

    theta_0_vals = np.linspace(-5, 30, 500)
    theta_1_vals = np.linspace(-5, 100, 500)
    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

    for i, theta_0 in enumerate(theta_0_vals):
        for j, theta_1 in enumerate(theta_1_vals):
            theta = np.array([theta_0, theta_1])
            J_vals[i, j] = LinearRegressor().cost(X_bias, Y, theta)

    plt.figure(figsize=(12, 8))
    plt.contour(theta_0_vals, theta_1_vals, J_vals, levels=np.logspace(-5, 10, 30), cmap='viridis')
  #  plt.contour(theta_0_vals, theta_1_vals, J_vals, levels=20, cmap='viridis')
    
    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title('Contours of Cost Function with Gradient Descent Path')

    for i, theta in enumerate(history):
        plt.scatter(theta[0], theta[1], color='red', s=5)
        if i == 0:
            plt.annotate("Start", (theta[0], theta[1]), color='blue')
        plt.pause(0.2)


    plt.show()
X = np.loadtxt("linearX.csv", delimiter=",", dtype=float)
if X.ndim == 1:  
    X = X.reshape(-1, 1)
Y = np.loadtxt("linearY.csv", delimiter=",", dtype=float)
split_index = int(0.7 * X.shape[0])

X1, X2 = X[:split_index], X[split_index:]
Y1, Y2 = Y[:split_index], Y[split_index:]

l = LinearRegressor()
model_parameters = l.fit(X, Y)
print("Cost on validation data:", l.cost(np.c_[np.ones((X2.shape[0], 1)), X2], Y2,model_parameters))
predictions =l.predict(X)
#plt.scatter(X, Y, color='blue', label='Data points')
#plt.plot(X, predictions, color='red', label='Hypothesis function')
#plt.xlabel('X')
#plt.ylabel('Y')
#plt.legend()
#plt.show()
#plot_cost_function(X,Y,l.history)
plot_contours_with_gradient_descent(X,Y,l.history)




import numpy as np 
import matplotlib.pyplot as plt 
from sampling_sgd import StochasticLinearRegressor,generate
import math 
def plot_parameter_movement(parameter_history):
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot(parameter_history[:, 0], parameter_history[:, 1], parameter_history[:, 2], marker='o')

        ax.set_xlabel('θ0 (bias)')
        ax.set_ylabel('θ1')
        ax.set_zlabel('θ2')

        ax.set_title('Movement of Parameters in 3D Space')
        plt.show()


    
N=10**6
theta =np.array([3,1,2])
input_mean=np.array([3,-1])
input_sigma =np.array([2,2])
noise_sigma =math.sqrt(2)    
split_index=800000

X,Y=generate(N,theta,input_mean,input_sigma,noise_sigma)
X1, X2 = X[:split_index], X[split_index:]
Y1, Y2 = Y[:split_index], Y[split_index:]

S = StochasticLinearRegressor()
#model_parameters2=S.fit_closed_form(X1,Y1)
model_parameters = S.fit(X1, Y1)
predictions1 =S.predict(X2)
predictions2=S.predict(X1)
m=200000
cost1 = np.sum((predictions1 - Y2) ** 2) / (2 * m)
m=800000
cost2 = np.sum((predictions2 - Y1) ** 2) / (2 * m)
print(cost1,cost2)
#print(model_parameters[0])
plot_parameter_movement(model_parameters[0])
#plt.scatter(X, Y, color='blue', label='Data points')
#plt.plot(X, predictions, color='red', label='Hypothesis function')
#plt.xlabel('X')
#plt.ylabel('Y')
#plt.legend()
#plt.show()







# Imports - you can add any other permitted libraries
import numpy as np
import math 
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features.

    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    def generate_data_in_loop(N, input_mean, input_sigma):
        X = np.zeros((N, 2))  

        for i in range(N):
            for j in range(2):  
                X[i][j] = np.random.normal(loc=input_mean[j], scale=input_sigma[j])
        
        return X
    X = generate_data_in_loop(N,input_mean,input_sigma)
    
    def generate_noise_in_loop(N, noise_sigma):
        noise = np.zeros(N) 

        for i in range(N):
            noise[i] = np.random.normal(loc=0, scale=noise_sigma)
    
        return noise
    noise = generate_noise_in_loop(N,noise_sigma)
    
    P=X@theta[1:]
    Y = theta[0] +P + noise

    return X, Y

class StochasticLinearRegressor:
    def __init__(self):
        self.parameters = []
        self.parameter_history=[]
    def gradient_magnitude(self, gradient):
        sum_of_squares = 0
        
        for g in gradient:
            sum_of_squares += g ** 2
        return np.sqrt(sum_of_squares)
    def compute_norm(self,vec1, vec2):
        
        sum_squared_diff = 0
        
        for i in range(len(vec1)):
            sum_squared_diff += (vec1[i] - vec2[i]) ** 2
        return math.sqrt(sum_squared_diff)

    def fit(self, X, Y, learning_rate=0.01):
        """
        Fit the linear regression model using Mini-batch Stochastic Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
        
        r : int
            Batch size for mini-batch gradient descent.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        max_epochs = 100000
        m,n =X.shape 
      #  r=800000
        X_bias = np.zeros((m, X.shape[1] + 1))  

        for i in range(m):
            X_bias[i][0] = 1

        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]        
        stopping_criteria = 1e-6
        parameter_history = []
        
     #   current_cost=0
        result_list=[]
        self.parameters=[]
        for r in [1,80,8000,800000]:
         #   print(r)
            print(self.parameters)
            x=False 
            it=0
            previous_cost = 1e15
            self.sub_parameters = np.zeros(n + 1)
            if r==1:
                for epoch in range(20):
                    indices = np.arange(m)
                    np.random.shuffle(indices)
                    X_shuffled = X_bias[indices]
                    Y_shuffled = Y[indices]

                    for i in range(0, m, r):
                    #    print(epoch,i)
                        # Select mini-batch of size r
                        X_batch = X_shuffled[i:i + r]
                        Y_batch = Y_shuffled[i:i + r]
                        predictions = X_batch @ self.sub_parameters
                        errors = predictions - Y_batch
                        gradient = (X_batch.T @ errors) / r
                        learning_rate=0.001
                        # Update parameters
                        self.sub_parameters =self.sub_parameters-learning_rate * gradient

                        # Record parameter history
                        
                        it+=1 
                        parameter_history.append(self.sub_parameters.copy())
                # parameter_change = self.compute_norm(self.parameters, previous_parameters)
                # print(parameter_change)
                # if parameter_change &lt; epsilon:
                #     print(f"Converged after {epoch} epochs, {it} iterations.")
                #     break
                # previous_parameters = np.copy(self.parameters)
                    # Compute cost for monitoring convergence
               #        gradient_magnitude_value = self.gradient_magnitude(gradient)
            #      print(gradient_magnitude_value,"gradient")
            #     print(self.parameters)
                #    if gradient_magnitude_value &lt; epsilon:
                #       print(f"Converged after {epoch} epochs, {it} iterations.")
                #      break
                    current_cost = self.cost(X_bias, Y,self.sub_parameters)
                #  print(previous_cost-current_cost)
                    # Check convergence
                    if abs(previous_cost - current_cost) &lt; stopping_criteria:
                #        print(f"Converged after {epoch} epochs.,{it-1} iterations ")
                        x=True
                        break
                    print(epoch,r)
                    previous_cost = current_cost
                    if x:
                        break 
                self.parameters.append(self.sub_parameters)
                result_list.append(np.array(parameter_history))
                parameter_history=[]
                continue 
            else:
                for epoch in range(max_epochs):
                    indices = np.arange(m)
                    np.random.shuffle(indices)
                    X_shuffled = X_bias[indices]
                    Y_shuffled = Y[indices]

                    for i in range(0, m, r):
                    #    print(epoch,i)
                        # Select mini-batch of size r
                        X_batch = X_shuffled[i:i + r]
                        Y_batch = Y_shuffled[i:i + r]

                        predictions = X_batch @ self.sub_parameters
                        errors = predictions - Y_batch

                        gradient = (X_batch.T @ errors) / r
                        learning_rate=0.001
                        # Update parameters
                        self.sub_parameters =self.sub_parameters- learning_rate * gradient

                        # Record parameter history
                        
                        it+=1 
                        parameter_history.append(self.sub_parameters.copy())
                # parameter_change = self.compute_norm(self.parameters, previous_parameters)
                # print(parameter_change)
                # if parameter_change &lt; epsilon:
                #     print(f"Converged after {epoch} epochs, {it} iterations.")
                #     break
                # previous_parameters = np.copy(self.parameters)
                    # Compute cost for monitoring convergence
            #       gradient_magnitude_value = self.gradient_magnitude(gradient)
            #      print(gradient_magnitude_value,"gradient")
            #     print(self.parameters)
                #    if gradient_magnitude_value &lt; epsilon:
                #       print(f"Converged after {epoch} epochs, {it} iterations.")
                #      break
                    current_cost = self.cost(X_bias, Y,self.sub_parameters)
                #  print(previous_cost-current_cost)
                    # Check convergence
                    if abs(previous_cost - current_cost) &lt; stopping_criteria:
                        print(f"Converged after {epoch} epochs.,{it-1} iterations ")
                        x=True
                        break
                    print(epoch,r)
                    previous_cost = current_cost
                    if x:
                        break 
                self.parameters.append(self.sub_parameters)   
                result_list.append(np.array(parameter_history))
             #   self.parameter_history=np.array(parameter_history)
                parameter_history=[]
    #    print(self.parameters)
     #   self.parameter_history = np.array(parameter_history)

        return result_list
    
    def cost(self, X_bias, Y,parameters):
        m = len(Y)
        predictions = X_bias @ parameters
        cost = np.sum((predictions - Y) ** 2) / (2 * m)
        return cost

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        print(self.parameters)
        res=[]
        for j in range(len(self.parameters)):
            m,n =X.shape 
            X_bias = np.zeros((m, X.shape[1] + 1))  
            for i in range(m):
                X_bias[i][0] = 1
            for i in range(m):
                for j in range(X.shape[1]):
                    X_bias[i][j + 1] = X[i][j]
            pred = X_bias@self.parameters[j]
            res.append(pred)
        return res
    def fit_closed_form(self, X, Y):
        
        m,n =X.shape 
        X_bias = np.zeros((m, X.shape[1] + 1))  
        for i in range(m):
            X_bias[i][0] = 1
        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]
        X_bias[:, 1:] = X  
        
        # θ = (X^T X)^-1 X^T y
        X_transpose = X_bias.T  
        theta = np.linalg.inv(X_transpose @ X_bias) @ (X_transpose @ Y)
        self.parameters=theta 
     #   print(theta)
        return theta
    



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt 
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    def __init__(self):
        self.parameters=[]
        pass
    def normalise(self,X):
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std
        return X
    def fit(self, X, Y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        parameter_history=[]
        m,n=X.shape 
        X_bias = np.zeros((m, X.shape[1] + 1))  

        for i in range(m):
            X_bias[i][0] = 1

        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]
        X=X_bias
        self.parameters=np.zeros(n+1)
        epsilon=1e-6 
        max_iterations=10000
        it=0
        while it&lt;=max_iterations:
            it+=1 
            z=X@self.parameters 
            h= 1/(1+np.exp(-z))
            gradient = X.T @(h-Y)
            W = np.zeros((m, m))  
            for i in range(m):
                W[i, i] = h[i] * (1 - h[i])
            H=X.T @W @X 
            try:
                delta = np.linalg.inv(H) @ gradient
            except np.linalg.LinAlgError:
            #    print("Hessian is singular, stopping iteration.")
                break
            self.parameters -=delta 
            parameter_history.append(self.parameters.copy())
            error =0.0 
            for i in delta :
                error+=abs(i)
            if error&lt;epsilon:
             #   print(f"Converged after {it + 1} iterations.")
                break
      #  print(self.parameters)
        return np.array(parameter_history)
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m,n =X.shape 
        X_bias = np.zeros((m, X.shape[1] + 1))  

        for i in range(m):
            X_bias[i][0] = 1
        for i in range(m):
            for j in range(X.shape[1]):
                X_bias[i][j + 1] = X[i][j]
        z=X_bias@self.parameters
        h=1/(1+np.exp(-z))
        pred = np.zeros_like(h)  

        for i in range(len(h)):
            if h[i] &gt;= 0.5:
                pred[i] = 1
            else:
                pred[i] = 0
        return pred 
        pass
    



import numpy as np 
import matplotlib.pyplot as plt 
import logistic_regression
from logistic_regression import LogisticRegressor
def plot_decision_boundary(X, Y,parameters):
        plt.figure(figsize=(12, 8))
        plt.scatter(X[Y == 0, 0], X[Y == 0, 1], color='red', marker='o', label='Label 0')
        plt.scatter(X[Y == 1, 0], X[Y == 1, 1], color='blue', marker='x', label='Label 1')

        theta_0, theta_1, theta_2 = parameters

        # θ0 + θ1 * x1 + θ2 * x2 = 0
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match180-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x1_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)
        x2_values = - (theta_0 + theta_1 * x1_values) / theta_2
</FONT>        plt.plot(x1_values, x2_values, color='black', linewidth=2, label='Decision Boundary')

        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.title('Logistic Regression Decision Boundary')
        plt.legend(loc='best')
        plt.show()


X = np.loadtxt("logisticX.csv", delimiter=",", dtype=float)
if X.ndim == 1:  
    X = X.reshape(-1, 1)
Y = np.loadtxt("logisticY.csv", delimiter=",", dtype=float)
split_index = int(0.8 * X.shape[0])

X1, X2 = X[:split_index], X[split_index:]
Y1, Y2 = Y[:split_index], Y[split_index:]
l = LogisticRegressor()
model_parameters = l.fit(X, Y)
plot_decision_boundary(X,Y,model_parameters[-1])




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt 
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu0=None 
        self.mu1=None 
        self.sigma0=None 
        self.sigma1=None 
        self.sigma=None 
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="2"></A><FONT color = #0000FF><A HREF="match180-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # (0 for Alaska, 1 for Canada)
        m = X.shape[0]
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
</FONT>        mu_0 = np.mean(X[y == 0], axis=0)  
<A NAME="0"></A><FONT color = #FF0000><A HREF="match180-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        mu_1 = np.mean(X[y == 1], axis=0)  
        if assume_same_covariance:
            sigma = np.zeros((X.shape[1], X.shape[1]))
            for i in range(m):
                xi = X[i].reshape(-1, 1)
                mu =mu_1
</FONT>                if y[i]==0:
                    mu =mu_0
                mu = mu.reshape(-1, 1)
                sigma += (xi - mu) @ (xi - mu).T
            sigma /= m
            self.mu0=mu_0
            self.mu1=mu_1
            self.sigma=sigma 
           
        else:
            X_0 = X[y == 0]
            X_1 = X[y == 1]

            sigma_0 = np.zeros((X.shape[1], X.shape[1]))
            sigma_1 = np.zeros((X.shape[1], X.shape[1]))

            N_0 = X_0.shape[0]
            N_1 = X_1.shape[0]
            for i in range(N_1):
                diff = (X_1[i] - mu_1).reshape(-1, 1)
                sigma_1 += diff @ diff.T
            sigma_1 /= N_1
            for i in range(N_0):
                diff = (X_0[i] - mu_0).reshape(-1, 1)
                sigma_0 += diff @ diff.T
            sigma_0 /= N_0

            

            self.mu0=mu_0
            self.mu1=mu_1
            self.sigma0 =sigma_0
            self.sigma1=sigma_1
        if assume_same_covariance:
            return mu_0, mu_1, sigma
        else:
            return mu_0, mu_1, sigma_0, sigma_1

        pass
        
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
            
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        sigma_inv = np.linalg.inv(self.sigma)
        w = sigma_inv @ (self.mu1 - self.mu0)
        b = 0.5 * (self.mu0.T @ sigma_inv @ self.mu0 - self.mu1.T @ sigma_inv @ self.mu1)
        decision_values = X @ w - b
        y_pred = (decision_values &gt; 0).astype(int)
        return y_pred
        pass





import numpy as np
import matplotlib.pyplot as plt 
from gda import GaussianDiscriminantAnalysis
def plot_with_decision_boundary(X, y, mu_0, mu_1, sigma):
    plt.figure(figsize=(8, 6))
    y_binary = np.array([0 if label == "Alaska" else 1 for label in y])
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    Sigma_inv = np.linalg.inv(sigma)
    w = Sigma_inv @ (mu_1 - mu_0)
    b = 0.5 * (mu_0 + mu_1) @ w  
    x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    x2_vals = (-w[0] * x1_vals + b) / w[1]

    # Scatter plot of data
    alaska_points = X[y_binary == 0]
    canada_points = X[y_binary == 1]
    plt.scatter(alaska_points[:, 0], alaska_points[:, 1], marker='o', label="Alaska", color='blue')
    plt.scatter(canada_points[:, 0], canada_points[:, 1], marker='x', label="Canada", color='red')
<A NAME="5"></A><FONT color = #FF0000><A HREF="match180-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot(x1_vals, x2_vals, 'k-', label="Decision Boundary")

    plt.xlabel("growth ring diameters in fresh water")
    plt.ylabel("growth ring diameters in marine")
    plt.title("Training Data & GDA Decision Boundary")
    plt.legend()
    plt.grid(True)
    plt.show()
</FONT>   

def quadratic_boundary_equation(mu_0, mu_1, sigma_0, sigma_1):
    """
    Computes and prints the quadratic decision boundary equation in standard form:
    ax1^2 + bx2^2 + c*x1*x2 + d*x1 + e*x2 + f = 0
    """
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)

    # Quadratic terms
    A = sigma_0_inv - sigma_1_inv  # 2x2 matrix

    # Linear terms
    B = 2 * (sigma_1_inv @ mu_1 - sigma_0_inv @ mu_0)  # 2x1 vector

    # Constant term
<A NAME="1"></A><FONT color = #00FF00><A HREF="match180-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    C = (mu_0.T @ sigma_0_inv @ mu_0) - (mu_1.T @ sigma_1_inv @ mu_1) + np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))

    a = A[0, 0]  # Coefficient of x1^2
</FONT>    b = A[1, 1]  # Coefficient of x2^2
    c = A[0, 1] + A[1, 0]  # Coefficient of x1 * x2 (since A is symmetric)
    d = B[0]  # Coefficient of x1
    e = B[1]  # Coefficient of x2
    f = C  # Constant term

    print("\nQuadratic Decision Boundary Equation:")
    print(f"{a:.4f} * x1² + {b:.4f} * x2² + {c:.4f} * x1*x2 + {d:.4f} * x1 + {e:.4f} * x2 + {f:.4f} = 0\n")

    return a, b, c, d, e, f

def plot_quadratic_boundary(X, y, mu_0, mu_1, sigma_0, sigma_1):
    
    a, b, c, d, e, f = quadratic_boundary_equation(mu_0, mu_1, sigma_0, sigma_1)

    plt.figure(figsize=(8, 6))
    y_binary = np.array([0 if label == "Alaska" else 1 for label in y])
    plt.scatter(X[y_binary == 0, 0], X[y_binary == 0, 1], marker='o', color='blue', label="Alaska")
    plt.scatter(X[y_binary == 1, 0], X[y_binary == 1, 1], marker='x', color='red', label="Canada")
    x_min, x_max = X[:, 0].min() - 10, X[:, 0].max() + 10
    y_min, y_max = X[:, 1].min() - 10, X[:, 1].max() + 10
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))

    # Compute decision function for each grid point
    Z = a * xx**2 + b * yy**2 + c * xx * yy + d * xx + e * yy + f
    plt.contour(xx, yy, Z, levels=[0], colors='green', linewidths=2, linestyles='dashed')
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("Training Data & Quadratic GDA Decision Boundary")
    plt.legend()
    plt.grid(True)
    plt.show()
def plot_combined_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1):

    plt.figure(figsize=(8, 6))
    y_binary = np.array(y)
  #  print(y_binary)
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    plt.scatter(X[y_binary == 0, 0], X[y_binary == 0, 1], marker='o', color='blue', label="Alaska")
    plt.scatter(X[y_binary == 1, 0], X[y_binary == 1, 1], marker='x', color='red', label="Canada")

    Sigma_inv = np.linalg.inv(sigma)
    w = Sigma_inv @ (mu_1 - mu_0)  # Weight vector
    b = 0.5 * (mu_0 + mu_1) @ w  # Bias term
    x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    x2_vals = (-w[0] * x1_vals + b) / w[1]
    plt.plot(x1_vals, x2_vals, 'k-', label="LDA Boundary (Linear)")
    a, b, c, d, e, f = quadratic_boundary_equation(mu_0, mu_1, sigma_0, sigma_1)
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))
    Z = a * xx**2 + b * yy**2 + c * xx * yy + d * xx + e * yy + f
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match180-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.contour(xx, yy, Z, levels=[0], colors='green', linewidths=2)
    plt.xlabel("growth ring diameters in fresh water")
    plt.ylabel("growth ring diameters in marine")
    plt.title("LDA & QDA Decision Boundaries")
</FONT>    plt.legend()
    plt.grid(True)
    plt.show()

   
   
x_file ="q4x.dat"
y_file ="q4y.dat"
X = np.loadtxt(x_file)
y = np.loadtxt(y_file, dtype=str) 
y_binary = []
for e in y:
    print(e)
    if e == "Alaska":
        y_binary.append(0)
    else:
        y_binary.append(1)  # Fix: Assign 1 for Canada
    

            

y = np.array(y_binary)
print(y)
model = GaussianDiscriminantAnalysis()

mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)
print("Shared Covariance Matrix", sigma)

mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)
print("Mean for Alaska", mu_0)
print("Mean for Canada ", mu_1)
print("Covariance Matrix for Alaska", sigma_0)
print("Covariance Matrix for Canada", sigma_1)
#print(y)
plot_combined_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1)

#plot_quadratic_boundary(X, y, mu_0, mu_1, sigma_0, sigma_1)

#Sigma_inv = np.linalg.inv(sigma)

# Compute weight vector w
#w = Sigma_inv @ (mu_1 - mu_0)

# Compute bias term
#b = 0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0)
#b = w.T @ (mu_0 + mu_1) / 2

# Decision boundary equation: w1 * x1 + w2 * x2 = b
# Solve for x2:  x2 = (-w1*x1 + b) / w2



# Convert y labels (if they are strings)
#y_binary = np.array([0 if label == "Alaska" else 1 for label in y])

## Separate points based on class
#alaska_points = X[y_binary == 0]
#canada_points = X[y_binary == 1]

# Plot Alaska points with 'o' marker
#plt.scatter(alaska_points[:, 0], alaska_points[:, 1], marker='o', label="Alaska", color='blue')

# Plot Canada points with 'x' marker
#plt.scatter(canada_points[:, 0], canada_points[:, 1], marker='x', label="Canada", color='red')

# Labels and legend
#plt.xlabel("growth ring diameters in fresh water")
#plt.ylabel("growth ring diameters in marine")
#plt.title("Training Data: Alaska vs. Canada")
#plt.legend()
#plt.grid(True)

# Show plot
#plt.show()
#plot_with_decision_boundary(X, y, mu_0, mu_1, sigma)

        
        


</PRE>
</PRE>
</BODY>
</HTML>
