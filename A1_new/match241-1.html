<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_HJFII.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_Z6YJ5.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# -----------------------------------------#
#        Linear Regressor Class            #
# -----------------------------------------#
class LinearRegressor:
    def __init__(self):
        self.theta = None  # Model parameters
    
    def fit(self, X, y, learning_rate=0.05):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
        The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        #initialising book-keeping variables
        loss_values = []
        loss_diff_values = []
        parameter_values = []
        m = len(y)
        
        #appending intercept feature in X with all 1's
        X = np.c_[np.ones(X.shape[0]), X]
        
        #reshaping y to (m,1)
        y = y.reshape(-1,1)
        
        #least-squared-loss-function
        def get_loss(X, y):
            m = len(y)
            errors = X @ self.theta - y  # (m,1) vector of residuals
            cost = (1 / (2 * m)) * (errors.T @ errors)  # Scalar value
            return cost.item()  # Convert from array to scalar
            
        #initialization of parameters
        self.theta = np.zeros((X.shape[1],1))
        parameter_values.append([self.theta[0][0], self.theta[1][0]])
        
        #finding the initial loss (i.e. loss with initialized parameters)
        curr_loss = get_loss(X, y)
        loss_values.append(curr_loss)
        
        
        iter = 0
        while True:
            iter+=1
            print('Iteration ',iter, ':',end=" ")
            prev_loss = curr_loss
            gradient = (1/m)* X.T @ (X @ self.theta - y) # Matrix form of batch gradient
            #update theta
            self.theta = self.theta - (learning_rate*gradient)
            curr_loss = get_loss( X, y)
            print('Loss: ',curr_loss)
            loss_diff = abs(prev_loss - curr_loss)
            loss_values.append(curr_loss)
            loss_diff_values.append(loss_diff)
            parameter_values.append([self.theta[0][0], self.theta[1][0]])
            if loss_diff&lt;1e-10:
                break
        
        print('Final parameters are:',self.theta)
        return np.array(parameter_values)#, np.array(loss_values), np.array(loss_diff_values)
                      
     
         
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        # Add a column of ones to X for the intercept term
        X = np.c_[np.ones(X.shape[0]), X]
        
        # Predict the target values using the learned parameters
        y_pred = X @ self.theta
        
        # Return the predictions as a 1D array
        return y_pred.flatten()
    

# -----------------------------------------#
#                SOLUTION 1.1.             #
# -----------------------------------------#
def question_one_one():
    
    # 1.1. (a)
    
    X  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearX.csv', header=None))    
    y  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearY.csv', header=None))
    y=y.reshape(-1,)
    
    lr = LinearRegressor()  # Creating an object of the class
    parameter_values  = lr.fit(X, y)
    
    
    # 1.1. (b), (c)
    
    #learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]
    learning_rates = [0.5]
    no_of_iterations = []
    for rate in learning_rates:
        print('Checking for rate :',rate)
        parameter_values, loss_values, loss_diff_values  = lr.fit(X, y, rate)
        no_of_iterations.append(len(parameter_values))
        print('Number of iterations to converge:',len(parameter_values))
    plt.plot(learning_rates, no_of_iterations)
    

# -----------------------------------------#
#                SOLUTION 1.2.             #
# -----------------------------------------#
def question_one_two(): 
    #plotting given data
    plt.plot(X,y)
    
    # Generate x values (range for plotting)
    x_vals = np.linspace(min(X.T[0]), max(X.T[0]), 100)  # 100 points between -10 and 10
    # Compute corresponding y values using the line equation
    theta = parameter_values[-1]
    y_vals = theta[0] + theta[1] * x_vals
    # Plot the line
    plt.plot(x_vals, y_vals, label=f"y = {theta[0]:.2f} + {theta[1]:.2f}x", color="red")
    # Labels and title
    plt.xlabel("acidity (x)")
    plt.ylabel("density (y)")
    plt.title("Plot of the Hypothesis Function")
    plt.legend()
    plt.grid(True)
    # Show the plot
    plt.show()

    
# ------------------------------------------#
#                SOLUTION 1.3.              #
# ------------------------------------------#
def question_one_three():
        
    X  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearX.csv', header=None))    
    y  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearY.csv', header=None))
    y=y.reshape(-1,1)
    
    lr = LinearRegressor()  # Creating an object of the class
    parameter_values, loss_values, loss_diff_values  = lr.fit(X, y, 0.5)

    # Extract theta and loss values
    gd_theta_0_vals = parameter_values[:, 0]
    gd_theta_1_vals = parameter_values[:, 1]
    gd_loss_vals = loss_values

    # Loss function used in Gradient Descent
    def get_loss(theta, X, y):
        m = len(y)
        X = np.c_[np.ones(X.shape[0]), X]
        errors = X @ theta - y
        cost = (1 / (2 * m)) * (errors.T @ errors)
        return cost.item()

    # Create a grid of theta_0 and theta_1 values
    theta_0_vals = np.linspace(-7, 20, 100)
    theta_1_vals = np.linspace(20, 40, 100)
    theta_0_grid, theta_1_grid = np.meshgrid(theta_0_vals, theta_1_vals)

    # Calculate the loss for each pair of theta_0, theta_1
    loss_vals = np.zeros(theta_0_grid.shape)

    for i in range(len(theta_0_vals)):
        for j in range(len(theta_1_vals)):
            theta = np.array([theta_0_grid[i, j], theta_1_grid[i, j]]).reshape(2, 1)
            loss_vals[i, j] = get_loss(theta, X, y)

    # Plotting the surface with color gradient
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    # Create a surface plot with a color gradient
    surface = ax.plot_surface(theta_0_grid, theta_1_grid, loss_vals, cmap='viridis', alpha=0.8)

    # Add a color bar for reference
    fig.colorbar(surface, ax=ax, shrink=0.5, aspect=5, label='Loss')

    # Update plot for each iteration of gradient descent
    for k in range(len(gd_loss_vals)):
        ax.clear()
        
        # Replot the surface with the color gradient
        surface = ax.plot_surface(theta_0_grid, theta_1_grid, loss_vals, cmap='viridis', alpha=0.8)
        
        # Plot the current theta and loss on the mesh plot
        ax.scatter(gd_theta_0_vals[k], gd_theta_1_vals[k], gd_loss_vals[k], color='r', s=100, label='Gradient Descent')
        
        # Add labels and title
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_zlabel('Loss')
        ax.set_title(f"Iteration {k+1}, Loss: {gd_loss_vals[k]:.4f}")
        
        # Add a legend
        ax.legend()
        
        # Pause to make the update visible
        plt.pause(0.5)

    plt.show()


# ------------------------------------------#
#                SOLUTION 1.4.              #
# ------------------------------------------#
def question_one_four(learning_rate):
    X  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearX.csv', header=None))    
    y  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q1/linearY.csv', header=None))
    y=y.reshape(-1,1)
    
    lr = LinearRegressor()  # Creating an object of the class
    parameter_values, loss_values, loss_diff_values  = lr.fit(X, y, learning_rate)

    # Extract theta and loss values
    gd_theta_0_vals = parameter_values[:, 0]
    gd_theta_1_vals = parameter_values[:, 1]
    gd_loss_vals = loss_values

    # Loss function used in Gradient Descent
    def get_loss(theta, X, y):
        m = len(y)
        X = np.c_[np.ones(X.shape[0]), X]
        errors = X @ theta - y
        cost = (1 / (2 * m)) * (errors.T @ errors)
        return cost.item()

    # Create a grid of theta_0 and theta_1 values
    theta_0_vals = np.linspace(-7, 20, 100)
    theta_1_vals = np.linspace(20, 40, 100)
    theta_0_grid, theta_1_grid = np.meshgrid(theta_0_vals, theta_1_vals)

    # Calculate the loss for each pair of theta_0, theta_1
    loss_vals = np.zeros(theta_0_grid.shape)

    for i in range(len(theta_0_vals)):
        for j in range(len(theta_1_vals)):
            theta = np.array([theta_0_grid[i, j], theta_1_grid[i, j]]).reshape(2, 1)
            loss_vals[i, j] = get_loss(theta, X, y)
            
    # Set up the plot for contours
    fig, ax = plt.subplots()
    
    # Draw the contour plot once
    cs=ax.contourf(theta_0_grid, theta_1_grid, loss_vals, cmap='viridis') #plot_surface
    fig.colorbar(cs)  # Add a color bar for the contours
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    
    # Initialize a scatter plot for the red dot
    point, = ax.plot([], [], 'ro', markersize=8)  # 'ro' means red circle


    # Update the red dot without clearing the plot
    for k in range(len(gd_loss_vals)):  #len(gd_loss_vals)
        point.set_data([gd_theta_0_vals[k]], [gd_theta_1_vals[k]])  # # Update red dot position # Ensure it's a sequence

        # Update the title to show the current iteration and loss value
        ax.set_title(f"Iteration {k+1}, Loss: {gd_loss_vals[k]:.4f}")
        
        # Pause to make the update visible
        plt.pause(0.1)

    plt.show()



# ------------------------------------------#
#                SOLUTION 1.4.              #
# ------------------------------------------#
def question_one_five():
    for lr in [0.025, 0.1]:
        question_one_four(lr)


def main():
    #question_one_three()
    #question_one_four(0.5)
    #question_one_five()
    pass

if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# -----------------------------------------#
#        Sample Generation Function        #
# -----------------------------------------#
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = []
    y = []
    for i in range (N):
        x_1   = np.random.normal(input_mean[0], input_sigma[0])
        x_2   = np.random.normal(input_mean[1], input_sigma[1])
        noise = np.random.normal(            0,    noise_sigma)
        X.append([x_1, x_2])
        y.append(theta[0]+(theta[1]*x_1)+(theta[2]*x_2)+noise)
    X=np.array(X)
    y=np.array(y)
    return X, y
        

# -----------------------------------------#
#                SGD Class                 #
# -----------------------------------------#
class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None  # Model parameters
    
    def fit(self, X, y, learning_rate=0.0001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        #initialising book-keeping variables
        loss_values = []
        loss_diff_values = []
        parameter_values = []
        m = len(y) #full data size
        r = 8000   #batch-size
        n_batches = int(m/r) #number of batches
        b_no=0
        
        #appending intercept feature in X with all 1's
        X = np.c_[np.ones(X.shape[0]), X]
        
        #reshaping y to (m,1)
        y = y.reshape(-1,1)
        
        #least-squared-loss-function
        def get_loss(X, y):
            m = len(y)
            errors = X @ self.theta - y  
            cost = (1 / (2 * m)) * (errors.T @ errors)  
            return cost.item()  
         
        #initialization of parameters
        self.theta = np.zeros((X.shape[1],1))
        parameter_values.append([self.theta[0][0], self.theta[1][0], self.theta[2][0]])

        #finding the initial loss (i.e. loss with initialized parameters)
        curr_loss = get_loss(X, y)
        losses = [curr_loss]
        iter=0
        while True:
            iter+=1
            print('Iteration ',iter, ':',end=" ")
            prev_loss = curr_loss
            
            #make batch
            if m==r:
                X_b = X
                y_b = y
            else:
                b_no=(b_no+1)%n_batches 
                st_ind = (b_no-1)*r
                X_b, y_b = X[st_ind:st_ind+r , :], y[st_ind:st_ind+r]
                print('Batch Details: Batch Number:',b_no, 'Batch Indices:',st_ind, 'to ',st_ind+r)
            
            #update theta
            gradient = (1/r)* X_b.T @ (X_b @ self.theta - y_b) # Matrix form of batch gradient
            self.theta = self.theta - (learning_rate*gradient)
            
            #calculate loss for current theta (for convergence check)
            curr_loss = get_loss( X, y)
            losses.append(curr_loss)
            print('batch-number: ',b_no, 'current loss :', curr_loss, 'parameters: ',self.theta)
            loss_diff = abs(prev_loss - curr_loss)
            parameter_values.append([self.theta[0][0], self.theta[1][0], self.theta[2][0]])
            if  np.mean(np.array(losses[-50:]))&lt;1.005: #iter&gt;10000:#loss_diff&lt;1e-7:
                break
               
        print('Final parameters at convergence are',self.theta)
        return np.array(parameter_values), np.array(losses)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        # Add a column of ones to X for the intercept term
        X = np.c_[np.ones(X.shape[0]), X]
        
        # Predict the target values using the learned parameters
        y_pred = X @ self.theta
        
        # Return the predictions as a 1D array
        return y_pred.flatten()
    
    

# -----------------------------------------#
#               SOLUTION 2.1.              #
# -----------------------------------------#   
def question_two_one():
    theta=np.array([3, 1, 2])
<A NAME="2"></A><FONT color = #0000FF><A HREF="match241-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    input_mean=np.array([3, -1])
    input_sigma=np.array([4,4])
    noise_sigma = np.sqrt(2)
    X, y = generate(1000000, theta, input_mean, input_sigma, noise_sigma)
    X_train, y_train = X[:800000, :], y[:800000]
</FONT>    X_test, y_test = X[-200000:, :], y[-200000:]
    print('Train Shape:', X_train.shape, y_train.shape)
    print('Test Shape:', X_test.shape, y_test.shape)
    return X_train, y_train, X_test, y_test
    
# -----------------------------------------#
#               SOLUTION 2.2.              #
# -----------------------------------------#  
def question_two_two():
    X_train, y_train, X_test, y_test = question_two_one()
    lr = StochasticLinearRegressor()  # Creating an object of the class
    parameters = lr.fit(X_train, y_train,0.001)
    print('Converged in ',len(parameters), ' iterations')
    print('Final set of parameters are  ', parameters[-1])
   
# -----------------------------------------#
#               SOLUTION 2.3.              #
# -----------------------------------------#  
def question_two_three():
    X_train, y_train, X_test, y_test = question_two_one()
    # Add a column of ones to X for the intercept term
    X_train = np.c_[np.ones(X_train.shape[0]), X_train]
    parameters = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
    print('Parameters obtained from the closed form are:',parameters)
 
# -----------------------------------------#
#               SOLUTION 2.4.              #
# -----------------------------------------# 
def mean_squared_error(y_true, y_pred):
    n = len(y_true)
    mse = np.sum((y_true - y_pred) ** 2) / n
    return mse

def question_two_four():
    X_train, y_train, X_test, y_test = question_two_one()
    lr = StochasticLinearRegressor()  # Creating an object of the class
    parameters = lr.fit(X_train, y_train,0.001)
    
    # Step : Predict on the training and test sets
    y_train_pred = lr.predict(X_train)
    y_test_pred = lr.predict(X_test)
    print('Parameters obtained from the closed form are:',parameters)
 
    # Calculate MSE for the training set
    train_mse = mean_squared_error(y_train, y_train_pred)

    # Calculate MSE for the test set
    test_mse = mean_squared_error(y_test, y_test_pred)

    print(f"Training MSE: {train_mse}")
    print(f"Test MSE: {test_mse}")
 
 
# -----------------------------------------#
#               SOLUTION 2.5.              #
# -----------------------------------------# 
def question_two_five():
    X_train, y_train, X_test, y_test = question_two_one()
    y_train=y_train.reshape(-1,1)
    
    lr = StochasticLinearRegressor()  # Creating an object of the class
    parameter_values, loss_values = lr.fit(X_train, y_train,0.001)

    from mpl_toolkits.mplot3d import Axes3D
    batch_size=8000
    fig = plt.figure(figsize=(15, 5))
    ax = fig.add_subplot(1, 3, i+1, projection='3d')
    theta_history = parameter_values
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', markersize=2)
    ax.set_title(f"Batch Size = {batch_size}")
    ax.set_xlabel("θ0")
    ax.set_ylabel("θ1")
    ax.set_zlabel("θ2")




    
def main():
    #X_train, y_train, X_test, y_test=question_two_one()
    question_two_two()
   



    
    
if __name__ == "__main__":
    main()




# Imports - you can add any other permitted libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None  # Model parameters
        self.normalization_mean=0
        self.normalization_std=0

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def normalize(self, X):
        self.normalization_mean = np.mean(X, axis=0)
        self.normalization_std = np.std(X, axis=0)
        # Avoid division by zero for constant features
        self.normalization_std[self.normalization_std == 0] = 1  
        X_norm = (X - self.normalization_mean) / self.normalization_std
        return X_norm  
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        
  

        n_samples, n_features = X.shape
        X=self.normalize(X)
        X = np.c_[np.ones(X.shape[0]), X]

        

            
        def compute_loss(y, y_pred):
            epsilon = 1e-10  # To prevent log(0)
            return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))

        
        #--------------------------#
        #   fitting starts here    #
        #--------------------------#
        
        # Initialize theta with zeros
        self.theta = np.zeros(n_features+1)
        self.theta = self.theta.reshape(-1,1)
        theta_history = [self.theta]
        loss_history = []  # Store loss values
        num_iter=50
        
        for _ in range(num_iter):
            # Compute predictions
            y_pred = self.sigmoid(X @ self.theta)
            y_pred = y_pred.reshape(-1, 1)
            
            # Compute loss
            loss = compute_loss(y, y_pred)
            loss_history.append(loss)
            print('Iteration-',_,' Loss:',loss)

            # Compute gradient
            gradient = X.T @ (y_pred - y) / n_samples

            # Compute Hessian
            W = np.diag((y_pred * (1 - y_pred)).flatten())  # Weight matrix
            hessian = X.T @ W @ X / n_samples

            # Update parameters using Newton's method
            self.theta -= np.linalg.inv(hessian) @ gradient
            theta_history.append(self.theta.copy())

        return np.array(theta_history)
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize the input data using the same parameters as training data
        X = (X - self.normalization_mean) / self.normalization_std
        
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept
        probabilities = self.sigmoid(X @ self.theta)
        return (probabilities &gt;= 0.5).astype(int)


import numpy as np
import matplotlib.pyplot as plt

def plot_decision_boundary(X, y, theta):
    """
    Plots the training data and the decision boundary for logistic regression, 
    along with shaded regions for h(x) &gt; 0.5 and h(x) &lt; 0.5.

    Parameters:
    X : numpy array of shape (n_samples, n_features)
        The input features.
    y : numpy array of shape (n_samples,)
        The labels (0 or 1).
    theta : numpy array of shape (n_features,)
        The learned logistic regression parameters.
    """

    # Scatter plot of the training data
    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], marker='x', color='blue', label="y=1")
    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], marker='o', color='red', label="y=0")

    # Define grid range
<A NAME="1"></A><FONT color = #00FF00><A HREF="match241-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1_range = np.linspace(X[:, 0].min()-5, X[:, 0].max()+5, 100)
    x2_range = np.linspace(X[:, 1].min()-5, X[:, 1].max()+5, 100)
    X1, X2 = np.meshgrid(x1_range, x2_range)

    # Compute the decision boundary function z = θ₀ + θ₁x₁ + θ₂x₂
    Z = theta[0] + theta[1] * X1 + theta[2] * X2
</FONT>
    # Plot decision boundary (where Z = 0)
    plt.contour(X1, X2, Z, levels=[0], colors='black', linewidths=2, label="Decision Boundary")

    # Fill regions: h(x) &gt; 0.5 (positive region) and h(x) &lt; 0.5 (negative region)
    plt.contourf(X1, X2, Z, levels=[-100, 0, 100], alpha=0.2, colors=['red', 'blue'])

    # Labels and legend
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.grid(True)
    plt.show()


# -----------------------------------------#
#               SOLUTION 3.1.              #
# -----------------------------------------# 
def question_three_one():
    # Fetch Data
    X  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q3/logisticX.csv', header=None))
    # Add a column of ones as the first column
    y  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q3/logisticY.csv', header=None))
    
    # Train logistic regression model
    lr = LogisticRegressor()
    theta_history = lr.fit(X, y)
    
    # Predict
    y_pred = lr.predict(X)

    # Accuracy
    accuracy = np.mean(y_pred == y)
    print("Model Accuracy:", accuracy)
    
    print('Final Parameters',theta_history[-1])
    

# -----------------------------------------#
#               SOLUTION 3.1.              #
# -----------------------------------------# 
def question_three_two():
   plot_decision_boundary(X,y, theta_history[-1])


def main():
    question_three_two()



    
    
if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None  # Prior probability of class 1
        self.normalization_mean=0
        self.normalization_std=0
    
    def normalize(self, X):
        self.normalization_mean = np.mean(X, axis=0)
        self.normalization_std = np.std(X, axis=0)
        # Avoid division by zero for constant features
        self.normalization_std[self.normalization_std == 0] = 1  
        X_norm = (X - self.normalization_mean) / self.normalization_std
        return X_norm  
    
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # Number of samples for each class
        N = X.shape[0]
        N_0 = np.sum(y == 0)
        N_1 = np.sum(y == 1)
        X = self.normalize(X)
        
        # Compute class prior
        self.phi = N_1 / N     # P(y=1)
        phi_0 = 1 - self.phi   # P(y=0)

        # Compute class means
        self.mu_0 = np.mean(X[y.flatten() == 0], axis=0)  # Mean of class 0 (for both features)
        self.mu_1 = np.mean(X[y.flatten() == 1], axis=0)  # Mean of class 1 (for both features)

        if assume_same_covariance:
            # Compute shared covariance matrix
            sigma = np.zeros((X.shape[1], X.shape[1]))
            for i in range(N): #looping over all examples one by one
                x_i = X[i].reshape(-1, 1)
                mu_class = self.mu_1.reshape(-1, 1) if y[i] == 1 else self.mu_0.reshape(-1, 1) #mu_class is the mean (of both features) for the class of the example 
                sigma += (x_i - mu_class) @ (x_i - mu_class).T
            self.sigma = sigma / N
            return self.phi, self.mu_0, self.mu_1, self.sigma
        else:
            # Compute class-specific covariance matrices
            X_0 = X[y.flatten() == 0]
            X_1 = X[y.flatten() == 1]
            self.sigma_0 = np.cov(X_0, rowvar=False)
            self.sigma_1 = np.cov(X_1, rowvar=False)
            return self.phi, self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
            
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize the input data using the same parameters as training data
        X = (X - self.normalization_mean) / self.normalization_std
        
        if self.sigma is not None:
            # Shared covariance case
<A NAME="0"></A><FONT color = #FF0000><A HREF="match241-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            sigma_inv = np.linalg.inv(self.sigma)
            w = sigma_inv @ (self.mu_1 - self.mu_0)
            b = (
                -0.5 * self.mu_1.T @ sigma_inv @ self.mu_1
                + 0.5 * self.mu_0.T @ sigma_inv @ self.mu_0
                + np.log(self.phi / (1 - self.phi))
            )
            g_X = X @ w + b
</FONT>        else:
            # Different covariance case
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            det_sigma_0 = np.linalg.det(self.sigma_0)
            det_sigma_1 = np.linalg.det(self.sigma_1)
            
            def quadratic_form(x, mu, sigma_inv):
                diff = x - mu
                return diff.T @ sigma_inv @ diff
            
            g_X = []
            for x in X:
                x = x.reshape(-1, 1)
                term_0 = -0.5 * quadratic_form(x, self.mu_0.reshape(-1, 1), sigma_0_inv) - 0.5 * np.log(det_sigma_0) + np.log(1 - self.phi)
                term_1 = -0.5 * quadratic_form(x, self.mu_1.reshape(-1, 1), sigma_1_inv) - 0.5 * np.log(det_sigma_1) + np.log(self.phi)
                g_X.append(term_1 - term_0)
            g_X = np.array(g_X).flatten()
        
        return (g_X &gt; 0).astype(int)
            
#part 4.3#   
def plot_gda_decision_boundary(X, y, mu_0, mu_1, sigma):
    """
    Plots the training data and the GDA decision boundary.

    Parameters:
    X     : numpy array of shape (n_samples, 2) - Input features.
    y     : numpy array of shape (n_samples,)   - Labels (0 or 1).
    mu_0  : numpy array of shape (2,)           - Mean vector of class 0.
    mu_1  : numpy array of shape (2,)           - Mean vector of class 1.
    sigma : numpy array of shape (2, 2)         - Shared covariance matrix.
    """
    # Ensure y is a 1D array
    y = y.flatten()

    # Scatter plot of the training data
    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], marker='o', color='red', label='Class 0 (Alaska)')
    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], marker='x', color='blue', label='Class 1 (Canada)')

    # Compute the decision boundary
    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, mu_1 - mu_0)
    x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
    X0, X1 = np.meshgrid(x0, x1)
    Z = np.c_[X0.ravel(), X1.ravel()].dot(w)
    Z = Z.reshape(X0.shape)
    plt.contour(X0, X1, Z, levels=[0], colors='green', linewidths=2)

    # Labels and legend
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.title('GDA Decision Boundary')
    plt.grid(True)
    plt.show()
    
#part 4.5#
def plot_two_gda_decision_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1):
    """
    Plots the training data, the GDA linear decision boundary (shared covariance),
    and the GDA quadratic decision boundary (non-shared covariance).

    Parameters:
    X      : numpy array of shape (n_samples, 2) - Input features.
    y      : numpy array of shape (n_samples,)   - Labels (0 or 1).
    mu_0   : numpy array of shape (2,)           - Mean vector of class 0.
    mu_1   : numpy array of shape (2,)           - Mean vector of class 1.
    sigma_0: numpy array of shape (2, 2)         - Covariance matrix of class 0.
    sigma_1: numpy array of shape (2, 2)         - Covariance matrix of class 1.
    """
    # Ensure y is a 1D array
    y = y.flatten()

    # Scatter plot of the training data
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='red', label='Class 0 (Alaska)')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='blue', label='Class 1 (Canada)')

    # Create a grid for plotting
    x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
    X0, X1 = np.meshgrid(x0, x1)
    grid = np.c_[X0.ravel(), X1.ravel()]

    # Compute the linear decision boundary (shared covariance)
    sigma_shared = (sigma_0 + sigma_1) / 2  # Assume shared covariance for linear boundary
    sigma_shared_inv = np.linalg.inv(sigma_shared)
    w = np.dot(sigma_shared_inv, mu_1 - mu_0)
    Z_linear = grid.dot(w)
    Z_linear = Z_linear.reshape(X0.shape)

    # Compute the quadratic decision boundary (non-shared covariance)
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    A = sigma_1_inv - sigma_0_inv
    b = -2 * (mu_1.T @ sigma_1_inv - mu_0.T @ sigma_0_inv)
    c = mu_0.T @ sigma_0_inv @ mu_0 - mu_1.T @ sigma_1_inv @ mu_1 + np.log(np.linalg.det(sigma_0) / np.linalg.det(sigma_1))
    Z_quadratic = np.array([x.T @ A @ x + b @ x + c for x in grid])
    Z_quadratic = Z_quadratic.reshape(X0.shape)

    # Plot the linear decision boundary
    plt.contour(X0, X1, Z_linear, levels=[0], colors='green', linewidths=2, linestyles='dashed', label='Linear Boundary (Shared Covariance)')

    # Plot the quadratic decision boundary
    plt.contour(X0, X1, Z_quadratic, levels=[0], colors='purple', linewidths=2, label='Quadratic Boundary (Non-Shared Covariance)')

    # Labels and legend
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.title('GDA Decision Boundaries')
    plt.grid(True)
    plt.show()


def question_four_one():
    # part 4.1
    # Fetch Data
    X  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q4/q4x.dat', delim_whitespace=True, header=None))
    # Add a column of ones as the first column
    y  = np.array(pd.read_csv('/Users/apple/PythonCodes/AssignmentML1/data/Q4/q4y.dat', delim_whitespace=True, header=None))
    y[y=='Alaska']=0
    y[y=='Canada']=1
    
    # Train logistic regression model
    gd = GaussianDiscriminantAnalysis()
    phi, mu_0, mu_1, sigma = gd.fit(X, y, True)
    print('Parameter Values are:',phi, mu_0, mu_1, sigma)
    phi, mu_0, mu_1, sigma_0, sigma_1 = gd.fit(X, y, False)
    print('Parameter Values are:',phi, mu_0, mu_1, sigma_0, sigma_1 )

    # question-4.2 # Plotting the data points
    X=gd.normalize(X)
    plot_gda_decision_boundary(X, y, mu_0, mu_1, sigma)
    plot_two_gda_decision_boundaries(X, y, mu_0, mu_1, sigma, sigma_0, sigma_1)

    


</PRE>
</PRE>
</BODY>
</HTML>
