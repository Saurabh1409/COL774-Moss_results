<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_4CB6J.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_4CB6J.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
from matplotlib.animation import FuncAnimation

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LinearRegressor:
    def __init__(self):
        self.weights=None
        self.bias=0
    
    def fit(self, X, y, learning_rate=0.1):
        if X.ndim==1:
            X = X.reshape(-1, 1)
        m=y.size       # samples 
        n=X.size//m    # features 
        weights=np.zeros(n)
        bias=0
        my_limit=1e-5          # convergence limit
        current_cost=0         
        previous_cost=0
        iterations=0
        max_iterations=10000    # max iterations allowed
        para=[]                 # will be storing  values of weights  in all iterations


        while(True):
            iterations+=1             
            loss=y-(X@weights +bias)  
            grad=-(loss@X)/m     # calculating gradient vector
            # i have mentioned in report that i am performing vector operations using @ and other np. functions
        
            weights =weights - learning_rate *grad     
            bias=bias-learning_rate*(-np.sum(loss)/m)

            cost=(y-(X@weights +bias))**2
            cost=np.sum(cost)/(2*m) 
            current_cost=cost
            para2=weights.tolist()  
            para2.insert(0,bias)
            para.append(para2)
            if(abs(current_cost-previous_cost)&lt;=my_limit or iterations&gt;=max_iterations):
                break

            previous_cost=current_cost

        print(f"converged after {iterations} iterations")
        print(f"prevError={previous_cost} , currectError={current_cost}")
        print(f"weights are {weights} bias is {bias}")
        para=np.array(para)
        self.weights=weights 
        self.bias=bias
        
        return para
            
    
    def predict(self, X):
        if X.ndim==1:
            X = X.reshape(-1, 1)

        y_pred=X@self.weights+self.bias
        return y_pred






def makeGraph(X,y,list_of_weights):
    plt.ion()
    cost_at_different_iterations=np.zeros(list_of_weights.shape[0])
    w_all=list_of_weights[:,1]
    b_all=list_of_weights[:,0]
    m=y.size

    for i in range(list_of_weights.shape[0]):
        cost_at_different_iterations[i]=np.sum((y-(w_all[i]*X+b_all[i]))**2)/(2*m)


    w = list_of_weights[-1][1] 
    b = list_of_weights[-1][0]
    w_range = np.linspace(w-30, w+30, 100)
    b_range = np.linspace(b-30, b+30, 100)
    J = np.zeros((len(w_range), len(b_range))) 

    for i, bi in enumerate(b_range):
        for j, wj in enumerate(w_range):
            y_graph = bi + wj*X
            J[i,j] = np.sum((y-y_graph)**2)/(2*m)
    T0, T1 = np.meshgrid(b_range, w_range)

    # Create figure
<A NAME="0"></A><FONT color = #FF0000><A HREF="match57-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(111, projection='3d')

    # Plot the surface
    surface = ax.plot_surface(T0, T1, J.T, cmap='jet', alpha=0.8)
</FONT>    # Create arrays to store the trajectory
    b_history = []
    w_history = []
    cost_history = []
    for i in range(len(list_of_weights)):
    # Get current parameters
        current_b = list_of_weights[i][0]
        current_w = list_of_weights[i][1]
        current_cost = cost_at_different_iterations[i]
        
        # Append to history
        b_history.append(current_b)
        w_history.append(current_w)
        cost_history.append(current_cost)
        
        # Clear previous plots
        ax.clear()
        
        # Replot the surface
        surface = ax.plot_surface(T0, T1, J.T, cmap='viridis', alpha=0.8)
        
        # Plot the trajectory line
        if len(b_history) &gt; 1:
            ax.plot(b_history, w_history, cost_history, 'r-', linewidth=2, label='Optimization Path')
        
        # Plot current point
        ax.scatter(current_b, current_w, current_cost, 
                color='red', s=100)
        
        # Update title and labels
        ax.set_title(f"3D Mesh of Cost Function J(θ)\nIteration {i}, Cost: {current_cost:.6f}")
        ax.set_xlabel("Bias (θ0)")
        ax.set_ylabel("Weight (θ1)")
        ax.set_zlabel("Cost Function J(θ)", labelpad=20)
        
        # Set viewing angle
        ax.view_init(elev=35, azim=130)
        
        # Set consistent axis limits
        ax.set_xlim([min(b_range), max(b_range)])
        ax.set_ylim([min(w_range), max(w_range)])
        ax.set_zlim([0, np.max(J)])
        
        if i &gt; 0:
            ax.legend()
        
        # Force drawing update
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(0.2)
    plt.ioff()
    plt.show()



def makeGraphContour(X, y, list_of_weights):
    plt.ion()
    cost_at_different_iterations = np.zeros(list_of_weights.shape[0])
    w_all = list_of_weights[:, 1]
    b_all = list_of_weights[:, 0]
    m = y.size

    for i in range(list_of_weights.shape[0]):
        cost_at_different_iterations[i] = np.sum((y - (w_all[i] * X + b_all[i])) ** 2) / (2 * m)

    w = list_of_weights[-1][1]
    b = list_of_weights[-1][0]
    w_range = np.linspace(w - 30, w + 30, 100)
    b_range = np.linspace(b - 30, b + 30, 100)
    J = np.zeros((len(b_range), len(w_range)))

    for i, bi in enumerate(b_range):
        for j, wj in enumerate(w_range):
            y_graph = bi + wj * X
            J[i, j] = np.sum((y - y_graph) ** 2) / (2 * m)

    T0, T1 = np.meshgrid(b_range, w_range)

    # 2D Contour Plot
    fig, ax = plt.subplots(figsize=(8, 6))
    contour = ax.contour(T0, T1, J.T, levels=np.logspace(-1, 3, 20), cmap='jet')
    ax.set_title("Contour Plot of Cost Function J(θ)")
    ax.set_xlabel("Bias (θ0)")
    ax.set_ylabel("Weight (θ1)")

    # Plot optimization path
    b_history = []
    w_history = []
    
    for i in range(len(list_of_weights)):
        current_b = list_of_weights[i][0]
        current_w = list_of_weights[i][1]
        b_history.append(current_b)
        w_history.append(current_w)

        ax.clear()
        ax.contour(T0, T1, J.T, levels=np.logspace(-1, 3, 20), cmap='plasma')
        ax.set_title(f"Gradient Descent Optimization\nIteration {i}")
        ax.set_xlabel("Bias (θ0)")
        ax.set_ylabel("Weight (θ1)")
        
        if len(b_history) &gt; 1:
            ax.plot(b_history, w_history, 'r-', linewidth=2, label='Optimization Path')
            ax.legend()
        ax.scatter(current_b, current_w, color='red', s=50)
        
        # ax.legend()
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(0.2)
    
    plt.ioff()
    plt.show()



    
    


     
    





# X= np.loadtxt("../data/Q1/linearX.csv")   
# y=np.loadtxt("../data/Q1/linearY.csv")


# a=LinearRegressor()
# b=a.fit(X,y) 
# # print(b)

# c=a.predict(X)
# print(b[-1])

# # np.savetxt("pred.txt", c, fmt="%.4f")

# makeGraph(X,y,b)

# # makeGraphContour(X,y,b)



#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
from matplotlib.animation import FuncAnimation


# In[2]:


class LinearRegressor:
    def __init__(self):
        self.weights=None
        self.bias=0
        
        pass
    
    def fit(self, X, y, learning_rate=0.1):
        if X.ndim==1:
            X = X.reshape(-1, 1)
        m=y.size
        n=X.size//m

        weights=np.zeros(n)
        bias=0
        my_limit=1e-5
        current_cost=0
        previous_cost=0
        iterations=0
        max_iterations=1000
        para=[]


        while(True):
            iterations+=1
            loss=y-(X@weights +bias)
            grad=-(loss@X)/m
            weights =weights - learning_rate *grad
            bias=bias-learning_rate*(-np.sum(loss)/m)

            cost=(y-(X@weights +bias))**2
            cost=np.sum(cost)/(2*m)
            current_cost=cost
            para2=weights.tolist()
            para2.append(bias)
            para.append(para2)
            if(np.linalg.norm(grad) &lt; my_limit or abs(current_cost-previous_cost)&lt;=my_limit or iterations&gt;=max_iterations):

                break

            previous_cost=current_cost

        print(f"converged after {iterations} iterations")
        print(f"prevError={previous_cost} , currectError={current_cost}")
        print(f"weights are {weights} bias is {bias}")
        para=np.array(para)
        self.weights=weights # it is weight only
        self.bias=bias
        
        return para
            
    
    def predict(self, X):
        if X.ndim==1:
            X = X.reshape(-1, 1)

        y_pred=X@self.weights+self.bias
        return y_pred


# In[3]:


X= np.loadtxt("../data/Q1/linearX.csv")   
y=np.loadtxt("../data/Q1/linearY.csv")


# In[4]:


lr=LinearRegressor()
list_of_weights=lr.fit(X,y) 


# In[5]:


print(list_of_weights[-1])


# In[6]:


y_pred=lr.predict(X)


# In[7]:


y_pred


# In[8]:


import matplotlib.pyplot as plt


# In[9]:


plt.figure(figsize=(30, 30))
plt.scatter(X, y,marker='.',color='blue', label='Data Points')   # Try other markers like 'o', '+', '*'
plt.plot(X,y_pred,color="red",label='Learned Hypothesis')


# Add title and labels
plt.title("Scatter Plot of Points")
plt.xlabel("X Values")
plt.ylabel("Y Values")

# Show the plot
x_ticks = np.arange(-2,2,0.25)  # From -1.5 to 2 with step 0.5
y_ticks=np.arange(-50,60,10)
plt.title("Linear Regression: Data vs Hypothesis Function")
plt.xticks(x_ticks)
plt.yticks(y_ticks)
plt.savefig('fig1', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:





# In[ ]:





# In[10]:


m=y.size
w=list_of_weights[-1][0] 
b=list_of_weights[-1][1]
w_range=np.linspace(w-20,w+20,100)
b_range=np.linspace(b-20,b+20,100)
J = np.zeros((len(w_range), len(b_range))) 
for i,bi in enumerate(b_range):
    for j,wj in enumerate(w_range):
        y_graph=bi+wj*X
        J[i,j]=np.sum((y-y_graph)**2)/(2*m)


# In[11]:


T0, T1 = np.meshgrid(b_range, w_range)

fig = plt.figure(figsize=(16, 14), constrained_layout=True)
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(T0, T1, J.T, cmap='jet', alpha=0.8)  # other cmap coolwarm,viridis,plasma,jet
ax.scatter(b, w, np.min(J), color='red', s=100, label="Trained Parameters")

ax.set_xlabel("Bias (θ0)")
ax.set_ylabel("Weight (θ1)")
ax.set_zlabel("Cost Function J(θ)")  # Increased padding
ax.set_title("3D Mesh of Cost Function J(θ)")


ax.view_init(elev=35, azim=120)  # Adjust viewing angle
plt.savefig('fig1', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[12]:


cost_at_different_iterations=np.zeros(list_of_weights.shape[0])


# In[13]:


cost_at_different_iterations.shape


# In[14]:


w_all=list_of_weights[:,0]
b_all=list_of_weights[:,1]
m=y.size

for i in range(list_of_weights.shape[0]):
    cost_at_different_iterations[i]=np.sum((y-(w_all[i]*X+b_all[i]))**2)/(2*m)
    


# In[15]:


cost_at_different_iterations


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[16]:



# Set the backend to interactive mode
plt.ion()

# Create the base 3D visualization
w = list_of_weights[-1][0] 
b = list_of_weights[-1][1]
w_range = np.linspace(w-30, w+30, 100)
b_range = np.linspace(b-30, b+30, 100)
J = np.zeros((len(w_range), len(b_range))) 

for i, bi in enumerate(b_range):
    for j, wj in enumerate(w_range):
        y_graph = bi + wj*X
        J[i,j] = np.sum((y-y_graph)**2)/(2*m)

T0, T1 = np.meshgrid(b_range, w_range)

# Create figure
<A NAME="1"></A><FONT color = #00FF00><A HREF="match57-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

fig = plt.figure(figsize=(12, 9))
ax = fig.add_subplot(111, projection='3d')

# Plot the surface
surface = ax.plot_surface(T0, T1, J.T, cmap='jet', alpha=0.8)
</FONT>
# Create arrays to store the trajectory
b_history = []
w_history = []
cost_history = []

# For each iteration, plot the trajectory
for i in range(len(list_of_weights)):
    # Get current parameters
    current_b = list_of_weights[i][1]
    current_w = list_of_weights[i][0]
    current_cost = cost_at_different_iterations[i]
    
    # Append to history
    b_history.append(current_b)
    w_history.append(current_w)
    cost_history.append(current_cost)
    
    # Clear previous plots
    ax.clear()
    
    # Replot the surface
    surface = ax.plot_surface(T0, T1, J.T, cmap='viridis', alpha=0.8)
    
    # Plot the trajectory line
    if len(b_history) &gt; 1:
        ax.plot(b_history, w_history, cost_history, 'r-', linewidth=2, label='Optimization Path')
    
    # Plot current point
    ax.scatter(current_b, current_w, current_cost, 
              color='red', s=100)
    
    # Update title and labels
    ax.set_title(f"3D Mesh of Cost Function J(θ)\nIteration {i}, Cost: {current_cost:.6f}")
    ax.set_xlabel("Bias (θ0)")
    ax.set_ylabel("Weight (θ1)")
    ax.set_zlabel("Cost Function J(θ)", labelpad=20)
    
    # Set viewing angle
    ax.view_init(elev=35, azim=130)
    
    # Set consistent axis limits
    ax.set_xlim([min(b_range), max(b_range)])
    ax.set_ylim([min(w_range), max(w_range)])
    ax.set_zlim([0, np.max(J)])
    
    if i &gt; 0:
        ax.legend()
    
    # Force drawing update
    fig.canvas.draw()
    fig.canvas.flush_events()
    
    # Add delay
#     time.sleep(0.2)

# Turn off interactive mode
plt.ioff()
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[93]:


import numpy as np
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
from matplotlib.animation import FuncAnimation

#manan


# In[94]:


class LinearRegressor:
    def __init__(self):
        self.weights=None
        self.bias=0
        
        pass
    
    def fit(self, X, y, learning_rate=0.1):
        if X.ndim==1:
            X = X.reshape(-1, 1)
        m=y.size
        n=X.size//m

        weights=np.zeros(n)
        bias=0
        my_limit=1e-5
        current_cost=0
        previous_cost=0
        iterations=0
        max_iterations=1000
        para=[]


        while(True):
            iterations+=1
            loss=y-(X@weights +bias)
            grad=-(loss@X)/m
            weights =weights - learning_rate *grad
            bias=bias-learning_rate*(-np.sum(loss)/m)

            cost=(y-(X@weights +bias))**2
            cost=np.sum(cost)/(2*m)
            current_cost=cost
            para2=weights.tolist()
            para2.append(bias)
            para.append(para2)
            if(np.linalg.norm(grad) &lt; my_limit or abs(current_cost-previous_cost)&lt;=my_limit or iterations&gt;=max_iterations):

                break

            previous_cost=current_cost

        print(f"converged after {iterations} iterations")
        print(f"prevError={previous_cost} , currectError={current_cost}")
        print(f"weights are {weights} bias is {bias}")
        para=np.array(para)
        self.weights=weights # it is weight only
        self.bias=bias
        
        return para
            
    
    def predict(self, X):
        if X.ndim==1:
            X = X.reshape(-1, 1)

        y_pred=X@self.weights+self.bias
        return y_pred


# In[95]:


X= np.loadtxt("../data/Q1/linearX.csv")   
y=np.loadtxt("../data/Q1/linearY.csv")


# In[96]:


lr=LinearRegressor()
list_of_weights=lr.fit(X,y) 


# In[97]:


print(list_of_weights[-1])


# In[98]:


y_pred=lr.predict(X)


# In[99]:


y_pred


# In[100]:


import matplotlib.pyplot as plt


# In[101]:


plt.figure(figsize=(30, 30))
plt.scatter(X, y,marker='.',color='blue', label='Data Points')   # Try other markers like 'o', '+', '*'
plt.plot(X,y_pred,color="red",label='Learned Hypothesis')


plt.title("Scatter Plot of Points")
plt.xlabel("X Values")
plt.ylabel("Y Values")

x_ticks = np.arange(-2,2,0.25)  # From -1.5 to 2 with step 0.5
y_ticks=np.arange(-50,60,10)
plt.title("Linear Regression: Data vs Hypothesis Function")
plt.xticks(x_ticks)
plt.yticks(y_ticks)
plt.savefig('fig1', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:





# In[ ]:





# In[116]:


m=y.size
w=list_of_weights[-1][0] 
b=list_of_weights[-1][1]
w_range=np.linspace(w-20,w+20,100)
b_range=np.linspace(b-20,b+20,100)
J = np.zeros((len(w_range), len(b_range))) 
for i,bi in enumerate(b_range):
    for j,wj in enumerate(w_range):
        y_graph=bi+wj*X
        J[i,j]=np.sum((y-y_graph)**2)/(2*m)


# In[117]:


T0, T1 = np.meshgrid(b_range, w_range)

fig = plt.figure(figsize=(16, 14), constrained_layout=True)
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(T0, T1, J.T, cmap='jet', alpha=0.8)  # other cmap coolwarm,viridis,plasma,jet
ax.scatter(b, w, np.min(J), color='red', s=100, label="Trained Parameters")

ax.set_xlabel("Bias (θ0)")
ax.set_ylabel("Weight (θ1)")
ax.set_zlabel("Cost Function J(θ)")  # Increased padding
ax.set_title("3D Mesh of Cost Function J(θ)")


ax.view_init(elev=35, azim=120)  # Adjust viewing angle
plt.savefig('fig1', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[119]:


cost_at_different_iterations=np.zeros(list_of_weights.shape[0])


# In[120]:


cost_at_different_iterations.shape


# In[121]:


w_all=list_of_weights[:,0]
b_all=list_of_weights[:,1]
m=y.size

for i in range(list_of_weights.shape[0]):
    cost_at_different_iterations[i]=np.sum((y-(w_all[i]*X+b_all[i]))**2)/(2*m)
    


# In[122]:


cost_at_different_iterations


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[124]:



plt.ion()  # for this graph i have used vs code it was better

w = list_of_weights[-1][0] 
b = list_of_weights[-1][1]
w_range = np.linspace(w-30, w+30, 100)
b_range = np.linspace(b-30, b+30, 100)
J = np.zeros((len(w_range), len(b_range))) 

for i, bi in enumerate(b_range):
    for j, wj in enumerate(w_range):
        y_graph = bi + wj*X
        J[i,j] = np.sum((y-y_graph)**2)/(2*m)

T0, T1 = np.meshgrid(b_range, w_range)

<A NAME="2"></A><FONT color = #0000FF><A HREF="match57-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

fig = plt.figure(figsize=(12, 9))
ax = fig.add_subplot(111, projection='3d')

surface = ax.plot_surface(T0, T1, J.T, cmap='jet', alpha=0.8)
</FONT>

b_history = []
w_history = []
cost_history = []

for i in range(len(list_of_weights)):

    current_b = list_of_weights[i][1]
    current_w = list_of_weights[i][0]
    current_cost = cost_at_different_iterations[i]
    
    b_history.append(current_b)
    w_history.append(current_w)
    cost_history.append(current_cost)
    

    ax.clear()
    

    surface = ax.plot_surface(T0, T1, J.T, cmap='viridis', alpha=0.8)
    

    if len(b_history) &gt; 1:
        ax.plot(b_history, w_history, cost_history, 'r-', linewidth=2, label='Optimization Path')
    
    ax.scatter(current_b, current_w, current_cost, 
              color='red', s=100)
    
 
    ax.set_title(f"3D Mesh of Cost Function J(θ)\nIteration {i}, Cost: {current_cost:.6f}")
    ax.set_xlabel("Bias (θ0)")
    ax.set_ylabel("Weight (θ1)")
    ax.set_zlabel("Cost Function J(θ)", labelpad=20)
    

    ax.view_init(elev=35, azim=130)
    
    ax.set_xlim([min(b_range), max(b_range)])
    ax.set_ylim([min(w_range), max(w_range)])
    ax.set_zlim([0, np.max(J)])
    
    if i &gt; 0:
        ax.legend()
    
  
    fig.canvas.draw()
    fig.canvas.flush_events()
    
    
    time.sleep(0.2)

plt.ioff()
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[23]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[24]:


def split(X,y,ratio): # 0.8
    n=X.shape[0]
    i_training=int(n*ratio)

    X_training=X[:i_training]
    X_test=X[i_training:]

    y_training=y[:i_training]
    y_test=y[i_training:]

    return X_training,X_test,y_training,y_test


def gety(weights,xi,sigma):
    hxx= np.dot(weights,xi)

    y=np.random.normal(hxx,sigma,1).item()
    return y

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1=np.random.normal(input_mean[0],input_sigma[0],N)
    x2=np.random.normal(input_mean[1],input_sigma[1],N)
    y=np.zeros(N)
    for i in range(N):
        xi=np.array([1,x1[i],x2[i]])
        y[i]=gety(theta,xi,noise_sigma)

    X=np.column_stack((x1,x2))

    return X,y


# In[167]:


class StochasticLinearRegressor:
    def __init__(self):
        self.weights=None

    
#     def fit(self, X, y, learning_rate=0.01):
#         my_limit=1e-8
#         n_samples=y.size   
#         n_features=X.size//n_samples 
#         weights=np.zeros(n_features+1) # including theta 0  i.e bias 
#         prev_avg_cost=0
#         costs=np.zeros(10)
#         current_avg_cost=0
#         current_cost=0
#         it=0
#         r=1 # batch size 
#         b_max=n_samples//r
#         epoch=0
#         max_epochs=1000
#         para=[]
#         X = np.column_stack((np.ones(X.shape[0]), X))


#         while(True):
#             epoch+=1
#             print(f"Epoch no. {epoch} current cost {current_cost:.4f}, and current_Avg_cost= {current_avg_cost:.4f}")
#             print(f"{weights[0]:.4},{weights[1]:.4},{weights[2]:.4}")
#             i=np.random.permutation(n_samples)
#             X=X[i]
#             y=y[i]

#             for ib in range(b_max):
#                 # b+=1                     # first batch b=0
#                 b_start=ib*r
#                 b_end=(ib+1)*r # non inclusive
#                 if ib == b_max-1:
#                     b_X_array=X[b_start:]
#                     b_y_array=y[b_start:]
#                 else:
#                     b_X_array=X[b_start:b_end]
#                     b_y_array=y[b_start:b_end]

#                 # b_X_array = np.column_stack((np.ones(b_X_array.shape[0]), b_X_array))
#                 loss=b_y_array-(b_X_array@weights)
#                 grad=-(loss@b_X_array)/b_X_array.shape[0]  
#                 weights=weights-learning_rate*grad

#             para2=weights.tolist()
#             para.append(para2)

#             current_cost=(y-X@weights)**2
#             current_cost=np.mean(current_cost)
#             prev_avg_cost=np.mean(costs)
#             # self.addCost(costs,current_cost,it)
#             costs[it%10]=current_cost
#             it=it+1
#             current_avg_cost =np.mean(costs)
#             weights_norm = np.linalg.norm(weights)  
#             # if(weights_norm&lt;my_limit ):
#             #     break
#             if((current_avg_cost-prev_avg_cost&lt;=my_limit  or epoch&gt;=max_epochs)):
#                 break

#         print(f"para size : {len(para)}")
#         print(f"weight size : {weights.size}")
#         print(f"converged after {epoch} epochs")
#         print(f"current_avg_cost={current_avg_cost} , currectAvgError={current_cost} ,prev_avg_cost={prev_avg_cost} ")
        
#         para=np.array(para)
#         self.weights=weights
#         print(self.weights)
#         return para

    def fit(self, X, y, learning_rate=0.001):
        my_limit=1e-5
        my_limit2=1e-1
        n_samples=y.size   
        n_features=X.size//n_samples 
        pweights=np.zeros(n_features+1)
        weights=np.zeros(n_features+1) # including theta 0  i.e bias 
        change_in_weights=np.zeros(10)
        average_change_weights=0
        prev_avg_cost=0
        costs=np.zeros(10)
        current_avg_cost=0
        current_cost=0
        it=0
        r=800000 # batch size   
        b_max=n_samples//r
        epoch=0
        max_epochs=10000
        para=[]
        X = np.column_stack((np.ones(X.shape[0]), X))

        if r==1:
            my_limit=1e-3
        
#         if r== n_samples:
#             my_limit=1e-3


        while(True):
            epoch+=1
#             print(f"Epoch no. {epoch} current cost {current_cost:.4f}, and current_Avg_cost= {current_avg_cost:.4f}")
#             print(f"difference in avg cost: {abs(current_avg_cost-prev_avg_cost)},difference in weights: {np.linalg.norm(change_in_weights,ord=2)}")
            # print(f"{weights[0]:.4},{weights[1]:.4},{weights[2]:.4}")
            i=np.random.permutation(n_samples)
            X=X[i]
            y=y[i]

            for ib in range(b_max):
                # b+=1                     # first batch b=0
                b_start=ib*r
                b_end=(ib+1)*r # non inclusive
                if ib == b_max-1:
                    b_X_array=X[b_start:]
                    b_y_array=y[b_start:]
                else:
                    b_X_array=X[b_start:b_end]
                    b_y_array=y[b_start:b_end]

                # b_X_array = np.column_stack((np.ones(b_X_array.shape[0]), b_X_array))
                loss=b_y_array-(b_X_array@weights)
                grad=-(loss@b_X_array)/b_X_array.shape[0]  
                weights=weights-learning_rate*grad

            para2=weights.tolist()
            para.append(para2)

            current_cost=(y-X@weights)**2
            current_cost=np.mean(current_cost)
            prev_avg_cost=np.mean(costs)
            # self.addCost(costs,current_cost,it)
            costs[it%10]=current_cost
            cw=weights-pweights
            cw= np.linalg.norm(cw,ord=2)
            change_in_weights[it%10]=cw
            it=it+1
            average_change_weights=np.mean(change_in_weights)
            current_avg_cost =np.mean(costs)

            pweights=weights 
           
            # if(weights_norm&lt;my_limit ):
            #     break
            # if((abs(current_avg_cost-prev_avg_cost)&lt;=my_limit )):
            #     print("avg cost limit crossed")

            # if()
            if(((abs(current_avg_cost-prev_avg_cost)&lt;=my_limit and average_change_weights&lt;=my_limit2  )  or epoch&gt;=max_epochs)):
                break

        print(f"para size : {len(para)}")
        print(f"weight size : {weights.size}")
        print(f"converged after {epoch} epochs")
        print(f"current_avg_cost={current_avg_cost} , currectAvgError={current_cost}")
        
        para=np.array(para)
        self.weights=weights
        print(self.weights)
        return para
    
    
    
    def predict(self, X):
        m=X.shape[0] # no of samples
        X = np.column_stack((np.ones(m), X))
        y_pred=X@self.weights
        return y_pred
    
    
    def get_error(self,X,y):
        y_pred=self.predict(X)
        error=(y-y_pred)**2
        error=np.mean(error)
        return error
        
        


# In[168]:


X=np.loadtxt("./X.txt")
y=np.loadtxt("./Y.txt")
X_training,X_test,y_training,y_test=split(X,y,0.8)

a=StochasticLinearRegressor()

print(X.size,y.size)
b=a.fit(X_training,y_training)
print(b[-1])
pred=a.predict(X_test)


# In[169]:


testError=a.get_error(X_test,y_test)
testError


# In[170]:


trainingError=a.get_error(X_training,y_training)
trainingError


# In[171]:


# using closed form
class closedForm:
    def __init__(self):
        self.weights=None
    
    def fit(self,X,y):
        X = np.column_stack((np.ones(X.shape[0]), X))
        v1=X.T@y
        v2=X.T@X
        v2=np.linalg.inv(v2)
        self.weights=v1@v2
        return self.weights
        
    def predict(self,X):
        X = np.column_stack((np.ones(X.shape[0]), X))
        y=X@self.weights
        return y


# In[172]:


a=closedForm()


# In[173]:


w=a.fit(X_training,y_training)


# In[174]:


w


# In[175]:


p=a.predict(X_test)


# In[176]:


p


# In[177]:


p.size


# In[178]:



b


# In[179]:


num_iterations = b.shape[0]  # Example: adjust as needed


# Extract θ0, θ1, and θ2 from 'b'
theta0, theta1, theta2 = b[:, 0], b[:, 1], b[:, 2]

# Create a 3D plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plot trajectory of θ updates
sc = ax.scatter(theta0, theta1, theta2, c=np.arange(len(theta0)), cmap='viridis', marker='o')
ax.plot(theta0, theta1, theta2, linestyle='-', color='black', alpha=0.6)

# Labels
ax.set_xlabel("θ0 (Bias)")
ax.set_ylabel("θ1 (Weight 1)")
ax.set_zlabel("θ2 (Weight 2)")
ax.set_title("3D Parameter Updates during SGD")

# Add color bar to indicate iteration progress
cbar = plt.colorbar(sc, ax=ax)
cbar.set_label("Iteration Progress")

# Adjust view angle for better visualization
ax.view_init(elev=25, azim=120)  # Adjust angles as needed

plt.savefig('fig2', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()


# In[166]:


b


# In[152]:


theta0, theta1, theta2 = b[:, 0], b[:, 1], b[:, 2]

# Create a 3D plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plot trajectory of θ updates
sc = ax.scatter(theta0, theta1, theta2, c=np.arange(len(theta0)), cmap='viridis', marker='o', s=50)
ax.plot(theta0, theta1, theta2, linestyle='-', color='black', alpha=0.6)

# Add iteration numbers as text labels
for i in range(num_iterations):
    ax.text(theta0[i], theta1[i], theta2[i], str(i), color='red', fontsize=10)

# Labels
ax.set_xlabel("θ0 (Bias)")
ax.set_ylabel("θ1 (Weight 1)")
ax.set_zlabel("θ2 (Weight 2)")
ax.set_title("3D Parameter Updates during SGD")

# Add color bar to indicate iteration progress
cbar = plt.colorbar(sc, ax=ax)
cbar.set_label("Iteration Progress")

# Adjust view angle for better visualization
ax.view_init(elev=25, azim=120)  # Adjust angles as needed

# Show the plot
plt.show()


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np
import math

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def split(X,y,ratio): # 0.8
    n=X.shape[0]
    i_training=int(n*ratio)

    X_training=X[:i_training]
    X_test=X[i_training:]

    y_training=y[:i_training]
    y_test=y[i_training:]

    return X_training,X_test,y_training,y_test


def gety(weights,xi,sigma):
     hxx= np.dot(weights,xi)

     y=np.random.normal(hxx,sigma,1).item()
     return y

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1=np.random.normal(input_mean[0],input_sigma[0],N)
    x2=np.random.normal(input_mean[1],input_sigma[1],N)
    y=np.zeros(N)
    for i in range(N):
        xi=np.array([1,x1[i],x2[i]])
        y[i]=gety(theta,xi,noise_sigma)

    X=np.column_stack((x1,x2))

    return X,y

class StochasticLinearRegressor:
    def __init__(self):
        self.weights_all=None

    
    def fit2(self, X, y,r, learning_rate):
        # r=8000 # batch size  
        my_limit=1e-5
        my_limit2=1e-1
        n_samples=y.size   
        n_features=X.size//n_samples 
        pweights=np.zeros(n_features+1)  # previous weights
        weights=np.zeros(n_features+1) # including theta 0  i.e bias 
        change_in_weights=np.zeros(10) # change in current and previous weight
        average_change_weights=0
        prev_avg_cost=0
        costs=np.zeros(10)  # using moving average of costs for last 10 iteration
        current_avg_cost=0
        current_cost=0
        it=0
        
        b_max=n_samples//r    # max no. of batches 
        epoch=0
        max_epochs=10000
        para=[]
        X = np.column_stack((np.ones(X.shape[0]), X))

        if r==1:
            my_limit=1e-3
        
        if r== n_samples:
            my_limit=1e-3


        while(True):
            epoch+=1
            i=np.random.permutation(n_samples)
            X=X[i]
            y=y[i]

            for ib in range(b_max):
                # b+=1                     # first batch b=0
                b_start=ib*r
                b_end=(ib+1)*r # non inclusive
                if ib == b_max-1:
                    b_X_array=X[b_start:]
                    b_y_array=y[b_start:]
                else:
                    b_X_array=X[b_start:b_end]
                    b_y_array=y[b_start:b_end]

                # b_X_array = np.column_stack((np.ones(b_X_array.shape[0]), b_X_array))
                loss=b_y_array-(b_X_array@weights)
                grad=-(loss@b_X_array)/b_X_array.shape[0]  
                weights=weights-learning_rate*grad

            para2=weights.tolist()
            para.append(para2)

            current_cost=(y-X@weights)**2
            current_cost=np.mean(current_cost)
            prev_avg_cost=np.mean(costs)
            # self.addCost(costs,current_cost,it)
            costs[it%10]=current_cost
            cw=weights-pweights
            cw= np.linalg.norm(cw,ord=2)
            change_in_weights[it%10]=cw
            it=it+1
            average_change_weights=np.mean(change_in_weights)
            current_avg_cost =np.mean(costs)

            pweights=weights 
           
            if(((abs(current_avg_cost-prev_avg_cost)&lt;=my_limit and average_change_weights&lt;=my_limit2  )  or epoch&gt;=max_epochs)):
                break

        
        # para=np.array(para)
        # self.weights=weights
        return para,weights


    def fit(self, X, y, learning_rate=0.001):
        batches=[1,80,8000,800000]
        weights_all=[]
        theta_learned=[]
        for r in batches:
            # print(f"starting batch size {r}")
            para,weights=self.fit2(X,y,r,learning_rate)
            weights_all.append(weights)
            para=np.array(para)
            theta_learned.append(para)
            # print(f"Learning for this batch completed")
            # print(weights)
        
        self.weights_all=np.array(weights_all)
        theta_learned=np.array(theta_learned,dtype=object)
        # print(theta_learned.shape)
        return theta_learned
        










    
    
    def predict(self, X):
        batches=[1,80,8000,800000]
        # batches=[80,8000]
        m=X.shape[0] # no of samples
        X = np.column_stack((np.ones(m), X))
        y_pred_all=[]
        for i in range(len(batches)):
          y_pred=X@self.weights_all[i]
          y_pred_all.append(y_pred)
        
        y_pred=np.array(y_pred)
        return y_pred_all
        
     
       
class closedForm:
    def __init__(self):
        self.weights=None
    
    def fit(self,X,y):
        X = np.column_stack((np.ones(X.shape[0]), X))
        v1=X.T@y
        v2=X.T@X
        v2=np.linalg.inv(v2)
        self.weights=v1@v2
        return self.weights
        
    def predict(self,X):
        X = np.column_stack((np.ones(X.shape[0]), X))
        y=X@self.weights
        return y


# n=1000000
# theta=np.array([3,1,2])
# input_mean=np.array([3,-1])
# input_sigma=np.array([math.sqrt(4),math.sqrt(4)])
# noise_sigma=math.sqrt(2)


# X,y=generate(n,theta,input_mean,input_sigma,noise_sigma)

# np.savetxt("X.txt", X, fmt="%.4f")
# np.savetxt("Y.txt", y, fmt="%.4f")
# X=np.loadtxt("./X.txt")
# y=np.loadtxt("./Y.txt")

# X_training= np.loadtxt("../data/Q1/linearX.csv")   
# y_training=np.loadtxt("../data/Q1/linearY.csv")

# X_training,X_test,y_training,y_test=split(X,y,0.8)

# a=StochasticLinearRegressor()

# # print(X.size,y.size)
# b=a.fit(X_training,y_training)
# # print(b[-1])
# pred=a.predict(X_test)

# # np.savetxt("pred.txt", pred, fmt="%.4f")
# # print(b)
# # np.savetxt("y_test_original.txt", y_test, fmt="%.4f")





#!/usr/bin/env python
# coding: utf-8

# In[23]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[24]:


def split(X,y,ratio): # 0.8
    n=X.shape[0]
    i_training=int(n*ratio)

    X_training=X[:i_training]
    X_test=X[i_training:]

    y_training=y[:i_training]
    y_test=y[i_training:]

    return X_training,X_test,y_training,y_test


def gety(weights,xi,sigma):
    hxx= np.dot(weights,xi)

    y=np.random.normal(hxx,sigma,1).item()
    return y

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1=np.random.normal(input_mean[0],input_sigma[0],N)
    x2=np.random.normal(input_mean[1],input_sigma[1],N)
    y=np.zeros(N)
    for i in range(N):
        xi=np.array([1,x1[i],x2[i]])
        y[i]=gety(theta,xi,noise_sigma)

    X=np.column_stack((x1,x2))

    return X,y


# In[77]:


class StochasticLinearRegressor:
    def __init__(self):
        self.weights=None

    
    def fit(self, X, y, learning_rate=0.001):
        my_limit=1e-8
        n_samples=y.size   
        n_features=X.size//n_samples 
        weights=np.zeros(n_features+1) # including theta 0  i.e bias 
        prev_avg_cost=0
        costs=np.zeros(10)
        current_avg_cost=0
        current_cost=0
        it=0
        r=80 # batch size 
        b_max=n_samples//r
        epoch=0
        max_epochs=1000
        para=[]
        X = np.column_stack((np.ones(X.shape[0]), X))


        while(True):
            epoch+=1
            print(f"Epoch no. {epoch} current cost {current_cost:.4f}, and current_Avg_cost= {current_avg_cost:.4f}")
            print(f"{weights[0]:.4},{weights[1]:.4},{weights[2]:.4}")
            i=np.random.permutation(n_samples)
            X=X[i]
            y=y[i]

            for ib in range(b_max):
                # b+=1                     # first batch b=0
                b_start=ib*r
                b_end=(ib+1)*r # non inclusive
                if ib == b_max-1:
                    b_X_array=X[b_start:]
                    b_y_array=y[b_start:]
                else:
                    b_X_array=X[b_start:b_end]
                    b_y_array=y[b_start:b_end]

                # b_X_array = np.column_stack((np.ones(b_X_array.shape[0]), b_X_array))
                loss=b_y_array-(b_X_array@weights)
                grad=-(loss@b_X_array)/b_X_array.shape[0]  
                weights=weights-learning_rate*grad

            para2=weights.tolist()
            para.append(para2)

            current_cost=(y-X@weights)**2
            current_cost=np.mean(current_cost)
            prev_avg_cost=np.mean(costs)
            # self.addCost(costs,current_cost,it)
            costs[it%10]=current_cost
            it=it+1
            current_avg_cost =np.mean(costs)
            weights_norm = np.linalg.norm(weights)  
            # if(weights_norm&lt;my_limit ):
            #     break
            if((current_avg_cost-prev_avg_cost&lt;=my_limit  or epoch&gt;=max_epochs)):
                break

        print(f"para size : {len(para)}")
        print(f"weight size : {weights.size}")
        print(f"converged after {epoch} epochs")
        print(f"current_avg_cost={current_avg_cost} , currectAvgError={current_cost} ,prev_avg_cost={prev_avg_cost} ")
        
        para=np.array(para)
        self.weights=weights
        print(self.weights)
        return para

#     def fit(self, X, y, learning_rate=0.001):
#         my_limit=1e-5
#         my_limit2=1e-1
#         n_samples=y.size   
#         n_features=X.size//n_samples 
#         pweights=np.zeros(n_features+1)
#         weights=np.zeros(n_features+1) # including theta 0  i.e bias 
#         change_in_weights=np.zeros(10)
#         average_change_weights=0
#         prev_avg_cost=0
#         costs=np.zeros(10)
#         current_avg_cost=0
#         current_cost=0
#         it=0
#         r=8000 # batch size   
#         b_max=n_samples//r
#         epoch=0
#         max_epochs=10000
#         para=[]
#         X = np.column_stack((np.ones(X.shape[0]), X))

#         if r==1:
#             my_limit=1e-3
        
#         if r== n_samples:
#             my_limit=1e-3


#         while(True):
#             epoch+=1
#             print(f"Epoch no. {epoch} current cost {current_cost:.4f}, and current_Avg_cost= {current_avg_cost:.4f}")
#             print(f"difference in avg cost: {abs(current_avg_cost-prev_avg_cost)},difference in weights: {np.linalg.norm(change_in_weights,ord=2)}")
#             # print(f"{weights[0]:.4},{weights[1]:.4},{weights[2]:.4}")
#             i=np.random.permutation(n_samples)
#             X=X[i]
#             y=y[i]

#             for ib in range(b_max):
#                 # b+=1                     # first batch b=0
#                 b_start=ib*r
#                 b_end=(ib+1)*r # non inclusive
#                 if ib == b_max-1:
#                     b_X_array=X[b_start:]
#                     b_y_array=y[b_start:]
#                 else:
#                     b_X_array=X[b_start:b_end]
#                     b_y_array=y[b_start:b_end]

#                 # b_X_array = np.column_stack((np.ones(b_X_array.shape[0]), b_X_array))
#                 loss=b_y_array-(b_X_array@weights)
#                 grad=-(loss@b_X_array)/b_X_array.shape[0]  
#                 weights=weights-learning_rate*grad

#             para2=weights.tolist()
#             para.append(para2)

#             current_cost=(y-X@weights)**2
#             current_cost=np.mean(current_cost)
#             prev_avg_cost=np.mean(costs)
#             # self.addCost(costs,current_cost,it)
#             costs[it%10]=current_cost
#             cw=weights-pweights
#             cw= np.linalg.norm(cw,ord=2)
#             change_in_weights[it%10]=cw
#             it=it+1
#             average_change_weights=np.mean(change_in_weights)
#             current_avg_cost =np.mean(costs)

#             pweights=weights 
           
#             # if(weights_norm&lt;my_limit ):
#             #     break
#             # if((abs(current_avg_cost-prev_avg_cost)&lt;=my_limit )):
#             #     print("avg cost limit crossed")

#             # if()
#             if(((abs(current_avg_cost-prev_avg_cost)&lt;=my_limit and average_change_weights&lt;=my_limit2  )  or epoch&gt;=max_epochs)):
#                 break

#         print(f"para size : {len(para)}")
#         print(f"weight size : {weights.size}")
#         print(f"converged after {epoch} epochs")
#         print(f"current_avg_cost={current_avg_cost} , currectAvgError={current_cost}")
        
#         para=np.array(para)
#         self.weights=weights
#         print(self.weights)
#         return para
    
    
    
    def predict(self, X):
        m=X.shape[0] # no of samples
        X = np.column_stack((np.ones(m), X))
        y_pred=X@self.weights
        return y_pred
    
    
    def get_error(self,X,y):
        y_pred=self.predict(X)
        error=(y-y_pred)**2
        error=np.mean(error)
        return error
        
        


# In[78]:


X=np.loadtxt("./X.txt")
y=np.loadtxt("./Y.txt")
X_training,X_test,y_training,y_test=split(X,y,0.8)

a=StochasticLinearRegressor()

print(X.size,y.size)
b=a.fit(X_training,y_training)
print(b[-1])
pred=a.predict(X_test)


# In[79]:


testError=a.get_error(X_test,y_test)
testError


# In[80]:


trainingError=a.get_error(X_training,y_training)
trainingError


# In[81]:


# using closed form
class closedForm:
    def __init__(self):
        self.weights=None
    
    def fit(self,X,y):
        X = np.column_stack((np.ones(X.shape[0]), X))
        v1=X.T@y
        v2=X.T@X
        v2=np.linalg.inv(v2)
        self.weights=v1@v2
        return self.weights
        
    def predict(self,X):
        X = np.column_stack((np.ones(X.shape[0]), X))
        y=X@self.weights
        return y


# In[82]:


a=closedForm()


# In[83]:


w=a.fit(X_training,y_training)


# In[84]:


w


# In[85]:


p=a.predict(X_test)


# In[86]:


p


# In[87]:


p.size


# In[71]:



b


# In[91]:


num_iterations = b.shape[0]  # Example: adjust as needed


# Extract θ0, θ1, and θ2 from 'b'
theta0, theta1, theta2 = b[:, 0], b[:, 1], b[:, 2]

# Create a 3D plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plot trajectory of θ updates
sc = ax.scatter(theta0, theta1, theta2, c=np.arange(len(theta0)), cmap='viridis', marker='o')
ax.plot(theta0, theta1, theta2, linestyle='-', color='black', alpha=0.6)

# Labels
ax.set_xlabel("θ0 (Bias)")
ax.set_ylabel("θ1 (Weight 1)")
ax.set_zlabel("θ2 (Weight 2)")
ax.set_title("3D Parameter Updates during SGD")

# Add color bar to indicate iteration progress
cbar = plt.colorbar(sc, ax=ax)
cbar.set_label("Iteration Progress")

# Adjust view angle for better visualization
ax.view_init(elev=25, azim=120)  # Adjust angles as needed

# plt.savefig('fig2', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()


# In[89]:


b


# In[90]:


theta0, theta1, theta2 = b[:, 0], b[:, 1], b[:, 2]

# Create a 3D plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plot trajectory of θ updates
sc = ax.scatter(theta0, theta1, theta2, c=np.arange(len(theta0)), cmap='viridis', marker='o', s=50)
ax.plot(theta0, theta1, theta2, linestyle='-', color='black', alpha=0.6)

# Add iteration numbers as text labels
for i in range(num_iterations):
    ax.text(theta0[i], theta1[i], theta2[i], str(i), color='red', fontsize=10)

# Labels
ax.set_xlabel("θ0 (Bias)")
ax.set_ylabel("θ1 (Weight 1)")
ax.set_zlabel("θ2 (Weight 2)")
ax.set_title("3D Parameter Updates during SGD")

# Add color bar to indicate iteration progress
cbar = plt.colorbar(sc, ax=ax)
cbar.set_label("Iteration Progress")

# Adjust view angle for better visualization
ax.view_init(elev=25, azim=120)  # Adjust angles as needed

# Show the plot
plt.show()


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.weights=None
    
    
    
    def fit(self, X, y, learning_rate=0.01):
        m=y.size
        n=X.size//m  # all features like theta1, theta 2 etc
        n=n+1 # adding bias term also as theta 0

        weights=np.zeros(n) # bias included
        X = np.column_stack((np.ones(y.size), X))  # including 1 for bias
        my_limit=1e-6
        max_iterations=1000
        iteration=0
        para=[]
        while(True):
            iteration+=1
            gx=1/(1+np.exp(-(X@weights)))

            grad=X.T@(y-gx)

            W=np.diag(gx*(1-gx))

            Hessian=-X.T @ W @ X

            try:
             change=np.linalg.solve(Hessian,grad)
            except:
             change=np.linalg.pinv(Hessian)@ grad

            
            weights=weights-change
            para2=weights.tolist()
            para.append(para2)

            if  iteration&gt;= max_iterations or np.linalg.norm(grad,ord=2)&lt;my_limit  or np.linalg.norm(change,ord=2)&lt;my_limit:
                break
        
        self.weights=weights
        para=np.array(para)

        return para

            
    
    def predict(self, X):

        m=X.shape[0] # samples
        y_pred=np.zeros(m,dtype=int)
        X = np.column_stack((np.ones(m), X))
        prob=1/(1+np.exp(-(X@self.weights)))
        y_pred = (prob &gt; 0.5).astype(int)
        return y_pred








# x = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
# y=np.loadtxt("../data/Q3/logisticY.csv")

# # print(x.shape)
# # print(y.shape)

# mean_x=np.mean(x,axis=0)  # shape of  (2, ) i.e mean of x1 and x2
# std_x=np.std(x,axis=0)

# X= (x-mean_x)/std_x # NumPy automatically expands mean (shape (2,)) to match X (shape (100,2)).
#                      #Effectively, it subtracts the mean from each row separately.

# # print(X)

# # X = np.column_stack((np.ones(y.size), X)) 
# # print(X)


# a= LogisticRegressor()

# a.fit(X,y)

# y_pred=a.predict(X)
# print(y_pred)

# np.savetxt("pred.txt", y_pred,fmt='%d')





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[29]:


class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    
    
    def fit(self, X, y, learning_rate=0.01):
        m=y.size
        n=X.size//m  # all features like theta1, theta 2 etc
        n=n+1 # adding bias term also as theta 0

        weights=np.zeros(n) # bias included
        X = np.column_stack((np.ones(y.size), X))  # including 1 for bias
        my_limit=1e-6
        max_iterations=1000
        iteration=0
        para=[]
        while(True):
            iteration+=1
            gx=1/(1+np.exp(-(X@weights)))

            grad=(X.T@(y-gx))

            W=np.diag(gx*(1-gx))

            Hessian=-(X.T @ W @ X)

            try:
             change=np.linalg.solve(Hessian,grad)
            except:
             change=np.linalg.pinv(Hessian)@ grad

            
            weights=weights-change
            para2=weights.tolist()
            para.append(para2)

            if  iteration&gt;= max_iterations or np.linalg.norm(grad,ord=2)&lt;my_limit  or np.linalg.norm(change,ord=2)&lt;my_limit:
                break
        
        self.weights=weights
        print(f"Converged after {iteration} \n")
        print(f"weights are {weights}")
        print(f" gradient was {grad} , change was {change}")
        para=np.array(para)
        return para
        
        
    def predict(self, X):

        m=X.shape[0] # samples
        y_pred=np.zeros(m,dtype=int)
        X = np.column_stack((np.ones(m), X))
        prob=1/(1+np.exp(-(X@self.weights)))
        y_pred = (prob &gt; 0.5).astype(int)
        return y_pred


# In[30]:


x = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y=np.loadtxt("../data/Q3/logisticY.csv")

print(x.shape)
print(y.shape)
mean_x=np.mean(x,axis=0)  # shape of  (2, ) i.e mean of x1 and x2
std_x=np.std(x,axis=0)
X= (x-mean_x)/std_x 


# In[31]:


a= LogisticRegressor()

a.fit(X,y)

<A NAME="5"></A><FONT color = #FF0000><A HREF="match57-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_pred=a.predict(X)


# In[32]:


y_pred


# In[35]:


plt.figure(figsize=(8, 6))

# Scatter plot of data points
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Class 0")
</FONT>plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Class 1")

# Plot decision boundary: θ0 + θ1*x1 + θ2*x2 = 0
theta0, theta1, theta2 = a.weights
x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
x2_vals = - (theta0 + theta1 * x1_vals) / theta2  # Solve for x2

plt.plot(x1_vals, x2_vals, 'r-', label="Decision Boundary")

plt.xlabel("x1 (Normalized)")
plt.ylabel("x2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.grid()
plt.savefig('fig2', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[29]:


class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    
    
    def fit(self, X, y, learning_rate=0.01):
        m=y.size
        n=X.size//m  # all features like theta1, theta 2 etc
        n=n+1 # adding bias term also as theta 0

        weights=np.zeros(n) # bias included
        X = np.column_stack((np.ones(y.size), X))  # including 1 for bias
        my_limit=1e-6
        max_iterations=1000
        iteration=0
        para=[]
        while(True):
            iteration+=1
            gx=1/(1+np.exp(-(X@weights)))

            grad=(X.T@(y-gx))

            W=np.diag(gx*(1-gx))

            Hessian=-(X.T @ W @ X)

            try:
             change=np.linalg.solve(Hessian,grad)
            except:
             change=np.linalg.pinv(Hessian)@ grad

            
            weights=weights-change
            para2=weights.tolist()
            para.append(para2)

            if  iteration&gt;= max_iterations or np.linalg.norm(grad,ord=2)&lt;my_limit  or np.linalg.norm(change,ord=2)&lt;my_limit:
                break
        
        self.weights=weights
        print(f"Converged after {iteration} \n")
        print(f"weights are {weights}")
        print(f" gradient was {grad} , change was {change}")
        para=np.array(para)
        return para
        
        
    def predict(self, X):

        m=X.shape[0] # samples
        y_pred=np.zeros(m,dtype=int)
        X = np.column_stack((np.ones(m), X))
        prob=1/(1+np.exp(-(X@self.weights)))
        y_pred = (prob &gt; 0.5).astype(int)
        return y_pred


# In[30]:


x = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y=np.loadtxt("../data/Q3/logisticY.csv")

print(x.shape)
print(y.shape)
mean_x=np.mean(x,axis=0)  # shape of  (2, ) i.e mean of x1 and x2
std_x=np.std(x,axis=0)
X= (x-mean_x)/std_x 


# In[31]:


a= LogisticRegressor()

a.fit(X,y)

<A NAME="6"></A><FONT color = #00FF00><A HREF="match57-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_pred=a.predict(X)


# In[32]:


y_pred


# In[34]:


plt.figure(figsize=(8, 6))

# Scatter plot of data points
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Class 0")
</FONT>plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Class 1")

# Plot decision boundary: θ0 + θ1*x1 + θ2*x2 = 0
theta0, theta1, theta2 = a.weights
x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
x2_vals = - (theta0 + theta1 * x1_vals) / theta2  # Solve for x2

plt.plot(x1_vals, x2_vals, 'r-', label="Decision Boundary")

plt.xlabel("x1 (Normalized)")
plt.ylabel("x2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.grid()
plt.savefig('fig2', dpi=300, bbox_inches='tight')
plt.show()


# In[ ]:








import numpy as np



class GaussianDiscriminantAnalysis:
    def __init__(self):
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        self.assume_same_covariance=assume_same_covariance
        m=y.size
        n=X.shape[1]

        self.phi=np.mean(y)
        self.mean0=np.mean(X[y==0],axis=0)
        self.mean1=np.mean(X[y==1],axis=0)

        if assume_same_covariance:
            sigma=np.zeros((n,n))
            for i in range(m):
                x=X[i]
                if y[i]:
                    u=self.mean1
                else:
                    u=self.mean0
                sig=np.outer((x-u),(x-u))
                sigma=sigma+sig
            
            sigma=sigma/m
            self.sigma=sigma

            self.sigma_inv=np.linalg.inv(self.sigma)
            return self.mean0, self.mean1, self.sigma
        else:
<A NAME="7"></A><FONT color = #0000FF><A HREF="match57-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            x0=X[y==0]
            x1=X[y==1]

            self.sigma_0=np.cov(x0,rowvar=False,bias=True)
</FONT>            self.sigma_1=np.cov(x1,rowvar=False,bias=True)

            self.sigma_0_inv=np.linalg.inv(self.sigma_0)
            self.sigma_1_inv=np.linalg.inv(self.sigma_1)
            return self.mean0, self.mean1, self.sigma_0, self.sigma_1

       
    
    def predict(self, X):
        if self.assume_same_covariance:
            x_given_y_0 = X @ (self.sigma_inv @ self.mean0) - 0.5 * self.mean0.T @ self.sigma_inv @ self.mean0
            x_given_y_1 = X @ (self.sigma_inv @ self.mean1) - 0.5 * self.mean1.T @ self.sigma_inv @ self.mean1

        else:
            x_given_y_0 = -0.5 * np.sum((X - self.mean0) @ self.sigma_0_inv * (X - self.mean0), axis=1)
            x_given_y_1 = -0.5 * np.sum((X - self.mean1) @ self.sigma_1_inv * (X - self.mean1), axis=1)



        
        A= x_given_y_1-x_given_y_0 + np.log(self.phi/(1-self.phi))
        if not self.assume_same_covariance:
         A -= 0.5 * np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))


        A=(A&gt;0).astype(int)
        # ypred = np.where(A == 1, "Alaska", "Canada")

        return A






# x = np.loadtxt("../data/Q4/q4x.dat")
# Y=np.loadtxt("../data/Q4/q4y.dat",dtype=str)

# mean_x=np.mean(x,axis=0)
# std_x=np.std(x,axis=0)
# X= (x-mean_x)/std_x 

# # print(X.shape)
# # print(y.shape)



# y=(Y=="Canada").astype(int)
# print(y)



# g=GaussianDiscriminantAnalysis()

# mean00, mean11, sigma00=g.fit(X,y,True)
# # print(mean00,mean11,sigma00)

# y_pred=g.predict(X)

# # # print("\n----------\n")
# print(y_pred)



# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# lda = LinearDiscriminantAnalysis()
# lda.fit(X, y)
# y_sklearn_pred = lda.predict(X)
# print(y_sklearn_pred)

# print("Sklearn LDA Accuracy:", np.mean(y_sklearn_pred == y_pred))



# print(y-y_pred)




#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[6]:


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        self.assume_same_covariance=assume_same_covariance
        m=y.size
        n=X.shape[1]

        self.phi=np.mean(y)
        self.mean0=np.mean(X[y==0],axis=0)
        self.mean1=np.mean(X[y==1],axis=0)

        if assume_same_covariance:
            sigma=np.zeros((n,n))
            for i in range(m):
                x=X[i]
                if y[i]:
                    u=self.mean1
                else:
                    u=self.mean0
                sig=np.outer((x-u),(x-u))
                sigma=sigma+sig
            
            sigma=sigma/m
            self.sigma=sigma

            self.sigma_inv=np.linalg.inv(self.sigma)
            return self.mean0, self.mean1, self.sigma
        else:
            x0=X[y==0]
            x1=X[y==1]

            self.sigma_0=np.cov(x0,rowvar=False,bias=True)
            self.sigma_1=np.cov(x1,rowvar=False,bias=True)

            self.sigma_0_inv=np.linalg.inv(self.sigma_0)
            self.sigma_1_inv=np.linalg.inv(self.sigma_1)
            return self.mean0, self.mean1, self.sigma_0, self.sigma_1

       
    
    def predict(self, X):
        if self.assume_same_covariance:
            x_given_y_0 = X @ (self.sigma_inv @ self.mean0) - 0.5 * self.mean0.T @ self.sigma_inv @ self.mean0
            x_given_y_1 = X @ (self.sigma_inv @ self.mean1) - 0.5 * self.mean1.T @ self.sigma_inv @ self.mean1

            # x_given_y_0=-0.5*np.sum((X@self.sigma_inv)*X,axis=1) + X @ (self.sigma_inv @ self.mean0)
            # x_given_y_1=-0.5*np.sum((X@self.sigma_inv)*X,axis=1) + X @ (self.sigma_inv @ self.mean1)

        else:
            # x_given_y_0=-0.5*np.sum((X@self.sigma_0_inv)*X,axis=1) + X @ (self.sigma_0_inv @ self.mean0)
            # x_given_y_1=-0.5*np.sum((X@self.sigma_1_inv)*X,axis=1) + X @ (self.sigma_1_inv @ self.mean1)
            x_given_y_0 = -0.5 * np.sum((X - self.mean0) @ self.sigma_0_inv * (X - self.mean0), axis=1)
            x_given_y_1 = -0.5 * np.sum((X - self.mean1) @ self.sigma_1_inv * (X - self.mean1), axis=1)



        
        A= x_given_y_1-x_given_y_0 + np.log(self.phi/(1-self.phi))
        if not self.assume_same_covariance:
             A -= 0.5 * np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))


        A=(A&gt;=0).astype(int)
        return A


# In[7]:


def gda_decision_boundary(phi, mu0, mu1, sigma1, sigma2, same_sigma):
  
    if same_sigma:
        # LDA case: Shared covariance matrix
        sigma_inv = np.linalg.inv(sigma1)  # Since sigma1 == sigma2
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match57-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        w = sigma_inv @ (mu1 - mu0)  # w = [w1, w2]
        b = -0.5 * (mu1.T @ sigma_inv @ mu1 - mu0.T @ sigma_inv @ mu0) + np.log(phi / (1 - phi))
</FONT>
        return f"Linear Decision Boundary: {w[0]:.3f} * x1 + {w[1]:.3f} * x2 + {b:.3f} = 0"

    else:
        # QDA case: Different covariance matrices
        sigma1_inv = np.linalg.inv(sigma1)
        sigma2_inv = np.linalg.inv(sigma2)
        
        A = 0.5 * (sigma1_inv - sigma2_inv)  # Quadratic term coefficients
        B = sigma2_inv @ mu1 - sigma1_inv @ mu0  # Linear term coefficients
        C = (
            -0.5 * (mu1.T @ sigma2_inv @ mu1 - mu0.T @ sigma1_inv @ mu0)
            - 0.5 * (np.log(np.linalg.det(sigma2)) - np.log(np.linalg.det(sigma1)))
            + np.log(phi / (1 - phi))
        )

        return (
            f"Quadratic Decision Boundary: "
            f"{A[0,0]:.3f} * x1^2 + {A[1,1]:.3f} * x2^2 + {2*A[0,1]:.3f} * x1 * x2 + "
            f"{B[0]:.3f} * x1 + {B[1]:.3f} * x2 + {C:.3f} = 0"
        )


# In[8]:



x = np.loadtxt("../data/Q4/q4x.dat")
y=np.loadtxt("../data/Q4/q4y.dat",dtype=str)

mean_x=np.mean(x,axis=0)
std_x=np.std(x,axis=0)
X= (x-mean_x)/std_x 

# print(X.shape)
# print(y.shape)

y=(y=="Alaska").astype(int)
# print(y)



g=GaussianDiscriminantAnalysis()

mean00, mean11, sigma00=g.fit(X,y,True)
print(mean00,mean11,sigma00)

y_pred=g.predict(X)

# print("\n----------\n")
# print(y_pred)


# In[9]:


gda_decision_boundary(g.phi,mean00,mean11,sigma00,sigma00,True)


# In[10]:


X_canada = X[y == 0]
X_alaska = X[y == 1]

plt.figure(figsize=(8, 6))
plt.scatter(X_canada[:, 0], X_canada[:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X_alaska[:, 0], X_alaska[:, 1], marker='x', label="Alaska", color="red")

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.title("Training Data: Canada vs Alaska")
plt.savefig('fig11', dpi=300, bbox_inches='tight')
plt.show()


# In[11]:


mean0 = g.mean0
mean1 = g.mean1
sigma_inv = g.sigma_inv
phi = g.phi

# Create a grid of points to evaluate the decision boundary
x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
y_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 200)

w = sigma_inv @ (mean1 - mean0)
b = 0.5 * (mean0 @ sigma_inv @ mean0 - mean1 @ sigma_inv @ mean1) + np.log(phi / (1 - phi))
x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
x2_boundary = -(w[0] * x1 + b) / w[1]
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Alaska", color="red")
plt.plot(x1, x2_boundary, label="Decision Boundary", color='black', linestyle='--')
plt.legend()

# Labels and title
plt.xlabel("Feature 1(x1)")
plt.ylabel("Feature 2(x2)")
plt.title("Training Data and GDA Decision Boundary (using plt.plot)")

plt.savefig('fig2', dpi=300, bbox_inches='tight')
# Show plot
plt.show()


# In[12]:


gda=GaussianDiscriminantAnalysis()

mean0m, mean1m, sigma_0m, sigma_1m=gda.fit(X,y,False)
print(mean0m, mean1m, sigma_0m, sigma_1m)

y_pred2=gda.predict(X)


# print(y_pred2)


# In[13]:


gda_decision_boundary(gda.phi,mean0m,mean1m,sigma_0m,sigma_1m,False)


# In[ ]:





# In[ ]:





# In[ ]:





# In[14]:


from matplotlib.lines import Line2D  
mean0 = gda.mean0
mean1 = gda.mean1
sigma_0_inv = gda.sigma_0_inv
sigma_1_inv = gda.sigma_1_inv
phi = gda.phi
x_margin = 0.1 * (max(X[:, 0]) - min(X[:, 0]))
y_margin = 0.1 * (max(X[:, 1]) - min(X[:, 1]))

x_vals = np.linspace(min(X[:, 0]) - x_margin, max(X[:, 0]) + x_margin, 200)
y_vals = np.linspace(min(X[:, 1]) - y_margin, max(X[:, 1]) + y_margin, 200)
xx, yy = np.meshgrid(x_vals, y_vals)
grid = np.c_[xx.ravel(), yy.ravel()]

# Compute the discriminant function for each point in the grid
def discriminant_function(x, mean0, mean1, sigma_0_inv, sigma_1_inv, phi):
    term_0 = (x - mean0) @ sigma_0_inv @ (x - mean0)
    term_1 = (x - mean1) @ sigma_1_inv @ (x - mean1)
    return term_1 - term_0 + np.log(phi / (1 - phi))

# Evaluate discriminant function for all grid points
grid_preds = np.array([discriminant_function(x, mean0, mean1, sigma_0_inv, sigma_1_inv, phi) for x in grid])

# Reshape the grid predictions to match the grid shape
grid_preds = grid_preds.reshape(xx.shape)

# Plot the data points
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Alaska", color="red")

# Plot the decision boundary using plt.contour
contour = plt.contour(xx, yy, grid_preds, levels=[0], colors='black', linestyles='--')

# Create a custom legend handle for the decision boundary line
decision_boundary_handle = Line2D([0], [0], color='black', linestyle='--', label="Decision Boundary")

# Add legend for scatter points and contour plot
plt.legend()

# Labels and title
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Training Data and GDA Decision Boundary (with different covariance matrices)")
plt.savefig('fig3', dpi=300, bbox_inches='tight')
# Show plot
plt.show()


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[39]:


import numpy as np
import math
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from matplotlib.animation import FuncAnimation


# In[47]:


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        self.assume_same_covariance=assume_same_covariance
        m=y.size
        n=X.shape[1]

        self.phi=np.mean(y)
        self.mean0=np.mean(X[y==0],axis=0)
        self.mean1=np.mean(X[y==1],axis=0)

        if assume_same_covariance:
            sigma=np.zeros((n,n))
            for i in range(m):
                x=X[i]
                if y[i]:
                    u=self.mean1
                else:
                    u=self.mean0
                sig=np.outer((x-u),(x-u))
                sigma=sigma+sig
            
            sigma=sigma/m
            self.sigma=sigma

            self.sigma_inv=np.linalg.inv(self.sigma)
            return self.mean0, self.mean1, self.sigma
        else:
            x0=X[y==0]
            x1=X[y==1]

            self.sigma_0=np.cov(x0,rowvar=False,bias=True)
            self.sigma_1=np.cov(x1,rowvar=False,bias=True)

            self.sigma_0_inv=np.linalg.inv(self.sigma_0)
            self.sigma_1_inv=np.linalg.inv(self.sigma_1)
            return self.mean0, self.mean1, self.sigma_0, self.sigma_1

       
    
    def predict(self, X):
        if self.assume_same_covariance:
            x_given_y_0 = X @ (self.sigma_inv @ self.mean0) - 0.5 * self.mean0.T @ self.sigma_inv @ self.mean0
            x_given_y_1 = X @ (self.sigma_inv @ self.mean1) - 0.5 * self.mean1.T @ self.sigma_inv @ self.mean1

            # x_given_y_0=-0.5*np.sum((X@self.sigma_inv)*X,axis=1) + X @ (self.sigma_inv @ self.mean0)
            # x_given_y_1=-0.5*np.sum((X@self.sigma_inv)*X,axis=1) + X @ (self.sigma_inv @ self.mean1)

        else:
            # x_given_y_0=-0.5*np.sum((X@self.sigma_0_inv)*X,axis=1) + X @ (self.sigma_0_inv @ self.mean0)
            # x_given_y_1=-0.5*np.sum((X@self.sigma_1_inv)*X,axis=1) + X @ (self.sigma_1_inv @ self.mean1)
            x_given_y_0 = -0.5 * np.sum((X - self.mean0) @ self.sigma_0_inv * (X - self.mean0), axis=1)
            x_given_y_1 = -0.5 * np.sum((X - self.mean1) @ self.sigma_1_inv * (X - self.mean1), axis=1)



        
        A= x_given_y_1-x_given_y_0 + np.log(self.phi/(1-self.phi))
        if not self.assume_same_covariance:
             A -= 0.5 * np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))


        A=(A&gt;0).astype(int)
        return A


# In[63]:


def gda_decision_boundary(phi, mu0, mu1, sigma1, sigma2, same_sigma):
  
    if same_sigma:
        # LDA case: Shared covariance matrix
        sigma_inv = np.linalg.inv(sigma1)  # Since sigma1 == sigma2
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match57-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        w = sigma_inv @ (mu1 - mu0)  # w = [w1, w2]
        b = -0.5 * (mu1.T @ sigma_inv @ mu1 - mu0.T @ sigma_inv @ mu0) + np.log(phi / (1 - phi))
</FONT>
        return f"Linear Decision Boundary: {w[0]:.3f} * x1 + {w[1]:.3f} * x2 + {b:.3f} = 0"

    else:
        # QDA case: Different covariance matrices
        sigma1_inv = np.linalg.inv(sigma1)
        sigma2_inv = np.linalg.inv(sigma2)
        
        A = 0.5 * (sigma1_inv - sigma2_inv)  # Quadratic term coefficients
        B = sigma2_inv @ mu1 - sigma1_inv @ mu0  # Linear term coefficients
        C = (
            -0.5 * (mu1.T @ sigma2_inv @ mu1 - mu0.T @ sigma1_inv @ mu0)
            - 0.5 * (np.log(np.linalg.det(sigma2)) - np.log(np.linalg.det(sigma1)))
            + np.log(phi / (1 - phi))
        )

        return (
            f"Quadratic Decision Boundary: "
            f"{A[0,0]:.3f} * x1^2 + {A[1,1]:.3f} * x2^2 + {2*A[0,1]:.3f} * x1 * x2 + "
            f"{B[0]:.3f} * x1 + {B[1]:.3f} * x2 + {C:.3f} = 0"
        )


# In[64]:



x = np.loadtxt("../data/Q4/q4x.dat")
y=np.loadtxt("../data/Q4/q4y.dat",dtype=str)

mean_x=np.mean(x,axis=0)
std_x=np.std(x,axis=0)
X= (x-mean_x)/std_x 

# print(X.shape)
# print(y.shape)

y=(y=="Alaska").astype(int)
# print(y)



g=GaussianDiscriminantAnalysis()

mean00, mean11, sigma00=g.fit(X,y,True)
print(mean00,mean11,sigma00)

y_pred=g.predict(X)

# print("\n----------\n")
# print(y_pred)


# In[65]:


gda_decision_boundary(g.phi,mean00,mean11,sigma00,sigma00,True)


# In[44]:


X_canada = X[y == 0]
X_alaska = X[y == 1]

plt.figure(figsize=(8, 6))
plt.scatter(X_canada[:, 0], X_canada[:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X_alaska[:, 0], X_alaska[:, 1], marker='x', label="Alaska", color="red")

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.title("Training Data: Canada vs Alaska")
plt.savefig('fig11', dpi=300, bbox_inches='tight')
plt.show()


# In[45]:


mean0 = g.mean0
mean1 = g.mean1
sigma_inv = g.sigma_inv
phi = g.phi

# Create a grid of points to evaluate the decision boundary
x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
y_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 200)

w = sigma_inv @ (mean1 - mean0)
b = 0.5 * (mean0 @ sigma_inv @ mean0 - mean1 @ sigma_inv @ mean1) + np.log(phi / (1 - phi))
x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
x2_boundary = -(w[0] * x1 + b) / w[1]
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Alaska", color="red")
plt.plot(x1, x2_boundary, label="Decision Boundary", color='black', linestyle='--')
plt.legend()

# Labels and title
plt.xlabel("Feature 1(x1)")
plt.ylabel("Feature 2(x2)")
plt.title("Training Data and GDA Decision Boundary (using plt.plot)")

plt.savefig('fig2', dpi=300, bbox_inches='tight')
# Show plot
plt.show()


# In[33]:


gda=GaussianDiscriminantAnalysis()

mean0m, mean1m, sigma_0m, sigma_1m=gda.fit(X,y,False)
print(mean0m, mean1m, sigma_0m, sigma_1m)

y_pred2=gda.predict(X)


# print(y_pred2)


# In[46]:


gda_decision_boundary(gda.phi,mean0m,mean1m,sigma_0m,sigma_1m,False)


# In[ ]:





# In[ ]:





# In[ ]:





# In[38]:


from matplotlib.lines import Line2D  
mean0 = gda.mean0
mean1 = gda.mean1
sigma_0_inv = gda.sigma_0_inv
sigma_1_inv = gda.sigma_1_inv
phi = gda.phi
x_margin = 0.1 * (max(X[:, 0]) - min(X[:, 0]))
y_margin = 0.1 * (max(X[:, 1]) - min(X[:, 1]))

x_vals = np.linspace(min(X[:, 0]) - x_margin, max(X[:, 0]) + x_margin, 200)
y_vals = np.linspace(min(X[:, 1]) - y_margin, max(X[:, 1]) + y_margin, 200)
xx, yy = np.meshgrid(x_vals, y_vals)
grid = np.c_[xx.ravel(), yy.ravel()]

# Compute the discriminant function for each point in the grid
def discriminant_function(x, mean0, mean1, sigma_0_inv, sigma_1_inv, phi):
    term_0 = (x - mean0) @ sigma_0_inv @ (x - mean0)
    term_1 = (x - mean1) @ sigma_1_inv @ (x - mean1)
    return term_1 - term_0 + np.log(phi / (1 - phi))

# Evaluate discriminant function for all grid points
grid_preds = np.array([discriminant_function(x, mean0, mean1, sigma_0_inv, sigma_1_inv, phi) for x in grid])

# Reshape the grid predictions to match the grid shape
grid_preds = grid_preds.reshape(xx.shape)

# Plot the data points
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Canada", color="blue")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Alaska", color="red")

# Plot the decision boundary using plt.contour
contour = plt.contour(xx, yy, grid_preds, levels=[0], colors='black', linestyles='--')

# Create a custom legend handle for the decision boundary line
decision_boundary_handle = Line2D([0], [0], color='black', linestyle='--', label="Decision Boundary")

# Add legend for scatter points and contour plot
plt.legend()

# Labels and title
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Training Data and GDA Decision Boundary (with different covariance matrices)")
plt.savefig('fig3', dpi=300, bbox_inches='tight')
# Show plot
plt.show()


# In[ ]:








#!/usr/bin/env python
# coding: utf-8


</PRE>
</PRE>
</BODY>
</HTML>
