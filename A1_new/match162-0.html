<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_PR1QW.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_PR1QW.py<p><PRE>


import numpy as np
import pandas as pd # type: ignore
import matplotlib.pyplot as plt # type: ignore
import time
from mpl_toolkits.mplot3d import Axes3D # type: ignore

def error_function(theta0, theta1, X, y):
    m = len(y)
    predictions = theta0 + theta1 * X
    return (1 / (2 * m)) * np.sum((predictions - y) ** 2)

def gradient(theta0, theta1, X, y):
    m = len(y)
    predictions = theta0 + theta1 * X
    d_theta0 = (1 / m) * np.sum(predictions - y)
    d_theta1 = (1 / m) * np.sum((predictions - y) * X)
    return d_theta0, d_theta1

dfx = pd.read_csv("Assignment1/data/Q1/linearX.csv", header=None)
valuesX = dfx.to_numpy()
dfy = pd.read_csv("Assignment1/data/Q1/linearY.csv", header=None)
valuesY = dfy.to_numpy()

theta0_values = np.linspace(-100, 100, 100)
theta1_values = np.linspace(-100, 100, 100)
theta0_grid, theta1_grid = np.meshgrid(theta0_values, theta1_values)

J_values = np.zeros_like(theta0_grid)
for i in range(theta0_values.size):
    for j in range(theta1_values.size):
        J_values[i, j] = error_function(theta0_grid[i, j], theta1_grid[i, j], dfx, dfy)

theta0, theta1 = -75, 35  
learning_rate = 0.1
iterations = 50

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

surf = ax.plot_surface(theta0_grid, theta1_grid, J_values, cmap='viridis', alpha=0.6, edgecolor='none')

ax.set_xlabel(r'$\theta_0$', fontsize=12)
ax.set_ylabel(r'$\theta_1$', fontsize=12)
ax.set_zlabel(r'$J(\theta)$', fontsize=12)
ax.set_title('Gradient Descent on Error Function Surface', fontsize=14)

fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)

for i in range(iterations):
    J = error_function(theta0, theta1, dfx, dfy)
    
    ax.scatter(theta0, theta1, J, color='red', s=10, label='Gradient Descent' if i == 0 else "")
    
    plt.draw()
    plt.pause(0.2)  
    
    grad_theta0, grad_theta1 = gradient(theta0, theta1, dfx, dfy)
    
    theta0 = theta0 - learning_rate * grad_theta0
    theta1 = theta1 - learning_rate * grad_theta1

plt.show()



from linear_regression import LinearRegressor
import pandas as pd #type: ignore
import numpy as np


dfx = pd.read_csv("Assignment1/data/Q1/linearX.csv", header=None)
valuesX = dfx.to_numpy()
dfy = pd.read_csv("Assignment1/data/Q1/linearY.csv", header=None)
valuesY = dfy.to_numpy()


model = LinearRegressor()
print(model.fit(dfx, dfy))



import numpy as np
import matplotlib.pyplot as plt # type: ignore
import pandas as pd #type: ignore
import time

def error_function(theta0, theta1, X, y):
    m = len(y)
    predictions = theta0 + theta1 * X
    return (1 / (2 * m)) * np.sum((predictions - y) ** 2)

def gradient(theta0, theta1, X, y):
    m = len(y)
    predictions = theta0 + theta1 * X
    d_theta0 = (1 / m) * np.sum(predictions - y)
    d_theta1 = (1 / m) * np.sum((predictions - y) * X)
    return d_theta0, d_theta1

dfx = pd.read_csv("Assignment1/data/Q1/linearX.csv", header=None)
X = dfx.to_numpy()
dfy = pd.read_csv("Assignment1/data/Q1/linearY.csv", header=None)
y = dfy.to_numpy()


theta0_values = np.linspace(-100, 100, 100)
theta1_values = np.linspace(-100, 100, 100)
theta0_grid, theta1_grid = np.meshgrid(theta0_values, theta1_values)

J_values = np.zeros_like(theta0_grid)
for i in range(theta0_values.size):
    for j in range(theta1_values.size):
        J_values[i, j] = error_function(theta0_grid[i, j], theta1_grid[i, j], X, y)

theta0, theta1 = -75, -75  
learning_rates = [0.001, 0.025, 0.1]
colors = ['red', 'blue','green']
iterations = 50

plt.figure(figsize=(10, 8))
plt.xlabel(r'$\theta_0$', fontsize=12)
plt.ylabel(r'$\theta_1$', fontsize=12)
plt.title('Gradient Descent on Error Function Contours', fontsize=14)

contour = plt.contour(theta0_grid, theta1_grid, J_values, levels=50, cmap='viridis')
plt.colorbar(contour, label=r'$J(\theta)$')

for i in range(iterations):
    for j in range(3):
        J = error_function(theta0, theta1, X, y)

        plt.scatter(theta0, theta1, color= colors[j], s=20, label='Gradient Descent' if i == 0 else "")
        plt.draw()
        plt.pause(0.2) 
    
        grad_theta0, grad_theta1 = gradient(theta0, theta1, X, y)
        
        theta0 = theta0 - learning_rates[j] * grad_theta0
        theta1 = theta1 - learning_rates[j] * grad_theta1

plt.legend()




# Imports - you can add any other permitted libraries
import numpy as np # type: ignore
import pandas as pd # type: ignore

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None 
        self.theta_history = None # Model parameters (including bias)
        pass
    
    def fit(self, X, y, learning_rate=0.0005):

        n_iter = 1000
        X = np.c_[np.ones(X.shape[0]), X]


        n_samples, n_features = X.shape
        self.theta = np.zeros((n_features,1))
        self.theta_list = np.zeros((1,n_features))
        stopcrit = 0.001

        # X is a stack of rows, each row corresponding to one data point
        # theta is a column vector with as many rows as X has features

        prevcost = 0
        self.theta = np.zeros((n_features,1))
        self.theta_list = np.empty((0, n_features))  # No rows initially

        for i in range(n_iter):
            y_pred = np.dot(X,self.theta)  # Compute predictions
            cost = (1/2)*((np.dot((y_pred-y).T,y_pred-y))[0,0])
            if abs(cost-prevcost)&lt;prevcost*(stopcrit) :
                break
            gradient = np.dot(X.T,y_pred-y)  # Compute gradient
            self.theta -= learning_rate*gradient  # Update parameters
            self.theta_list = np.vstack((self.theta_list, self.theta.T))
            prevcost = cost
        return self.theta_list
        """
        Fit the linear regression model to the data using Gradient Descent. 
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.     
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
            The learning rate to use in the update rule.      
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
    def predict(self, X):
        """
        Predict the target values for the input data. 
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.    
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        n_samples = X.shape[0]
        X = np.c_[np.ones(n_samples), X]  # Add bias term
        return np.dot(X,self.theta)
    










from sampling_sgd import StochasticLinearRegressor
import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):

    np.random.seed(42)
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    X= np.column_stack((x1, x2))
    return [X,y]


means = np.array([[3],[-1]])
sigmas = np.array([[4],[4]])
noise = np.sqrt(2)
theta = np.array([[3],[1],[2]])
data = generate(1000000, theta, means, sigmas, noise )

trainX = data[0][:800000]
trainy = data[1][:800000]
valX = data[0][800000:]
valy = data[1][800000:]


model = StochasticLinearRegressor()
model.fit(trainX,trainy)

'''CLOSED FORM SOLUTION
closedform = np.matmul(np.matmul(np.linalg.inv(np.matmul(trainX.T, trainX)), trainX.T), trainy)
print(closedform)
'''

'''
VALIDATION ERROR
y_pred = model.predict(valX).astype(np.float32).reshape(valy.shape[0])
valy = valy.astype(np.float32).reshape(valy.shape[0],)
cost = np.dot((y_pred - valy), (y_pred - valy)) / (2 * valy.shape[0])
print(cost)
'''



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder. 48.3

def generate(N, theta, input_mean, input_sigma, noise_sigma):

    np.random.seed(42)
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    X= np.column_stack((x1, x2))
    return [X,y]
    

class StochasticLinearRegressor:

    def __init__(self):
        self.theta = None
        self.theta_list = None
        pass
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
            The learning rate to use in the update rule. 
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        X = np.c_[np.ones(X.shape[0]), X]
        self.theta = np.zeros((X.shape[1], 1))
        prevtheta = np.copy(self.theta)
        m,n_features = X.shape

        batch_sizes = [1,80,8000,800000]
        batch_size = batch_sizes[2]

        iterations = 5000
        prevcost = 0
        self.theta_list = np.empty((0, n_features)) 



        for i in range(iterations) :
            indices = np.random.permutation(X.shape[0])
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            for j in range(0, m, batch_size):
                X_batch = X_shuffled[j:j + batch_size]
                y_batch = y_shuffled[j:j + batch_size].reshape(-1,1)
                y_pred = np.dot(X_batch,self.theta)
                cost = (1/(2*batch_size))*((np.dot((y_pred-y_batch).T,y_pred-y_batch))[0,0])
                grady = (1/batch_size)*np.dot(X_batch.T,(y_pred-y_batch))
                self.theta -= learning_rate * grady
                

            self.theta_list = np.vstack((self.theta_list, self.theta.T))


            if batch_size in [1,80]: 
                if abs(np.dot((self.theta - prevtheta).T, self.theta - prevtheta)) &lt; 1e-5 :
                    break
            prevtheta = np.copy(self.theta)
            if batch_size in [8000 , 800000]:
                if abs(cost - prevcost ) &lt; 1e-4 :
                    break
            prevcost = cost
        pass
    
    def predict(self, X):

        X = np.c_[np.ones(X.shape[0]), X]
        y_pred = np.dot(X,self.theta)
        return y_pred


        """
        Predict the target values for the input data.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.  
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        pass










# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd #type: ignore

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

dfx = pd.read_csv("Assignment1/data/Q3/logisticX.csv", header=None)
valuesX = dfx.to_numpy()
dfy = pd.read_csv("Assignment1/data/Q3/logisticY.csv", header=None)
valuesY = dfy.to_numpy()

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def log_likelihood(X, y, theta):
    h = sigmoid(np.dot(X,theta))
    return np.sum(y*np.log(h) + (1-y)*np.log(1-h))

def gradient(X, y, theta):
    h = sigmoid(X.dot(theta))
    return np.dot(X.T,(h-y))

def hessian(X, theta):
    h = sigmoid(np.dot(X,theta))  # Compute hypothesis
    D = np.diag((h * (1 - h)).flatten())  # Compute diagonal matrix
    return X.T @ D @ X


class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.theta_history = None
        pass
    
    def fit(self, X, y, learning_rate=0.01):

<A NAME="1"></A><FONT color = #00FF00><A HREF="match162-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        n_iter = 10
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        X = np.c_[np.ones(X.shape[0]), X]
        n_samples, n_features = X.shape
</FONT>        self.theta = np.zeros((n_features,1))
        self.theta_history = np.zeros((1,n_features))



        for i in range(n_iter):
        # Compute the gradient and Hessian
            grad = gradient(X, y, self.theta)
            H = hessian(X, self.theta)
            # Update theta using Newton's method
            self.theta = self.theta - np.linalg.inv(H).dot(grad)
            self.theta_history = np.vstack((self.theta_history, self.theta.T))
            
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.    
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        learning_rate : float
            The learning rate to use in the update rule.
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        return self.theta_history
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.

        """
<A NAME="2"></A><FONT color = #0000FF><A HREF="match162-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.c_[np.ones(X.shape[0]), X]

        probs = self.sigmoid(X @ self.theta)
</FONT>        return (probs &gt;= 0.5).astype(int)
        

model = LogisticRegressor()
print (model.fit(dfx,dfy))



import numpy as np
import matplotlib.pyplot as plt #type:ignore
import pandas as pd #type:ignore

def plot_decision_boundary(X, y, theta):
    y = y.flatten()  # Ensure y is a 1D array

    pos = y == 1
    neg = y == 0

    plt.figure(figsize=(8, 6))

    plt.scatter(X[pos, 0], X[pos, 1], marker='o', label="Class 1 (y=1)", color='blue')
    plt.scatter(X[neg, 0], X[neg, 1], marker='x', label="Class 0 (y=0)", color='red')

    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]  # Solve for x2

    plt.plot(x1_vals, x2_vals, 'k-', label="Decision Boundary")

    plt.xlabel("Feature x1")
    plt.ylabel("Feature x2")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.grid(True)
    plt.show()


dfx = pd.read_csv("Assignment1/data/Q3/logisticX.csv", header=None)
X_train = dfx.to_numpy()
dfy = pd.read_csv("Assignment1/data/Q3/logisticY.csv", header=None)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match162-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_train = dfy.to_numpy()
X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)

theta_learned = np.array([ 4.01253161e-01 , 2.58854770e+00 ,-2.72558849e+00])  
</FONT>plot_decision_boundary(X_train, y_train, theta_learned)




import numpy as np
import matplotlib.pyplot as plt #type: ignore
from pathlib import Path

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.scaler_mean = None
        self.scaler_std = None
        self.assume_same_covariance = None
        
    def normalize_data(self, X):
        return (X - self.scaler_mean) / self.scaler_std
    
    def fit(self, X, y, assume_same_covariance=False):
        self.assume_same_covariance = assume_same_covariance
        
        self.scaler_mean = np.mean(X, axis=0)
        self.scaler_std = np.std(X, axis=0)
        
        X_normalized = self.normalize_data(X)
        
        X_0 = X_normalized[y == 0]
        X_1 = X_normalized[y == 1]
        
        n_0 = len(X_0)
        n_1 = len(X_1)
        n = len(X)
        
        self.phi = n_1 / n
        
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)
        
        if assume_same_covariance:
            X_0_centered = X_0 - self.mu_0
            X_1_centered = X_1 - self.mu_1
            
            self.sigma = (X_0_centered.T @ X_0_centered + X_1_centered.T @ X_1_centered) / n
            return self.mu_0, self.mu_1, self.sigma
        else:
            X_0_centered = X_0 - self.mu_0
            X_1_centered = X_1 - self.mu_1
            
            self.sigma_0 = (X_0_centered.T @ X_0_centered) / n_0
            self.sigma_1 = (X_1_centered.T @ X_1_centered) / n_1
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
      
        X_normalized = self.normalize_data(X)
        
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match162-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
            
            term_0 = -0.5 * np.sum(((X_normalized - self.mu_0) @ sigma_inv) * 
</FONT>                                 (X_normalized - self.mu_0), axis=1)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match162-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            term_1 = -0.5 * np.sum(((X_normalized - self.mu_1) @ sigma_inv) * 
                                 (X_normalized - self.mu_1), axis=1)
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            
            term_0 = -0.5 * (np.sum(((X_normalized - self.mu_0) @ sigma_0_inv) * 
</FONT>                                   (X_normalized - self.mu_0), axis=1) + 
                            np.log(np.linalg.det(self.sigma_0)))
            term_1 = -0.5 * (np.sum(((X_normalized - self.mu_1) @ sigma_1_inv) * 
                                   (X_normalized - self.mu_1), axis=1) + 
                            np.log(np.linalg.det(self.sigma_1)))
        
        log_prob_0 = term_0 + np.log(1 - self.phi)
        log_prob_1 = term_1 + np.log(self.phi)
        
        return (log_prob_1 &gt; log_prob_0).astype(int)










from gda import GaussianDiscriminantAnalysis
import numpy as np
import matplotlib.pyplot as plt #type: ignore



def load_data():
    X = np.loadtxt('Assignment1/data/Q4/q4x.dat')
    y_str = np.loadtxt('Assignment1/data/Q4/q4y.dat', dtype=str)
    
    y = (y_str == 'Canada').astype(int)
    
    return X, y

def plot_decision_boundary(X, y, model, title):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', label='Alaska')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Canada')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.legend()
    plt.show()


X, y = load_data()

print("\nTraining model with shared covariance...")
gda_shared = GaussianDiscriminantAnalysis()
params_shared = gda_shared.fit(X, y, assume_same_covariance=True)
print(params_shared)
y_pred_shared = gda_shared.predict(X)
accuracy_shared = np.mean(y_pred_shared == y)
print(f"Accuracy (shared covariance): {accuracy_shared:.4f}")

# Train and evaluate model with separate covariances
print("\nTraining model with separate covariances...")
gda_separate = GaussianDiscriminantAnalysis()
params_separate = gda_separate.fit(X, y, assume_same_covariance=False)
print(params_separate)
y_pred_separate = gda_separate.predict(X)
accuracy_separate = np.mean(y_pred_separate == y)
print(f"Accuracy (separate covariances): {accuracy_separate:.4f}")

# Plot decision boundaries
#plot_decision_boundary(X, y, gda_shared, 'GDA with Shared Covariance')
#plot_decision_boundary(X, y, gda_separate, 'GDA with Separate Covariances')


</PRE>
</PRE>
</BODY>
</HTML>
