<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_N8QMF.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_YHRMJ.py<p><PRE>


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LinearRegressor:
    def __init__(self):
        self.theta_init = np.zeros(2)
        self.cost_history = []
        self.theta_history = []
        self.max_iters = 10000
        self.tol = 1e-10


    def load_data(self, x_filename='./data/Q1/linearX.csv', y_filename='./data/Q1/linearY.csv'):
        """
        Load the feature and label data from CSV files.
        Returns:
            X: numpy array of shape (m, 2) where the first column is 1's (for the intercept)
            and the second column contains the original feature values.
            y: numpy array of shape (m,)
        """
        # Read data using pandas
        print(x_filename)
        dfX = pd.read_csv(x_filename, header=None)
        dfY = pd.read_csv(y_filename, header=None)
        x = dfX.values.flatten()  # shape (m,)
        y = dfY.values.flatten()  # shape (m,)
        m = len(x)
        # Add intercept term: X will be m x 2 with first column ones and second column x.
        
        return x, y
    

    def compute_cost(self, theta, X, y):
        """
        Compute the cost function J(θ) for linear regression.
        J(θ) = (1/(2*m)) * sum((hθ(x^(i)) - y^(i))^2)
        """
        m = len(y)
        predictions = X.dot(theta)
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        theta = self.theta_init.copy()
        m = len(y)
        
        X = np.column_stack((np.ones(m), X))
        prev_cost = self.compute_cost(theta, X, y)
        self.cost_history = []
        self.theta_history = []
        self.cost_history.append(prev_cost)
        self.theta_history.append(theta.copy())

        for iter in range(self.max_iters):
            # Compute gradient: (1/m) * X^T * (Xθ - y)
            error = X.dot(theta) - y
            gradient = (1 / m) * (X.T.dot(error))
            theta = theta - learning_rate * gradient
            cost = self.compute_cost(theta, X, y)
            self.cost_history.append(cost)
            self.theta_history.append(theta.copy())
            # stopping criteria
            if abs(prev_cost - cost) &lt; self.tol:
                print(f"Stopping gradient descent at iteration {iter+1} as cost change {abs(prev_cost - cost):.2e} &lt; tol {self.tol:.2e}")
                break
            prev_cost = cost
        return self.theta_history
    

    def plot_regression_line(self, X, y, theta):
        """
        Plot the original data points and the regression line with a black background.
        """
        plt.style.use("dark_background")  # Dark background for visibility

        plt.figure(figsize=(8, 6))

        # Scatter plot for data points
        plt.scatter(X[:, 1], y, color='#17becf', alpha=0.8, label='Data', s=5)

        # Regression line
<A NAME="5"></A><FONT color = #FF0000><A HREF="match83-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        y_vals = theta[0] + theta[1] * x_vals
        plt.plot(x_vals, y_vals, color='magenta', linewidth=2.5, label='Regression Line')

        # Labels and Title 
        plt.xlabel('Acidity', fontsize=12, fontweight='bold', color='white')
</FONT>        plt.ylabel('Density', fontsize=12, fontweight='bold', color='white')
        plt.title('Wine Density vs Acidity with Learned Regression Line', fontsize=14, fontweight='bold', color='white')

        # Grid and legend 
        plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.5)
        plt.legend()

        plt.show()

    def get_theta_grid(self, theta_history, padding=0.5, num_points=50):
        """
        Create a meshgrid for theta0 and theta1 based on the history of theta values.
        """
        theta_history = np.array(theta_history)
        t0_vals = theta_history[:, 0]
        t1_vals = theta_history[:, 1]
        t0_min, t0_max = t0_vals.min() - padding, t0_vals.max() + padding
        t1_min, t1_max = t1_vals.min() - padding, t1_vals.max() + padding
        theta0_vals = np.linspace(t0_min, t0_max, num_points)
        theta1_vals = np.linspace(t1_min, t1_max, num_points)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        return Theta0, Theta1


    def compute_cost_grid(self, Theta0, Theta1, X, y):
        """
        Compute the cost function J(θ) over a grid of theta values.
        Returns:
            J_vals: matrix of cost values with the same shape as Theta0/Theta1.
        """
        m = len(y)
        J_vals = np.zeros(Theta0.shape)
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                predictions = X.dot(theta_temp)
                error = predictions - y
                J_vals[i, j] = (1 / (2 * m)) * np.sum(error ** 2)
        return J_vals   
    

    def animate_3d(self, theta_history, X, y):
        """
        Animate the gradient descent path on a 3D surface plot of the cost function J(θ).
        """
        Theta0, Theta1 = self.get_theta_grid(theta_history, padding=0.5, num_points=100)
        J_vals = self.compute_cost_grid(Theta0, Theta1, X, y)
        
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel(r'$\theta_0$', fontsize=12)
        ax.set_ylabel(r'$\theta_1$', fontsize=12)
        ax.set_zlabel('Cost J(θ)', fontsize=12)
        ax.set_title('3D Surface of Cost Function with Gradient Descent Path')
        
        # Plotting the surface
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.8)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        
        theta_history = np.array(theta_history)
        points = []
        
        for theta in theta_history:
            cost = self.compute_cost(theta, X, y)
            # Plot the current point
            point = ax.scatter(theta[0], theta[1], cost, color='red', s=20)
            points.append(point)  #retain the point
        
        # path of gradient descent
        final_theta = theta_history[-1]
        final_cost = self.compute_cost(final_theta, X, y)
        ax.scatter(final_theta[0], final_theta[1], final_cost, color='pink', s=80, label='Final θ')
        
        ax.legend()
        plt.show()


    def animate_contour(self, theta_history, X, y):
        """
        Animate the gradient descent path on a contour plot of the cost function J(θ).
        """
        Theta0, Theta1 = self.get_theta_grid(theta_history, padding=0.5, num_points=100)
        J_vals = self.compute_cost_grid(Theta0, Theta1, X, y)
        
        # Create the contour plot
        plt.figure(figsize=(8, 6))
        cp = plt.contourf(Theta0, Theta1, J_vals, 20, cmap='viridis')
        plt.xlabel(r'$\theta_0$', fontsize=12)
        plt.ylabel(r'$\theta_1$', fontsize=12)
        plt.title('Contour Plot of Cost Function with Gradient Descent Path')
        plt.colorbar(cp)
        
        # Plotting all gradient descent points at once
        theta_history = np.array(theta_history)
        plt.plot(theta_history[:, 0], theta_history[:, 1], 'ro-', markersize=5, label='Gradient Descent Path')  # Plot path
        final_theta = theta_history[-1]
        plt.plot(final_theta[0], final_theta[1], 'p', color='pink', markersize=8, label='Final θ')  # Pink final marker
        plt.legend()
        plt.show()


    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = len(X)
        X = np.column_stack((np.ones(m), X))
        theta = self.theta_history[-1].reshape(-1, 1)
        Y = X@theta
        return Y
    
        
# def main():    
#     # learning_rate = 0.01
#     m = LinearRegressor()
#     x, y = m.load_data()
#     tolerances = [1e-9]
#     learning_rates = [0.5]

#     for tol in tolerances:
#         for learning_rate in learning_rates:
#             print(f"\nStarting gradient descent with learning rate (η) = {learning_rate} and tolerance = {tol}")
#             m.tol = tol
#             theta_history = m.fit(x, y, learning_rate)
#             final_theta = theta_history[-1]
#             print("\nFinal parameters obtained from gradient descent:")
#             print(f"θ0 = {final_theta[0]:.4f}, θ1 = {final_theta[1]:.4f}")
            
#             # Testing predict:
#             Y = m.predict(x)
#             length = len(x)
#             X = np.column_stack((np.ones(length), x))
            
#             # Plot the data and the learned hypothesis (regression line)
#             m.plot_regression_line(X, y, final_theta)
    
#     print("\nAnimating gradient descent on 3D cost surface...")
#     m.animate_3d(theta_history, X, y)
    
#     print("\nAnimating gradient descent on contour plot...")
#     m.animate_contour(theta_history, X, y)
        
#     print("Done...... he he he")


# if __name__ == "__main__":
#     main()



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LinearRegressor:
    def __init__(self):
        self.theta_init = np.zeros(2)
        self.cost_history = []
        self.theta_history = []
        self.max_iters = 10000
        self.tol = 1e-10


    def load_data(self, x_filename='./data/Q1/linearX.csv', y_filename='./data/Q1/linearY.csv'):
        """
        Load the feature and label data from CSV files.
        Returns:
            X: numpy array of shape (m, 2) where the first column is 1's (for the intercept)
            and the second column contains the original feature values.
            y: numpy array of shape (m,)
        """
        # Read data using pandas
        print(x_filename)
        dfX = pd.read_csv(x_filename, header=None)
        dfY = pd.read_csv(y_filename, header=None)
        x = dfX.values.flatten()  # shape (m,)
        y = dfY.values.flatten()  # shape (m,)
        m = len(x)
        # Add intercept term: X will be m x 2 with first column ones and second column x.
        
        return x, y
    

    def compute_cost(self, theta, X, y):
        """
        Compute the cost function J(θ) for linear regression.
        J(θ) = (1/(2*m)) * sum((hθ(x^(i)) - y^(i))^2)
        """
        m = len(y)
        predictions = X.dot(theta)
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        theta = self.theta_init.copy()
        m = len(y)
        
        X = np.column_stack((np.ones(m), X))
        prev_cost = self.compute_cost(theta, X, y)
        self.cost_history = []
        self.theta_history = []
        self.cost_history.append(prev_cost)
        self.theta_history.append(theta.copy())

        for iter in range(self.max_iters):
            # Compute gradient: (1/m) * X^T * (Xθ - y)
            error = X.dot(theta) - y
            gradient = (1 / m) * (X.T.dot(error))
            theta = theta - learning_rate * gradient
            cost = self.compute_cost(theta, X, y)
            self.cost_history.append(cost)
            self.theta_history.append(theta.copy())
            # stopping criteria
            if abs(prev_cost - cost) &lt; self.tol:
                print(f"Stopping gradient descent at iteration {iter+1} as cost change {abs(prev_cost - cost):.2e} &lt; tol {self.tol:.2e}")
                break
            prev_cost = cost
        return self.theta_history
    

    def plot_regression_line(self, X, y, theta):
        """
        Plot the original data points and the regression line with a black background.
        """
        plt.style.use("dark_background")  # Dark background for visibility

        plt.figure(figsize=(8, 6))

        # Scatter plot for data points
        plt.scatter(X[:, 1], y, color='#17becf', alpha=0.8, label='Data', s=5)

        # Regression line
        x_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        y_vals = theta[0] + theta[1] * x_vals
        plt.plot(x_vals, y_vals, color='magenta', linewidth=2.5, label='Regression Line')

        # Labels and Title 
        plt.xlabel('Acidity', fontsize=12, fontweight='bold', color='white')
        plt.ylabel('Density', fontsize=12, fontweight='bold', color='white')
        plt.title('Wine Density vs Acidity with Learned Regression Line', fontsize=14, fontweight='bold', color='white')

        # Grid and legend 
        plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.5)
        plt.legend()

        plt.show()

    def get_theta_grid(self, theta_history, padding=0.5, num_points=50):
        """
        Create a meshgrid for theta0 and theta1 based on the history of theta values.
        """
        theta_history = np.array(theta_history)
        t0_vals = theta_history[:, 0]
        t1_vals = theta_history[:, 1]
        t0_min, t0_max = t0_vals.min() - padding, t0_vals.max() + padding
        t1_min, t1_max = t1_vals.min() - padding, t1_vals.max() + padding
        theta0_vals = np.linspace(t0_min, t0_max, num_points)
        theta1_vals = np.linspace(t1_min, t1_max, num_points)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        return Theta0, Theta1


    def compute_cost_grid(self, Theta0, Theta1, X, y):
        """
        Compute the cost function J(θ) over a grid of theta values.
        Returns:
            J_vals: matrix of cost values with the same shape as Theta0/Theta1.
        """
        m = len(y)
        J_vals = np.zeros(Theta0.shape)
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                predictions = X.dot(theta_temp)
                error = predictions - y
                J_vals[i, j] = (1 / (2 * m)) * np.sum(error ** 2)
        return J_vals   
    

    def animate_3d(self, theta_history, X, y):
        """
        Animate the gradient descent path on a 3D surface plot of the cost function J(θ).
        """
        Theta0, Theta1 = self.get_theta_grid(theta_history, padding=0.5, num_points=100)
        J_vals = self.compute_cost_grid(Theta0, Theta1, X, y)
        
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel(r'$\theta_0$', fontsize=12)
        ax.set_ylabel(r'$\theta_1$', fontsize=12)
        ax.set_zlabel('Cost J(θ)', fontsize=12)
        ax.set_title('3D Surface of Cost Function with Gradient Descent Path')
        
        # Plotting the surface
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.8)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        
        theta_history = np.array(theta_history)
        points = []
        
        for theta in theta_history:
            cost = self.compute_cost(theta, X, y)
            # Plot the current point
            point = ax.scatter(theta[0], theta[1], cost, color='red', s=20)
            points.append(point)  #retain the point
        
        # path of gradient descent
        final_theta = theta_history[-1]
        final_cost = self.compute_cost(final_theta, X, y)
        ax.scatter(final_theta[0], final_theta[1], final_cost, color='pink', s=80, label='Final θ')
        
        ax.legend()
        plt.show()


    def animate_contour(self, theta_history, X, y):
        """
        Animate the gradient descent path on a contour plot of the cost function J(θ).
        """
        Theta0, Theta1 = self.get_theta_grid(theta_history, padding=0.5, num_points=100)
        J_vals = self.compute_cost_grid(Theta0, Theta1, X, y)
        
        # Create the contour plot
        plt.figure(figsize=(8, 6))
        cp = plt.contourf(Theta0, Theta1, J_vals, 20, cmap='viridis')
        plt.xlabel(r'$\theta_0$', fontsize=12)
        plt.ylabel(r'$\theta_1$', fontsize=12)
        plt.title('Contour Plot of Cost Function with Gradient Descent Path')
        plt.colorbar(cp)
        
        # Plotting all gradient descent points at once
        theta_history = np.array(theta_history)
        plt.plot(theta_history[:, 0], theta_history[:, 1], 'ro-', markersize=5, label='Gradient Descent Path')  # Plot path
        final_theta = theta_history[-1]
        plt.plot(final_theta[0], final_theta[1], 'p', color='pink', markersize=8, label='Final θ')  # Pink final marker
        plt.legend()
        plt.show()


    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = len(X)
        X = np.column_stack((np.ones(m), X))
        theta = self.theta_history[-1].reshape(-1, 1)
        Y = X@theta
        return Y
    
        
# def main():    
#     # learning_rate = 0.01
#     m = LinearRegressor()
#     x, y = m.load_data()
#     tolerances = [1e-9]
#     learning_rates = [0.5]

#     for tol in tolerances:
#         for learning_rate in learning_rates:
#             print(f"\nStarting gradient descent with learning rate (η) = {learning_rate} and tolerance = {tol}")
#             m.tol = tol
#             theta_history = m.fit(x, y, learning_rate)
#             final_theta = theta_history[-1]
#             print("\nFinal parameters obtained from gradient descent:")
#             print(f"θ0 = {final_theta[0]:.4f}, θ1 = {final_theta[1]:.4f}")
            
#             # Testing predict:
#             Y = m.predict(x)
#             length = len(x)
#             X = np.column_stack((np.ones(length), x))
            
#             # Plot the data and the learned hypothesis (regression line)
#             m.plot_regression_line(X, y, final_theta)
    
#     print("\nAnimating gradient descent on 3D cost surface...")
#     m.animate_3d(theta_history, X, y)
    
#     print("\nAnimating gradient descent on contour plot...")
#     m.animate_contour(theta_history, X, y)
        
#     print("Done...... he he he")


# if __name__ == "__main__":
#     main()



# Imports - you can add any other permitted libraries
import sys
import numpy as np
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match83-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
</FONT>    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    # Generate x1 and x2
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)

    # Create X matrix with intercept term
    x = np.vstack([x1,x2]).T
    #adding offset for y to be computed
    X = np.vstack([np.ones(N), x1, x2]).T
    
    # Generate y with Gaussian noise
    noise = np.random.normal(0, np.sqrt(noise_sigma), N)
    y = np.dot(X, theta) + noise
    
    return x, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = []
        self.epochs = 250000
        self.batch_size = 80
        self.batch_sizes = [80]
        self.tol = 1e-6
    def compute_cost(self, theta, X, y):
        """
        Compute the cost function J(θ) for linear regression.
        """
        m = len(y)
        predictions = X.dot(theta)
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        X = np.hstack([np.ones((m, 1)), X])
        m, n = X.shape
        theta = np.zeros(n)
        cost_history = []
        theta_history = []

        # Initial cost
        prev_cost = self.compute_cost(theta, X, y)
        cost_history.append(prev_cost)
        theta_history.append(theta.copy())

        for epoch in range(self.epochs):
            # Shuffle the data
            sys.stdout.write('\rEpoch: ' + str(epoch + 1))
            sys.stdout.flush()
            indices = np.random.permutation(m)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match83-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            X_shuffled = X[indices]
            y_shuffled = y[indices]

            # Update theta using each batch
            for b in range(0, m, self.batch_size):
                X_batch = X_shuffled[b:b+self.batch_size]
</FONT>                y_batch = y_shuffled[b:b+self.batch_size]

                # Compute the gradient
                error = X_batch.dot(theta) - y_batch
                gradients = (1 / self.batch_size) * X_batch.T.dot(error)

                # Update the parameters
                theta -= learning_rate * gradients

            cost = self.compute_cost(theta, X, y)
            cost_history.append(cost)
            theta_history.append(theta.copy())

            # Checking for convergence
            if abs(prev_cost - cost) &lt; self.tol:
                print(f"Converged at epoch {epoch+1} with cost change {abs(prev_cost - cost):.2e}")
                break
            prev_cost = cost

        return theta_history


    def plot_regression_line(self, X, y, theta):
        """
        Plot the original data points and the regression line.
        """
        plt.figure(figsize=(8, 6))
        plt.scatter(X[:, 1], y, color='blue', label='Data',s=10)
        x_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        y_vals = theta[0] + theta[1] * x_vals + theta[2] * x_vals
        plt.plot(x_vals, y_vals, color='red', label='Regression Line')
        plt.xlabel('x1')
        plt.ylabel('y')
        plt.title('Regression Line from SGD')
        plt.legend()
        plt.show()

    def plot_3d_movement(self, theta_history):
        """
        Plot the movement of θ in the 3D parameter space as the parameters are updated.
        """
        theta_history = np.array(theta_history)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel('Theta0')
        ax.set_ylabel('Theta1')
        ax.set_zlabel('Theta2')
        
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], color='blue', label='SGD Path')
        ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2], color='red', s=100, label='Start')
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], color='green', s=100, label='End')
        
        ax.legend()
        plt.title('Parameter Movement in 3D Space')
        plt.show()

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape
        X = np.hstack([np.ones((m, 1)), X]) 
        return X.dot(self.theta)


def closed_form_solution(X, y):
    """
    Compute the closed-form solution for Linear Regression
    θ = (X^T X)^-1 X^T y
    """
    X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term
    theta_closed_form = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
    return theta_closed_form

def mse(X, y, theta):
        m,n = X.shape
        X = np.hstack([np.ones((m, 1)), X])
        y_pred = X.dot(theta)
        return np.mean((y - y_pred) ** 2)

# def main():
#     N = 1000000
#     true_theta = np.array([3, 1, 2])
#     # Generate synthetic data
#     x, y = generate(N=N, theta=np.array([3, 1, 2]), input_mean=np.array([3, -1]), input_sigma=np.array([4, 4]), noise_sigma=2)
    
#     # Saving the generated data to CSV files to reduce varibles across tests
#     np.savetxt('x_data.csv', x, delimiter=',')
#     np.savetxt('y_data.csv', y, delimiter=',')
    
#     # Read the data back from the CSV files
#     x = np.loadtxt('x_data.csv', delimiter=',')
#     y = np.loadtxt('y_data.csv', delimiter=',')
    
#     # Split the data into training (80%) and test (20%) sets
#     train_size = int(0.8 * len(y))
#     x_train, x_test = x[:train_size], x[train_size:]
#     y_train, y_test = y[:train_size], y[train_size:]
    
#     # Initialize parameters
#     model = StochasticLinearRegressor()
#     for batch_size in model.batch_sizes:
#         print(f"\nRunning SGD with batch size = {batch_size}...")
#         # start_time = time.time()
#         theta_history = model.fit(X=x_train, y=y_train, learning_rate=0.001)
#         # end_time = time.time()
#         # print(f"Time taken for SGD with batch size {batch_size}: {end_time - start_time:.2f} seconds")
        
#         # Plotting regression line
#         theta_learned = theta_history[-1]
#         model.plot_regression_line(x_train, y_train, theta_learned)
        
#         # Plotting parameter movement in 3D space
#         model.plot_3d_movement(theta_history)
        
#         print(f"Learned parameters for batch size {batch_size}: {theta_learned}")

#         test_error = mse(x_test, y_test, theta_learned)
#         train_error = mse(x_train, y_train, theta_learned)
        
#         print(f"\nTest Error: {test_error:.4f}")
#         print(f"Training Error: {train_error:.4f}")
    
#     theta_closed_form = closed_form_solution(x_train, y_train)
#     print(f"Closed-form Parameters: {theta_closed_form}")


#     train_error_closed_form = model.compute_cost(theta_closed_form, np.hstack([np.ones((x_train.shape[0], 1)), x_train]), y_train)
#     test_error_closed_form = model.compute_cost(theta_closed_form, np.hstack([np.ones((x_test.shape[0], 1)), x_test]), y_test)
#     print(f"Training Error (Closed-form): {train_error_closed_form:.4f}")
#     print(f"Test Error (Closed-form): {test_error_closed_form:.4f}")


# if __name__ == "__main__":
#     main()



# Imports - you can add any other permitted libraries
import sys
import numpy as np
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    # Generate x1 and x2
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)

    # Create X matrix with intercept term
    x = np.vstack([x1,x2]).T
    #adding offset for y to be computed
    X = np.vstack([np.ones(N), x1, x2]).T
    
    # Generate y with Gaussian noise
    noise = np.random.normal(0, np.sqrt(noise_sigma), N)
    y = np.dot(X, theta) + noise
    
    return x, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = []
        self.epochs = 250000
        self.batch_size = 80
        self.batch_sizes = [80]
        self.tol = 1e-6
    def compute_cost(self, theta, X, y):
        """
        Compute the cost function J(θ) for linear regression.
        """
        m = len(y)
        predictions = X.dot(theta)
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        X = np.hstack([np.ones((m, 1)), X])
        m, n = X.shape
        theta = np.zeros(n)
        cost_history = []
        theta_history = []

        # Initial cost
        prev_cost = self.compute_cost(theta, X, y)
        cost_history.append(prev_cost)
        theta_history.append(theta.copy())

        for epoch in range(self.epochs):
            # Shuffle the data
            sys.stdout.write('\rEpoch: ' + str(epoch + 1))
            sys.stdout.flush()
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            # Update theta using each batch
            for b in range(0, m, self.batch_size):
                X_batch = X_shuffled[b:b+self.batch_size]
                y_batch = y_shuffled[b:b+self.batch_size]

                # Compute the gradient
                error = X_batch.dot(theta) - y_batch
                gradients = (1 / self.batch_size) * X_batch.T.dot(error)

                # Update the parameters
                theta -= learning_rate * gradients

            cost = self.compute_cost(theta, X, y)
            cost_history.append(cost)
            theta_history.append(theta.copy())

            # Checking for convergence
            if abs(prev_cost - cost) &lt; self.tol:
                print(f"Converged at epoch {epoch+1} with cost change {abs(prev_cost - cost):.2e}")
                break
            prev_cost = cost

        return theta_history


    def plot_regression_line(self, X, y, theta):
        """
        Plot the original data points and the regression line.
        """
        plt.figure(figsize=(8, 6))
        plt.scatter(X[:, 1], y, color='blue', label='Data',s=10)
        x_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        y_vals = theta[0] + theta[1] * x_vals + theta[2] * x_vals
        plt.plot(x_vals, y_vals, color='red', label='Regression Line')
        plt.xlabel('x1')
        plt.ylabel('y')
        plt.title('Regression Line from SGD')
        plt.legend()
        plt.show()

    def plot_3d_movement(self, theta_history):
        """
        Plot the movement of θ in the 3D parameter space as the parameters are updated.
        """
        theta_history = np.array(theta_history)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_xlabel('Theta0')
        ax.set_ylabel('Theta1')
        ax.set_zlabel('Theta2')
        
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], color='blue', label='SGD Path')
        ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2], color='red', s=100, label='Start')
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], color='green', s=100, label='End')
        
        ax.legend()
        plt.title('Parameter Movement in 3D Space')
        plt.show()

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape
        X = np.hstack([np.ones((m, 1)), X]) 
        return X.dot(self.theta)


def closed_form_solution(X, y):
    """
    Compute the closed-form solution for Linear Regression
    θ = (X^T X)^-1 X^T y
    """
    X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term
    theta_closed_form = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
    return theta_closed_form

def mse(X, y, theta):
        m,n = X.shape
        X = np.hstack([np.ones((m, 1)), X])
        y_pred = X.dot(theta)
        return np.mean((y - y_pred) ** 2)

# def main():
#     N = 1000000
#     true_theta = np.array([3, 1, 2])
#     # Generate synthetic data
#     x, y = generate(N=N, theta=np.array([3, 1, 2]), input_mean=np.array([3, -1]), input_sigma=np.array([4, 4]), noise_sigma=2)
    
#     # Saving the generated data to CSV files to reduce varibles across tests
#     np.savetxt('x_data.csv', x, delimiter=',')
#     np.savetxt('y_data.csv', y, delimiter=',')
    
#     # Read the data back from the CSV files
#     x = np.loadtxt('x_data.csv', delimiter=',')
#     y = np.loadtxt('y_data.csv', delimiter=',')
    
#     # Split the data into training (80%) and test (20%) sets
#     train_size = int(0.8 * len(y))
#     x_train, x_test = x[:train_size], x[train_size:]
#     y_train, y_test = y[:train_size], y[train_size:]
    
#     # Initialize parameters
#     model = StochasticLinearRegressor()
#     for batch_size in model.batch_sizes:
#         print(f"\nRunning SGD with batch size = {batch_size}...")
#         # start_time = time.time()
#         theta_history = model.fit(X=x_train, y=y_train, learning_rate=0.001)
#         # end_time = time.time()
#         # print(f"Time taken for SGD with batch size {batch_size}: {end_time - start_time:.2f} seconds")
        
#         # Plotting regression line
#         theta_learned = theta_history[-1]
#         model.plot_regression_line(x_train, y_train, theta_learned)
        
#         # Plotting parameter movement in 3D space
#         model.plot_3d_movement(theta_history)
        
#         print(f"Learned parameters for batch size {batch_size}: {theta_learned}")

#         test_error = mse(x_test, y_test, theta_learned)
#         train_error = mse(x_train, y_train, theta_learned)
        
#         print(f"\nTest Error: {test_error:.4f}")
#         print(f"Training Error: {train_error:.4f}")
    
#     theta_closed_form = closed_form_solution(x_train, y_train)
#     print(f"Closed-form Parameters: {theta_closed_form}")


#     train_error_closed_form = model.compute_cost(theta_closed_form, np.hstack([np.ones((x_train.shape[0], 1)), x_train]), y_train)
#     test_error_closed_form = model.compute_cost(theta_closed_form, np.hstack([np.ones((x_test.shape[0], 1)), x_test]), y_test)
#     print(f"Training Error (Closed-form): {train_error_closed_form:.4f}")
#     print(f"Test Error (Closed-form): {test_error_closed_form:.4f}")


# if __name__ == "__main__":
#     main()



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.max_iters = 100000
        self.tol = 1e-7
        self.theta = None


    def sigmoid(self, z):
        """Compute the sigmoid of z."""
        return 1 / (1 + np.exp(-z))


    def compute_gradient(self, X, y, theta):
        """Compute the gradient of the log-likelihood function."""
        m = len(y)  
        predictions = self.sigmoid(X.dot(theta))
        gradient = (1 / m) * X.T.dot(predictions - y)
        return gradient


    def compute_hessian(self, X, y, theta):
        """Compute the Hessian matrix of the log-likelihood function."""
        m = len(y)
        predictions = self.sigmoid(X.dot(theta))
        R = np.diag(predictions * (1 - predictions))  # Diagonal matrix of probabilities
        hessian = (1 / m) * X.T.dot(R).dot(X)  # Hessian = X^T * R * X
        return hessian


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])  # Add intercept term (column of 1s)

        # Initialize parameters (theta)
        theta_new = np.zeros(X_norm.shape[1])
        self.theta = theta_new.copy()
        history = []

        for iter in range(self.max_iters):
            gradient = self.compute_gradient(X_norm, y, theta_new)
            hessian = self.compute_hessian(X_norm, y, theta_new)

            # Update rule: θ = θ - eta*H^-1 * ∇L(θ)
            theta_new -= learning_rate*np.linalg.inv(hessian).dot(gradient)
            history.append(theta_new.copy())
            theta = theta_new.copy()

            if np.linalg.norm(gradient) &lt; self.tol:
                print(f"Stopping criterion met: Gradient norm below threshold. at iteration {iter}")
                break

                

        return np.array(history)
    

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm]) 
        predictions = self.sigmoid(X_norm.dot(self.theta)) &gt;= 0.5  
        return predictions.astype(int)
    
    def plot_decision_boundary(self, X, y, theta):
        """Plot the decision boundary."""
        x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
        x2_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        xx1, xx2 = np.meshgrid(x1_vals, x2_vals)
        X_grid = np.c_[xx1.ravel(), xx2.ravel()]
        
        X_norm = (X_grid - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])
        predictions = self.sigmoid(X_norm.dot(theta)).reshape(xx1.shape)

        plt.contour(xx1, xx2, predictions, levels=[0.5], cmap="RdBu", alpha=0.7)
        plt.scatter(X[:, 0], X[:, 1], c=y, cmap="RdBu", edgecolors='k', marker='o')
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.title("Logistic Regression Decision Boundary")
        plt.show()


def load_data():
    """Load data from the CSV files and return normalized X and y."""
    # Load the data
    X = pd.read_csv('./data/Q3/logisticX.csv', header=None).values
    y = pd.read_csv('./data/Q3/logisticY.csv', header=None).values.flatten()

    return X, y

# def main():
#     # Load data
#     X, y = load_data()

#     # Initialize the logistic regressor
#     model = LogisticRegressor()

#     # Fit the model to the data
#     history = model.fit(X, y)

#     # Extract the final learned parameters
#     theta = history[-1]
#     print(f"Learned parameters: {theta}")

#     # Plot the training data and decision boundary
#     model.plot_decision_boundary(X, y, theta)

# if __name__ == "__main__":
#     main()




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.max_iters = 100000
        self.tol = 1e-7
        self.theta = None


    def sigmoid(self, z):
        """Compute the sigmoid of z."""
        return 1 / (1 + np.exp(-z))


    def compute_gradient(self, X, y, theta):
        """Compute the gradient of the log-likelihood function."""
        m = len(y)  
        predictions = self.sigmoid(X.dot(theta))
        gradient = (1 / m) * X.T.dot(predictions - y)
        return gradient


    def compute_hessian(self, X, y, theta):
        """Compute the Hessian matrix of the log-likelihood function."""
        m = len(y)
        predictions = self.sigmoid(X.dot(theta))
        R = np.diag(predictions * (1 - predictions))  # Diagonal matrix of probabilities
        hessian = (1 / m) * X.T.dot(R).dot(X)  # Hessian = X^T * R * X
        return hessian


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])  # Add intercept term (column of 1s)

        # Initialize parameters (theta)
        theta_new = np.zeros(X_norm.shape[1])
        self.theta = theta_new.copy()
        history = []

        for iter in range(self.max_iters):
            gradient = self.compute_gradient(X_norm, y, theta_new)
            hessian = self.compute_hessian(X_norm, y, theta_new)

            # Update rule: θ = θ - eta*H^-1 * ∇L(θ)
            theta_new -= learning_rate*np.linalg.inv(hessian).dot(gradient)
            history.append(theta_new.copy())
            theta = theta_new.copy()

            if np.linalg.norm(gradient) &lt; self.tol:
                print(f"Stopping criterion met: Gradient norm below threshold. at iteration {iter}")
                break

                

        return np.array(history)
    

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm]) 
        predictions = self.sigmoid(X_norm.dot(self.theta)) &gt;= 0.5  
        return predictions.astype(int)
    
    def plot_decision_boundary(self, X, y, theta):
        """Plot the decision boundary."""
        x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
        x2_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        xx1, xx2 = np.meshgrid(x1_vals, x2_vals)
        X_grid = np.c_[xx1.ravel(), xx2.ravel()]
        
        X_norm = (X_grid - np.mean(X, axis=0)) / np.std(X, axis=0)
        X_norm = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])
        predictions = self.sigmoid(X_norm.dot(theta)).reshape(xx1.shape)

        plt.contour(xx1, xx2, predictions, levels=[0.5], cmap="RdBu", alpha=0.7)
        plt.scatter(X[:, 0], X[:, 1], c=y, cmap="RdBu", edgecolors='k', marker='o')
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.title("Logistic Regression Decision Boundary")
        plt.show()


def load_data():
    """Load data from the CSV files and return normalized X and y."""
    # Load the data
    X = pd.read_csv('./data/Q3/logisticX.csv', header=None).values
    y = pd.read_csv('./data/Q3/logisticY.csv', header=None).values.flatten()

    return X, y

# def main():
#     # Load data
#     X, y = load_data()

#     # Initialize the logistic regressor
#     model = LogisticRegressor()

#     # Fit the model to the data
#     history = model.fit(X, y)

#     # Extract the final learned parameters
#     theta = history[-1]
#     print(f"Learned parameters: {theta}")

#     # Plot the training data and decision boundary
#     model.plot_decision_boundary(X, y, theta)

# if __name__ == "__main__":
#     main()




import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = []
        self.mu_1 = []
        self.sigma_0 = []
        self.sigma_1 = []
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Normalize the data X before fitting.
        """
        #initially i normnalized the data but it was not required
        X_norm = X
        
        # Calculate means of each class
        self.mu_0 = np.mean(X_norm[y == 0], axis=0)
        self.mu_1 = np.mean(X_norm[y == 1], axis=0)

        # Calculate covariance matrix (shared or different for each class)
        if assume_same_covariance:
            self.sigma_0 = np.cov(X_norm.T)  # Shared covariance matrix
            self.sigma_1 = self.sigma_0
            return (self.mu_0, self.mu_1, self.sigma_0)
        else:
            # Covariance for each class
            self.sigma_0 = np.cov(X_norm[y == 0].T)
            self.sigma_1 = np.cov(X_norm[y == 1].T)
            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        if self.sigma_0 is None or self.sigma_1 is None:
            raise ValueError("Model not yet trained. Please fit the model first.")
        
        # For the case where we assume same covariance matrix
<A NAME="0"></A><FONT color = #FF0000><A HREF="match83-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        if hasattr(self, 'sigma_1') and self.sigma_0.shape == self.sigma_1.shape:
            inv_sigma = np.linalg.inv(self.sigma_0)
            
            # Calculate the discriminant function for each class
            diff_0 = X - self.mu_0
            diff_1 = X - self.mu_1

            # Compute discriminant functions for both classes
            g_0 = np.sum(diff_0 @ inv_sigma * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma * diff_1, axis=1)
</FONT>
            # Predict the class with higher likelihood
            y_pred = (g_0 &lt; g_1).astype(int)  # Predict class 1 if g_1 &gt; g_0, else class 0
            return y_pred
        
        else:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match83-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            diff_0 = X - self.mu_0
            diff_1 = X - self.mu_1

            # Compute discriminant functions for both classes
            g_0 = np.sum(diff_0 @ inv_sigma_0 * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma_1 * diff_1, axis=1)
</FONT>
            # Predict the class with higher likelihood
            y_pred = (g_0 &lt; g_1).astype(int)
            return y_pred

    
    def plot_decision_boundary(self, X, y, mu_0, mu_1, sigma, sigma_0=None, sigma_1=None, assume_same_covariance=True):
        """
        Plot the decision boundary obtained from the Gaussian Discriminant Analysis model.
        """
        # Plot data points for class 0 and class 1
        plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Canada", color='blue', marker='o')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Alaska", color='red', marker='x')
        
        # Generate a grid of points to plot the decision boundary
        x_min, x_max = X[:, 0].min() - 1 - 10, X[:, 0].max() + 1 + 10
        y_min, y_max = X[:, 1].min() - 1 - 10, X[:, 1].max() + 1 + 10
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))
        grid = np.c_[xx.ravel(), yy.ravel()]
        
        # Predict class labels for each point in the grid
        if assume_same_covariance:
            inv_sigma = np.linalg.inv(sigma)
            diff_0 = grid - mu_0
            diff_1 = grid - mu_1
            g_0 = np.sum(diff_0 @ inv_sigma * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma * diff_1, axis=1)
            y_pred = (g_0 &lt; g_1).astype(int)
            # for i in range(len(y_pred)):
            #     print(y_pred[i])
        else:
            inv_sigma_0 = np.linalg.inv(sigma_0)
            inv_sigma_1 = np.linalg.inv(sigma_1)
            diff_0 = grid - mu_0
            diff_1 = grid - mu_1
            g_0 = np.sum(diff_0 @ inv_sigma_0 * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma_1 * diff_1, axis=1)
            y_pred = (g_0 &lt; g_1).astype(int)
            # for i in range(len(y_pred)):
            #     print(y_pred[i])
        
        y_pred = y_pred.reshape(xx.shape)

        # Plot the decision boundary (where g_0 = g_1)
        plt.contour(xx, yy, y_pred, levels=[0.5], colors='black', linewidths=2)
        plt.title("GDA Decision Boundary")
        plt.xlabel("Feature 1(fresh_water)")
        plt.ylabel("Feature 2(marine water)")
        plt.legend()
        plt.show()

def load_data():
    """Load data from the files q4x.dat and q4y.dat."""
    X = np.loadtxt('./data/Q4/q4x.dat')
    y_raw = np.loadtxt('./data/Q4/q4y.dat', dtype=str)

    y = np.where(y_raw == 'Alaska', 0, 1)
    return X, y

# def main():
#     # Load the data
#     X, y = load_data()

#     # Initialize the model
#     model = GaussianDiscriminantAnalysis()

#     # Fit the model assuming both classes have the same covariance matrix
#     mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)

#     # Print the learned parameters (mean vectors and covariance matrix)
#     print(f"mu_0: {mu_0}")
#     print(f"mu_1: {mu_1}")
#     print(f"sigma: {sigma}")

#     # Plot the decision boundary for the linear case (same covariance matrix)
#     model.plot_decision_boundary(X, y, mu_0, mu_1, sigma, assume_same_covariance=True)

#     # Fit the model assuming different covariance matrices for each class
#     mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)

#     # Print the learned parameters (mean vectors and covariance matrices)
#     print(f"mu_0: {mu_0}")
#     print(f"mu_1: {mu_1}")
#     print(f"sigma_0: {sigma_0}")
#     print(f"sigma_1: {sigma_1}")

#     # Plot the decision boundary for the quadratic case (different covariance matrices)
#     model.plot_decision_boundary(X, y, mu_0, mu_1, sigma=None, sigma_0=sigma_0, sigma_1=sigma_1, assume_same_covariance=False)

# if __name__ == "__main__":
#     main()




import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = []
        self.mu_1 = []
        self.sigma_0 = []
        self.sigma_1 = []
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Normalize the data X before fitting.
        """
        #initially i normnalized the data but it was not required
        X_norm = X
        
        # Calculate means of each class
        self.mu_0 = np.mean(X_norm[y == 0], axis=0)
        self.mu_1 = np.mean(X_norm[y == 1], axis=0)

        # Calculate covariance matrix (shared or different for each class)
        if assume_same_covariance:
            self.sigma_0 = np.cov(X_norm.T)  # Shared covariance matrix
            self.sigma_1 = self.sigma_0
            return (self.mu_0, self.mu_1, self.sigma_0)
        else:
            # Covariance for each class
            self.sigma_0 = np.cov(X_norm[y == 0].T)
            self.sigma_1 = np.cov(X_norm[y == 1].T)
            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        """
        if self.sigma_0 is None or self.sigma_1 is None:
            raise ValueError("Model not yet trained. Please fit the model first.")
        
        # For the case where we assume same covariance matrix
        if hasattr(self, 'sigma_1') and self.sigma_0.shape == self.sigma_1.shape:
<A NAME="2"></A><FONT color = #0000FF><A HREF="match83-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            inv_sigma = np.linalg.inv(self.sigma_0)
            
            # Calculate the discriminant function for each class
            diff_0 = X - self.mu_0
            diff_1 = X - self.mu_1

            # Compute discriminant functions for both classes
            g_0 = np.sum(diff_0 @ inv_sigma * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma * diff_1, axis=1)
</FONT>
            # Predict the class with higher likelihood
            y_pred = (g_0 &lt; g_1).astype(int)  # Predict class 1 if g_1 &gt; g_0, else class 0
            return y_pred
        
        else:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            diff_0 = X - self.mu_0
            diff_1 = X - self.mu_1

            # Compute discriminant functions for both classes
            g_0 = np.sum(diff_0 @ inv_sigma_0 * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma_1 * diff_1, axis=1)

            # Predict the class with higher likelihood
            y_pred = (g_0 &lt; g_1).astype(int)
            return y_pred

    
    def plot_decision_boundary(self, X, y, mu_0, mu_1, sigma, sigma_0=None, sigma_1=None, assume_same_covariance=True):
        """
        Plot the decision boundary obtained from the Gaussian Discriminant Analysis model.
        """
        # Plot data points for class 0 and class 1
        plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Canada", color='blue', marker='o')
        plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Alaska", color='red', marker='x')
        
        # Generate a grid of points to plot the decision boundary
        x_min, x_max = X[:, 0].min() - 1 - 10, X[:, 0].max() + 1 + 10
        y_min, y_max = X[:, 1].min() - 1 - 10, X[:, 1].max() + 1 + 10
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))
        grid = np.c_[xx.ravel(), yy.ravel()]
        
        # Predict class labels for each point in the grid
        if assume_same_covariance:
            inv_sigma = np.linalg.inv(sigma)
            diff_0 = grid - mu_0
            diff_1 = grid - mu_1
            g_0 = np.sum(diff_0 @ inv_sigma * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma * diff_1, axis=1)
            y_pred = (g_0 &lt; g_1).astype(int)
            # for i in range(len(y_pred)):
            #     print(y_pred[i])
        else:
            inv_sigma_0 = np.linalg.inv(sigma_0)
            inv_sigma_1 = np.linalg.inv(sigma_1)
            diff_0 = grid - mu_0
            diff_1 = grid - mu_1
            g_0 = np.sum(diff_0 @ inv_sigma_0 * diff_0, axis=1)
            g_1 = np.sum(diff_1 @ inv_sigma_1 * diff_1, axis=1)
            y_pred = (g_0 &lt; g_1).astype(int)
            # for i in range(len(y_pred)):
            #     print(y_pred[i])
        
        y_pred = y_pred.reshape(xx.shape)

        # Plot the decision boundary (where g_0 = g_1)
        plt.contour(xx, yy, y_pred, levels=[0.5], colors='black', linewidths=2)
        plt.title("GDA Decision Boundary")
        plt.xlabel("Feature 1(fresh_water)")
        plt.ylabel("Feature 2(marine water)")
        plt.legend()
        plt.show()

def load_data():
    """Load data from the files q4x.dat and q4y.dat."""
    X = np.loadtxt('./data/Q4/q4x.dat')
    y_raw = np.loadtxt('./data/Q4/q4y.dat', dtype=str)

    y = np.where(y_raw == 'Alaska', 0, 1)
    return X, y

# def main():
#     # Load the data
#     X, y = load_data()

#     # Initialize the model
#     model = GaussianDiscriminantAnalysis()

#     # Fit the model assuming both classes have the same covariance matrix
#     mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)

#     # Print the learned parameters (mean vectors and covariance matrix)
#     print(f"mu_0: {mu_0}")
#     print(f"mu_1: {mu_1}")
#     print(f"sigma: {sigma}")

#     # Plot the decision boundary for the linear case (same covariance matrix)
#     model.plot_decision_boundary(X, y, mu_0, mu_1, sigma, assume_same_covariance=True)

#     # Fit the model assuming different covariance matrices for each class
#     mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)

#     # Print the learned parameters (mean vectors and covariance matrices)
#     print(f"mu_0: {mu_0}")
#     print(f"mu_1: {mu_1}")
#     print(f"sigma_0: {sigma_0}")
#     print(f"sigma_1: {sigma_1}")

#     # Plot the decision boundary for the quadratic case (different covariance matrices)
#     model.plot_decision_boundary(X, y, mu_0, mu_1, sigma=None, sigma_0=sigma_0, sigma_1=sigma_1, assume_same_covariance=False)

# if __name__ == "__main__":
#     main()


</PRE>
</PRE>
</BODY>
</HTML>
