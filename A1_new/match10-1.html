<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_O1KGG.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_Q0XYE.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def J(X,theta,y):
    return 0.5*np.mean((X.dot(theta)- y)**2)

class LinearRegressor:
    def __init__(self):
        self.theta=None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match10-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m,n=X.shape
        X_with_intercept=np.c_[np.ones(m),X]
        theta=np.zeros(n+1)
</FONT>        theta_array=[theta.copy()]
        prev_loss=float('inf')
        epsilon=1e-8
        while True:
            gradient = (1.0/m)* X_with_intercept.T.dot(X_with_intercept.dot(theta) - y)
            theta -= learning_rate * gradient
            theta_array.append(theta.copy())
            loss = J(X_with_intercept,theta,y)
            if(np.abs(loss-prev_loss)&lt;epsilon):
                break
            prev_loss=loss
            # print(f"theta: {theta}, loss: {loss}")
        self.theta = theta_array[-1]
        return np.array(theta_array)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match10-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.c_[np.ones(X.shape[0]),X].dot(self.theta)
</FONT>




import linear_regression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def generate_plots(X,y,thetas):

    X_with_intercept=np.c_[np.ones(X.shape[0]),X]

    plt.scatter(X, y, color='blue', label='Data points')
    y_plot = thetas[-1][0] + thetas[-1][1] * X
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match10-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot(X, y_plot, color='red', label='Learned function')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Linear Regression: Data and Learned Function')
    plt.legend()
    plt.show()

    theta_mesh = np.meshgrid( np.linspace(-1, thetas[-1][0]+1, 200),np.linspace(-1, thetas[-1][1]+1, 200))
</FONT>    Z = np.array([ linear_regression.J(X_with_intercept,np.array([t0,t1]),y) for t0, t1 in zip(np.ravel(theta_mesh[0]), np.ravel(theta_mesh[1]))]).reshape(theta_mesh[0].shape)

    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(theta_mesh[0], theta_mesh[1], Z, cmap='viridis', alpha=0.8)
    ax.set_xlabel(r'$\theta_0$ (Intercept)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_zlabel('Loss value')
    fig.colorbar(surf)

    loss_values = [linear_regression.J(X_with_intercept,theta,y) for theta in thetas]
    plt.title('Gradient Descent Trajectory on Surface')

    for i in range(len(thetas)):
        ax.scatter(thetas[i][0], thetas[i][1], loss_values[i],color='red', s=50)
        plt.pause(0.02) 
    plt.show()

    plt.figure(figsize=(12, 8))
    contour = plt.contour(theta_mesh[0], theta_mesh[1], Z, levels=50, cmap='viridis')
    plt.colorbar(contour, label='Loss value')
    plt.xlabel(r'$\theta_0$ (Intercept)')
    plt.ylabel(r'$\theta_1$ (Slope)')
    plt.title('Gradient Descent Trajectory on Contour')

    for i in range(len(thetas)):
        plt.plot(thetas[i, 0], thetas[i, 1], 'ro', markersize=5)
        if i &gt; 0:
            plt.plot(thetas[i-1:i+1, 0], thetas[i-1:i+1, 1], 'r-')
        plt.pause(0.2) 
    plt.show()

X=pd.read_csv("../data/Q1/linearX.csv").values
y=pd.read_csv("../data/Q1/linearY.csv").values.flatten()
best_lr=0
min_loss=float('inf')
lr=linear_regression.LinearRegressor()
for learning_rate in range(1,1000):
    learning_rate= learning_rate/10000.0
    t=lr.fit(X,y,learning_rate)
    if(linear_regression.J(np.c_[np.ones(X.shape[0]),X],t[-1],y)&lt;min_loss):
        min_loss=linear_regression.J(np.c_[np.ones(X.shape[0]),X],t[-1],y)
        best_lr=learning_rate
print(best_lr)
print(min_loss)
lr.fit(X,y,best_lr)
print(f"best parameters: {lr.theta}")


for learningRate in [0.001, 0.025, 0.1]:
    lr=linear_regression.LinearRegressor()
    t=lr.fit(X,y,learningRate)
    print(f"Learning Rate: {learningRate}")
    print(f"Learned Parameters: {t[-1]}")
    print(f"prediction loss: {0.5*np.mean((lr.predict(X)- y)**2)}")
    generate_plots(X,y,t)




import numpy as np
import matplotlib.pyplot as plt
import sampling_sgd

def plot_theta_movement(theta, batches):

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    for i, batch_size in enumerate(batches):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match10-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_history=theta[i]
        theta_1 = theta_history[:, 0]
        theta_2 = theta_history[:, 1]
        theta_3 = theta_history[:, 2]
    
        ax.plot(theta_1, theta_2, theta_3, marker='o', label=f'Batch Size {batch_size}')
</FONT>        ax.scatter(theta_1[0], theta_2[0], theta_3[0], color='red', label='Start', s=100)
        ax.scatter(theta_1[-1], theta_2[-1], theta_3[-1], color='green', label='End', s=100)
    
    ax.set_xlabel(r'$\theta_1$')
    ax.set_ylabel(r'$\theta_2$')
    ax.set_zlabel(r'$\theta_3$')
    ax.set_title(f'Movement of $\\theta$ in Parameter Space (Batch Size {batch_size})')
    ax.legend()
    plt.show()

X,y=sampling_sgd.generate(1000000,[3,1,2],[3,-1],[2,2],np.sqrt(2))
X_train = X[:int(0.8*X.shape[0]), :]
y_train = y[:int(0.8*y.shape[0])]
X_test = X[int(0.8*X.shape[0]):, :]
y_test = y[int(0.8*y.shape[0]):]

closed_theta=sampling_sgd.closed_form_solution(X,y)
print(closed_theta)

slr = sampling_sgd.StochasticLinearRegressor()
t=slr.fit(X_train,y_train,0.001)
batches=[1,80,8000,800000]
for i,theta in enumerate(t):
    print(f"batch size {batches[i]}: theta: {theta[-1]}, train_loss: {sampling_sgd.J(X_train,y_train,theta[-1])}, test loss: {sampling_sgd.J(X_test,y_test,theta[-1])}")

plot_theta_movement(t,batches)





# Imports - you can add any other permitted libraries
import numpy as np
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    X_with_intercept = np.c_[np.ones(N), X]
    y = np.dot(X_with_intercept, theta) + np.random.normal(0, noise_sigma, N)
    
    return X, y

def J(X, y, theta):
    predictions = np.dot(np.c_[np.ones(X.shape[0]), X], theta)
    return 0.5*np.mean((predictions - y) ** 2)

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        batch_sizes = [1,80,8000,800000]
        theta_array = []
        loss_tolerance = 1e-4
        # gradient_tolerance = 1e-5
        alpha = 0.1
        X_with_intercept = np.c_[np.ones(m), X]
        
        for batch_size in batch_sizes:
            theta = np.zeros(n + 1)  
            avg_loss = None          
            theta_history=[theta.copy()]
            start_time = time.time()
            epochs=0
            while True:
                epochs+=1
                permutation = np.random.permutation(m)
                X_shuffled = X_with_intercept[permutation]
                y_shuffled = y[permutation]
                
                for i in range(0, m, batch_size):
                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]
                    predictions = np.dot(X_batch, theta)
                    gradient = np.dot(X_batch.T, (predictions-y_batch))/batch_size
                    theta -= learning_rate*gradient
                theta_history.append(theta.copy())
                loss= J(X,y,theta)

                if avg_loss is None:
                    avg_loss = loss  
                    continue
                else:
                    avg_loss = alpha*loss+(1-alpha)*avg_loss
                
                if abs(loss-avg_loss) &lt; loss_tolerance:
                    end_time = time.time()
                    print(f"batch size {batch_size}: time taken: {end_time-start_time}")
                    print(f"Number of Epochs: {epochs}, Gradient: {gradient}")
                    self.theta = theta.copy()
                    break
<A NAME="10"></A><FONT color = #FF0000><A HREF="match10-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            theta_array.append(np.array(theta_history))
        return theta_array
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return np.dot(np.c_[np.ones(X.shape[0]), X], self.theta)
    
def closed_form_solution(X, y):

    X_with_intercept = np.c_[np.ones(X.shape[0]), X]
    theta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
    
    return theta



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def sigmoid(z):
    return 1.0/(1+np.exp(-z))

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
<A NAME="0"></A><FONT color = #FF0000><A HREF="match10-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X_normalized = (X-np.mean(X, axis=0))/np.std(X, axis=0)
</FONT>        X_normalized = np.c_[np.ones(X_normalized.shape[0]), X_normalized]
        tolerance = 1e-8
        theta = np.zeros(X_normalized.shape[1])
        theta_array = [theta.copy()]
    
        while(True):
            pred = sigmoid(np.dot(X_normalized, theta))
            gradient = X_normalized.T.dot(y-pred)
            Hessian = X_normalized.T.dot(np.diag(pred*(1-pred))).dot(X_normalized)
            delta = np.linalg.solve(Hessian, gradient)
            theta -= delta
            theta_array.append(theta.copy())
            if np.linalg.norm(delta) &lt; tolerance:
                break
        self.theta = theta.copy()
        self.mean = np.mean(X,axis=0)
        self.std = np.std(X,axis=0)
        return theta_array
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_normalized = (X-self.mean)/self.std
        X_normalized = np.c_[np.ones(X_normalized.shape[0]), X_normalized]

        y_pred = sigmoid(X_normalized.dot(self.theta))
        y_pred = np.where(y_pred&gt;0.5, 1, 0)
        return y_pred



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import logistic_regression

X = pd.read_csv('..\data\Q3\logisticX.csv', header=None).values
y = pd.read_csv('..\data\Q3\logisticY.csv', header=None).values.flatten()

lr = logistic_regression.LogisticRegressor()
t = lr.fit(X,y)
print("Optimal parameters: ", t[-1])
print(f"Prediction loss: {np.sum(np.abs(lr.predict(X)-y))}")

X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_normalized = (X-X_mean)/X_std
X_normalized = np.c_[np.ones((X_normalized.shape[0], 1)), X_normalized]

plt.figure(figsize=(10, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Class 0')
<A NAME="7"></A><FONT color = #0000FF><A HREF="match10-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Class 1')

x_mesh = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))
</FONT>X_grid = np.c_[x_mesh[0].ravel(), x_mesh[1].ravel()]

X_grid_normalized = (X_grid - X_mean) / X_std
X_grid_normalized = np.c_[np.ones(X_grid_normalized.shape[0]), X_grid_normalized]

probabilities = logistic_regression.sigmoid(X_grid_normalized @ t[-1]).reshape(x_mesh[0].shape)

plt.contour(x_mesh[0], x_mesh[1], probabilities, levels=[0.5], colors='red')
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
plt.title('Logistic Regression: Data and Decision Boundary')
plt.legend()
plt.show()




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.phi=None
        self.mu_0=None
        self.mu_1=None
        self.sigma_0=None
        self.sigma_1=None
        self.mean=None
        self.std=None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """

        X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match10-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X_class_0 = X_normalized[y == 0]
        X_class_1 = X_normalized[y == 1]

        mu_0 = np.mean(X_class_0, axis=0)
        mu_1 = np.mean(X_class_1, axis=0)
</FONT>
        self.mu_0 = mu_0
        self.mu_1 = mu_1
        self.phi = np.mean(y)
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)

        if(assume_same_covariance):
            m = len(y)
            sigma = np.zeros((X.shape[1], X.shape[1]))
            for i in range(m):
                x_i = X_normalized[i]
                mu_yi = mu_0 if (y[i]==0) else mu_1
                sigma += np.outer(x_i-mu_yi, x_i-mu_yi)
            sigma /= m
            self.sigma_0=sigma
            self.sigma_1=sigma
            return (mu_0,mu_1,sigma)
        else:
            sigma_0 = np.mean(np.array([np.outer(x_i-mu_0, x_i-mu_0) for x_i in X_class_0]), axis=0)
            sigma_1 = np.mean(np.array([np.outer(x_i-mu_1, x_i-mu_1) for x_i in X_class_1]), axis=0)
            self.sigma_0=sigma_0
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match10-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            self.sigma_1=sigma_1
            return (mu_0,mu_1,sigma_0,sigma_1)

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X-self.mean)/self.std
        sigma_0_inv = np.linalg.inv(self.sigma_0)
        sigma_1_inv = np.linalg.inv(self.sigma_1)
        sigma_0_det = np.linalg.det(self.sigma_0)
        sigma_1_det = np.linalg.det(self.sigma_1)
        
        log_likelihood_0 = -0.5*np.sum((X-self.mu_0)@ sigma_0_inv*(X-self.mu_0), axis=1)- 0.5*np.log(sigma_0_det)+ np.log(1-self.phi)
        log_likelihood_1 = -0.5*np.sum((X-self.mu_1)@ sigma_1_inv*(X - self.mu_1), axis=1)- 0.5*np.log(sigma_1_det)+ np.log(self.phi)
        y_pred =np.where(log_likelihood_1&gt;log_likelihood_0, 1, 0) 
        
        return y_pred



import matplotlib.pyplot as plt
import numpy as np
import gda

<A NAME="2"></A><FONT color = #0000FF><A HREF="match10-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def quadratic_boundary(x, mu_0, mu_1, sigma_0, sigma_1, phi):
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    term1 = -0.5 * (x - mu_1).T @ sigma_1_inv @ (x - mu_1)
</FONT>    term2 = 0.5 * (x - mu_0).T @ sigma_0_inv @ (x - mu_0)
    term3 = np.log(phi / (1 - phi)) - 0.5 * (np.log(np.linalg.det(sigma_1)) - np.log(np.linalg.det(sigma_0)))
<A NAME="11"></A><FONT color = #00FF00><A HREF="match10-0.html#11" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    return term1 + term2 + term3

X = np.loadtxt('../data/Q4/q4x.dat') 
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y_int = np.astype(y == "Canada",np.int64) 
</FONT>
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_normalized = (X - X_mean) / X_std
X_alaska = X_normalized[y=="Alaska"]
X_canada = X_normalized[y=="Canada"]

gda_analysis=gda.GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma= gda_analysis.fit(X,y_int,True)
print("Mean for Class 0 (mu_0):", mu_0)
print("Mean for Class 1 (mu_1):", mu_1)
print("Shared Covariance Matrix (Sigma):\n", sigma)
print(f"Prediction loss: {np.sum(np.abs(gda_analysis.predict(X)-y_int))}")


plt.figure(figsize=(10, 6))
plt.scatter(X_alaska[:, 0], X_alaska[:, 1], marker='o', label='Alaska')
plt.scatter(X_canada[:, 0], X_canada[:, 1], marker='^', label='Canada')

xx, yy = np.meshgrid(np.arange(X_normalized[:, 0].min() - 1, X_normalized[:, 0].max() + 1, 0.1), np.arange(X_normalized[:, 1].min() - 1, X_normalized[:, 1].max() + 1, 0.1))

Z = (mu_1 - mu_0).T.dot(np.linalg.inv(sigma)).dot(np.c_[xx.ravel(), yy.ravel()].T).reshape(xx.shape)

plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='dashed')
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match10-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.title('Salmon Growth Ring Diameters: Alaska vs Canada with GDA Decision Boundary')
plt.xlabel('Normalized Growth Ring Diameter (Fresh Water)')
plt.ylabel('Normalized Growth Ring Diameter (Marine Water)')
plt.legend()
plt.grid(True)
plt.show()

mu_0, mu_1, sigma_0, sigma_1 = gda_analysis.fit(X,y_int,False)
</FONT>print("Mean for Class 0 (mu_0):", mu_0)
print("Mean for Class 1 (mu_1):", mu_1)
print("Covariance Matrix for Class 0 (sigma_0):\n", sigma_0)
print("Covariance Matrix for Class 1 (sigma_1):\n", sigma_1)
print(f"Prediction loss: {np.sum(np.abs(gda_analysis.predict(X)-y_int))}")


plt.figure(figsize=(10, 6))
plt.scatter(X_alaska[:, 0], X_alaska[:, 1], marker='o', label='Alaska')
plt.scatter(X_canada[:, 0], X_canada[:, 1], marker='^', label='Canada')
grid = np.c_[xx.ravel(), yy.ravel()]

phi = np.mean(y_int)
Z = np.array([quadratic_boundary(xi, mu_0, mu_1, sigma_0, sigma_1, phi) for xi in grid])
Z = Z.reshape(xx.shape)

plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='dashed')
plt.xlabel('Normalized Growth Ring Diameter (Fresh Water)')
plt.ylabel('Normalized Growth Ring Diameter (Marine Water)')
plt.title('Salmon Growth Ring Diameters: Alaska vs Canada with GDA Decision Boundary')
plt.legend()
plt.grid(True)
plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
