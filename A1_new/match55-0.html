<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_0VNUY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_0VNUY.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = None
        self.cost_history = None
        
    def compute_cost(self, X, y, theta):
        m = len(y)
        predictions = X.dot(theta)
        return 1/(2*m) * np.sum(np.square(predictions - y.flatten()))
    
    def compute_gradient(self, X, y, theta):
        m = len(y)
        predictions = X.dot(theta)
        return 1/m * X.T.dot(predictions - y.flatten())
    
    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
            
        X = np.c_[np.ones(X.shape[0]), X]
        
        threshold = 1e-8
        self.theta = np.zeros(X.shape[1])
        self.theta_history = [self.theta.copy()]
        self.cost_history = [self.compute_cost(X, y, self.theta)]
        
        iteration = 0
        prev_cost = float('inf')
        max_iterations = 10000
        
        while iteration &lt; max_iterations:
            self.theta = self.theta - learning_rate * self.compute_gradient(X, y, self.theta)
            self.theta_history.append(self.theta.copy())
            
            current_cost = self.compute_cost(X, y, self.theta)
            self.cost_history.append(current_cost)
            
            # Check convergence
            if abs(prev_cost - current_cost) &lt; threshold:
                break
                
            prev_cost = current_cost
            iteration += 1
        
        return np.array(self.theta_history)

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.theta is None:
            raise Exception("Model not trained yet.")
        
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
            
        X = np.c_[np.ones(X.shape[0]), X]
        y_pred = X.dot(self.theta)
        return y_pred




# Imports - you can add any other permitted libraries
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match55-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
</FONT>    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*X[:, 0] + theta[2]*X[:, 1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
<A NAME="2"></A><FONT color = #0000FF><A HREF="match55-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.01, batch_size=1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        stop_threshold = 1e-6
        
        # Add bias column to X
        X = np.c_[np.ones(X.shape[0]), X]  # Shape: (n_samples, n_features+1)
        y = y.reshape(-1, 1)  # Ensure y is a column vector

        # Initialize theta with zeros, shape (n_features+1, 1)
        n_features = X.shape[1]
        self.theta = np.zeros((n_features, 1))

        # Shuffle data
        data = np.hstack((y, X))
        np.random.shuffle(data)

        num_samples, dim = data.shape
        num_batches = num_samples // batch_size
        data_batched = data.reshape((num_batches, batch_size, dim))

        loss_history = []
        epochs = 0
        theta_history = [self.theta.flatten()]
        window_size = 10
        
        while True:
            total_loss = 0
            for batch in data_batched:
                Y_batch = batch[:, 0].reshape(-1, 1)  # Target values
                X_batch = batch[:, 1:]  # Feature matrix

                # Compute gradient and update theta
                gradient = (X_batch.T @ (X_batch @ self.theta - Y_batch)) / (2 * X_batch.shape[0])
                self.theta -= learning_rate * gradient

                # Compute loss
                total_loss += np.linalg.norm(Y_batch - X_batch @ self.theta) ** 2 / (2 * X_batch.shape[0])
                theta_history.append(self.theta.flatten())

            loss_history.append(total_loss / num_batches)
            epochs += 1
            # print(f'epochs {epochs} , curr_loss {curr_loss}')
            
            if len(loss_history) &gt;= 2 * window_size:
                avg_old = np.mean(loss_history[-2 * window_size : -window_size])
                avg_new = np.mean(loss_history[-window_size :])
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match55-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

                if abs(avg_old - avg_new) &lt; stop_threshold:
                    break

        return np.array(theta_history)
                    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.c_[np.ones(X.shape[0]), X]
<A NAME="5"></A><FONT color = #FF0000><A HREF="match55-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (X @ self.theta).flatten()



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LogisticRegressor:
    def __init__(self):
        self.theta = None
        
    def normalize(self, X):
</FONT>        return (X-np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def add_intercept(self, X):
        return np.column_stack([np.ones(X.shape[0]), X])
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = self.normalize(X)
        
        X = self.add_intercept(X)
        
        self.theta = np.zeros(X.shape[1])
        
        n_iter = 10
        theta_history = []
        tolerance = 1e-6
        
        for _ in range(n_iter):
            z = np.dot(X, self.theta)
            h = self.sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / y.size
            H = np.dot(X.T, X * (h * (1 - h)).reshape(-1, 1)) / y.size
            prev_loss = float('inf')
            try:
                theta_update = np.linalg.pinv(H).dot(gradient)
                self.theta -= theta_update
                theta_history.append(self.theta.copy())
                loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
                if abs(prev_loss - loss) &lt; tolerance:
                    break
                prev_loss = loss
            except np.linalg.LinAlgError:
                print("Hessian is not invertible. Stopping optimization.")
                break
            
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        
        X = self.add_intercept(X)
        
<A NAME="6"></A><FONT color = #00FF00><A HREF="match55-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        prob = self.sigmoid(np.dot(X, self.theta))
        
        return (prob &gt;= 0.5).astype(int)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
</FONT>        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
            
    def normalize(self, X):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match55-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="7"></A><FONT color = #0000FF><A HREF="match55-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = self.normalize(X)
        
        m = y.shape[0]
</FONT>        
        indicator_0 = 1*(y == 0).reshape(-1, 1)
        indicator_1 = 1*(y == 1).reshape(-1, 1)
        
        self.phi = np.sum(indicator_1) / m
        
        self.mu_0 = np.sum(X * indicator_0, axis=0) / np.sum(indicator_0)
        self.mu_1 = np.sum(X * indicator_1, axis=0) / np.sum(indicator_1)
        
        if assume_same_covariance:
            mu = indicator_0*self.mu_0 + indicator_1*self.mu_1
            self.sigma = np.dot((X - mu).T, (X - mu)) / m
            
            # print(f'mu_0: {self.mu_0}')
            # print(f'mu_1: {self.mu_1}')
            # print(f'sigma: {self.sigma}')
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.dot((X - self.mu_0).T, indicator_0* (X - self.mu_0)) / np.sum(indicator_0)
            self.sigma_1 = np.dot((X - self.mu_1).T, indicator_1* (X - self.mu_1)) / np.sum(indicator_1)
            # print(f'mu_0: {self.mu_0}')
            # print(f'mu_1: {self.mu_1}')
            # print(f'sigma_0: {self.sigma_0}')
            # print(f'sigma_1: {self.sigma_1}')
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="0"></A><FONT color = #FF0000><A HREF="match55-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        if self.sigma is not None:
            sigma_inv = np.linalg.pinv(self.sigma)
</FONT>            coef = -np.dot((self.mu_1 - self.mu_0).T, sigma_inv)
            linear_term = np.dot(X, coef)
            
            c_1 = np.dot(np.dot(self.mu_1.T, sigma_inv), self.mu_1)/2
            c_2 = np.dot(np.dot(self.mu_0.T, sigma_inv), self.mu_0)/2
            c_3 = np.log((1 - self.phi)/self.phi)
            constant_term = (c_1 - c_2) + c_3
            
            decision_values = linear_term + constant_term
            
        else:
            sigma_0_inv = np.linalg.pinv(self.sigma_0)
            sigma_1_inv = np.linalg.pinv(self.sigma_1)
            
            quad_term = np.array([0.5 * np.dot(np.dot(x, sigma_1_inv - sigma_0_inv), x) for x in X])
            
            coef = -(np.dot(self.mu_1, sigma_1_inv) - np.dot(self.mu_0, sigma_0_inv))
            linear_term = np.dot(X, coef)
            
            c_1 = np.dot(np.dot(self.mu_1.T, sigma_1_inv), self.mu_1)/2
            c_2 = np.dot(np.dot(self.mu_0.T, sigma_0_inv), self.mu_0)/2
            c_3 = np.log((1 - self.phi)/self.phi)
            c_4 = 0.5 * np.log(np.linalg.det(self.sigma_1)/np.linalg.det(self.sigma_0))
            constant_term = (c_1 - c_2) + c_3 + c_4
            
            decision_values = quad_term + linear_term + constant_term
            
        return (decision_values &lt; 0).astype(int)
    

</PRE>
</PRE>
</BODY>
</HTML>
