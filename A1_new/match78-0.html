<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_6DF52.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_6DF52.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

class LinearRegressor:
    def init(self): 
        self.theta = None
    
<A NAME="2"></A><FONT color = #0000FF><A HREF="match78-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.01):
        m = X.shape[0]
        
        X_with_bias = np.hstack((np.ones((m, 1)), X))
        num_params = X_with_bias.shape[1]

        theta = np.zeros(num_params)
</FONT>
        def cost(Xb, targets, params):
            predictions = Xb @ params
            diff = predictions - targets
            return (1 / (2 * m)) * np.sum(diff ** 2)

        params_history = []
        params_history.append(theta.copy())
        
        prev_cost = cost(X_with_bias, y, theta)

        for i in range(10000):
            gradients = (1 / m) * X_with_bias.T @ (X_with_bias @ theta - y)
            theta -= learning_rate * gradients

            params_history.append(theta.copy())
            current_cost = cost(X_with_bias, y, theta)

            if abs(prev_cost - current_cost) &lt; 1e-8:
                break
            prev_cost = current_cost

        self.theta = theta
        return np.array(params_history)

    def predict(self, X):
        m = X.shape[0]
        X_with_bias = np.hstack((np.ones((m, 1)), X))
        return X_with_bias @ self.theta


def main():
    X = np.loadtxt('../data/Q1/linearX.csv', delimiter=',')
    y = np.loadtxt('../data/Q1/linearY.csv', delimiter=',')
    
    if len(X.shape) == 1:
        X = X.reshape(-1, 1)

    reg = LinearRegressor()
    
    learning_rate = 0.01
    max_iter = 10000
    tol = 1e-8
    
    params_history = reg.fit(X, y, learning_rate=learning_rate)
    
    final_theta = reg.theta
    print("1.")
    print(" (a) and (b) Results:")
    print(f"Learning Rate  : {learning_rate}")
    print(f"Stopping Tol   : {tol}")
    print(f"Max Iterations : {max_iter}")
    print("(c) Final theta (parameters):", final_theta)
    print("\n")


    print("2.")
    plt.figure(figsize=(6,4))
    plt.scatter(X, y, color='blue', label='Data')

    X_sorted = np.sort(X, axis=0)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match78-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y_pred_line = reg.predict(X_sorted)
    plt.plot(X_sorted, y_pred_line, color='red', label='Learned Hypothesis')
    
    plt.xlabel('X (Feature)')
    plt.ylabel('y (Target)')
    plt.title('Data and Learned Linear Regression Line')
    plt.legend()
    plt.show()
</FONT>

    print("3.")
    def compute_error_for_plot(theta0, theta1, X, y):
        m = len(y)
        X_augment = np.hstack((np.ones((m,1)), X))
        t = np.array([theta0, theta1])
<A NAME="6"></A><FONT color = #00FF00><A HREF="match78-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        predictions = X_augment.dot(t)
        errors = predictions - y
        return (1/(2*m)) * np.sum(errors**2)

    theta0_vals = params_history[:,0]
</FONT>    theta1_vals = params_history[:,1]

    theta0_min, theta0_max = theta0_vals.min() - 1, theta0_vals.max() + 1
    theta1_min, theta1_max = theta1_vals.min() - 1, theta1_vals.max() + 1

    T0, T1 = np.meshgrid(np.linspace(theta0_min, theta0_max, 100), np.linspace(theta1_min, theta1_max, 100))

    Z = np.zeros_like(T0)
    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            Z[i,j] = compute_error_for_plot(T0[i,j], T1[i,j], X, y)

    fig = plt.figure(figsize=(7,5))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(T0, T1, Z, cmap='viridis', alpha=0.7)

    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel('Error J(theta0, theta1)')
    ax.set_title('3D Surface of Cost Function + GD Updates')

    # Animation
    for i, th in enumerate(params_history):
        # print(f"Step {i}: theta0 = {th[0]:.4f}, theta1 = {th[1]:.4f}")
        t0, t1 = th[0], th[1]
        c = compute_error_for_plot(t0, t1, X, y)
        ax.scatter(t0, t1, c, color='r', marker='o', s=30)
        plt.pause(0.2)

    plt.show()



    print("4.")
    fig, ax = plt.subplots(figsize=(6,5))
    CS = ax.contour(T0, T1, Z, levels=np.logspace(-2, 3, 20), cmap='viridis')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_title('Contour Plot of Cost Function + GD Updates')
    
    for i, th in enumerate(params_history):
        # print(f"Step {i}: theta0 = {th[0]:.4f}, theta1 = {th[1]:.4f}")
        t0, t1 = th[0], th[1]
        c = compute_error_for_plot(t0, t1, X, y)
        ax.scatter(t0, t1, color='r', marker='o', s=30)
        plt.pause(0.2)
    plt.show()



    print("5.")
    learning_rates = [0.001, 0.025, 0.1]
    for lr in learning_rates:
        reg_temp = LinearRegressor()
        params_hist_lr = reg_temp.fit(X, y, learning_rate=lr)
        
        fig, ax = plt.subplots(figsize=(6,5))
        CS = ax.contour(T0, T1, Z, levels=np.logspace(-2, 3, 20), cmap='viridis')
        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_title(f'Contour Plot (lr={lr}) + GD Updates')
        
        for i, th in enumerate(params_hist_lr):
            # print(f"Step {i}: theta0 = {th[0]:.4f}, theta1 = {th[1]:.4f}")
            t0, t1 = th[0], th[1]
            c = compute_error_for_plot(t0, t1, X, y)
            ax.scatter(t0, t1, color='r', marker='o', s=30)
            plt.pause(0.2)
        
        plt.show()
        print(f"Learning rate = {lr}, final parameters = {reg_temp.theta}")



if __name__ == "__main__":
    main()




import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N,2))
<A NAME="0"></A><FONT color = #FF0000><A HREF="match78-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(loc=0.0, scale=noise_sigma, size=N)
    y = theta[0] + theta[1]*X[:,0] + theta[2]*X[:,1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self, batch_size=1, max_epochs=1000, tol=1e-6):
</FONT>        self.batch_size = batch_size
        self.max_epochs = max_epochs
        self.tol = tol
        self.theta = None
        self.param_history_ = []
    
    def fit(self, X, y, learning_rate=0.01):
        n_samples, n_features = X.shape
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        self.theta = np.zeros(n_features+1)
        self.param_history_ = []

        for epoch in range(self.max_epochs):
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_aug = X_aug[indices]
            y = y[indices]
            old_theta = self.theta.copy()

            for start_idx in range(0, n_samples, self.batch_size):
                end_idx = start_idx + self.batch_size
                X_batch = X_aug[start_idx:end_idx]
                y_batch = y[start_idx:end_idx]
                y_pred = X_batch @ self.theta
                error = (y_pred - y_batch)
                grad = (X_batch.T @ error) / len(X_batch)
                self.theta -= learning_rate * grad
                self.param_history_.append(self.theta.copy())

            if np.linalg.norm(self.theta - old_theta) &lt; self.tol:
                break

        return np.array(self.param_history_)
    
    def predict(self, X):
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])
        y_pred = X_aug @ self.theta
        return y_pred



def main():
    N = 1_000_000
    theta_true = np.array([3,1,2])
    input_mean = np.array([3,-1])
    input_sigma = np.array([2, 2])
    noise_sigma = np.sqrt(2)
    X_all, y_all = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

    split_index = int(0.8*N)
    X_train, y_train = X_all[:split_index], y_all[:split_index]
    X_test,  y_test = X_all[split_index:], y_all[split_index:]


    batch_sizes = [1,80, 8000, 800000]
    learning_rate = 0.001
    results_sgd = {}

    for r in batch_sizes:
        print(f"Training with batch size = {r}")
        reg = StochasticLinearRegressor(batch_size=r, max_epochs=50, tol=1e-8)
        param_hist = reg.fit(X_train, y_train, learning_rate=learning_rate)
        results_sgd[r] = {
            "theta": reg.theta.copy(),
            "param_hist": param_hist
        }
        print(f"Batch size = {r}, final learned theta = {reg.theta}")

    X_train_aug = np.hstack([np.ones((X_train.shape[0],1)), X_train])
    theta_closed_form = np.linalg.inv(X_train_aug.T @ X_train_aug) @ (X_train_aug.T @ y_train)
    print("Closed-form theta:", theta_closed_form)
    print("True theta:", theta_true)



    def mse(y_true, y_pred):
        return np.mean((y_true - y_pred)**2)

    for r in batch_sizes:
        reg_pred_train = reg.predict(X_train)
        reg_pred_test = reg.predict(X_test)
        
        train_error = mse(y_train, reg_pred_train)
        test_error  = mse(y_test,  reg_pred_test)

        print(f"Batch size = {r}: train MSE={train_error:.4f}, test MSE={test_error:.4f}")

    theta_cf = theta_closed_form
    train_pred_cf = X_train_aug @ theta_cf
    test_pred_cf  = np.hstack([np.ones((X_test.shape[0],1)), X_test]) @ theta_cf
    train_mse_cf = mse(y_train, train_pred_cf)
    test_mse_cf  = mse(y_test, test_pred_cf)

    print(f"Closed-form: train MSE={train_mse_cf:.4f}, test MSE={test_mse_cf:.4f}")






    r = 1
    param_hist = results_sgd[r]["param_hist"]
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(param_hist[:,0], param_hist[:,1], param_hist[:,2], marker='o', markersize=2, label=f'SGD trajectory (batch={r})')
    ax.scatter(param_hist[-1,0], param_hist[-1,1], param_hist[-1,2],color='red', s=50, label='Final')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    plt.legend()
    plt.title(f"Parameter Updates in 3D (batch={r})")
    plt.show()

    r = 80
    param_hist = results_sgd[r]["param_hist"]
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(param_hist[:,0], param_hist[:,1], param_hist[:,2], marker='o', markersize=2, label=f'SGD trajectory (batch={r})')
    ax.scatter(param_hist[-1,0], param_hist[-1,1], param_hist[-1,2],color='red', s=50, label='Final')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    plt.legend()
    plt.title(f"Parameter Updates in 3D (batch={r})")
    plt.show()

    r = 8000
    param_hist = results_sgd[r]["param_hist"]
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(param_hist[:,0], param_hist[:,1], param_hist[:,2], marker='o', markersize=2, label=f'SGD trajectory (batch={r})')
    ax.scatter(param_hist[-1,0], param_hist[-1,1], param_hist[-1,2],color='red', s=50, label='Final')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    plt.legend()
    plt.title(f"Parameter Updates in 3D (batch={r})")
    plt.show()

    r = 800000
    param_hist = results_sgd[r]["param_hist"]
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(param_hist[:,0], param_hist[:,1], param_hist[:,2], marker='o', markersize=2, label=f'SGD trajectory (batch={r})')
    ax.scatter(param_hist[-1,0], param_hist[-1,1], param_hist[-1,2],color='red', s=50, label='Final')
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    plt.legend()
    plt.title(f"Parameter Updates in 3D (batch={r})")
    plt.show()
            
if __name__ == "__main__":
    main()



import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match78-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std  = None
    
    
    def fit(self, X, y, learning_rate=0.01):
</FONT>        self.mean = X.mean(axis=0)
        self.std  = X.std(axis=0, ddof=1)
        X_norm = (X - self.mean) / self.std
        m, n = X_norm.shape
        X_with_intercept = np.hstack([np.ones((m, 1)), X_norm])
        theta = np.zeros(n + 1)
        param_history = []
        for _ in range(20):
            z = X_with_intercept.dot(theta)
            preds = sigmoid(z)
            grad = X_with_intercept.T.dot(preds - y)
            R = preds * (1 - preds)
            X_R = X_with_intercept * R[:, np.newaxis]
            Hessian = X_with_intercept.T.dot(X_R)
            update = np.linalg.solve(Hessian, grad)
            new_theta = theta - update
            if np.linalg.norm(new_theta - theta, 2) &lt; 1e-6:
                theta = new_theta
                param_history.append(theta.copy())
                break
            theta = new_theta
            param_history.append(theta.copy())
        self.theta = theta
        return param_history

    def predict_proba(self, X):
        X_norm = (X - self.mean) / self.std
        m = X_norm.shape[0]
        X_with_intercept = np.hstack([np.ones((m, 1)), X_norm])

        return sigmoid(X_with_intercept.dot(self.theta))
    
    def predict(self, X):
        return (self.predict_proba(X) &gt;= 0.5).astype(int)


def main():

    X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
    y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')


    reg = LogisticRegressor()
    history = reg.fit(X, y)
    print("Final theta:", reg.theta)


    X0 = X[y == 0]
    X1 = X[y == 1]

    plt.figure(figsize=(8,6))
    plt.scatter(X0[:, 0], X0[:, 1], marker='o', color='red', label='y = 0')
    plt.scatter(X1[:, 0], X1[:, 1], marker='x', color='blue', label='y = 1')

    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.title('Training Data')

    x1_vals = np.linspace(min(X[:,0]), max(X[:,0]), 200)
    x1_vals_norm = (x1_vals - reg.mean[0]) / reg.std[0]
    t0, t1, t2 = reg.theta
    x2_vals_norm = -(t0 + t1*x1_vals_norm) / t2
    x2_vals = x2_vals_norm * reg.std[1] + reg.mean[1]

    plt.plot(x1_vals, x2_vals, 'g--', label='Decision Boundary')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()




import numpy as np
import matplotlib.pyplot as plt


class GaussianDiscriminantAnalysis:
    def __init__(self):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match78-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.fitted_ = False
    
    def fit(self, X, y, assume_same_covariance=False):
        self.mean_ = np.mean(X, axis=0)
        self.std_  = np.std(X, axis=0)
        X_norm = (X - self.mean_) / self.std_
</FONT>
        m = X_norm.shape[0]
        y0 = (y == 0)
        y1 = (y == 1)
        n0 = np.sum(y0)
        n1 = np.sum(y1)

        self.phi_ = n1 / m
        self.mu0_ = np.mean(X_norm[y0], axis=0)
        self.mu1_ = np.mean(X_norm[y1], axis=0)

        if assume_same_covariance:
            sigma = np.zeros((X_norm.shape[1], X_norm.shape[1]))
            for i in range(m):
                if y[i] == 0:
                    diff = (X_norm[i] - self.mu0_).reshape(-1,1)
                else:
                    diff = (X_norm[i] - self.mu1_).reshape(-1,1)
                sigma += diff @ diff.T
            sigma /= m

            self.sigma_ = sigma
            self.fitted_ = True
            return (self.mu0_, self.mu1_, self.sigma_)

        else:
            X0 = X_norm[y0]
            X1 = X_norm[y1]

            sigma0 = np.zeros((X_norm.shape[1], X_norm.shape[1]))
            for i in range(n0):
                diff = (X0[i] - self.mu0_).reshape(-1,1)
                sigma0 += diff @ diff.T
            sigma0 /= n0

            sigma1 = np.zeros((X_norm.shape[1], X_norm.shape[1]))
            for i in range(n1):
                diff = (X1[i] - self.mu1_).reshape(-1,1)
                sigma1 += diff @ diff.T
            sigma1 /= n1

            self.sigma0_ = sigma0
            self.sigma1_ = sigma1
            self.fitted_ = True
            return (self.mu0_, self.mu1_, self.sigma0_, self.sigma1_)
    
    def predict(self, X):
        if not self.fitted_:
            raise ValueError("Model has not been fitted yet.")

        X_norm = (X - self.mean_) / self.std_
        if hasattr(self, 'sigma_'):
            sigma_inv = np.linalg.inv(self.sigma_)
            log_phi = np.log(self.phi_)
            log_1_minus_phi = np.log(1 - self.phi_)

            g1 = []
            g0 = []
            for x_i in X_norm:
                diff1 = (x_i - self.mu1_).reshape(-1,1)
                diff0 = (x_i - self.mu0_).reshape(-1,1)
                g1_val = -0.5 * diff1.T @ sigma_inv @ diff1 + log_phi
                g0_val = -0.5 * diff0.T @ sigma_inv @ diff0 + log_1_minus_phi
                g1.append(g1_val.item())
                g0.append(g0_val.item())

            g1 = np.array(g1)
            g0 = np.array(g0)
            y_pred = (g1 &gt; g0).astype(int)
            return y_pred

        else:
            sigma0_inv = np.linalg.inv(self.sigma0_)
            sigma1_inv = np.linalg.inv(self.sigma1_)
            det_sigma0 = np.linalg.det(self.sigma0_)
            det_sigma1 = np.linalg.det(self.sigma1_)

            log_phi = np.log(self.phi_)
            log_1_minus_phi = np.log(1 - self.phi_)

            g1 = []
            g0 = []
            for x_i in X_norm:
                diff1 = (x_i - self.mu1_).reshape(-1,1)
                diff0 = (x_i - self.mu0_).reshape(-1,1)
                g1_val = -0.5 * diff1.T @ sigma1_inv @ diff1 \
                         - 0.5 * np.log(det_sigma1) \
                         + log_phi
                g0_val = -0.5 * diff0.T @ sigma0_inv @ diff0 \
                         - 0.5 * np.log(det_sigma0) \
                         + log_1_minus_phi
                g1.append(g1_val.item())
                g0.append(g0_val.item())

            g1 = np.array(g1)
            g0 = np.array(g0)
            y_pred = (g1 &gt; g0).astype(int)
            return y_pred


def main():
    print("1.")
    X = np.loadtxt('../data/Q4/q4x.dat')
    y_str = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
    y = np.array([1 if label == 'Canada' else 0 for label in y_str])
    gda_same_cov = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda_same_cov.fit(X, y, assume_same_covariance=True)
    print(f"phi (fraction labeled 'Canada'):  {gda_same_cov.phi_:.4f}")
    print("mu0 (Alaska) =", mu0)
    print("mu1 (Canada) =", mu1)
    print("Sigma =\n", sigma)


    print("2.")
    plt.figure(figsize=(8,6))
    X_alaska = X[y == 0]
    X_canada = X[y == 1]
    plt.scatter(X_alaska[:,0], X_alaska[:,1], marker='o', 
                edgecolors='k', facecolors='none', label='Alaska (y=0)')
    plt.scatter(X_canada[:,0], X_canada[:,1], marker='x', 
                color='red', label='Canada (y=1)')
    # plt.xlabel('x1 (Original Scale)')
    # plt.ylabel('x2 (Original Scale)')
    # plt.title('Training Data Distribution')
    # plt.legend()
    # plt.savefig('gda_data.png')
    # plt.close()

    print("3.")
    x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5
    x2_min, x2_max = X[:,1].min()-0.5, X[:,1].max()+0.5
    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match78-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                           np.linspace(x2_min, x2_max, 200))
    grid = np.column_stack([xx1.ravel(), xx2.ravel()])
    y_pred_same = gda_same_cov.predict(grid).reshape(xx1.shape)
</FONT>
    plt.contour(xx1, xx2, y_pred_same, levels=[0.5], 
                colors='g', linewidths=2, 
                linestyles='solid', 
                label='Linear Boundary')
    # plt.xlabel('x1 (Original Scale)')
    # plt.ylabel('x2 (Original Scale)')
    # plt.title('GDA with Linear Decision Boundary')
    # plt.legend()
    # plt.savefig('gda_linear.png')
    # plt.close()

    print("4.")
    gda_sep_cov = GaussianDiscriminantAnalysis()
    mu0_sep, mu1_sep, sigma0_sep, sigma1_sep = gda_sep_cov.fit(X, y, assume_same_covariance=False)
    print(f"phi (fraction labeled 'Canada'):  {gda_sep_cov.phi_:.4f}")
    print("mu0 (Alaska) =", mu0_sep)
    print("mu1 (Canada) =", mu1_sep)
    print("Sigma0 =\n", sigma0_sep)
    print("Sigma1 =\n", sigma1_sep)
    print("-------------------------------------------------------------")

    print("5.")
    y_pred_sep = gda_sep_cov.predict(grid).reshape(xx1.shape)
    plt.contour(xx1, xx2, y_pred_sep, levels=[0.5], 
                colors='r', linewidths=2,
                linestyles='dashdot', 
                label='Quadratic Boundary')
    # plt.xlabel('x1 (Original Scale)')
    # plt.ylabel('x2 (Original Scale)')
    # plt.title('GDA with Quadratic Decision Boundary')
    # plt.legend()
    # plt.savefig('gda_quadratic.png')
    # plt.close()

    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('GDA Decision Boundaries: Linear vs. Quadratic')
    plt.legend()
    plt.show()


if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
