<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_7N54O.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_AAPWP.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta=None
        self.theta_list=None
        self.error_list=None
        pass

    def calculate_error(self,X,y,theta):
        m=len(y)
        h=X@theta
        return (1/(2*m))*(np.sum((h-y)**2))

    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match186-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Gradient Descent.
        """
        epsilon=1e-7
        max_itrs=500

        n_samples=X.shape[0]
        n_features=X.shape[1]
        X_b=np.c_[np.ones((n_samples,1)),X]
</FONT>        m=len(y)

        self.theta=np.zeros(n_features+1)
        self.theta_list=[self.theta.copy()]
        self.error_list=[self.calculate_error(X_b,y,self.theta)]

        prev_error=float('inf')

        for i in range(max_itrs):
            h=X_b@self.theta
            grad=(1/m)*(X_b.T@(h-y))

            self.theta=self.theta-(learning_rate*grad)

            self.theta_list.append(self.theta.copy())
            curr_error=self.calculate_error(X_b,y,self.theta)
            self.error_list.append(curr_error)

            if abs(curr_error-prev_error)&lt;epsilon:
                break
            prev_error=curr_error
        print(self.theta)
        return np.array(self.theta_list)


        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        return X_b @ self.theta
        pass



import numpy as np
import csv
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from linear_regression import LinearRegressor

def load_csv_files(x_file='linearX.csv',y_file='linearY.csv'):
    X=np.genfromtxt(x_file,delimiter=',').reshape(-1, 1)
    y=np.genfromtxt(y_file,delimiter=',')
    return X,y

def plot_and_fit(X,y,regressor):
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color='blue', alpha=0.5, label='Training Data')
    
    X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_pred = regressor.predict(X_test)
    
    plt.plot(X_test, y_pred, color='red', linewidth=2, label='Hypothesis Function')
    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Wine Density vs Acidity with Fitted Line')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.savefig('regression_fit.png', dpi=300, bbox_inches='tight')
    plt.show()


<A NAME="2"></A><FONT color = #0000FF><A HREF="match186-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def plot_error_surface_3d(X, y, regressor, pause_time=0.2):
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    margin = 2
</FONT>    theta0_range = np.linspace(regressor.theta_list[0][0]-margin, 
                              regressor.theta_list[-1][0]+margin, 100)
    theta1_range = np.linspace(regressor.theta_list[0][1]-margin, 
                              regressor.theta_list[-1][1]+margin, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    J_mesh = np.zeros(theta0_mesh.shape)
    for i in range(len(theta0_range)):
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match186-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(len(theta1_range)):
            theta = np.array([theta0_mesh[i,j], theta1_mesh[i,j]])
            h = X_b @ theta
</FONT><A NAME="1"></A><FONT color = #00FF00><A HREF="match186-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            J_mesh[i,j] = (1/(2*len(y))) * np.sum((h - y)**2)
    
    surf = ax.plot_surface(theta0_mesh, theta1_mesh, J_mesh, 
                          cmap='viridis', alpha=0.6)
    fig.colorbar(surf, label='Cost J(θ)')
</FONT>    ii=0
    for theta in regressor.theta_list:
        h = X_b @ theta
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match186-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        cost = (1/(2*len(y))) * np.sum((h - y)**2)
        ax.scatter(theta[0], theta[1], cost, color='red', s=50)
</FONT>        if pause_time &gt; 0:
            plt.pause(pause_time)
        if ii%50==0:
            print(ii)
        ii=ii+1
    
    ax.set_xlabel('θ₀ (Intercept)')
    ax.set_ylabel('θ₁ (Slope)')
    ax.set_zlabel('Cost J(θ)')
    plt.title('Error Surface with Gradient Descent Path')
    
    plt.savefig('error_surface_3d.png', dpi=300, bbox_inches='tight')
    plt.show()


def plot_contours(X, y, regressor, learning_rate, pause_time=0.2):
    """Plot contours of the error function with gradient descent iterations"""
    plt.figure(figsize=(10, 6))
    
    # Create mesh grid for theta values
    margin = 2
    theta0_range = np.linspace(regressor.theta_list[0][0]-2, 
                              regressor.theta_list[-1][0]+2, 100)
    theta1_range = np.linspace(regressor.theta_list[0][1]-2, 
                              regressor.theta_list[-1][1]+2, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    
    # Compute cost for each theta combination
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    J_mesh = np.zeros(theta0_mesh.shape)
    for i in range(len(theta0_range)):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match186-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(len(theta1_range)):
            theta = np.array([theta0_mesh[i,j], theta1_mesh[i,j]])
            h = X_b @ theta
</FONT>            J_mesh[i,j] = (1/(2*len(y))) * np.sum((h - y)**2)
    
    # Plot contours
    contours = plt.contour(theta0_mesh, theta1_mesh, J_mesh, levels=50)
    plt.colorbar(contours, label='Cost J(θ)')
    
    # Animate gradient descent path
    path_x = [theta[0] for theta in regressor.theta_list]
    path_y = [theta[1] for theta in regressor.theta_list]
    
    # Plot path with animation
    for i in range(len(path_x)):
        plt.scatter(path_x[i], path_y[i], color='red', s=50)
        if i &lt; len(path_x)-1:
            plt.plot([path_x[i], path_x[i+1]], [path_y[i], path_y[i+1]], 
                     'r--', alpha=0.3)
        if pause_time &gt; 0:
            plt.pause(pause_time)
    
    plt.xlabel('θ₀ (Intercept)')
    plt.ylabel('θ₁ (Slope)')
    plt.title(f'Contour Plot with Gradient Descent Path (η={learning_rate})')
    plt.grid(True, alpha=0.3)
    

    plt.savefig(f'contour_plot_lr_{learning_rate}.png', dpi=300, bbox_inches='tight')
    plt.show()


def compare_learning_rates(X, y, learning_rates=[0.01]):
    """Compare the convergence for different learning rates"""
    results = []
    
    for lr in learning_rates:
        regressor = LinearRegressor()
        theta_list = regressor.fit(X, y, learning_rate=lr)
        
        # Plot contours for this learning rate
        plot_contours(X, y, regressor, lr)
        results.append({
            'learning_rate': lr,
            'final_theta': regressor.theta,
            'n_iterations': len(regressor.theta_list),
            'final_cost': regressor.error_list[-1]
        })
        
        print(f"\nLearning Rate: {lr}")
        print(f"Final parameters: θ₀={regressor.theta[0]:.4f}, θ₁={regressor.theta[1]:.4f}")
        print(f"Number of iterations: {len(regressor.theta_list)}")
        print(f"Final cost: {regressor.error_list[-1]:.8f}")
    
    return results


if __name__ == "__main__":

    
    X, y = load_csv_files()
    regressor = LinearRegressor()
    theta_list = regressor.fit(X, y, learning_rate=0.025)
    
    plot_and_fit(X, y, regressor)
    # plot_error_surface_3d(X, y, regressor)
    
    results = compare_learning_rates(X, y)



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor, generate


N = 1000000  
theta_true = np.array([3, 1, 2])  
input_mean = np.array([3, -1])  
input_sigma = np.array([2, 2]) 
noise_sigma = np.sqrt(2)

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
train_size = int(0.8 * N)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]


def run_experiment( learning_rate=0.001):
    results = {}
    x=True
    model = StochasticLinearRegressor()
    theta_history = model.fit(X_train, y_train,
                            learning_rate=learning_rate)
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    batch_size={0:1,1:80,2:8000,3:800000}
    for i in range(4):
        train_mse = np.mean((train_pred[i] - y_train) ** 2)
        test_mse = np.mean((test_pred[i] - y_test) ** 2)
        
        results[batch_size[i]] = {
            'final_theta': model.ls[i][-1],
            'theta_history': theta_history[i],
            'train_mse': train_mse,
            'test_mse': test_mse
        }
        
        print(f"Final theta: {model.ls[i][-1]}")
        print(f"Training MSE: {train_mse:.4f}")
        print(f"Test MSE: {test_mse:.4f}")
    
    return results

def plot_parameter_trajectories(results):
    """Plot 3D trajectories of parameter updates."""
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    colors = ['b', 'g', 'r', 'c']
    for (batch_size, result), color in zip(results.items(), colors):
        theta_history = np.array(result['theta_history'])
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2],
                label=f'Batch size={batch_size}', color=color)
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2],
                  color=color, marker='*', s=100)
    
    ax.set_xlabel('θ₀')
    ax.set_ylabel('θ₁')
    ax.set_zlabel('θ₂')
    ax.legend()
    plt.title('Parameter Trajectories for Different Batch Sizes')
    plt.savefig('plt2.png')
    plt.show()


batch_sizes = [1,80,8000,800000]
results = run_experiment()

# Compute closed-form solution
model = StochasticLinearRegressor()
theta_closed_form = model.closed_form_solution(X_train, y_train)
print("\nClosed-form solution:")
print(f"Theta: {theta_closed_form}")
plot_parameter_trajectories(results)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
batch_size=80
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    X=np.random.normal(input_mean,input_sigma,size=(N,2))
    X_intercept=np.column_stack([np.ones(N),X])
    y=X_intercept@theta+np.random.normal(0,noise_sigma,N)
    return X,y
    pass

class StochasticLinearRegressor:
    def __init__(self):
        self.theta=None
        self.theta_list=None
        pass

    def compute_batchgrad(self,X_batch,y_batch):
        X_batch_with_intercept = np.column_stack([np.ones(len(X_batch)), X_batch])
        predictions = X_batch_with_intercept @ self.theta
        errors = predictions - y_batch
        return (X_batch_with_intercept.T @ errors) / len(X_batch)
    
    def _check_convergence(self, old_theta, new_theta, tol):
        return np.all(np.abs(new_theta - old_theta) &lt; tol)

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        # remove batch_size from argument
        num_epochs=500
        summ=0
        prev_loss=float('inf')
        tol=1e-6

        num_samples,num_features=X.shape
        self.theta=np.zeros(num_features + 1)
        self.theta_list=[self.theta.copy()]
        num_batches=int(np.ceil(num_samples/batch_size))
        indices=np.random.permutation(num_samples)
        X_shuffled=X[indices]
        y_shuffled=y[indices]
        for i in range(num_epochs):
            epoch_loss=0

            old_theta=self.theta.copy()
            for batch_idx in range(num_batches):
                start=batch_idx*batch_size
                end=min(num_samples,batch_size*(batch_idx+1))
                X_batch=X_shuffled[start:end]
                y_batch=y_shuffled[start:end]
                X_batch_intercept = np.column_stack([np.ones(len(X_batch)), X_batch])
                y_pred=X_batch_intercept@self.theta
                gradient=(X_batch_intercept.T @ (y_pred-y_batch)) / len(X_batch)
                self.theta-=learning_rate*gradient
                self.theta_list.append(self.theta.copy())
                batch_loss = np.mean((y_batch - y_pred) ** 2) / 2
                epoch_loss += batch_loss
            epoch_loss/=num_batches
            summ+=epoch_loss
            # if abs(self.theta - old_theta) &lt; tol:
            #     print(f"Converged after {i+1} epochs")
            #     break
            if abs(epoch_loss-prev_loss) &lt; tol:
                print(f"Converged after {i+1} epochs")
                break
            if  np.linalg.norm(self.theta - old_theta) &lt; tol:
                print(f"Converged after {i+1} epochs")
                break
            prev_loss=epoch_loss
            if i%100==0:
                print(i)
            # print(f"conv :{abs(prev_loss - (summ/(i+1)))}")
        return self.theta_list
        pass
    
    def predict(self, X):
        X_with_intercept = np.column_stack([np.ones(len(X)), X])
        return X_with_intercept @ self.theta
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        pass

    def closed_form_solution(self,X,y):
        X_with_intercept = np.column_stack([np.ones(len(X)), X])
        self.theta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
        return self.theta



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std = None
    
    def normalize_features(self, X):
        """Normalize features by standardization (zero mean, unit variance)"""
        if self.mean is None:
            self.mean = np.mean(X, axis=0)
            self.std = np.std(X, axis=0)
        return (X - self.mean) / (self.std + 1e-8)  # Add small epsilon to avoid division by zero
    
    def sigmoid(self, z):
        """Compute sigmoid function"""
        return 1 / (1 + np.exp(-z))
    
    def compute_gradient(self, X, y, theta):
        """Compute gradient of log-likelihood"""
        m = len(y)
        h = self.sigmoid(X @ theta)
        return X.T @ (h - y) / m
    
    def compute_hessian(self, X, theta):
        """Compute Hessian matrix of log-likelihood"""
        m = len(X)
        h = self.sigmoid(X @ theta)
        diag = h * (1 - h)
        return (X.T @ (X * diag.reshape(-1, 1))) / m
    
    def compute_log_likelihood(self, X, y, theta):
        """Compute log-likelihood"""
        h = self.sigmoid(X @ theta)
        return np.mean(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8))

    
    def fit(self, X, y, learning_rate=0.1):

        tol=1e-8
        max_iter=1000

        X = np.c_[np.ones(len(X)), self.normalize_features(X)]
        m, n = X.shape
        
        # Initialize parameters
        theta = np.zeros(n)
        theta_history = [theta.copy()]
        
        # Newton's Method iterations
        for i in range(max_iter):
            # Compute gradient and Hessian
            gradient = self.compute_gradient(X, y, theta)
            hessian = self.compute_hessian(X, theta)
            
            # Compute update
            delta = np.linalg.solve(hessian, gradient)
            theta_new = theta - learning_rate * delta
            
            # Store parameters
            theta_history.append(theta_new.copy())
            
            # Check convergence
            if np.linalg.norm(theta_new - theta) &lt; tol:
                break
                
            theta = theta_new
        
        self.theta = theta
        print(theta)
        return np.array(theta_history)
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        pass
    
    def predict(self, X):
        X = np.c_[np.ones(len(X)), self.normalize_features(X)]
        probabilities = self.sigmoid(X @ self.theta)
        return (probabilities &gt;= 0.5).astype(int)
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        pass



import matplotlib.pyplot as plt
import numpy as np
from logistic_regression import LogisticRegressor

def plot_decision_boundary(X, y, model):
    """Plot the decision boundary and data points"""
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    
    # Generate a grid of points
    xx = np.linspace(x_min, x_max, 100)
    
    # Calculate the corresponding y values for the decision boundary
    theta = model.theta
    if theta[2] != 0:  # Avoid division by zero
        yy = - (theta[0] + theta[1] * xx) / theta[2]
    else:
        yy = np.full_like(xx, np.nan)  # If theta[2] is zero, we can't define a line
    
    # Plot the decision boundary
    plt.figure(figsize=(10, 8))
    plt.plot(xx, yy, 'k--', label='Decision Boundary')
    
    # Plot the data points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='o', label='Class 0')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='x', label='Class 1')
    
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Logistic Regression Decision Boundary')
    plt.legend()
    plt.grid(True)
    plt.savefig('plt3.png')
    plt.show()
    
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
model = LogisticRegressor()
theta_history = model.fit(X, y)
plot_decision_boundary(X, y, model)



import numpy as np

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = None
        self.scaler_mean = None
        self.scaler_std = None
    
    def normalize_data(self, X):
        if self.scaler_mean is None:
            self.scaler_mean = np.mean(X, axis=0)
            self.scaler_std = np.std(X, axis=0)
        return (X - self.scaler_mean) / self.scaler_std
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the GDA model to the data.
        
        Parameters:
        X: Input features (n_samples, n_features)
        y: Target labels (n_samples,) with values 0 or 1
        assume_same_covariance: Boolean indicating whether to use same covariance for both classes
        
        Returns:
        Parameters depending on assume_same_covariance:
        - If True: (mu_0, mu_1, sigma)
        - If False: (mu_0, mu_1, sigma_0, sigma_1)
        """
        self.assume_same_covariance = assume_same_covariance
        
        X = self.normalize_data(X)
        
        self.phi = np.mean(y)
        
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        if assume_same_covariance:
            n = X.shape[0]
            self.sigma = np.zeros((X.shape[1], X.shape[1]))
            
            for i in range(n):
                x_i = X[i]
                mu_i = self.mu_0 if y[i] == 0 else self.mu_1
                diff = (x_i - mu_i).reshape(-1, 1)
                self.sigma += np.dot(diff, diff.T)
            
            self.sigma /= n
            return self.mu_0, self.mu_1, self.sigma
        
        else:
            n_0 = np.sum(y == 0)
            n_1 = np.sum(y == 1)
            
            self.sigma_0 = np.zeros((X.shape[1], X.shape[1]))
            self.sigma_1 = np.zeros((X.shape[1], X.shape[1]))
            
            for i in range(len(y)):
                if y[i] == 0:
                    diff = (X[i] - self.mu_0).reshape(-1, 1)
                    self.sigma_0 += np.dot(diff, diff.T)
            self.sigma_0 /= n_0
            
            for i in range(len(y)):
                if y[i] == 1:
                    diff = (X[i] - self.mu_1).reshape(-1, 1)
                    self.sigma_1 += np.dot(diff, diff.T)
            self.sigma_1 /= n_1
            
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict class labels for samples in X.
        
        Parameters:
        X: Input features (n_samples, n_features)
        
        Returns:
        y_pred: Predicted labels (n_samples,)
        """

        X = (X - self.scaler_mean) / self.scaler_std
        
        y_pred = np.zeros(X.shape[0])
        
        for i in range(X.shape[0]):
            x = X[i]
            
            if self.assume_same_covariance:
                sigma_inv = np.linalg.inv(self.sigma)
                diff_0 = x - self.mu_0
                diff_1 = x - self.mu_1
                
                log_prob_0 = (-0.5 * np.dot(np.dot(diff_0, sigma_inv), diff_0.T) + 
                            np.log(1 - self.phi))
                log_prob_1 = (-0.5 * np.dot(np.dot(diff_1, sigma_inv), diff_1.T) + 
                            np.log(self.phi))
            
            else:
                sigma_0_inv = np.linalg.inv(self.sigma_0)
                sigma_1_inv = np.linalg.inv(self.sigma_1)
                
                diff_0 = x - self.mu_0
                diff_1 = x - self.mu_1
                
                log_prob_0 = (-0.5 * np.dot(np.dot(diff_0, sigma_0_inv), diff_0.T) - 
                             0.5 * np.log(np.linalg.det(self.sigma_0)) + 
                             np.log(1 - self.phi))
                log_prob_1 = (-0.5 * np.dot(np.dot(diff_1, sigma_1_inv), diff_1.T) - 
                             0.5 * np.log(np.linalg.det(self.sigma_1)) + 
                             np.log(self.phi))
            
            y_pred[i] = 1 if log_prob_1 &gt; log_prob_0 else 0
            
        return y_pred



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis
from plot import plot_data, plot_linear_boundary, plot_quadratic_boundary, plot_combined_boundaries
def normalize_data(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X - mean) / std, mean, std

# Load data
X_nn = np.loadtxt('q4x.dat')
X,std,mean = normalize_data(X_nn)
y_original = np.loadtxt('q4y.dat',dtype=str)
y = np.where(y_original == 'Alaska', 0, 1)
# Fit linear model (shared covariance)
gda_linear = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda_linear.fit(X_nn, y, assume_same_covariance=True)
print(f"mu_0 : {mu_0}")
print(f"mu_1 : {mu_1}")
print(f"sigma : {sigma}")

# Fit quadratic model (separate covariances)
gda_quadratic = GaussianDiscriminantAnalysis()
mu_0_q, mu_1_q, sigma_0_q, sigma_1_q = gda_quadratic.fit(X_nn, y, assume_same_covariance=False)

print(f"mu_0 : {mu_0_q}")
print(f"mu_1 : {mu_1_q}")
print(f"sigma_0 : {sigma_0_q}")
print(f"sigma_1 : {sigma_1_q}")
# Create visualizations
plot_data(X_nn, y)

plot_linear_boundary(X_nn,X, y ,mu_0,mu_1,sigma,np.mean(y))

plot_quadratic_boundary(X_nn,X,y,mu_0_q,mu_1_q,sigma_0_q,sigma_1_q)

plot_combined_boundaries(X_nn,X,y,mu_0,mu_1,sigma,np.mean(y),sigma_0_q,sigma_1_q)
# plot_comparison(X, y, gda_linear, gda_quadratic)

# plot_together(X,y,gda_linear,gda_quadratic)



import numpy as np
import matplotlib.pyplot as plt

def plot_data(X, y):
    """
    Plot the training data with different markers for each class.
    
    Parameters:
    X: Input features (n_samples, n_features)
    y: Target labels (n_samples,) with values 0 or 1
    """
    plt.figure(figsize=(10, 6))
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Alaska', marker='o')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Canada', marker='x')
    plt.xlabel('x1: Freshwater Growth Ring Diameter')
    plt.ylabel('x2: Marine Water Growth Ring Diameter')
    plt.title('Training Data')
    plt.legend()
    plt.grid(True)

def plot_linear_boundary(X_nn,X, y, mu_0, mu_1, sigma, phi):
    """
    Plot the decision boundary for the linear case (same covariance matrix).
    
    Parameters:
    X: Input features (n_samples, n_features)
    y: Target labels (n_samples,) with values 0 or 1
    mu_0: Mean vector for class 0
    mu_1: Mean vector for class 1
    sigma: Covariance matrix (same for both classes)
    """
    
    # Calculate the decision boundary
    mean = np.mean(X_nn, axis=0)
    std = np.std(X_nn, axis=0)
    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, (mu_0 - mu_1))
    b = (-0.5 * np.dot(np.dot(mu_0.T, sigma_inv), mu_0) +
          0.5 * np.dot(np.dot(mu_1.T, sigma_inv), mu_1) +
          np.log(phi / (1 - phi)))
    
    # Generate points for the decision boundary
    x1_n = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
    x2_n = -(w[0] * x1_n + b) / w[1]
    x1=x1_n*std[0]+mean[0]
    x2=x2_n*std[1]+mean[1]
    plt.figure(figsize=(10, 6))
    plt.scatter(X_nn[y == 0][:, 0], X_nn[y == 0][:, 1], label='Alaska', marker='o')
    plt.scatter(X_nn[y == 1][:, 0], X_nn[y == 1][:, 1], label='Canada', marker='x')
    plt.xlabel('x1: Freshwater Growth Ring Diameter')
    plt.ylabel('x2: Marine Water Growth Ring Diameter')
    plt.plot(x1, x2, label='Linear Decision Boundary', color='purple')
    plt.legend()
    plt.savefig('linear.png')
def plot_quadratic_boundary(X_nn,X, y, mu_0, mu_1, sigma_0, sigma_1):
    """
    Plot the decision boundary for the quadratic case (different covariance matrices).
    
    Parameters:
    X: Input features (n_samples, n_features)
    y: Target labels (n_samples,) with values 0 or 1
    mu_0: Mean vector for class 0
    mu_1: Mean vector for class 1
    sigma_0: Covariance matrix for class 0
    sigma_1: Covariance matrix for class 1
    """
    mean = np.mean(X_nn, axis=0)
    std = np.std(X_nn, axis=0)
    plt.figure(figsize=(10, 6))
    plt.scatter(X_nn[y == 0][:, 0], X_nn[y == 0][:, 1], label='Alaska', marker='o')
    plt.scatter(X_nn[y == 1][:, 0], X_nn[y == 1][:, 1], label='Canada', marker='x')
    plt.xlabel('x1: Freshwater Growth Ring Diameter')
    plt.ylabel('x2: Marine Water Growth Ring Diameter')
    
    # Generate a grid of points
    x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
    x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
    X1, X2 = np.meshgrid(x1, x2)
    grid = np.c_[X1.ravel(), X2.ravel()]
    
    # Calculate the decision boundary
    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    
    diff_0 = grid - mu_0
    diff_1 = grid - mu_1
    
    # Quadratic decision boundary equation
    log_prob_0 = -0.5 * np.sum(np.dot(diff_0, sigma_0_inv) * diff_0, axis=1) - 0.5 * np.log(np.linalg.det(sigma_0))
    log_prob_1 = -0.5 * np.sum(np.dot(diff_1, sigma_1_inv) * diff_1, axis=1) - 0.5 * np.log(np.linalg.det(sigma_1))
    
    Z = log_prob_1 - log_prob_0
    Z = Z.reshape(X1.shape)
    X1=X1*std[0]+mean[0]
    X2=X2*std[1]+mean[1]
    plt.contour(X1, X2, Z, levels=[0], colors='purple', label='Quadratic Decision Boundary')
    plt.legend()
    plt.savefig('quadratic.png')




def plot_combined_boundaries(X_nn, X, y, mu_0, mu_1, sigma, phi, sigma_0, sigma_1):
    """
    Plot the training data with both linear and quadratic decision boundaries.
    
    Parameters:
    X_nn: Original (denormalized) data points
    X: Normalized input features (n_samples, n_features)
    y: Target labels (n_samples,) with values 0 or 1
    mu_0: Mean vector for class 0
    mu_1: Mean vector for class 1
    sigma: Common covariance matrix for linear case
    phi: Prior probability for class 1
    sigma_0: Covariance matrix for class 0 (quadratic case)
    sigma_1: Covariance matrix for class 1 (quadratic case)
    """
    mean = np.mean(X_nn, axis=0)
    std = np.std(X_nn, axis=0)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(X_nn[y == 0][:, 0], X_nn[y == 0][:, 1], label='Alaska', marker='o')
    plt.scatter(X_nn[y == 1][:, 0], X_nn[y == 1][:, 1], label='Canada', marker='x')
    plt.xlabel('x1: Freshwater Growth Ring Diameter')
    plt.ylabel('x2: Marine Water Growth Ring Diameter')

    # ------ Linear Decision Boundary ------
    sigma_inv = np.linalg.inv(sigma)
    w = np.dot(sigma_inv, (mu_0 - mu_1))
    b = (-0.5 * np.dot(np.dot(mu_0.T, sigma_inv), mu_0) +
         0.5 * np.dot(np.dot(mu_1.T, sigma_inv), mu_1) +
         np.log(phi / (1 - phi)))
    
    x1_n = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
    x2_n = -(w[0] * x1_n + b) / w[1]
    
    x1 = x1_n * std[0] + mean[0]  # Denormalizing
    x2 = x2_n * std[1] + mean[1]
    
    plt.plot(x1, x2, label='Linear Decision Boundary', color='blue')

    # ------ Quadratic Decision Boundary ------
    x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
    x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
    X1, X2 = np.meshgrid(x1, x2)
    grid = np.c_[X1.ravel(), X2.ravel()]

    sigma_0_inv = np.linalg.inv(sigma_0)
    sigma_1_inv = np.linalg.inv(sigma_1)
    
    diff_0 = grid - mu_0
    diff_1 = grid - mu_1
    
    log_prob_0 = -0.5 * np.sum(np.dot(diff_0, sigma_0_inv) * diff_0, axis=1) - 0.5 * np.log(np.linalg.det(sigma_0))
    log_prob_1 = -0.5 * np.sum(np.dot(diff_1, sigma_1_inv) * diff_1, axis=1) - 0.5 * np.log(np.linalg.det(sigma_1))
    
    Z = log_prob_1 - log_prob_0
    Z = Z.reshape(X1.shape)
    
    X1 = X1 * std[0] + mean[0]  # Denormalizing
    X2 = X2 * std[1] + mean[1]

    plt.contour(X1, X2, Z, levels=[0], colors='purple', label='Quadratic Decision Boundary')

    plt.legend()
    plt.title('Linear & Quadratic Decision Boundaries')
    plt.grid(True)
    plt.savefig('combined.png')


</PRE>
</PRE>
</BODY>
</HTML>
