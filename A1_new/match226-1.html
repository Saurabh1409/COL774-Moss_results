<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_ARW4B.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_EE26Q.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        # Model parameters. Will take nice values when fit is called.
        self.theta = None
        self.max_iter = 1000000
        self.eps = 1e-8 # Stopping criterion
        self.stopped = None # 0 for convergence, 1 for large lr, 2 for reached max_iter
        self.n_iter = None # Number of iterations gradient descent ran for

    # Method to get MSE for given parameters
    def mean_squared_error(self, X, y, parameters, add_bias):
        if add_bias is True:
            X = np.hstack((np.ones((X.shape[0], 1)), X))
        if parameters is None:
            parameters = self.theta
        n_samples = X.shape[0]
        error_vector = X @ parameters - y
        return (error_vector.T @ error_vector) / (2 * n_samples)

    # Method to find the Analaytical Solution
    def analytical_solution(self, X, y):
<A NAME="1"></A><FONT color = #00FF00><A HREF="match226-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = np.hstack((np.ones(((X.shape[0], 1))), X))
        theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y
        return theta_analytical

    def fit(self, X, y, learning_rate=0.01):
</FONT>        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        # Extract Info
        n_samples = X.shape[0]
        n_features = X.shape[1]

        # Add dummy feature to X
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        # Declare the return value: numpy array of shape (max_iter, n_features+1) 
        # theta_history[0] is the inital value
        theta_history = np.zeros((self.max_iter, n_features+1))

        # Gradient Descent
        for iter in range(1, self.max_iter):
            # Calculate new parameters
            theta_old = theta_history[iter-1]
            gradient = X.T @ (X @ theta_old - y) / n_samples
            theta_new = theta_old - learning_rate * gradient

            # Calculate MSE for old and new parameters
            mse_old = self.mean_squared_error(X, y, parameters=theta_old, add_bias=False)
            mse_new = self.mean_squared_error(X, y, parameters=theta_new, add_bias=False)

            # Stopping Criterion
            if mse_new &gt; mse_old:
                # Learning Rate is too large
                self.stopped = 1
                self.n_iter = iter
                break
            elif abs(mse_new - mse_old) &lt; self.eps:
                # Converged
                self.stopped = 0
                theta_history[iter] = theta_new
                self.n_iter = iter+1
                break
            else:
                theta_history[iter] = theta_new

        # Get sub array of proper iterations
        if self.n_iter is None:
            # Learningn rate was too small, reached max_iter before convergence
            self.stopped = -1
            self.n_iter = self.max_iter
        else:
            theta_history = theta_history[:self.n_iter, :]

        # Update self.theta after n_iter iterations of gradient descent
        self.theta = theta_history[-1]

        # Return list of parameters, the whole history
        return theta_history

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="2"></A><FONT color = #0000FF><A HREF="match226-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        if self.theta is None:
            raise RuntimeError("The model has not been trained. Call fit() before making predictions.")

        # np.hstack and np.ones, both take tuples as argument
        X = np.hstack((np.ones((X.shape[0], 1)), X))
</FONT>
        # Predicted values of y
        y_pred = X @ self.theta

        return y_pred
        



#!/usr/bin/env python
# coding: utf-8

# # Q1: Linear Regression

# ## Step I — Import and Load

# ### Import Libraries and the Model 

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation

from linear_regression import LinearRegressor


# ### Load Data and Report Dimensions

# In[ ]:


X = pd.read_csv('../data/Q1/linearX.csv', header=None).values  # shape: (n_samples, 1)
y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.ravel()  # convert to 1D array

n_samples = X.shape[0]
n_features = X.shape[1]

print("Number of training examples: ", n_samples)
print("Number of features: ", n_features)


# ## Step II — Hyperparameter Selection

# ### Create the Object of Class LinearRegressor

# In[ ]:


lr = LinearRegressor()


# ### Calculate the Analytical Solution and Minimum MSE

# In[ ]:


theta_analytical = lr.analytical_solution(X, y)
mse_min = lr.mean_squared_error(X, y, parameters=theta_analytical, add_bias=True)
print("Analytical Solution: ", theta_analytical)
print("Minimum Mean Squared Error: ", mse_min)


# ### Calculate MSE on Validation Set for different hyperparameters

# In[ ]:


learning_rates = [0.001, 0.010, 0.025, 0.1000]
eps_vals = [1e-8, 1e-6, 1e-4]

m = len(learning_rates)
n = len(eps_vals)

mse_vals = np.zeros((m, n))
n_iter_vals = np.zeros((m, n))

n_shuffles = 3

for _ in range(n_shuffles):

    # Shuffle
    perm = np.random.permutation(len(X))
    X_shuffle = X[perm]
    y_shuffle = y[perm]

    # Training Set
    X_train = X_shuffle[:800, :]
    y_train = y_shuffle[:800]

    # Validation Set
    X_valid = X_shuffle[800:, :]
    y_valid = y_shuffle[800:]

    for i in range(m):
        for j in range(n):
            lr.eps = eps_vals[j]
            lr.fit(X_train, y_train, learning_rate=learning_rates[i])
            y_pred = lr.predict(X_valid)
            mse_vals[i][j] += np.mean((y_pred - y_valid) ** 2) / 2
            n_iter_vals[i][j] += lr.n_iter

mse_vals /= n_shuffles
n_iter_vals /= n_shuffles


# ### Print 

# In[ ]:


# for i in range(m):
#     for j in range(n):
#         print("Learning Rate: ", learning_rates[i])
#         print("Stopping Criterion", eps_vals[j])
#         print("MSE: ", mse_vals[i][j])
#         print("Number of Iterations: ", n_iter_vals[i][j])
#         print("\n")


# In[ ]:


# I like lr = 0.1 and eps = 1e-6
lr.eps = 1e-6
theta_history = lr.fit(X, y, learning_rate=0.1)
theta = lr.theta
print("Theta: ", theta)
print("Number of Iterations: ", lr.n_iter)
print("MSE: ", lr.mean_squared_error(X, y, parameters=theta, add_bias=True))


# ## Step III — Plots

# ### Plot 1
# Q2

# #### Data and Hypothesis

# In[ ]:


# Data
x1 = X[:, 0]
y1 = y
# Hypothesis
x2 = np.linspace(x1.min(), x1.max(), 1000)
y2 = theta[0] + theta[1] * x2

# Plots
plt.scatter(x1, y1, color='blue', marker='.', label="Data")
plt.plot(x2, y2, color='red', label="Hypothesis")

# Display
plt.legend() 
plt.show()


# #### Zoomed In

# In[ ]:


# Now a zoomed in plot around x = 0.0 

# Mask
mask = (x1 &gt; -0.1) & (x1 &lt; 0.1)

# Data
x1_zoom = x1[mask]
y1_zoom = y1[mask]
# Hypothesis
x2_zoom = np.linspace(-0.1, 0.1, 100)
y2_zoom = theta[0] + theta[1] * x2_zoom

# Plots
plt.scatter(x1_zoom, y1_zoom, color='blue', marker='.', label="Data")
plt.plot(x2_zoom, y2_zoom, color='red', label="Hypothesis")

# Display
plt.legend()
plt.show()


# ### Plot 2 — 3D Mesh
# Q3

# In[ ]:


# Grids
range_0 = 5 * abs(theta[0])  
range_1 = 1 * abs(theta[1])
t0 = np.linspace(theta[0] - range_0, theta[0] + range_0, 100)
t1 = np.linspace(theta[1] - range_1, theta[1] + range_1, 100)
T0, T1 = np.meshgrid(t0, t1)
J = np.zeros_like (T0)

# Calculate Error for each point on the Grid
for i in range(J.shape[0]):
    for j in range(J.shape[1]):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match226-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_vector = np.array([T0[i, j], T1[i, j]])
        J[i, j] = lr.mean_squared_error(X, y, parameters=theta_vector, add_bias=True)
</FONT>
# Compute J_history
J_history = np.zeros(theta_history.shape[0])
for i in range(theta_history.shape[0]):
    J_history[i] = lr.mean_squared_error(X, y, parameters=theta_history[i], add_bias=True)

# Set up figure
fig = plt.figure(figsize=(10, 7.5))
ax = fig.add_subplot(111, projection='3d')
ax.set_xlabel(r"$\theta_0$")
ax.set_ylabel(r"$\theta_1$")
ax.set_zlabel(r"$J(\theta)$")

# Plot
ax.plot_surface(T0, T1, J, cmap='viridis', alpha=0.7)
ax.view_init(elev=30, azim=30)

# Animation
path, = ax.plot([], [], [], marker='.', color='black', label='Gradient Descent Path')
def animate(i):
    path.set_data(theta_history[:i+1, 0], theta_history[:i+1, 1])
    path.set_3d_properties(J_history[:i+1])
    return path,

# anim = animation.FuncAnimation(fig, animate, frames=len(theta_history), interval=200)
# anim.save("gradient_descent_3d_mesh.gif", writer=animation.PillowWriter(fps=5))

# Display
plt.legend()
plt.show()


# #### Plot 3 — Contour Plot

# In[ ]:


# learning_rates = [0.1, 0.025, 0.01, 0.001]

# for learning_rate in learning_rates:

learning_rate = 0.001

# Fit the model
lr.eps = 1e-6
if learning_rate in [0.001, 0.01]:
    lr.eps = 1e-4

theta_history = lr.fit(X, y, learning_rate=learning_rate)
theta = lr.theta

# Grids
range_0 = 5 * abs(theta[0])  
range_1 = 1 * abs(theta[1])
t0 = np.linspace(theta[0] - range_0, theta[0] + range_0, 100)
t1 = np.linspace(theta[1] - range_1, theta[1] + range_1, 100)
T0, T1 = np.meshgrid(t0, t1)
J = np.zeros_like (T0)

# Calculate Error for each point on the Grid
for i in range(J.shape[0]):
    for j in range(J.shape[1]):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match226-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_vector = np.array([T0[i, j], T1[i, j]])
        J[i, j] = lr.mean_squared_error(X, y, parameters=theta_vector, add_bias=True)
</FONT>
# Compute J_history
J_history = np.zeros(theta_history.shape[0])
for i in range(theta_history.shape[0]):
    J_history[i] = lr.mean_squared_error(X, y, parameters=theta_history[i], add_bias=True)

# Set up Figure
fig, ax = plt.subplots()

# Plot
contour = ax.contour(T0, T1, J, levels=20, cmap='viridis')
plt.colorbar(contour)
plt.plot(theta_history[:, 0], theta_history[:, 1], marker='.', color='black')
plt.xlabel(r"$\theta_0$")
plt.ylabel(r"$\theta_1$")
plt.title(
    "Gradient Descent Path\n"
    "Learning Rate: " + str(learning_rate) + ", Stopping Criterion: " + str(lr.eps) + "\n"
    "Parameters Found: " + str(theta) + "\n"
    "Number of Iterations: " + str(lr.n_iter)
)

plt.legend()
plt.show()

# Animation
# path, = ax.plot([], [], marker='.', color='red')
# def animate(i):
#     ax.clear()
#     ax.set_xlabel(r"$\theta_0$")
#     ax.set_ylabel(r"$\theta_1$")
#     # ax.set_aspect('equal')

#     # Draw the full error function contours in the background
#     ax.contour(T0, T1, J, levels=100, cmap='viridis', alpha=0.8)
#     ax.contourf(T0, T1, J, levels=100, cmap='viridis', alpha=0.6)

#     # Draw the contour corresponding to the current iteration's cost
#     current_cost = J_history[i]
#     ax.contour(T0, T1, J, levels=[current_cost], colors='red', linewidths=2)

#     # Plot the gradient descent path up to the current iteration
#     ax.plot(theta_history[:i+1, 0], theta_history[:i+1, 1], marker='.', color='black')

#     return ax,


# # Animate and Write gif
# interval = 200
# if learning_rate == 0.001:
#     pass

# # anim = animation.FuncAnimation(fig, animate, frames=len(theta_history), interval=200)
# # anim.save("gradient_descent_contour-lr-" + str(learning_rate) + ".gif", writer=animation.PillowWriter(fps=5))





#!/usr/bin/env python
# coding: utf-8

# # Q2: SGD

# ## Step I — Import and Generate Data

# ## Import Modules

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation

from sampling_sgd import StochasticLinearRegressor
from sampling_sgd import generate


# ### Generate Synthetic Data

# In[ ]:


N = 1000000 # 1 Million
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)
X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)


# ### Split Training and Test Data

# In[ ]:


N_train = 800000 # Eight hundred thousand
N_test = 200000 # Two hundred thousand
X_train = X[:N_train, :]
X_test = X[N_train:, :]
y_train = y[:N_train]
y_test = y[N_train:]


# ## Step II — Train the Model

# ### Create the Object

# In[ ]:


slr = StochasticLinearRegressor()


# ### Train

# In[ ]:


# For later use, no need to pass to fit()
batch_sizes = [1, 80, 8000, 800000]

# Obatain theta_history for each batch size, keep default learning rate
list_of_parameters = slr.fit(X=X_train, y=y_train, learning_rate=0.001) 

print("\n\nTheta learned with batch size 1 = ", slr.theta[0])
print("Number of iterations taken for batch size 1 = ", slr.n_iter[0])
print("Convergence criterion for batch size 1 = ", slr.eps[0])
print("Mean Squared Error at the end = ", slr.mean_squared_error(X_train, y_train, parameters=slr.theta[0], add_bias=True))

print("\n\nTheta learned with batch size 80 = ", slr.theta[1])
print("Number of iterations taken for batch size 80 = ", slr.n_iter[1])
print("Convergence criterion for batch size 80 = ", slr.eps[1])
print("Mean Squared Error at the end = ", slr.mean_squared_error(X_train, y_train, parameters=slr.theta[1], add_bias=True))

print("\n\nTheta learned with batch size 8000 = ", slr.theta[2])
print("Number of iterations taken for batch size 8000 = ", slr.n_iter[2])
print("Convergence criterion for batch size 8000 = ", slr.eps[2])
print("Mean Squared Error at the end = ", slr.mean_squared_error(X_train, y_train, parameters=slr.theta[2], add_bias=True))

print("\n\nTheta learned with batch size 800000 = ", slr.theta[3])
print("Number of iterations taken for batch size 800000 = ", slr.n_iter[3])
print("Convergence criterion for batch size 800000 = ", slr.eps[3])
print("Mean Squared Error at the end = ", slr.mean_squared_error(X_train, y_train, parameters=slr.theta[3], add_bias=True))


# ### Calculate the Analytical Solution

# In[ ]:


X_train_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
analytical_theta = np.linalg.inv(X_train_bias.T @ X_train_bias) @ X_train_bias.T @ y_train


# ### Print Analytical solution and corresponing MSE

# In[ ]:



print("\n\nAnalytical Solution = ", analytical_theta)
print("MSE of Analytical Solution = ", slr.mean_squared_error(X_train, y_train, parameters=analytical_theta, add_bias=True))


# ### Print Test Errors

# In[ ]:


print("Test error for batch size 1 = ", slr.mean_squared_error(X_test, y_test, parameters=slr.theta[0], add_bias=True))
print("Test error for batch size 80 = ", slr.mean_squared_error(X_test, y_test, parameters=slr.theta[1], add_bias=True))
print("Test error for batch size 8000 = ", slr.mean_squared_error(X_test, y_test, parameters=slr.theta[2], add_bias=True))
print("Test error for batch size 800000 = ", slr.mean_squared_error(X_test, y_test, parameters=slr.theta[3], add_bias=True))


# ## Plot SGD Paths

# In[ ]:



batch_sizes = [1, 80, 8000, 800000]

# Create Figure
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot each trajectory for different batch sizes
for batch_size, theta_history in zip(batch_sizes, list_of_parameters):

    theta_0, theta_1, theta_2 = theta_history[:, 0], theta_history[:, 1], theta_history[:, 2]

    # Plot the trajectory of theta updates
    ax.plot(theta_0, theta_1, theta_2, label=f"Batch size: {batch_size}")


# Labels and title
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$\theta_2$')
ax.set_title("Gradient Descent Trajectories in 3D Parameter Space")
ax.legend()

# Show the plot
plt.show()





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match226-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    # Generate X
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N,2))

    # Generate Noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=(N,))
</FONT>
    # Calculate y
    y = (np.hstack((np.ones((N, 1)), X))) @ theta + noise

    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = [None] * 4
        self.n_iter = [None] * 4
        self.max_iter = 8000000 # 1 epoch of SGD
        self.eps = [1e-4, 1e-4, 1e-6, 1e-8]
        self.converged = [None] * 4
        self.moving_window_size = [80000, 1000, 10, 1] # Last one is batch gradient descent, can be 1
        # batch sizes [1, 80, 8000, 800000]


    # Method to get MSE for given parameters
    def mean_squared_error(self, X, y, parameters, add_bias=True):
        if add_bias is True:
            X = np.hstack((np.ones((X.shape[0], 1)), X))
        n_samples = X.shape[0]
        error_vector = X @ parameters - y
        return (error_vector.T @ error_vector) / (2 * n_samples)


    # Method to find the Analaytical Solution
    def analytical_solution(self, X, y):
        X = np.hstack((np.ones(((X.shape[0], 1))), X))
        theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y
        return theta_analytical


    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        """
        This method will get 800,000 samples that form the training set.
        I will create n_samples / batch_size numnber of batches.
        """

        # Shuffle once
        perm = np.random.permutation(len(X))
        X = X[perm]
        y = y[perm]

        # Extract Info
        n_samples = X.shape[0]
        n_features = X.shape[1]

        # Add dummy feature to X
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        # Initialize theta
        self.theta = [np.zeros(n_features+1) for _ in range(4)]

        # Return of List of numpy arrays. Each array of shape (n_iter, n_features + 1)
        list_of_parameters = []


        # Mini-Batch Gradient Descent
        for index, batch_size in enumerate([1, 80, 8000, 800000]):
            # Average MSE after 10 iterations
            old_avg_mse = float('inf')
            new_avg_mse = 0
            # This weirdly named p_iter keeps track of the moving average window
            p_iter = 0
            ############################### IMPORTANT PARAMETER!!! Moving average Window #######################
            theta_history = np.zeros((self.max_iter, n_features+1))
            # Number of batches
            n_batch = n_samples // batch_size
            reached_end = False
            for iter in range(1, self.max_iter):
                if iter == self.max_iter-1:
                    reached_end = True # Can't converge on last iteration, not divisble by 10
                # This iteration corresponds to bath_num-th batch
                batch_num = iter % n_batch
                # Batch indices
                batch_start = batch_num * batch_size
                batch_end = (batch_num + 1) * batch_size
                # Take slices of X and y
                X_batch = X[batch_start:batch_end, :]
                y_batch = y[batch_start:batch_end]
                theta_old = theta_history[iter-1]
                gradient = X_batch.T @ (X_batch @ theta_old - y_batch) / batch_size
                theta_new = theta_old - learning_rate * gradient
                theta_history[iter] = theta_new
                # Calculate avg MSE if 10 iterations complete, compare with previous average
                mse_current = self.mean_squared_error(X, y, parameters=theta_new, add_bias=False)
                if (p_iter &lt; self.moving_window_size[index]):
                    p_iter += 1
                    new_avg_mse += mse_current
                elif p_iter == self.moving_window_size[index]:
                    new_avg_mse += mse_current
                    new_avg_mse /= self.moving_window_size[index]
                    if abs(new_avg_mse-old_avg_mse) &lt; self.eps[index]:
                        self.theta[index] = theta_new
                        self.n_iter[index] = iter
                        theta_history = theta_history[:iter+1, :]
                        list_of_parameters.append(theta_history)
                        break;
                    else:
                        old_avg_mse = new_avg_mse
                        new_avg_mse = 0
                        p_iter = 0

            if reached_end is True:
                self.theta[index] = theta_history[-1] # or self.max_iter-1
                self.n_iter[index] = self.max_iter-1
                list_of_parameters.append(theta_history)
                print("\n\nSGD for batch size = " + str(batch_size) + " does not converge after " + str(self.max_iter) + " iterations.")
                print("Try something else. Increase the learning rate or maximum iterations or relax the stopping criterion.")
                print("MSE = " + str(self.mean_squared_error(X, y, parameters=self.theta[index], add_bias=False)) + ".")


        return list_of_parameters



    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match226-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        if self.theta is None:
            raise RuntimeError("The model has not been trained. Call fit() before making predictions.")

        X = np.hstack((np.ones((X.shape[0], 1)), X))
</FONT>
        y_predS = [(X @ theta).tolist() for theta in self.theta]

        return y_predS



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        pass



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        pass

</PRE>
</PRE>
</BODY>
</HTML>
