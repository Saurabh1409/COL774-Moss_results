<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_G6YHS.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_G6YHS.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import csv 
import pandas as pd
import seaborn as sns
from matplotlib.animation import FuncAnimation

class LinearRegressor:
    def __init__(self):
        self.theta = None 
        self.tol = 1e-3
        self.theta_history = []
        self.loss_values = []
    #1_1 
    def fit(self, X, y, learning_rate=0.001):
        n_samples, n_features = X.shape
        X_eff = np.hstack((np.ones((n_samples, 1)), X))
        self.theta = np.zeros((n_features + 1, 1))
        n_iter = 1000
        for _ in range(n_iter):
            y_pred = np.dot(X_eff, self.theta)
            error = y_pred - y
            gradient = (1 / n_samples) * np.dot(X_eff.T, error)
            self.theta -= learning_rate * gradient
            self.theta_history.append(self.theta.copy())
            loss_val = (1 / (2 * n_samples)) * (np.sum(error ** 2))            
            self.loss_values.append(loss_val.copy())
            if np.linalg.norm(gradient, ord=2) &lt; self.tol:
                print(f"Stops at itr = {_}")
                return np.array(self.theta_history)
        return np.array(self.theta_history)

    def predict(self, X):
        n_samples, n_features = X.shape
        X_eff = np.hstack((np.ones((n_samples, 1)), X))
        return np.dot(X_eff, self.theta)
    
    #1_2
    def plot_data(self, X, y, saveIT = True):
        np.random.seed(301)
        sample_size = X.shape[0] // 10
        indices = np.random.choice(X.shape[0], sample_size, replace=False)
        X_subset = X[indices]
        y_subset = y[indices]
        data = pd.DataFrame({'X': X_subset.flatten(), 'y': y_subset.flatten()})
        sns.scatterplot(x='X', y='y', data=data)
        y_pred_subset = self.predict(X_subset)
        data = pd.DataFrame({'X': X_subset.flatten(), 'y': y_pred_subset.flatten()})
        sns.scatterplot(x='X', y='y', data=data)
        if saveIT:
            plt.savefig('Q_1_2.png')  
        # plt.show()

    #1_3_a
    def plot_3d(self, X, y, theta0_range=(-100, 100), theta1_range=(-100, 100), resolution=50):
        """Plots the error function J(Î¸) in 3D for a linear regression model with one feature."""

        n_samples = X.shape[0]
        X_eff = np.hstack((np.ones((n_samples, 1)), X))  # Add bias term

        theta0_vals = np.linspace(*theta0_range, resolution)
        theta1_vals = np.linspace(*theta1_range, resolution)
        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

        J_vals = np.zeros_like(T0)
        
        for i in range(resolution):
            for j in range(resolution):
                theta = np.array([[T0[i, j]], [T1[i, j]]])
                y_pred = np.dot(X_eff, theta)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match42-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                error = y_pred - y
                J_vals[i, j] = (1 / (2 * n_samples)) * np.sum(error ** 2)

        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.8)
</FONT>
        ax.set_xlabel(r'$\theta_0$ (Bias)')
        ax.set_ylabel(r'$\theta_1$ (Weight)')
        ax.set_zlabel(r'$J(\theta)$ (Cost Function)')
        ax.set_title('3D Visualization of Cost Function')

        plt.show()

    def animate_plot(self, X, y, resolution=50, margin=1.0):
        theta_history = self.theta_history
        loss_values = self.loss_values
        theta_history = np.squeeze(np.array(theta_history))  # shape (n_iter, 2)
        loss_values = np.squeeze(np.array(loss_values))      # shape (n_iter,)

        theta0_vals = theta_history[:, 0]
        theta1_vals = theta_history[:, 1]
        theta0_min, theta0_max = theta0_vals.min() - margin, theta0_vals.max() + margin
        theta1_min, theta1_max = theta1_vals.min() - margin, theta1_vals.max() + margin

        theta0_grid = np.linspace(min(-50, theta0_min), max(50, theta0_max), resolution)
        theta1_grid = np.linspace(min(-50, theta1_min), max(50, theta1_max), resolution)
        T0, T1 = np.meshgrid(theta0_grid, theta1_grid)
        n_samples = X.shape[0]
        X_eff = np.hstack((np.ones((n_samples, 1)), X))
        J_vals = np.zeros_like(T0)
        for i in range(resolution):
            for j in range(resolution):
                theta_ij = np.array([[T0[i, j]], [T1[i, j]]])
                y_pred = np.dot(X_eff, theta_ij)
                error = y_pred - y
                J_vals[i, j] = (1 / (2 * n_samples)) * np.sum(error ** 2)

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.5)
        
        ax.set_xlabel(r'$\theta_0$ (Bias)')
        ax.set_ylabel(r'$\theta_1$ (Weight)')
        ax.set_zlabel('Cost J')
        ax.set_title('Gradient Descent Animation')

        point, = ax.plot([theta_history[0, 0]], [theta_history[0, 1]], [loss_values[0]],
                marker='o', markersize=8, color='red')

        for i in range(len(theta_history)):
            point.set_data([theta_history[i, 0]], [theta_history[i, 1]])
            point.set_3d_properties(loss_values[i])
            
            plt.draw()
            plt.pause(0.2)

        plt.show()

    def animate_2d(self, X, y,learning_rate, resolution=50, margin=1.0):
        self.fit(X, y, learning_rate)
        theta_history = np.squeeze(np.array(self.theta_history))
        loss_values = np.squeeze(np.array(self.loss_values))
        
        theta0_vals = theta_history[:, 0]   
        theta1_vals = theta_history[:, 1]
        theta0_min, theta0_max = theta0_vals.min() - margin, theta0_vals.max() + margin
        theta1_min, theta1_max = theta1_vals.min() - margin, theta1_vals.max() + margin

        theta0_grid = np.linspace(min(theta0_min, -40), max(70, theta0_max), resolution)
        theta1_grid = np.linspace(min(theta0_min, -40), max(70, theta1_max), resolution)
        T0, T1 = np.meshgrid(theta0_grid, theta1_grid)

        n_samples = X.shape[0]
        X_eff = np.hstack((np.ones((n_samples, 1)), X))

        J_vals = np.zeros(T0.shape)
        for i in range(resolution):
            for j in range(resolution):
                theta_ij = np.array([[T0[i, j]], [T1[i, j]]])
                y_pred = np.dot(X_eff, theta_ij)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match42-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                error = y_pred - y
                J_vals[i, j] = (1 / (2 * n_samples)) * np.sum(error ** 2)

        # Set up the figure for plotting
        plt.figure(figsize=(8, 6))
</FONT>        levels = np.logspace(np.log10(J_vals.min() + 1e-6), np.log10(J_vals.max()), 30)
        contour_plot = plt.contour(T0, T1, J_vals, levels=levels, cmap='viridis')
        plt.clabel(contour_plot, inline=True, fontsize=8)

        plt.xlabel(r'$\theta_0$ (Bias)')
        plt.ylabel(r'$\theta_1$ (Weight)')
        plt.title('Gradient Descent on Contour Plot of Error Function')

        # Initialize a marker for the current theta and a line for the path.
        current_point, = plt.plot([theta_history[0, 0]], [theta_history[0, 1]], 'ro', markersize=8)
        path_line, = plt.plot([theta_history[0, 0]], [theta_history[0, 1]], 'r-', linewidth=2)

        # Animate the descent
        for i in range(len(theta_history)):
            current_point.set_data([theta_history[i, 0]], [theta_history[i, 1]])
            path_line.set_data(theta_history[:i+1, 0], theta_history[:i+1, 1])
            
            plt.draw()
            plt.pause(0.05)  # Pause to show animation

        plt.show()

    def test_and_training_errors(self, X_test, X_train, y_test, y_train, true_theta):
        
        # for i in range(self.total_batches):
            self.theta = self.theta_history[i][-1]
            y_test_pred = self.predict(X_test)
            test_mse = np.mean((y_test_pred - y_test)**2)
            y_train_pred = self.predict(X_train)
            train_mse = np.mean((y_train_pred - y_train)**2)
            print(f"For batch of size = {self.batch_sizes[i]}    test_error = {test_mse:.6f}   and train_error = {train_mse:.6f} \n")
            param_error = np.linalg.norm(self.theta - true_theta)
            print(f"  Euclidean error between true and learned theta: {param_error:.6f} \n")
            print(f" Theta = {self.theta}")
            print(f" It took num of iterations = {len(self.theta_history[i])} \n\n")



X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
y = pd.read_csv('../data/Q1/linearY.csv', header=None).values


model = LinearRegressor()
model.fit(X, y)
model.plot_data(X, y, True)
# model.plot_3d(X, y)
# model.animate_plot(X, y)
# model.animate_2d(X, y, learning_rate = 0.001)
# model.animate_2d(X, y, learning_rate = 0.025)
# model.animate_2d(X, y, learning_rate = 0.1)



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

#2_1
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.empty((N, 2))
    X[:, 0] = np.random.normal(input_mean[0], input_sigma[0], N)
    X[:, 1] = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    y = X @ theta[1:] + theta[0] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.epochs = 100
        self.total_batches = 4
        self.batch_sizes = [1, 80, 8000, 800000]
        self.tol = 1e-4
        self.theta = None
        self.theta_history = []
    
    def fit(self, X, y, learning_rate=0.001):

        n_samples, n_features = X.shape
        X_new = np.hstack([np.ones((n_samples, 1)), X])
        n_params = n_features + 1
        self.theta_history = []

        for i, batch_size in enumerate(self.batch_sizes):
            
            theta = np.zeros(n_params)
            batch_thetas = [theta.copy()]
            converged = False
            indices = np.arange(n_samples)

            print(f"batch_size = {batch_size}")

            for epoch in range(self.epochs):

                if(epoch% 10 == 0):
                    print(f"epoch = {epoch} out of {self.epochs}")

                np.random.shuffle(indices)
                X_shuff = X_new[indices]
                y_shuff = y[indices]
                theta_prev = theta.copy()

                y_pred_prev = X_new.dot(theta_prev)
                loss_prev = np.mean((y_pred_prev - y) ** 2) / 2
                
                for b in range(int(np.ceil(n_samples / batch_size))):
                    start = b * batch_size
                    end = min(start + batch_size, n_samples)
                    X_batch = X_shuff[start:end]
                    y_batch = y_shuff[start:end]
                    y_pred = X_batch.dot(theta)
                    errors = y_pred - y_batch
                    gradient = (X_batch.T.dot(errors)) / len(y_batch)
                    theta -= learning_rate * gradient    

                y_pred_now = X_new.dot(theta)
                loss_now = np.mean((y_pred_now - y) ** 2) / 2

                batch_thetas.append(theta.copy())

                if abs(loss_now - loss_prev) &lt; self.tol:
                    converged = True
                    break
            
            print(f"Ending batch {batch_size} - Converged: {converged}")
            self.theta_history.append(np.array(batch_thetas))

        return np.array(self.theta_history)
    
    def predict(self, X):
        
        n_samples = X.shape[0]
        X_new = np.hstack([np.ones((n_samples, 1)), X])
        return self.theta[0] + X.dot(self.theta[1:])

    #2_4
    def test_and_training_errors(self, X_test, X_train, y_test, y_train, true_theta):
        for i in range(self.total_batches):

            self.theta = self.theta_history[i][-1]
            
            y_test_pred = self.predict(X_test)
            test_mse = np.mean((y_test_pred - y_test)**2)

            y_train_pred = self.predict(X_train)
            train_mse = np.mean((y_train_pred - y_train)**2)
            
            print(f"For batch of size = {self.batch_sizes[i]}    test_error = {test_mse:.6f}   and train_error = {train_mse:.6f} \n")
            param_error = np.linalg.norm(self.theta - true_theta)
            print(f"  Euclidean error between true and learned theta: {param_error:.6f} \n")
            print(f" Theta = {self.theta}")
            print(f" It took num of iterations = {len(self.theta_history[i])} \n\n")

    #2_5 
    def plot_theta_trajectories(self, all_thetas):
        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')

        colors = ['r', 'g', 'b', 'm']
        markers = ['o', 'x', '^', 's']
        batch_sizes = [1, 10, 50, 100]  # Example batch sizes (adjust accordingly)

<A NAME="2"></A><FONT color = #0000FF><A HREF="match42-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for i, theta_updates in enumerate(all_thetas):
            theta_updates = np.array(theta_updates)  # Ensure it's an array
            ax.plot(theta_updates[:, 0], theta_updates[:, 1], theta_updates[:, 2], 
                    marker=markers[i % len(markers)], linestyle='-', 
</FONT>                    color=colors[i % len(colors)], label=f'Batch Size {batch_sizes[i]}')

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$\theta_2$')
        ax.set_title('Comparison of Î¸ Updates for Different Batch Sizes')
        ax.legend()
        plt.show()

    #2_3_b
    def closed_form_solution(self, X, y):
<A NAME="6"></A><FONT color = #00FF00><A HREF="match42-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        n_samples = X.shape[0]
        X_eff = np.hstack([np.ones((n_samples, 1)), X])
        XtX = X_eff.T @ X_eff
</FONT>        Xty = X_eff.T @ y    
        
        theta = np.linalg.inv(XtX) @ Xty
        self.theta = theta

        print(f"Testing Closed form solution \n")

        y_pred = self.predict(X)
        mse = np.mean((y_pred - y)**2)

        print(f" mse = {mse:.6f} \n")
        param_error = np.linalg.norm(self.theta - true_theta)
        print(f"  Euclidean error between true and learned theta: {param_error:.6f} \n")
        print(f" Theta = {self.theta}")
        return theta


def main_runner():
    N_tot = 10000000
    N_train = 800000
    N_test = 200000

    actual_theta = np.array([3.0, 1.0, 2.0])
    input_mean = np.array([3.0, -1.0])
    input_sigma = np.array([2.0, 2.0])
    noise_sigma = 2.0

    X_test , y_test = generate(N_test, actual_theta, input_mean, input_sigma, noise_sigma)
    X_train , y_train = generate(N_train, actual_theta, input_mean, input_sigma, noise_sigma)

    model = StochasticLinearRegressor()
    theta_array = model.fit(X_train, y_train)

    model.test_and_training_errors(X_test, X_train, y_test, y_train, actual_theta)    

main_runner()



# 
# 

# print(f"Data generated ... \n")
# C = 
# print(f"Regressor initialized ... \n")
# all_thetas = C.fit(X_train, y_train)
# C.test_and_training_errors(X_test, X_train, y_test, y_train, true_theta)
# C.plot_theta_trajectories(all_thetas)


# C:\Users\Pushpraj\OneDrive\Desktop\SEM8\col774\Assignment1\Assignment1\Q2&gt;python sampling_sgd.py
# batch_size = 1
# epoch = 0 out of 100
# epoch = 10 out of 100
# epoch = 20 out of 100
# epoch = 30 out of 100
# epoch = 40 out of 100
# epoch = 50 out of 100
# epoch = 60 out of 100
# epoch = 70 out of 100
# epoch = 80 out of 100
# epoch = 90 out of 100
# Ending batch 1 - Converged: False
# batch_size = 80
# epoch = 0 out of 100
# Ending batch 80 - Converged: True
# batch_size = 8000
# epoch = 0 out of 100
# epoch = 10 out of 100
# epoch = 20 out of 100
# epoch = 30 out of 100
# epoch = 40 out of 100
# epoch = 50 out of 100
# epoch = 60 out of 100
# epoch = 70 out of 100
# epoch = 80 out of 100
# epoch = 90 out of 100
# Ending batch 8000 - Converged: False
# batch_size = 800000
# epoch = 0 out of 100
# epoch = 10 out of 100
# epoch = 20 out of 100
# epoch = 30 out of 100
# epoch = 40 out of 100
# epoch = 50 out of 100
# epoch = 60 out of 100
# epoch = 70 out of 100
# epoch = 80 out of 100
# epoch = 90 out of 100
# Ending batch 800000 - Converged: False
# Traceback (most recent call last):
#   File "C:\Users\Pushpraj\OneDrive\Desktop\SEM8\col774\Assignment1\Assignment1\Q2\sampling_sgd.py", line 162, in &lt;module&gt;
#     main_runner()
#   File "C:\Users\Pushpraj\OneDrive\Desktop\SEM8\col774\Assignment1\Assignment1\Q2\sampling_sgd.py", line 158, in main_runner
#     theta_array = model.fit(X_train, y_train)
#                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
#   File "C:\Users\Pushpraj\OneDrive\Desktop\SEM8\col774\Assignment1\Assignment1\Q2\sampling_sgd.py", line 74, in fit
#     return np.array(self.theta_history)
#            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
# ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.max_iter = 100
        self.tol = 1e-6
        self.theta_history = []

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def compute_gradient(self, theta, X, y):
        n_samples = y.shape[0]
        h = self.sigmoid(X.dot(theta))
        gradient = X.T.dot(h - y)
        return gradient

    def compute_hessian(self, theta, X):
        n_samples = X.shape[0]
        h = self.sigmoid(X.dot(theta))
        D = np.diag(h * (1 - h))
        hessian = X.T.dot(D).dot(X)
        return hessian

    def fit(self, X, y, learning_rate=0.01):
        n_samples, n_features = X.shape
        mu = np.mean(X, axis=0)
        sigma = np.std(X, axis=0)
        X_norm = (X - mu) / sigma
        X_eff = np.hstack((np.ones((n_samples, 1)), X_norm))
        
        self.theta = np.zeros(n_features + 1)
        
        for i in range(self.max_iter):
            self.theta_history.append(self.theta.copy())
            h = self.sigmoid(X_eff.dot(self.theta))
            grad = self.compute_gradient(self.theta, X_eff, y)
            H = self.compute_hessian(self.theta, X_eff)
            
            delta = np.linalg.solve(H, grad)
            theta_new = self.theta - delta
            
            if np.linalg.norm(delta, ord=2) &lt; self.tol:
                print(f"theta_final = {self.theta_history[-1]}")
                return np.array(self.theta_history)
            self.theta = theta_new
        
        print(f"theta_final = {self.theta_history[-1]}")
        return np.array(self.theta_history)
    
    def predict(self, X):
        n_samples, n_features = X.shape
        mu = np.mean(X, axis=0)
        sigma = np.std(X, axis=0)
        X_norm = (X - mu) / sigma
        X_eff = np.hstack((np.ones((n_samples, 1)), X_norm))
        
        y_pred = self.sigmoid(X_eff.dot(self.theta))
        return (y_pred &gt;= 0.5).astype(int)
    
    def plot_it(self, X, y):
        pos = y == 1
        neg = y == 0

        theta_final = self.theta_history[-1]

        plt.scatter(X[pos, 0], X[pos, 1], marker='x', c='red', label='y = 1')
        plt.scatter(X[neg, 0], X[neg, 1], marker='o', c='blue', label='y = 0')

        x1_vals = np.array([X[:, 0].min() - 1, X[:, 0].max() + 1])
<A NAME="0"></A><FONT color = #FF0000><A HREF="match42-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]

        plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend()
        plt.title('Training Data and Decision Boundary')
        plt.savefig('data_points.png')
        plt.show()
    

X = pd.read_csv('../data/Q3/logisticX.csv', header=None).values
</FONT>y = pd.read_csv('../data/Q3/logisticY.csv', header=None).values.flatten()

cls = LogisticRegressor()
cls.fit(X, y)
cls.plot_it(X, y)



import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
        self.phi = None
        self.sigma_inv = None
        self.w = None 
        self.b = None 

    def plot_4_2(self, X, y):
        X0 = X[y == 0]
        X1 = X[y == 1]
        plt.figure(figsize=(8, 6))
        plt.scatter(X0[:, 0], X0[:, 1], marker='o', color='black', label='Alaska')
        plt.scatter(X1[:, 0], X1[:, 1], marker='x', color='red', label='Canada')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('Training Data: Salmons from Alaska vs. Canada')
        plt.legend()
        plt.grid(True)
        plt.savefig('data_points.png')
        plt.show()
    
    def plot_4_3(self, X, y):
        X0 = X[y == 0]
        X1 = X[y == 1]
        plt.scatter(X0[:, 0], X0[:, 1], marker='o', color='black', label='Alaska')
        plt.scatter(X1[:, 0], X1[:, 1], marker='x', color='red', label='Canada')
<A NAME="7"></A><FONT color = #0000FF><A HREF="match42-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x1_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)
        x2_vals = -(self.b + self.w[0]*x1_vals) / self.w[1]
</FONT>        plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match42-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('GDA Decision Boundary')
        plt.legend()
        plt.savefig('gda_decision_boundary.png')
        plt.show()

    def plot_4_5(self, X, y):
        X0 = X[y == 0]
</FONT>        X1 = X[y == 1]
        
        plt.figure(figsize=(10, 6))
        
        plt.scatter(X0[:, 0], X0[:, 1], marker='o', color='black', label='Alaska')
        plt.scatter(X1[:, 0], X1[:, 1], marker='x', color='red', label='Canada')
        
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))
        
        linear_boundary = self.w[0]*xx + self.w[1]*yy + self.b
        plt.contour(xx, yy, linear_boundary, levels=[0], colors='green', linestyles='dashed', label='Linear Boundary')
        
<A NAME="5"></A><FONT color = #FF0000><A HREF="match42-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        Z = np.zeros_like(xx)
        sigma0_inv = np.linalg.inv(self.sigma0)
        sigma1_inv = np.linalg.inv(self.sigma1)
        
        for i in range(xx.shape[0]):
</FONT>            for j in range(xx.shape[1]):
                x = np.array([xx[i,j], yy[i,j]])                
                d0 = (x - self.mu0).T @ sigma0_inv @ (x - self.mu0)
                d1 = (x - self.mu1).T @ sigma1_inv @ (x - self.mu1)
                Z[i,j] = -0.5*(d1 - d0) + np.log(self.phi/(1-self.phi)) - 0.5*np.log(np.linalg.det(self.sigma1)/np.linalg.det(self.sigma0))
        
        plt.contour(xx, yy, Z, levels=[0], colors='purple', label='Quadratic Boundary')

        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('Quadratic Decision Boundary')
        plt.legend()
        plt.savefig('quadratic_decision_boundary.png')
        plt.show()

    # 4_1 and 4_4 
    def fit(self, X, y, assume_same_covariance=False):
        n = X.shape[0]
        self.phi = np.mean(y)
        X0 = X[y == 0]
        X1 = X[y == 1]
        self.mu0 = np.mean(X0, axis=0)
        self.mu1 = np.mean(X1, axis=0)
        
        D = X.copy()
        D[y == 0] -= self.mu0
        D[y == 1] -= self.mu1

        self.sigma = (D.T @ D) / n

        D0 = X0 - self.mu0 
        D1 = X1 - self.mu1
        
        n0 = X0.shape[0]
        n1 = X1.shape[0]
        self.sigma0 = (D0.T @ D0) / n0
        self.sigma1 = (D1.T @ D1) / n1

        self.sigma_inv = np.linalg.inv(self.sigma)

        self.w = self.sigma_inv @ (self.mu1 - self.mu0)
        self.b = 0.5 * (self.mu0 @ self.sigma_inv @ self.mu0 - self.mu1 @ self.sigma_inv @ self.mu1) + np.log(self.phi / (1 - self.phi))

        return (self.mu0, self.mu1, self.sigma) if assume_same_covariance else (self.mu0, self.mu1, self.sigma0, self.sigma1)

    def predict(self, X):
        linear_scores = X @ self.w + self.b
        return (linear_scores &gt; 0).astype(int)        

C = GaussianDiscriminantAnalysis()
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
C.fit(X, y, True)
C.plot_4_2(X, y)
C.plot_4_3(X, y)
C.plot_4_5(X, y)

</PRE>
</PRE>
</BODY>
</HTML>
