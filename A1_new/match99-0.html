<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_8TOF2.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_8TOF2.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    learning_rate=0.001
    regressor = LinearRegressor()
    param_history = regressor.fit(x, y, learning_rate)
    param_history = np.array(param_history)
    m = len(y)

    def compute_error(weight, bias):
        predictions = weight * x + bias
        errors = predictions - y
            
        return (1/(2*m)) * np.sum(errors ** 2)
    
    w_min = np.min(param_history[:, 0]) - 50
    w_max = np.max(param_history[:, 0]) + 50
    b_min = np.min(param_history[:, 1]) - 20
    b_max = np.max(param_history[:, 1]) + 20

<A NAME="2"></A><FONT color = #0000FF><A HREF="match99-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    weights = np.linspace(w_min, w_max, 200)
    biases = np.linspace(b_min, b_max, 200)
    W, B = np.meshgrid(weights, biases)

    Z = np.zeros(W.shape)
</FONT>    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            Z[i, j] = compute_error(W[i, j], B[i, j])

    fig, ax = plt.subplots(figsize=(10, 8))
    contour_levels = np.logspace(-2, 3, 50)
    contf = ax.contourf(W, B, Z, levels=contour_levels, cmap='viridis', alpha=0.7)
    cont = ax.contour(W, B, Z, levels=contour_levels, colors='black', linewidths=0.5)
    fig.colorbar(contf, label='Error J')

    ax.set_xlabel('Weight (w)')
    ax.set_ylabel('Bias (b)')
    ax.set_title('Contour Plot of Error Function with Gradient Descent Trajectory')

    path_w = []
    path_b = []
    
    trajectory, = ax.plot([], [], 'b-o', linewidth=2, markersize=4, label='Trajectory')
    current_point, = ax.plot([], [], 'ro', markersize=8, label='Current Point')
    ax.legend()

    for i in range(len(param_history)):
        w_curr = param_history[i, 0]
        b_curr = param_history[i, 1]
        
        path_w.append(w_curr)
        path_b.append(b_curr)
        
        trajectory.set_data(path_w, path_b)
        current_point.set_data([w_curr], [b_curr])
        
        plt.draw()
        plt.pause(0.2)

    plt.show()
    # plt.savefig('plots/Q1/contour_graph.png')




import numpy as np
import os

class LinearRegressor:
    def __init__(self):
        self.ABS = 1e-5
        self.w = 0
        self.b = 0

    def fit(self, X, y, learning_rate=0.01):
        X=np.array(X)
        y=np.array(y)
        
        m=X.shape[0]
        weights=[np.hstack((0,0))]
        J_prev=np.inf

        while True:
            predited=self.w*X+self.b
            error=predited-y

            J_current=(1/(2*m))*np.sum(error**2)

            grad_w=(1/m)*(X.T @ error)
            grad_b=(1/m)*np.sum(error)

            self.w-=learning_rate*grad_w
            self.b-=learning_rate*grad_b

            weights.append(np.hstack((self.w,self.b)))

            if abs(J_current-J_prev) &lt; self.ABS:
                break

            J_prev=J_current

        return weights
    
    def predict(self, X):
        X = np.array(X)
        y_pred= X* self.w + self.b
        return y_pred

if __name__=='__main__':
    regressor = LinearRegressor()
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    parameter_history = regressor.fit(x, y, learning_rate=0.01)
    print("Final parameters (weights and intercept):", parameter_history[-1])




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    regressor = LinearRegressor()
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    weights = regressor.fit(x, y, learning_rate=0.01)
    weights = np.array(weights)
    m = y.shape[0]

    def compute_error(weight, bias):
        predictions = weight * x + bias
        errors = predictions - y
            
        return (1/(2*m)) * np.sum(errors ** 2)

    w_min = np.min(weights[:, 0]) - 10
    w_max = np.max(weights[:, 0]) + 10
    b_min = np.min(weights[:, 1]) - 10
    b_max = np.max(weights[:, 1]) + 10

    weight_vals = np.linspace(w_min, w_max, 200)
    bias_vals = np.linspace(b_min, b_max, 200)
    W, B = np.meshgrid(weight_vals, bias_vals)

    J_vals = np.zeros(W.shape)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match99-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            J_vals[i, j] = compute_error(W[i, j], B[i, j])

    fig = plt.figure(figsize=(10,8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(W, B, J_vals, cmap='viridis', alpha=0.7,edgecolor=None)
</FONT>    fig.colorbar(surf, shrink=0.5, aspect=5, label='Error J(w, b)')
    ax.set_xlabel('Weight (w)')
    ax.set_ylabel('Bias (b)')
    ax.set_zlabel('Cost J')
    ax.set_title('Cost Function Surface and Gradient Descent Path')

    path_weights = weights[:, 0]
    path_biases = weights[:, 1]
    path_costs = np.array([compute_error(w, b) for w, b in zip(path_weights, path_biases)])

    for i in range(len(weights)):
        current_w = weights[i, 0]
        current_b = weights[i, 1]
        current_cost = compute_error(current_w, current_b)
        current_point = ax.scatter(current_w, current_b, current_cost, color='k', s=50)
        plt.draw()
        plt.pause(0.2)
    plt.show()
    # plt.savefig('plots/Q1/error_curve.png')




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    regressor = LinearRegressor()
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    weights = regressor.fit(x, y, learning_rate=0.01)

    print("Final parameters:")
    print("weight (w):", regressor.w)
    print("Intercept (b):", regressor.b)

    plt.scatter(x, y, color='blue', label='Data Points')

    x_vals = np.linspace(min(x), max(x), 100)

    y_vals = regressor.w * x_vals + regressor.b

    plt.plot(x_vals, y_vals, color='red', label='Learned Hypothesis')

    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Linear Regression')
    plt.legend()

    plt.show()
    # plt.savefig('plots/Q1/datapoints.png')



import numpy as np
import matplotlib.pyplot as plt

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x0=theta[0]
    x1=np.random.normal(input_mean[0],input_sigma[0],N)
    x2=np.random.normal(input_mean[1],input_sigma[1],N)
    X=np.column_stack((x1,x2))
    eps=np.random.normal(0,noise_sigma,N)
    theta=theta[1:]
    y=x0+X @ theta+eps
    return X,y

class ClosedForm:
    def __init__(self):
        self.theta = None

    def fit(self, X, y):
        X = np.column_stack((np.ones(X.shape[0]), X))
        self.theta = np.linalg.inv(X.T @ X) @ (X.T @ y)

    def predict(self, X):
        X = np.column_stack((np.ones(X.shape[0]), X)) 
        return X @ self.theta
    
    def compute_loss(self, X, y):
        if X.shape[1]!=self.theta.shape[0]:
            predictions=self.predict(X)
        else:
            predictions = X @ self.theta
        errors = predictions - y
        return (1 / (2 * X.shape[0])) * np.sum(errors ** 2)

class StochasticLinearRegressor:
    def __init__(self,batch_size=1):
        self.batch_size=batch_size
        self.ABS=1e-7
        self.theta = None
        self.bias=None

    def compute_loss(self, X, y):
        errors = (X @ self.theta + self.bias)-y
        return (1 / (2 * X.shape[0])) * np.sum(errors ** 2)

    def fit(self, X, y, learning_rate=0.001):
        X = np.array(X)
        y = np.array(y)
        N, F = X.shape

        self.theta = np.zeros(F)
        self.bias=0.0
        loss_history = []
        theta_history = [np.insert(self.theta.copy(),0,self.bias)]

        epoch = 0
        updates=N//self.batch_size

        while True:

            # shuffle
            indices = np.arange(N)
            np.random.shuffle(indices)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match99-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            X = X[indices]
            y = y[indices]

            # split into batches
            for i in range(0, N, self.batch_size):
                X_batch= X[i:i+self.batch_size]
</FONT>                y_batch= y[i:i+self.batch_size]

                errors = (X_batch @ self.theta+self.bias) - y_batch

                grad_w = (1 / self.batch_size) * X_batch.T @ errors
                grad_b = (1 / self.batch_size) * np.sum(errors)

                self.theta -= learning_rate * grad_w
                self.bias -= learning_rate * grad_b

                loss = 1/(2*self.batch_size) * np.sum(errors**2)
                loss_history.append(loss)
            
            if len(loss_history)&gt;=100000:
                loss_history=loss_history[-100000:]

            epoch += 1
            theta_history.append(np.insert(self.theta.copy(),0,self.bias))

            
            print(f'epoch={epoch}, theta={theta_history[-1]}')

            if epoch &gt;=10000:
                break

            elif self.batch_size &gt;= 5000 and np.linalg.norm(theta_history[-1]-theta_history[-2])&lt;1e-3 * np.sqrt(learning_rate):
                break

            elif updates*epoch&gt;1e7:
                break

            elif len(loss_history)&gt;=100000:
                    x1=np.mean(np.asarray(loss_history[-50000:]))
                    x2=np.mean(np.asarray(loss_history[-100000:-50000]))
                    avg_diff = abs(x1-x2 )
                    # avg_diff = abs(np.mean(np.asarray(loss_history[-5000:])) - np.mean(np.asarray(loss_history[-10000:-5000])))
                    print(f'x1={x1}, x2={x2}, avg_diff={avg_diff}')
                    print()
                    if avg_diff&lt;1e-5:
                        break

        theta_history=np.array(theta_history)
        return theta_history

    def predict(self, X):
        return X @ self.theta + self.bias

def plot_theta_history(theta_history, savepath=None):
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', linestyle='-', color='b', label='Trajectory')

    ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2], color='green', s=100, label='Start')
    ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], color='red', s=100, label='Converged')

    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.legend()

    if savepath:
        plt.savefig(savepath)
    plt.show()

if __name__ == '__main__':

    X_train,y_train = generate(800000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)
    X_test,y_test = generate(200000,np.array([3,1,2]),np.array([3,-1]),np.array([4,4]),2)

    batch_sizes = [1, 80, 8000, 800000]

    for batch_size in batch_sizes:
        print(f"\nTraining with batch size: {batch_size}")
        directsol = ClosedForm()
        regressor = StochasticLinearRegressor(batch_size=batch_size)
        theta_history = regressor.fit(X_train, y_train, learning_rate=0.001)
        directsol.fit(X_train, y_train)
        print('stochastic gradient descent:')
        print("$theta$:", regressor.bias,regressor.theta[0],regressor.theta[1])
        print('Training error is ',regressor.compute_loss(X_train,y_train))
        print('Test error is ',regressor.compute_loss(X_test,y_test))
        print('closed form solution:')
        print("$theta$:", directsol.theta)
        print('Training error is ',directsol.compute_loss(X_train,y_train))
        print('Test error is ',directsol.compute_loss(X_test,y_test))
        plot_theta_history(theta_history,f'plots/Q2/plot_{batch_size}.png')
    




<A NAME="5"></A><FONT color = #FF0000><A HREF="match99-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import os
class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mean = None
        self.std = None
</FONT>        self.ABS=1e-8

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01):
        X = np.array(X)
        y = np.array(y)
        N, F = X.shape

        self.mean = X.mean(axis=0)
        self.std = X.std(axis=0)
        X = (X - self.mean) / self.std

        X = np.column_stack((np.ones(N), X))
        F = X.shape[1]

        self.theta = np.zeros(F)
        theta_history = [self.theta.copy()]

        max_iter = 100

        for iteration in range(max_iter):
            z = X @ self.theta
            h = self.sigmoid(z)

            grad = X.T @ (y - h)

            D = np.diag(h * (1 - h))

            H = X.T @ D @ X

            try:
                delta = np.linalg.inv(H) @ grad
            except np.linalg.LinAlgError:
                break

            self.theta += learning_rate * delta

            theta_history.append(self.theta.copy())

            if np.linalg.norm(delta) &lt; self.ABS:
                print(f"Convergence reached at iteration {iteration+1}.")
                break

        return theta_history

    def predict(self, X):
        X = np.array(X)
        N = X.shape[0]
        X = (X - self.mean) / self.std
        X = np.column_stack((np.ones(N), X))
        prob = self.sigmoid(X @ self.theta)
        return (prob &gt; 0.5).astype(int)


if __name__=='__main__':
    X = np.genfromtxt(os.path.abspath('data/Q3/logisticX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q3/logisticY.csv'), delimiter=',')

    regressor = LogisticRegressor()
    theta_history = regressor.fit(X, y, learning_rate=1)
    print("Learned coefficients (theta):", regressor.theta)

    y_pred = regressor.predict(X)
    accuracy = np.mean(y_pred == y)
    print("Training accuracy:", accuracy)




import numpy as np
import matplotlib.pyplot as plt
import os
from logistic_regression import LogisticRegressor

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q3/logisticX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q3/logisticY.csv'), delimiter=',')

    regressor = LogisticRegressor()
    theta = regressor.fit(X, y, learning_rate=1)
    print("Learned coefficients (theta):", regressor.theta)

    plt.figure(figsize=(8, 6))
    idx0 = np.where(y == 0)[0]
    idx1 = np.where(y == 1)[0]
    
    plt.scatter(X[idx0, 0], X[idx0, 1], marker='o', color='blue', label='Class 0')
    plt.scatter(X[idx1, 0], X[idx1, 1], marker='x', color='red', label='Class 1')
    
    if regressor.theta[2] != 0:
        x1_vals = np.linspace(np.min(X[:, 0]) - 1, np.max(X[:, 0]) + 1, 100)
        mu1, mu2 = regressor.mean[0], regressor.mean[1]
        sigma1, sigma2 = regressor.std[0], regressor.std[1]
        x2_vals = mu2 - (sigma2 / regressor.theta[2]) * (regressor.theta[0] + regressor.theta[1] * ((x1_vals - mu1) / sigma1))
        plt.plot(x1_vals, x2_vals, 'k-', linewidth=2, label='Decision Boundary')
    else:
        mu1, sigma1 = regressor.mean[0], regressor.std[0]
        if regressor.theta[1] != 0:
            x1_const = mu1 - sigma1 * (regressor.theta[0] / regressor.theta[1])
            plt.axvline(x=x1_const, color='k', linewidth=2, label='Decision Boundary')
        else:
            print("Both theta[1] and theta[2] are zero; decision boundary cannot be determined.")

    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.title("Logistic Regression: Training Data and Decision Boundary")
    plt.legend()
    # plt.savefig('plots/Q3/plot.png')
    plt.show()




import numpy as np
import os

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.same_covariance=None
        self.w=None
        self.b=None
        self.mean=None
        self.std=None
        self.phi0=None
        self.phi1=None
        self.mu0=None
        self.mu1=None
        self.sigma0=None
        self.sigma1=None
        self.sigma=None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = np.array(X)
        y = np.array(y)
        self.same_covariance=assume_same_covariance
        N,F=X.shape

        self.mean = X.mean(axis=0)
        self.std = X.std(axis=0)
        X = (X - self.mean) / self.std
        X0 = X[y == 'Alaska']
        X1 = X[y == 'Canada']
        
        self.phi0,self.phi1 = X0.shape[0]/N,X1.shape[0]/N
        
        self.mu0,self.mu1 = X0.mean(axis=0), X1.mean(axis=0)
        
        # sigma0 = (X0-mu0).T @ (X0-mu0) / X0.shape[0]
        # sigma1 = (X1-mu1).T @ (X1-mu1) / X1.shape[0]
        self.sigma0 = np.cov(X0, rowvar=False, bias=True)
        self.sigma1 = np.cov(X1, rowvar=False, bias=True)
        self.sigma = (self.sigma0*X0.shape[0] + self.sigma1*X1.shape[0]) / N

        if assume_same_covariance:
            sigma_inv=np.linalg.inv(self.sigma)
            self.w = (self.mu0-self.mu1).T @ sigma_inv
            self.b = np.log(X0.shape[0] / X1.shape[0]) + 1/2 * ((self.mu1.T @ sigma_inv @ self.mu1)- (self.mu0.T @ sigma_inv @ self.mu0))
            return self.mu0,self.mu1,self.sigma
        else:
            return self.mu0,self.mu1,self.sigma0,self.sigma1


    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match99-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = np.array(X)
        N = X.shape[0]
        X = (X - self.mean) / self.std
</FONT>        predictions = []

        if self.same_covariance:
            for x in X:
                predictions.append('Alaska' if self.w @ x + self.b &gt;=0 else 'Canada')
        else:
            sigma0_inv = np.linalg.inv(self.sigma0)
            sigma1_inv = np.linalg.inv(self.sigma1)
            log_det0 = np.log(np.linalg.det(self.sigma0))
            log_det1 = np.log(np.linalg.det(self.sigma1))

            quad_term0 = 0.5 * self.mu0 @ sigma0_inv @ self.mu0
            quad_term1 = 0.5 * self.mu1 @ sigma1_inv @ self.mu1

            log_p0 = np.log(self.phi0) - 0.5 * log_det0
            log_p1 = np.log(self.phi1) - 0.5 * log_det1

            for x in X:
                logp0 = x @ sigma0_inv @ self.mu0 - quad_term0 + log_p0
                logp1 = x @ sigma1_inv @ self.mu1 - quad_term1 + log_p1
                predictions.append('Canada' if logp1 &gt;= logp0 else 'Alaska')

        return np.array(predictions)

if __name__=='__main__':
    X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
    y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'),dtype=str)

    gda = GaussianDiscriminantAnalysis()
    gda.fit(X, y, assume_same_covariance=True)
    y_pred=gda.predict(X)
    print(f'accuracy = {np.mean(y_pred==y)}')



import numpy as np
import os
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
    y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)
    
    gda = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda.fit(X, y, assume_same_covariance=True)
    normalized_graph=False
    
    print("Normalization parameters:")
    print("Estimated class means in normalized space:")
    print("mu0 (Alaska):", mu0)
    print("mu1 (Canada):", mu1)
    print("Pooled covariance matrix (in normalized space):")
    print(sigma)
    print()

    # linear separator
    A,B,C = gda.w[0], gda.w[1], gda.b

    if normalized_graph:
        X=(X-gda.mean)/gda.std
    else:
        A /= gda.std[0]
        B /= gda.std[1]
        C -= A*gda.mean[0] + B*gda.mean[1]

    alaska_indices = X[y == 'Alaska']
    canada_indices = X[y == 'Canada']
    plt.figure(figsize=(8, 6))
    plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
    plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')    
    

    if np.abs(B) &gt; 1e-10:
        x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)
        y_vals = -(A / B) * x_vals - (C / B)
        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')
    elif np.abs(A) &gt; 1e-10:
        x_val = -C / A
        plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')
    else:
        print("Invalid decision boundary: both A and B are zero.")

    plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
    plt.ylabel('x2 (Marine Growth Ring Diameter)')
    plt.title('GDA Training Data: Alaska vs. Canada')
    plt.legend()

    plt.savefig('plots/Q4/linear_separator_notnormal.png')
    # plt.show()



import numpy as np
import matplotlib.pyplot as plt
import os

X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)

alaska_indices = X[y == 'Alaska']
canada_indices = X[y == 'Canada']

plt.figure(figsize=(8, 6))
plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
<A NAME="6"></A><FONT color = #00FF00><A HREF="match99-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')

plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
plt.ylabel('x2 (Marine Growth Ring Diameter)')
plt.title('GDA Training Data: Alaska vs. Canada')
</FONT>plt.legend()

# plt.savefig('plots/Q4/datapoints.png')
plt.show()




import numpy as np
import os
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
    y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)
    
    gda = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda.fit(X, y, assume_same_covariance=True)
    normalized_graph = True
    
    print("Normalization parameters:")
    print("Estimated class means in normalized space:")
    print("mu0 (Alaska):", mu0)
    print("mu1 (Canada):", mu1)
    print("Pooled covariance matrix (in normalized space):")
    print(sigma)
    print()

    A, B, C = gda.w[0], gda.w[1], gda.b

    if normalized_graph:
        X_plot = (X - gda.mean) / gda.std
    else:
        A = A / gda.std[0]
        B = B / gda.std[1]
        C = C - ((gda.w[0] / gda.std[0]) * gda.mean[0] + (gda.w[1] / gda.std[1]) * gda.mean[1])
        X_plot = X

    alaska_indices = X_plot[y == 'Alaska']
    canada_indices = X_plot[y == 'Canada']
    plt.figure(figsize=(8, 6))
    plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
    plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')    
    
    if np.abs(B) &gt; 1e-10:
        x_vals = np.linspace(np.min(X_plot[:, 0]) - 0.5, np.max(X_plot[:, 0]) + 0.5, 200)
        y_vals = -(A / B) * x_vals - (C / B)
        plt.plot(x_vals, y_vals, 'k--', label='Linear Boundary')
    elif np.abs(A) &gt; 1e-10:
        x_val = -C / A
        plt.axvline(x=x_val, color='k', linestyle='--', label='Linear Boundary')
    else:
        print("Invalid decision boundary: both A and B are zero.")

    # --- Quadratic Boundary ---
    gda_quad = GaussianDiscriminantAnalysis()
    mu0_quad, mu1_quad, sigma0, sigma1 = gda_quad.fit(X, y, assume_same_covariance=False)
    
    if not normalized_graph:
        mu0_quad_orig = mu0_quad * gda_quad.std + gda_quad.mean
        mu1_quad_orig = mu1_quad * gda_quad.std + gda_quad.mean
        sigma0_orig = np.diag(gda_quad.std) @ sigma0 @ np.diag(gda_quad.std)
        sigma1_orig = np.diag(gda_quad.std) @ sigma1 @ np.diag(gda_quad.std)
    else:
        mu0_quad_orig = mu0_quad
        mu1_quad_orig = mu1_quad
        sigma0_orig = sigma0
        sigma1_orig = sigma1

    inv_sigma0 = np.linalg.inv(sigma0_orig)
    inv_sigma1 = np.linalg.inv(sigma1_orig)
    det_sigma0 = np.linalg.det(sigma0_orig)
    det_sigma1 = np.linalg.det(sigma1_orig)

    constant_term = np.log(det_sigma0 / det_sigma1) - 2 * np.log(gda_quad.phi0 / gda_quad.phi1)

<A NAME="0"></A><FONT color = #FF0000><A HREF="match99-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1_min, x1_max = np.min(X_plot[:, 0]) - 0.5, np.max(X_plot[:, 0]) + 0.5
    x2_min, x2_max = np.min(X_plot[:, 1]) - 0.5, np.max(X_plot[:, 1]) + 0.5
</FONT>    x1_grid, x2_grid = np.meshgrid(np.linspace(x1_min, x1_max, 300), np.linspace(x2_min, x2_max, 300))

    F = np.zeros_like(x1_grid)
    for i in range(x1_grid.shape[0]):
        for j in range(x1_grid.shape[1]):
            x_point = np.array([x1_grid[i, j], x2_grid[i, j]])
            diff0 = x_point - mu0_quad_orig
            diff1 = x_point - mu1_quad_orig
            F[i, j] = (diff0.T @ inv_sigma0 @ diff0) - (diff1.T @ inv_sigma1 @ diff1) + constant_term

    quad_contour = plt.contour(x1_grid, x2_grid, F, levels=[0], colors='green', linestyles='-', linewidths=2)
    quad_contour.collections[0].set_label('Quadratic Boundary')

    plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
    plt.ylabel('x2 (Marine Growth Ring Diameter)')
    plt.title('GDA Training Data: Alaska vs. Canada')
    plt.legend()
    # plt.show()
    plt.savefig('plots/Q4/quadratic.png')


</PRE>
</PRE>
</BODY>
</HTML>
