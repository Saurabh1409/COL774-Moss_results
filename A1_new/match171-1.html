<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_2EL0I.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_N8QMF.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd
import os

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None

        self.threshold = 1e-5
        self.max_iter = 1_000
        self.learning_rate = None
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m, n = X.shape
        
        X_aug = np.hstack((np.ones((m, 1)), X))

        y = y.reshape(-1, 1)
        
        theta = np.zeros((n + 1, 1))

        params_history = []
        prev_loss = float('inf')
        
        for _ in range(self.max_iter):
            prediction = X_aug @ theta

            loss = (1 / (2 * m)) * np.sum((prediction - y) ** 2)

            gradient = (1 / m) * (X_aug.T @ (prediction - y))
            theta -= learning_rate * gradient
            
            params_history.append(theta.copy().flatten())
            
            if abs(prev_loss - loss) &lt; self.threshold:
                break
            prev_loss = loss
        self.theta = theta
        return np.array(params_history)
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        
        X_aug = np.hstack((np.ones((m, 1)), X))

        return X_aug.dot(self.theta)
        pass
    
def compute_cost_grid(X, y, t0_bounds=(-20, 40), t1_bounds=(-10, 60), num=100):
    m = X.shape[0]
    t0_range = np.linspace(t0_bounds[0], t0_bounds[1], num)
    t1_range = np.linspace(t1_bounds[0], t1_bounds[1], num)
    T0, T1 = np.meshgrid(t0_range, t1_range)
    X_flat = X.flatten()[None, None, :]
    y_flat = y.flatten()[None, None, :]
    preds = T0[:, :, None] + T1[:, :, None] * X_flat
    J_vals = np.sum((preds - y_flat) ** 2, axis=2) / (2 * m)
    return T0, T1, J_vals


def a(X, y):
    regressor = LinearRegressor()
    params_history = regressor.fit(X, y, learning_rate=0.01)
    final_theta = regressor.theta.flatten()
    print("Learning Rate: 0.01")
    print("Stopping Criteria (change in J):", 1e-6)
    print("Final parameters (theta):", final_theta)
    print("Number of iterations:", len(params_history))
    return params_history, final_theta


def b(X, y):
    _, final_theta = a(X, y)
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.scatter(X, y, color='blue', s=20, edgecolors='black', linewidths=0.5, label='Data Points')
    x_vals = np.linspace(X.min(), X.max(), 100)
    y_vals = final_theta[0] + final_theta[1] * x_vals
    ax.plot(x_vals, y_vals, color='red', label='Fitted Line')
    ax.set(xlabel="X", ylabel="y", title="Linear Regression Fit")
    ax.legend()
    plt.show()


def c(X, y):
    m = X.shape[0]
    regressor = LinearRegressor()
    params_history = regressor.fit(X, y, learning_rate=0.01)
    T0, T1, J_vals = compute_cost_grid(X, y)
    
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.7)
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_zlabel('Cost J(theta)')
    ax.set_title('3D Mesh of Cost Function')
    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)
    
    plt.show()
    
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.7)
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_zlabel('Cost J(theta)')
    ax.set_title('3D Mesh of Cost Function with Gradient Descent Trajectory')
    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)
    
    for theta in params_history:
        theta0, theta1 = theta
        cost = np.sum(((theta0 + theta1 * X.flatten()) - y.flatten()) ** 2) / (2 * m)
        ax.scatter(theta0, theta1, cost, color='r', s=20)
        plt.pause(0.02)
    plt.show()


def d(X, y):
    m = X.shape[0]
    regressor = LinearRegressor()
    params_history = regressor.fit(X, y, learning_rate=0.1)
    T0, T1, J_vals = compute_cost_grid(X, y)
    
    fig, ax = plt.subplots(figsize=(8, 6))
    cp = ax.contourf(T0, T1, J_vals, levels=50, cmap='viridis')
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_title(f'Contour Plot of Cost Function')
    fig.colorbar(cp, ax=ax)
    plt.show()

    fig, ax = plt.subplots(figsize=(8, 6))
    cp = ax.contourf(T0, T1, J_vals, levels=50, cmap='viridis')
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_title('Contour Plot of Cost Function with Gradient Descent Trajectory')
    fig.colorbar(cp, ax=ax)
    
    theta0_list, theta1_list = [], []
    for theta in params_history:
        theta0, theta1 = theta
        theta0_list.append(theta0)
        theta1_list.append(theta1)
        ax.plot(theta0_list, theta1_list, 'ro-', markersize=1)
        plt.pause(0.2)
    plt.show()


def e(X, y):
    learning_rates = [0.1, 0.025, 0.001]
    m = X.shape[0]
    for lr in learning_rates:
        print(f"\nLearning Rate: {lr}")
        regressor = LinearRegressor()
        params_history = regressor.fit(X, y, learning_rate=lr)
        T0, T1, J_vals = compute_cost_grid(X, y)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        cp = ax.contourf(T0, T1, J_vals, levels=50, cmap='viridis')
        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_title(f'Contour Plot of Cost Function with Gradient Descent (lr = {lr})')
        fig.colorbar(cp, ax=ax)
        
        theta0_list, theta1_list = [], []
        for theta in params_history:
            theta0, theta1 = theta
            theta0_list.append(theta0)
            theta1_list.append(theta1)
            ax.plot(theta0_list, theta1_list, 'ro-', markersize=1)
            plt.pause(0.2)
        plt.show()


if __name__ == "__main__":
    try:
        script_dir = os.path.dirname(os.path.realpath(__file__))
        X_csv = os.path.join(script_dir, '../data/Q1', 'linearX.csv')
        y_csv = os.path.join(script_dir, '../data/Q1', 'linearY.csv')
        X = pd.read_csv(X_csv, header=None).values
        y = pd.read_csv(y_csv, header=None).values
    except Exception as ex:
        print("Can't open file!!", ex)
        exit(1)
    # a(X, y)
    # b(X, y)
    # c(X, y)
    # d(X, y)
    e(X, y)



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # needed for 3D plotting


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
<A NAME="0"></A><FONT color = #FF0000><A HREF="match171-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)
    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)
    X = np.column_stack((x1, x2))
</FONT>    
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match171-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)

    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
</FONT>    
    return X, y
    pass

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_size = 100
        self.threshold = 1e-6
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        max_iter = 1_000
        m, n = X.shape
        theta = np.zeros((n + 1, 1))
        
        X_aug = np.hstack((np.ones((m, 1)), X))
        y = np.reshape(y, (-1, 1))
        
        params_history = [theta.copy().flatten()]
        prev_epoch_loss = float('inf')
        
        for epoch in range(1, max_iter + 1):
            indices = np.random.permutation(m)
            X_shuffled = X_aug[indices]
            y_shuffled = y[indices]
            
            for i in range(0, m, self.batch_size):
                X_batch = X_shuffled[i:i + self.batch_size]
                y_batch = y_shuffled[i:i + self.batch_size]
                
                prediction = X_batch @ theta
                batch_size = X_batch.shape[0]
                gradient = (1 / batch_size) * (X_batch.T @ (prediction - y_batch))
                
                theta -= learning_rate * gradient
                
                params_history.append(theta.copy().flatten())
            
            epoch_predictions = X_aug @ theta
            epoch_avg_loss = np.sum((y - epoch_predictions) ** 2) / (2 * m)
            
            if abs(prev_epoch_loss - epoch_avg_loss) &lt; self.threshold:
                break
            prev_epoch_loss = epoch_avg_loss
            # print(f"epoch: {epoch} loss {epoch_avg_loss}")

        self.theta = theta
        return np.array(params_history)

        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X_aug = np.hstack((np.ones((m, 1)), X))
        return (X_aug @ self.theta).flatten()
        pass
        


TRAIN_DATA = None
TEST_DATA = None

def a():
    N = 1_000_000
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2])  
    noise_sigma = np.sqrt(2)
    
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
    
    indices = np.random.permutation(N)
    train_size = int(0.8 * N)
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]
    
    X_train = X[train_indices]
    y_train = y[train_indices]
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    global TRAIN_DATA, TEST_DATA
    TRAIN_DATA = (X_train, y_train)
    TEST_DATA = (X_test, y_test)
    
    return (X_train, y_train), (X_test, y_test)

def b():
    global TRAIN_DATA
    if TRAIN_DATA is None:
        (X_train, y_train), _ = a()
    else:
        X_train, y_train = TRAIN_DATA

    batch_sizes = [1, 80, 8000, 800000]
    lr_mapping = {
        1: 0.000001,
        80: 0.00005,
        8000: 0.0001,
        800000: 0.05
    }
    models = {}

    for r in batch_sizes:
        regressor = StochasticLinearRegressor()
        regressor.batch_size = r
        theta_history = regressor.fit(X_train, y_train, learning_rate=lr_mapping[r])
        final_theta = regressor.theta.flatten()
        models[r] = {"theta": final_theta, "history": theta_history}
        n_iters = theta_history.shape[0]
        print(f"  - Batch size {r:&gt;7}: Final θ = {final_theta}, Iterations = {n_iters}")
        
    return models

def c(sgd_models):

    global TRAIN_DATA
    if TRAIN_DATA is None:
        (X_train, y_train), _ = a()
    else:
        X_train, y_train = TRAIN_DATA
        
    m = X_train.shape[0]
    X_train_aug = np.hstack((np.ones((m, 1)), X_train))
    y_train_vec = y_train.reshape(-1, 1)
    closed_theta = np.linalg.inv(X_train_aug.T @ X_train_aug) @ (X_train_aug.T @ y_train_vec)
    closed_theta = closed_theta.flatten()
    print(f"\n[Part c] Closed-form solution θ: {closed_theta}")

    
    true_theta = np.array([3, 1, 2])
    print("\n[Part c] Comparison of parameters:")
    print("        True θ          Closed-form θ          SGD θ (by batch size)")
    for batch_size, model in sgd_models.items():
        sgd_theta = model["theta"]
        print(f"  Batch {batch_size:&gt;7}: {true_theta} vs {closed_theta} vs {sgd_theta}")
    
def d():
    global TRAIN_DATA, TEST_DATA
    if TRAIN_DATA is None or TEST_DATA is None:
        (X_train, y_train), (X_test, y_test) = a()
    else:
        X_train, y_train = TRAIN_DATA
        X_test, y_test = TEST_DATA

    regressor = StochasticLinearRegressor()
    regressor.fit(X_train, y_train, learning_rate=0.01)  
    
    y_train_pred = regressor.predict(X_train)
    train_mse = np.mean((y_train - y_train_pred) ** 2)
    
    y_test_pred = regressor.predict(X_test)
    test_mse = np.mean((y_test - y_test_pred) ** 2)
    
    print(f"\n      Training MSE:     {train_mse:.4f}")
    print(f"           Test MSE:       {test_mse:.4f}")
    
    return train_mse, test_mse

def e(sgd_models):
    
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    colors = {1: 'red', 80: 'green', 8000: 'blue', 800000: 'purple'}
    
    for batch_size, model in sgd_models.items():
        theta_history = model["history"]  

        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2],
                color=colors.get(batch_size, None), label=f"Batch size {batch_size}")

        ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2],
                   color=colors.get(batch_size, None), marker='o', s=50)
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2],
                   color=colors.get(batch_size, None), marker='*', s=100)
    
    ax.set_xlabel(r"$\theta_0$")
    ax.set_ylabel(r"$\theta_1$")
    ax.set_zlabel(r"$\theta_2$")
    ax.set_title("Trajectory of θ in 3D Parameter Space for Different Batch Sizes")
    ax.legend()
    plt.show()
if __name__ == "__main__":
    # a()         
    # sgd_models = b() 
    # c(sgd_models)   
    d()
    # e(sgd_models)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.sigmoid = lambda z: 1 / (1 + np.exp(-z))
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        n_samples, n_features = X.shape

        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0, ddof=0)
        X_norm = (X - self.mean) / self.std

        X_aug = np.hstack((np.ones((n_samples, 1)), X_norm))
        
        theta = np.zeros(n_features + 1)
        params_history = []
        
        max_iter = 100
        tol = 1e-6
        
        for _ in range(max_iter):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match171-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            z = X_aug @ theta
            h = self.sigmoid(z)
            
            gradient = X_aug.T @ (y - h)
            
            R = h * (1 - h)
</FONT>            Hessian = -X_aug.T @ (R[:, np.newaxis] * X_aug)
        
            try:
                delta = np.linalg.solve(-Hessian, gradient)
            except np.linalg.LinAlgError:
                delta = np.linalg.pinv(-Hessian) @ gradient
            
            theta_new = theta + learning_rate * delta
            params_history.append(theta_new.copy())
            
            if np.linalg.norm(theta_new - theta) &lt; tol:
                theta = theta_new
                break
            
            theta = theta_new
        
        self.theta = theta
        return np.array(params_history)
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = (X - self.mean) / self.std
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match171-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        n_samples = X_norm.shape[0]
        X_aug = np.hstack((np.ones((n_samples, 1)), X_norm))
        
        probs = self.sigmoid(X_aug @ self.theta)
</FONT>        y_pred = (probs &gt;= 0.5).astype(int)
        return y_pred
        pass
    
def a(X, y):
    model = LogisticRegressor()
    model.fit(X, y.ravel(), learning_rate=1.0)
    print("Resulting Coefficient:")
    print(model.theta)
    return model

def b(X, y):
    model = a(X, y)
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y.ravel() == 0, 0], X[y.ravel() == 0, 1], marker='o', label='Class 0')
    plt.scatter(X[y.ravel() == 1, 0], X[y.ravel() == 1, 1], marker='x', label='Class 1')
    theta = model.theta
    
    x1_vals = np.linspace(np.min(X[:, 0]) - 1, np.max(X[:, 0]) + 1, 200)
    if np.abs(theta[2]) &gt; 1e-8:
        x2_vals = (-theta[0] - theta[1] * x1_vals) / theta[2]
        plt.plot(x1_vals, x2_vals, color='red', label='Decision Boundary')
    
    plt.xlabel("x1 (normalized)")
    plt.ylabel("x2 (normalized)")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary (Normalized Data)")
    plt.show()


if __name__ == "__main__":
    try:
        script_dir = os.path.dirname(os.path.realpath(__file__))
        X_csv = os.path.join(script_dir, '../data/Q3', 'logisticX.csv')
        y_csv = os.path.join(script_dir, '../data/Q3', 'logisticY.csv')
        X = pd.read_csv(X_csv, header=None).values
        y = pd.read_csv(y_csv, header=None).values
    except Exception as ex:
        print("Can't open file!!", ex)
        exit(1)
    
    X_mean = np.mean(X, axis=0)
    X_std = np.std(X, axis=0, ddof=0)
    X_norm = (X - X_mean) / X_std
    
    b(X_norm, y)



# Imports - you can add any other permitted libraries
import numpy as np
import os
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    def __init__(self):
        
        self.mu0 = None
        self.mu1 = None
        
        self.sigma0 = None
        self.sigma1 = None
        self.sigma = None 
        
        self.phi0 = None
        self.phi1 = None
        
        self.assume_same_covariance = None
        
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.assume_same_covariance = assume_same_covariance
        n_samples, n_features = X.shape

        idx0 = (y == 0)
        idx1 = (y == 1)
        n0 = np.sum(idx0)
        n1 = np.sum(idx1)
        self.phi0 = n0 / n_samples
        self.phi1 = n1 / n_samples

<A NAME="1"></A><FONT color = #00FF00><A HREF="match171-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu0 = np.mean(X[idx0], axis=0)
        self.mu1 = np.mean(X[idx1], axis=0)
        
        if assume_same_covariance:
            sigma = np.zeros((n_features, n_features))
</FONT>            diff0 = X[idx0] - self.mu0  
            diff1 = X[idx1] - self.mu1  
            sigma += diff0.T @ diff0
<A NAME="2"></A><FONT color = #0000FF><A HREF="match171-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            sigma += diff1.T @ diff1
            sigma /= n_samples
            self.sigma = sigma
            return self.mu0, self.mu1, self.sigma
        else:
            diff0 = X[idx0] - self.mu0  
</FONT>            diff1 = X[idx1] - self.mu1  
            sigma0 = (diff0.T @ diff0) / n0
            sigma1 = (diff1.T @ diff1) / n1
            self.sigma0 = sigma0
            self.sigma1 = sigma1
            return self.mu0, self.mu1, self.sigma0, self.sigma1
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        n_samples = X.shape[0]
        
        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
            diff0 = X - self.mu0  
            diff1 = X - self.mu1

            term0 = np.sum((diff0 @ sigma_inv) * diff0, axis=1)
            term1 = np.sum((diff1 @ sigma_inv) * diff1, axis=1)

            g0 = np.log(self.phi0) - 0.5 * term0
            g1 = np.log(self.phi1) - 0.5 * term1
        else:
            sigma0_inv = np.linalg.inv(self.sigma0)
            sigma1_inv = np.linalg.inv(self.sigma1)

            eps = 1e-6
            det_sigma0 = np.linalg.det(self.sigma0)
            det_sigma1 = np.linalg.det(self.sigma1)
            if det_sigma0 &lt; eps:
                det_sigma0 = eps
            if det_sigma1 &lt; eps:
                det_sigma1 = eps

            diff0 = X - self.mu0
            diff1 = X - self.mu1
            term0 = np.sum((diff0 @ sigma0_inv) * diff0, axis=1)
            term1 = np.sum((diff1 @ sigma1_inv) * diff1, axis=1)

            g0 = np.log(self.phi0) - 0.5 * np.log(det_sigma0) - 0.5 * term0
            g1 = np.log(self.phi1) - 0.5 * np.log(det_sigma1) - 0.5 * term1
        
        y_pred = np.where(g0 &gt; g1, 0, 1)
        return y_pred
        pass
    

def a(X, y):
    model = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = model.fit(X, y, assume_same_covariance=True)
    
    print("----------GDA with Same Covariance----------")
    print("mu0 (class 0 - Alaska):", mu0)
    print("mu1 (class 1 - Canada):", mu1)
    print("Sigma:\n", sigma)
    

def b(X, y):
    fig, ax = plt.subplots(figsize=(8,6))

    class0 = X[y==0]
    class1 = X[y==1]
    
    ax.scatter(class0[:, 0], class0[:, 1], marker='o', color='blue', label='Alaska')
    ax.scatter(class1[:, 0], class1[:, 1], marker='x', color='red', label='Canada')
    
    ax.set_xlabel('Fresh Water')
    ax.set_ylabel('Marine Water')
    ax.set_title('Training Data (Normalized)')
    ax.legend()

def c(X, y):
    model = GaussianDiscriminantAnalysis()
    model.fit(X, y, assume_same_covariance=True)

    sigma_inv = np.linalg.inv(model.sigma)
    w = sigma_inv @ (model.mu1 - model.mu0)
    b_val = -0.5 * (model.mu1.T @ sigma_inv @ model.mu1 - model.mu0.T @ sigma_inv @ model.mu0) \
            + np.log(model.phi1/model.phi0)

    if np.abs(w[1]) &gt; 1e-6:
        slope = -w[0]/w[1]
        intercept = -b_val/w[1]
        
        ax = plt.gca()
        x_vals = np.linspace(np.min(X[:,0]) - 1, np.max(X[:,0]) + 1, 200)
        y_vals = slope * x_vals + intercept
        ax.plot(x_vals, y_vals, 'g-', linewidth=2, label='Linear Decision Boundary')
        ax.legend()
    else:
        ax = plt.gca()
        x_const = -b_val/w[0]
        ax.axvline(x=x_const, color='g', linestyle='-', linewidth=2, label='Linear Decision Boundary')
        ax.legend()

def d(X, y):
    model = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma0, sigma1 = model.fit(X, y, assume_same_covariance=False)
    
    print("\n----------GDA with Different Covariances----------")
    print("mu0 (class 0 - Alaska):", mu0)
    print("mu1 (class 1 - Canada):", mu1)
    print("Sigma0:\n", sigma0)
    print("Sigma1:\n", sigma1)
    

def e(X, y):
    model = GaussianDiscriminantAnalysis()
    model.fit(X, y, assume_same_covariance=False)
    
    x_min, x_max = np.min(X[:,0]) - 1, np.max(X[:,0]) + 1
    y_min, y_max = np.min(X[:,1]) - 1, np.max(X[:,1]) + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    sigma0_inv = np.linalg.inv(model.sigma0)
    sigma1_inv = np.linalg.inv(model.sigma1)
    
    eps = 1e-6
    det_sigma0 = np.linalg.det(model.sigma0)
    det_sigma1 = np.linalg.det(model.sigma1)
    if det_sigma0 &lt; eps:
        det_sigma0 = eps
    if det_sigma1 &lt; eps:
        det_sigma1 = eps
    
    diff0 = grid_points - model.mu0
    diff1 = grid_points - model.mu1
    term0 = np.sum((diff0 @ sigma0_inv) * diff0, axis=1)
    term1 = np.sum((diff1 @ sigma1_inv) * diff1, axis=1)
    
    g0 = np.log(model.phi0) - 0.5 * np.log(det_sigma0) - 0.5 * term0
    g1 = np.log(model.phi1) - 0.5 * np.log(det_sigma1) - 0.5 * term1
    
    diff_g = (g1 - g0).reshape(xx.shape)
    
    ax = plt.gca()
    cs = ax.contour(xx, yy, diff_g, levels=[0], colors='magenta', linestyles='--', linewidths=2)
    try:
        cs.collections[0].set_label('Quadratic Decision Boundary')
    except Exception:
        pass
    ax.legend()

if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.realpath(__file__))

    q4x_dat = os.path.join(script_dir, '../data/Q4', 'q4x.dat')
    q4y_dat = os.path.join(script_dir, '../data/Q4', 'q4y.dat')
    
    try:
        X = np.loadtxt(q4x_dat)
    except Exception as e:
        print("Error loading q4x.dat:", e)
        exit(1)
    
    try:
        y_text = np.loadtxt(q4y_dat, dtype=str)
    except Exception as e:
        print("Error loading q4y.dat:", e)
        exit(1)
    
    y = np.array([0 if label.strip().lower() == 'alaska' else 1 for label in y_text])
    
    # Normalize the data 
    X_mean = np.mean(X, axis=0)
    X_std = np.std(X, axis=0)
    X_std[X_std == 0] = 1
    X = (X - X_mean) / X_std

    # a(X, y)
    
    b(X, y)
    
    # c(X, y)
    
    d(X, y)
    
    e(X, y)
    
    plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
