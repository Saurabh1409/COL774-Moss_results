<HTML>
<HEAD>
<TITLE>./A1_processed_new_hash_2/combined_8OSK3.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new_hash_2/combined_8OSK3.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

tolerance = 1e-6

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        n_samples, n_features = X.shape
        X = np.c_[np.ones(n_samples), X]  # Add bias term (column of ones)
        self.theta = np.zeros(n_features + 1)  # Initialize parameters to zero

        param_history = []  # Store parameters after each iteration

        while (True):
            for i in range(n_samples):
                x_i = X[i]  # Single sample (including bias term)
                y_i = y[i]  # Corresponding target value
                prediction = np.dot(self.theta, x_i)  # Compute prediction
                
                # Update rule for each weight θj
                self.theta += learning_rate * (y_i - prediction) * x_i
            
            param_history.append(self.theta.copy())  # Store parameters

            if len(param_history) &gt; 1 and np.linalg.norm(param_history[-2] - self.theta)/learning_rate &lt; tolerance:
                break

        
        return np.array(param_history)
    
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        X = np.c_[np.ones(n_samples), X]  # Add bias term
        return np.dot(X, self.theta)







import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from linear_regression import LinearRegressor

def plot_regression_line(X, y, param_history):
    theta = param_history[-1]
    plt.scatter(X, y, color='blue', s=5, label="Data points")
    x_range = np.linspace(X.min(), X.max(), 100)
    y_pred = theta[0] + theta[1] * x_range
    plt.plot(x_range, y_pred, color='red', label="Learned hypothesis")
    plt.xlabel("X")
    plt.ylabel("y")
    plt.legend()
    plt.title("Regression Line")
    plt.show()

def plot_error_surface(X, y, param_history):
    """Plots a 3D surface of the cost function and the gradient descent path."""
    theta0_vals = np.linspace(-10, 20, 500)
    theta1_vals = np.linspace(15, 45, 500)
    J_vals = np.zeros((len(theta1_vals), len(theta0_vals)))  

    # Compute J(theta) over a grid of theta0, theta1 values
    for i, theta1 in enumerate(theta1_vals): 
        for j, theta0 in enumerate(theta0_vals):                
            theta = np.array([theta0, theta1] + [0] * (X.shape[1] - 1))  # Extend for higher dims
            predictions = np.column_stack((np.ones(X.shape[0]), X)) @ theta
            J_vals[i, j] = np.mean((y - predictions) ** 2)

    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta0_grid, theta1_grid, J_vals, cmap="viridis", alpha=0.7)

    # Compute cost for each iteration in param_history
    param_history = np.array(param_history)
    cost_values = np.array([
        np.mean((y - (np.column_stack((np.ones(X.shape[0]), X)) @ theta)) ** 2)
        for theta in param_history
    ])

    # Initialize the scatter plot
    scatter = ax.scatter([], [], [], color='red', marker='o')

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Cost Function J(θ)")
    plt.title("Error Surface and Gradient Descent Path")

    # Animation function
    def update(frame):
        scatter._offsets3d = (
            param_history[:frame, 0],  # Theta0 values up to current frame
            param_history[:frame, 1],  # Theta1 values up to current frame
            cost_values[:frame]        # Cost values up to current frame
        )

    ani = animation.FuncAnimation(fig, update, frames=len(param_history), interval=200)  # 0.2 sec intervals
    plt.show()

def plot_contour(X, y, param_history):
    theta0_vals = np.linspace(0, 40, 500)
    theta1_vals = np.linspace(0, 40, 500)
    J_vals = np.zeros((len(theta1_vals), len(theta0_vals))) 

    for i, theta1 in enumerate(theta1_vals): 
        for j, theta0 in enumerate(theta0_vals):
            theta = np.array([theta0, theta1])
            predictions = np.column_stack((np.ones(X.shape[0]), X)) @ theta
            J_vals[i, j] = np.mean((y - predictions) ** 2)

    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    fig, ax = plt.subplots()
    contour = ax.contour(theta0_grid, theta1_grid, J_vals, levels=np.logspace(-2, 3, 20), cmap="viridis")

    param_history = np.array(param_history)
    line, = ax.plot([], [], 'ro-', markersize=5)

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_title("Cost Function Contour and Gradient Descent Path")

    # Animation function
    def update(frame):
        line.set_data(param_history[:frame, 0], param_history[:frame, 1])

    ani = animation.FuncAnimation(fig, update, frames=len(param_history), interval=200)  # 0.2 sec intervals
    plt.show()


# Load X and ensure it is 2D
X = np.loadtxt("data\Q1\linearX.csv", delimiter=",")
if X.ndim == 1:
    X = X.reshape(-1, 1)  # Convert (n_samples,) to (n_samples, 1)

# Load y and ensure it is 1D
y = np.loadtxt("data\Q1\linearY.csv", delimiter=",").flatten()

# Initialize and train the model
model = LinearRegressor()
param_history = model.fit(X, y, learning_rate=0.001)

# Print final parameters
print("Final Parameters:", param_history[-1])
print("No of iterations:", len(param_history))

# Make predictions on training data
y_pred = model.predict(X)

# Mean relative error
mre = np.mean(np.abs((y - y_pred) / y)) * 100

print(f"Mean Relative Error: {mre}%")

# plot_regression_line(X, y, param_history)
# plot_error_surface(X, y, param_history)
plot_contour(X, y, param_history)





import numpy as np
from sampling_sgd import StochasticLinearRegressor


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    # Generate normally distributed input features
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))

    # Generate noise
<A NAME="0"></A><FONT color = #FF0000><A HREF="match106-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute target values: y = θ0 + θ1 * x1 + θ2 * x2 + ϵ
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise

    return X, y


def closed_form_solution(X, y):
    # Computes theta using the closed-form solution: theta = (X^T X)^(-1) X^T y

    X = np.column_stack((np.ones(X.shape[0]), X))  # Add bias term (column of ones)
</FONT>    theta = np.linalg.inv(X.T @ X) @ X.T @ y
    return theta


# Set parameters
N = 1000000  # Number of samples
theta_true = np.array([3, 1, 2])  # True parameters (θ0, θ1, θ2)
input_mean = np.array([3, -1])  # Mean for x1, x2
input_sigma = np.array([4, 4])  # Standard deviation for x1, x2
noise_sigma = 2  # Noise standard deviation

# Generate dataset
X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train the model
sgd_regressor = StochasticLinearRegressor()
param_history = sgd_regressor.fit(X[:800000], y[:800000], learning_rate=0.001)

# Print final learned parameters
print("Final Parameters:", sgd_regressor.theta)

# Predict on training data
y_pred_train = sgd_regressor.predict(X[:800000])
mse = np.mean((y[:800000] - y_pred_train) ** 2) 
print(f"Mean Squared Error (Training): {mse}")

# Predict on test data
y_pred_test = sgd_regressor.predict(X[800000:])
mse = np.mean((y[800000:] - y_pred_test) ** 2) 
print(f"Mean Squared Error (Test): {mse}")


# Theta (closed-form solution): [3.00005329 1.00111048 2.00072717]
theta_closed_form = closed_form_solution(X[:800000], y[:800000])
print("Theta (closed-form solution):", theta_closed_form)




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# Model parameters
batch_size = 800000      # 1, 80, 8000, 800000
num_epochs = 5000
tolerance = 1e-6

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.tolerance = tolerance
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        n_samples, n_features = X.shape
        X = np.column_stack((np.ones(n_samples), X))  # Add bias term
        self.theta = np.zeros(n_features + 1)  # Initialize parameters

        param_history = []  # To store parameter updates
        prev_loss = float("inf")  # Initialize previous loss as infinity
        
        converged = False
        for epoch in range(self.num_epochs): 
            indices = np.arange(n_samples)
            np.random.shuffle(indices)  # Shuffle the data

            for i in range(0, n_samples, self.batch_size):
                # print(i)
                batch_indices = indices[i:i + self.batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                predictions = X_batch @ self.theta
                self.theta += learning_rate * (X_batch.T @ (y_batch - predictions)) / self.batch_size
                param_history.append(self.theta.copy())

                # Compute current loss 
                loss = np.mean((y - (X @ self.theta)) ** 2) / 2  

                # Check stopping condition
                if abs(prev_loss - loss) &lt; self.tolerance:
                    print("SGD converged!")
                    converged = True
                    break
                prev_loss = loss

            if converged:
                print("Epoch:",epoch)
                break

        return param_history
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.column_stack((np.ones(X.shape[0]), X))  # Add bias term
        return X @ self.theta  # Compute predictions






# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# Hyperparameters
num_epochs = 10
tolerance = 1e-6

# I don't get why we're normalizing, and also should i min-max normalize or mean-std??

def normalize(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized_data = (data - mean) / std
    return normalized_data

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = normalize(X)
        n_samples, n_features = X.shape
        X = np.hstack((np.ones((n_samples, 1)), X))  # Add bias term
        self.theta = np.zeros(n_features + 1)  # Initialize parameters

        param_history = np.zeros((num_epochs, n_features + 1))  # Store parameter updates

        for i in range(num_epochs):
            z = X @ self.theta
            p = sigmoid(z)  # Predicted probabilities

            gradient = X.T @ (p - y)  # Gradient of log-likelihood
            H = X.T @ np.diag(p * (1 - p)) @ X  # Hessian matrix

            delta_theta = np.linalg.inv(H) @ gradient  # Newton-Raphson update
            self.theta -= delta_theta  # Update parameters

            param_history[i] = self.theta.copy()

            if np.linalg.norm(delta_theta) &lt; tolerance:  # Convergence check
                param_history = param_history[:i+1]
                print("Newton's Method converged!")
                break

        return param_history

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match106-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = normalize(X)
        n_samples = X.shape[0]
        X = np.hstack((np.ones((n_samples, 1)), X))  # Add bias term
</FONT>        return (sigmoid(X @ self.theta) &gt;= 0.5).astype(int)  # Classify based on threshold







import numpy as np
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

# Load data
X = np.loadtxt("data\Q3\logisticX.csv", delimiter=",")
y = np.loadtxt("data\Q3\logisticY.csv", delimiter=",").reshape(-1)  # Ensure y is a 1D array

train_X = X[:80]
train_y = y[:80]
test_X = X[80:]
test_y = y[80:]

# Train and predict
model = LogisticRegressor()
param_history = model.fit(train_X, train_y)
train_y_pred = model.predict(train_X)
test_y_pred = model.predict(test_X)

# Compute accuracy
train_accuracy = np.mean(train_y_pred == train_y) * 100  
test_accuracy = np.mean(test_y_pred == test_y) * 100  

# Output results
print("No of iterations:", param_history.shape[0])
print("Parameter list:", param_history)
print("Final Parameters:", model.theta)
print(f"Train Accuracy: {train_accuracy:.2f}%")
print(f"Test Accuracy: {test_accuracy:.2f}%")

# Plot training data points
X = train_X
y = train_y

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label="Label 0", edgecolors='k', facecolors='none')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label="Label 1")

# Plot decision boundary
<A NAME="2"></A><FONT color = #0000FF><A HREF="match106-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_vals = -(model.theta[0] + model.theta[1] * x1_vals) / model.theta[2]

plt.plot(x1_vals, x2_vals, 'r-', label="Decision Boundary")
</FONT>plt.xlabel("x1 (Normalized)")
plt.ylabel("x2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.grid()
plt.show()



# Imports - you can add any other permitted libraries
<A NAME="5"></A><FONT color = #FF0000><A HREF="match106-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# Normalize input data by centering and scaling
def normalize(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized_data = (data - mean) / std
</FONT>    return normalized_data
    
class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.assume_same_covariance = assume_same_covariance

        # Normalize X
        X = normalize(X)

        # Compute class priors
        self.phi = np.mean(y)

        # Compute class means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)

        # Compute covariance matrices

        m, n = X.shape
        
        if assume_same_covariance:
            # Initialize n x n matrix
            sigma = np.zeros((n, n))
            # For each sample, add covariance term
            for i in range(m):
                x_i = X[i].reshape(-1, 1)  # Convert to column vector
                mu_y = self.mu_1 if y[i] == 1 else self.mu_0
                sigma += (x_i - mu_y.reshape(-1, 1)) @ (x_i - mu_y.reshape(-1, 1)).T

            self.sigma = sigma / m
            return self.mu_0, self.mu_1, self.sigma
        
        else:
            sigma_0 = np.zeros((n, n))
            sigma_1 = np.zeros((n, n))
            n_0 = np.sum(y == 0)
            n_1 = np.sum(y == 1)

            for i in range(m):
                x_i = X[i].reshape(-1, 1)
                if y[i] == 0:
                    sigma_0 += (x_i - self.mu_0.reshape(-1, 1)) @ (x_i - self.mu_0.reshape(-1, 1)).T
                else:
                    sigma_1 += (x_i - self.mu_1.reshape(-1, 1)) @ (x_i - self.mu_1.reshape(-1, 1)).T

            self.sigma_0 = sigma_0 / n_0
            self.sigma_1 = sigma_1 / n_1
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize X 
        X = normalize(X)
        
        m, _ = X.shape
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match106-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred = np.zeros(m)

        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
            
            for i in range(m):
                x_i = X[i].reshape(-1, 1)
</FONT>                
                # Compute log probability density of p(x|y).p(y) for the cases (ignoring constant terms)
                # if y = 0
                log_p0 = -0.5 * (x_i - self.mu_0.reshape(-1, 1)).T @ sigma_inv @ (x_i - self.mu_0.reshape(-1, 1)) 
                + np.log(1 - self.phi)
                # if y = 1
                log_p1 = -0.5 * (x_i - self.mu_1.reshape(-1, 1)).T @ sigma_inv @ (x_i - self.mu_1.reshape(-1, 1)) 
<A NAME="1"></A><FONT color = #00FF00><A HREF="match106-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

                + np.log(self.phi)
                
                y_pred[i] = 1 if log_p1 &gt; log_p0 else 0
        
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            det_sigma_0 = np.linalg.det(self.sigma_0)
</FONT>            det_sigma_1 = np.linalg.det(self.sigma_1)

            for i in range(m):
                x_i = X[i].reshape(-1, 1)
                
                # Similar to above case, but compute log probability densities with different covariances
                # Since covariances are different we cannot ignore det(sigma)
                log_p0 = -0.5 * np.log(det_sigma_0) 
                - 0.5 * (x_i - self.mu_0.reshape(-1, 1)).T @ sigma_0_inv @ (x_i - self.mu_0.reshape(-1, 1)) 
                + np.log(1 - self.phi)
                log_p1 = -0.5 * np.log(det_sigma_1) 
                - 0.5 * (x_i - self.mu_1.reshape(-1, 1)).T @ sigma_1_inv @ (x_i - self.mu_1.reshape(-1, 1)) 
                + np.log(self.phi)
                
                y_pred[i] = 1 if log_p1 &gt; log_p0 else 0
        
        return y_pred




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Plot data points X and y
def plot_data(X, y, title):
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title(title)
    plt.legend()

# Decision boundary is given by probability density of y = 0.5
# Or p(y=1|x) = p(y=0|x) =&gt; p(x|y=1)p(y=1) = p(x|y=0)p(y=0)
# This equation has been solved in the report to get a linear equation in x
# w x^T + b = 0, w and b are as calculated below
def plot_linear_boundary(mu_0, mu_1, sigma, X, y):
    inv_sigma = np.linalg.inv(sigma)
    # slope 
    w = np.dot(inv_sigma, mu_1 - mu_0)
    # intercept
    b = -0.5 * (mu_1.T @ inv_sigma @ mu_1 - mu_0.T @ inv_sigma @ mu_0)
    # log ratio of y=1 to y=0, added to intercept
    phi = np.mean(y)
    b += np.log(phi / (1-phi))
    x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
    x2_vals = (-w[0] * x1_vals - b) / w[1]
    plt.plot(x1_vals, x2_vals, 'r-', label='Linear Decision Boundary')


# For the case in which covariances are different,
# The equation p(x|y=1)p(y=1) = p(x|y=0)p(y=0) has been solved in the report to get a quadratic in x
# x Q x^T + x L + C = 0  
# Compute the function values for each sample
def quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y):
    inv_sigma_0 = np.linalg.inv(sigma_0)
    inv_sigma_1 = np.linalg.inv(sigma_1)
    
    Q = inv_sigma_0 - inv_sigma_1
    L = 2 * (mu_1.T @ inv_sigma_1 - mu_0.T @ inv_sigma_0)
    phi = np.mean(y)
    C = (mu_0.T @ inv_sigma_0 @ mu_0 - mu_1.T @ inv_sigma_1 @ mu_1) 
    + np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
    + 2 * np.log(phi / (1-phi))
    # Compute quadratic function for each input point (x1, x2)
    function_values = np.einsum('ij,ji-&gt;i', X @ Q, X.T) + X @ L + C
    return function_values

# Takes values computed above and plots them
def plot_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y):
    x1_vals, x2_vals = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
                                    np.linspace(X[:, 1].min(), X[:, 1].max(), 100))
    
    # Flatten the meshgrid for easier evaluation
    grid_points = np.c_[x1_vals.ravel(), x2_vals.ravel()]

    # Get predictions for the grid points
    preds = quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, grid_points, y)
    preds = preds.reshape(x1_vals.shape)
    plt.contour(x1_vals, x2_vals, preds, levels=[0], colors='g', label='Quadratic Decision Boundary')


X = np.loadtxt("data\Q4\q4x.dat")
y = np.loadtxt("data\Q4\q4y.dat", dtype=str)
y = np.where(y == 'Canada', 1, 0)  # Convert labels to binary (0: Alaska, 1: Canada)

gda = GaussianDiscriminantAnalysis()

# Fit model assuming same covariance
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
y_pred = gda.predict(X)
accuracy = np.mean(y_pred == y) * 100
print('--- SAME COVARIANCE ---')
print(f'Accuracy: {accuracy:.2f}%')
print(f'Mu_0:', mu_0)
print(f'Mu_1:', mu_1)
print(f'Sigma:', sigma)

# Fit model assuming different covariances
mu_0_diff, mu_1_diff, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)
y_pred = gda.predict(X)
accuracy = np.mean(y_pred == y) * 100
print('--- DIFFERENT COVARIANCE ---')
print(f'Accuracy: {accuracy:.2f}%')
print(f'Mu_0:', mu_0_diff)
print(f'Mu_1:', mu_1_diff)
print(f'Sigma_0:', sigma_0)
print(f'Sigma_1:', sigma_1)

# Plot data points, linear decision boundary and quadratic decision boundary
plt.figure(figsize=(8, 6))
plot_data(X, y, 'GDA with Linear and Quadratic Decision Boundaries')
plot_linear_boundary(mu_0, mu_1, sigma, X, y)
plot_quadratic_boundary(mu_0_diff, mu_1_diff, sigma_0, sigma_1, X, y)
plt.legend()
plt.show()

</PRE>
</PRE>
</BODY>
</HTML>
