<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_KR4W0.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_KR4W0.py<p><PRE>


import time
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        if learning_rate &lt;= 0:
            raise ValueError("Learning rate must be positive - check your input!")
        features_with_bias = self._add_bias_term(X)
        self.theta = np.zeros(features_with_bias.shape[1])
        previous_loss = float('inf')
        param_evolution = []
        convergence_threshold = 1e-8
        max_training_steps = 10000
        
        for step in range(max_training_steps):
            predictions = self._forward_pass(features_with_bias)
            param_updates = self._calculate_updates(features_with_bias, y, predictions)
            self.theta -= learning_rate * param_updates
            
            if step % 10 == 0:
                param_evolution.append(self.theta.copy())
            
            current_loss = self._calculate_loss(features_with_bias, y)
            if abs(previous_loss - current_loss) &lt; convergence_threshold:
                if not param_evolution or not np.array_equal(param_evolution[-1], self.theta):
                    param_evolution.append(self.theta.copy())
                break
            previous_loss = current_loss
            
        return np.array(param_evolution)
    
    def _add_bias_term(self, X):
        return np.c_[np.ones((X.shape[0], 1)), X]

    def predict(self, X):
        return self._forward_pass(self._add_bias_term(X))
    
    
    def _calculate_updates(self, X, y, predictions):
        m = X.shape[0]
        return (1/m) * X.T.dot(predictions - y)
    
    def _forward_pass(self, X):
        return np.dot(X, self.theta)
    
    def _calculate_loss(self, X, y):
        m = X.shape[0]
        predictions = self._forward_pass(X)
        return (1/(2*m)) * np.sum((predictions - y)**2)

def compute_error_landscape(X, y, param1_range, param2_range):
    param1_grid, param2_grid = np.meshgrid(param1_range, param2_range)
    X_bias = np.c_[np.ones((X.shape[0], 1)), X]
    error_surface = np.zeros(param1_grid.shape)
    
    for i in range(len(param1_range)):
        for j in range(len(param2_range)):
            current_params = np.array([param1_grid[i,j], param2_grid[i,j]])
            predictions = np.dot(X_bias, current_params)
            error_surface[i,j] = (1/(2*X.shape[0])) * np.sum((predictions - y)**2)
            
    return param1_grid, param2_grid, error_surface, X_bias

def visualize_wine_regression(X, y, model):
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color='yellow', alpha=0.5, label='Wine Samples')
    
    X_regression = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match179-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y_regression = model.predict(X_regression)
    
    plt.plot(X_regression, y_regression, color='Navy', label='Density Predictor')
    plt.xlabel('Wine Acidity ---&gt;')
    plt.ylabel('Wine Density ---&gt;')
    plt.title('Linear Regn Plot: Wine Density vs Acidity Relationship')
    plt.legend()
    plt.grid(True, alpha=0.3)
</FONT>    plt.show()

def visualize_error_contours(X, y, param_history, landscape_data=None):
    plt.ion()
    
    if landscape_data is None:
        param2_range = np.linspace(-10, 70, 100)
        param1_range = np.linspace(-10, 20, 100)
        landscape_data = compute_error_landscape(X, y, param1_range, param2_range)
    
    param1_grid, param2_grid, error_surface, X_bias = landscape_data
    
    fig = plt.figure(figsize=(10, 8))
    plt.contour(param1_grid, param2_grid, error_surface, levels=20, cmap='viridis')
    plt.colorbar(label='Error Value')
    
    path_line = None
    try:
        for step in range(len(param_history)):
            if not plt.fignum_exists(fig.number):
                break
            if path_line:
                path_line[0].remove()
            
            path_line = plt.plot(param_history[:step+1, 0], param_history[:step+1, 1], 
                               'r.-', linewidth=2, markersize=5)
            
            plt.xlabel('Bias (θ₀)')
            plt.ylabel('Weight (θ₁)')
            plt.title(f'Error Landscape Contours\nStep {step*10}')
            plt.draw()
            plt.pause(0.2)
    except:
        pass
    finally:
        plt.ioff()
        plt.close(fig)

def visualize_error_surface_3d(X, y, param_history, landscape_data=None):
    plt.ion()
    
    if landscape_data is None:
        param2_range = np.linspace(-10, 70, 100)
        param1_range = np.linspace(-10, 20, 100)
        landscape_data = compute_error_landscape(X, y, param1_range, param2_range)
    
    param1_grid, param2_grid, error_surface, X_bias = landscape_data
    
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    surface = ax.plot_surface(param1_grid, param2_grid, error_surface, 
                            cmap='viridis', alpha=0.8)
    plt.colorbar(surface)
    ax.set_ylabel('Weight (θ₁)')
    ax.set_xlabel('Bias (θ₀)')
    ax.set_zlabel('Error Value')
    
    path_line = None
    try:
        for step in range(len(param_history)):
            if not plt.fignum_exists(fig.number):
                break
            if path_line:
                path_line.remove()
            
            errors = [compute_step_error(X_bias, y, theta) for theta in param_history[:step+1]]
            path_line, = ax.plot(param_history[:step+1, 0], param_history[:step+1, 1], errors,
                               color='red', linewidth=2, marker='o', markersize=5)
            
            ax.set_title(f'Error Landscape Evolution\nStep {step*10}')
            fig.canvas.draw()
            fig.canvas.flush_events()
            plt.pause(0.2)
    except:
        pass
    finally:
        plt.ioff()
        plt.close(fig)


def compute_step_error(X, y, theta):
    predictions = np.dot(X, theta)
    return (1/(2*X.shape[0])) * np.sum((predictions - y)**2)

def analyze_learning_rates(X, y, learning_rates=[0.001, 0.025, 0.1]):
    param1_range = np.linspace(-10, 20, 100)
    param2_range = np.linspace(-10, 70, 100)
    landscape_data = compute_error_landscape(X, y, param1_range, param2_range)
    
    for rate in learning_rates:
        model = LinearRegressor()
        param_history = model.fit(X, y, learning_rate=rate)
        
        print(f"\nModel Parameters (η={rate}):")
        print(f"Bias (θ₀): {model.theta[0]:.4f}")
        print(f"Weight (θ₁): {model.theta[1]:.4f}")
        
        plt.ion()
        fig = plt.figure(figsize=(10, 8))
        param1_grid, param2_grid, error_surface, _ = landscape_data
        plt.contour(param1_grid, param2_grid, error_surface, levels=20, cmap='viridis')
        plt.colorbar(label='Error Value')
        
        path_line = None
        try:
            for step in range(len(param_history)):
                if not plt.fignum_exists(fig.number):
                    break
                if path_line:
                    path_line[0].remove()
                
                path_line = plt.plot(param_history[:step+1, 0], param_history[:step+1, 1], 
                                   'r.-', linewidth=2, markersize=5)
                
                plt.xlabel('Bias (θ₀)')
                plt.ylabel('Weight (θ₁)')
                plt.title(f'Learning Rate Analysis (η={rate})\nStep {step*10}')
                plt.draw()
                plt.pause(0.2)
        except:
            pass
        finally:
            plt.ioff()
            plt.close(fig)
            plt.pause(0.5)

def _print_iteration_info(iteration, error, params):
    """Personal style progress printing"""
    print(f"[Training Step {iteration:4d}] "
          f"Error: {error:.6f} | "
          f"Parameters: [{params[0]:.4f}, {params[1]:.4f}]")

wine_targets = np.loadtxt('../data/Q1/linearY.csv')
wine_features = np.loadtxt('../data/Q1/linearX.csv').reshape(-1, 1)
wine_model = LinearRegressor()
param_history = wine_model.fit(wine_features, wine_targets)
# final parameters
print(f"\nFinal Wine Density Predictor Parameters:")
print(f"Weight (θ₁): {wine_model.theta[1]:.4f}")
print(f"Bias (θ₀): {wine_model.theta[0]:.4f}")


# plot final
landscape_data = compute_error_landscape(wine_features, wine_targets, 
                                      np.linspace(-10, 20, 100), 
                                      np.linspace(-10, 70, 100))

visualize_wine_regression(wine_features, wine_targets, wine_model)
visualize_error_surface_3d(wine_features, wine_targets, param_history, landscape_data)
visualize_error_contours(wine_features, wine_targets, param_history, landscape_data)
analyze_learning_rates(wine_features, wine_targets)



from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    if N &lt;= 0:
        raise ValueError("Sample size must be positive")
    if len(input_mean) != len(input_sigma):
        raise ValueError("Input mean and sigma dimensions must match")
    input_data = np.random.normal(input_mean, input_sigma, size=(N, 2))
    design_matrix = np.column_stack([np.ones(N), input_data])
    random_noise = np.random.normal(0, noise_sigma, size=N)
    targets = np.dot(design_matrix, theta) + random_noise
    
    return input_data, targets

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
<A NAME="0"></A><FONT color = #FF0000><A HREF="match179-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.01, batch_size=1, max_epochs=100, tol=1e-6):
        n_samples, n_features = X.shape
        X_augmented = np.column_stack([np.ones(n_samples), X])
        self.theta = np.zeros(n_features + 1)
</FONT>        phst = [self.theta.copy()]
        n_batches = n_samples // batch_size
        
        for epoch in range(max_epochs):
            old_theta = self.theta.copy()
            rand_idx = np.random.permutation(n_samples)
            total_loss = 0
            
            # diff batches
            btc = min(n_batches, 1000) if batch_size &lt; 100 else n_batches
            
            for b in range(btc):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match179-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                batch_idx = rand_idx[b * batch_size : (b + 1) * batch_size]
                X_batch = X_augmented[batch_idx]
                y_batch = y[batch_idx]
                predictions = np.dot(X_batch, self.theta)
                batch_loss = np.mean((y_batch - predictions) ** 2) / 2
</FONT>
                grads = -np.dot(X_batch.T, (y_batch - predictions)) / batch_size
                grad_norm = np.linalg.norm(grads)
                if grad_norm &gt; 1.0:
                    grads *= 1.0 / grad_norm
                
                # Updating parameters
                self.theta -= learning_rate * grads
                phst.append(self.theta.copy())
                
                total_loss += batch_loss
            
            # Check convergence
            pnew = np.linalg.norm(self.theta - old_theta)
            if pnew &lt; tol:
                print(f"Converged at epoch {epoch}, parameter change: {pnew:.8f}")
                break
        
        return np.array(phst)
    
    def predict(self, X):
        """Make predictions on new data"""
        X_augmented = np.column_stack([np.ones(len(X)), X])
        return np.dot(X_augmented, self.theta)

    def _match_inp(self, X, y):
        if len(X) != len(y):
            raise ValueError("X and y must have same number of samples")
        if len(X.shape) != 2:
            raise ValueError("X must be 2-dimensional")

if __name__ == "__main__":
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match179-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    N = 1_000_000
    true_params = np.array([3, 1, 2])
    feature_means = np.array([3, -1])
    feature_stds = np.array([4, 4])
</FONT>    noise_std = np.sqrt(2)  
    
    print("\nGenerating synthetic dataset...")
    X, y = generate(N, true_params, feature_means, feature_stds, noise_std)
    
    train_idx = int(0.8 * N)
    X_train, y_train = X[:train_idx], y[:train_idx]
    X_test, y_test = X[train_idx:], y[train_idx:]
    print(f"Dataset split: {train_idx} training, {N-train_idx} testing samples")
    
    batch_configs = [1, 80, 8000, 800000]
    learn_rate = 0.001  
    experiment_results = {}
  
    for batch_size in batch_configs:
        print(f"\n{'='*20} Batch Size: {batch_size} {'='*20}")
        
        model = StochasticLinearRegressor()
        max_iters = 10000 if batch_size &gt;= 800000 else 50
        
        param_trajectory = model.fit(
            X_train, y_train,
            learning_rate=learn_rate,
            batch_size=batch_size,
            max_epochs=max_iters,
            tol=1e-8
        )
        
        # Evaluate performance
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match179-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        train_preds = model.predict(X_train)
        test_preds = model.predict(X_test)
        
        train_err = np.mean((y_train - train_preds) ** 2)
        test_err = np.mean((y_test - test_preds) ** 2)
</FONT>        
        # Store results
        experiment_results[batch_size] = {
            'parameters': model.theta,
            'train_error': train_err,
            'test_error': test_err,
            'trajectory': param_trajectory
        }
        
        # Print summary
        print(f"\nResults for batch_size = {batch_size}")
        print(f"Final parameters: {model.theta}")
        print(f"Training MSE: {train_err:.6f}")
        print(f"Testing MSE:  {test_err:.6f}")
    
    # closed-form
    print("\nComputing closed-form solution...")
    X_train_bias = np.column_stack([np.ones(len(X_train)), X_train])
    closed_form_theta = np.linalg.inv(X_train_bias.T @ X_train_bias) @ X_train_bias.T @ y_train
    
    # Visualize results
    for batch_size in batch_configs:
        plt.figure(figsize=(10, 8))
        ax = plt.axes(projection='3d')
        
        trajectory = experiment_results[batch_size]['trajectory']
        ax.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2],
                color='blue', label='Parameter Path', linewidth=1)
        
        ax.set_xlabel('θ₀')
        ax.set_ylabel('θ₁')
        ax.set_zlabel('θ₂')
        ax.grid(True, alpha=0.3)
        
        plt.title(f'Parameter Trajectory (Batch Size = {batch_size})')
        ax.legend()
        plt.tight_layout()
        plt.show()




import matplotlib.pyplot as plt
import numpy as np

class LogisticRegressor:
    def __init__(self):
        self.params = None
        self.feature_means = None 
        self.feature_stds = None
    
    def _logistic_function(self, z):
        z = np.clip(z, -250, 250)
        return 1.0 / (1.0 + np.exp(-z))
    
    def _standardize_data(self, X):
        if self.feature_means is None or self.feature_stds is None:
            self.feature_means = np.mean(X, axis=0)
            self.feature_stds = np.std(X, axis=0)
            if np.any(self.feature_stds == 0):
                raise ValueError("Found constant features - cannot standardize")
            
        return (X - self.feature_means) / (self.feature_stds + 1e-8)
     
    
    def _calculate_cost(self, X, y, params):
        predictions = self._logistic_function(X @ params)
        eps = 1e-15
        predictions = np.clip(predictions, eps, 1-eps)
        return np.sum(y * np.log(predictions) + (1-y) * np.log(1-predictions))
    
    def _get_gradient(self, X, y, params):
        error = y - self._logistic_function(X @ params)
        return X.T @ error
    
    def _get_hessian(self, X, params):
        pred = self._logistic_function(X @ params)
        W = np.diag(pred * (1 - pred))
        return -X.T @ W @ X
        
    def fit(self, X, y, max_iterations=100, tolerance=1e-6):
        # Add input validation
        if X.shape[0] != y.shape[0]:
            raise ValueError("Number of samples in X and y must match")
        if not np.all(np.unique(y) == [0, 1]):
            raise ValueError("Labels must be binary (0 or 1)")
        X_norm = self._standardize_data(X)
        X_with_bias = np.column_stack([np.ones(X.shape[0]), X_norm])
        self.params = np.zeros(X_with_bias.shape[1])
        for iter_count in range(max_iterations):
            grad = self._get_gradient(X_with_bias, y, self.params)
            hess = self._get_hessian(X_with_bias, self.params)
            param_update = np.linalg.solve(hess, grad)
            self.params = self.params - param_update
            if np.all(abs(param_update) &lt; tolerance):
                break
        
        return self.params
    
    def predict(self, X):
        X_norm = self._standardize_data(X)
        X_with_bias = np.column_stack([np.ones(X.shape[0]), X_norm])
        # Get probabilities
        probs = self._logistic_function(X_with_bias @ self.params)
        return (probs &gt;= 0.5).astype(int)
    
    # debugging & log for every 10 iterations
    def _print_iteration_info(self, iter_count, grad_norm):
        if iter_count % 10 == 0:
            print(f"Iteration {iter_count}, Gradient norm: {grad_norm:.6f}")

    '''
    grad_norm = np.linalg.norm(grad)
    if grad_norm &lt; tolerance:
        print(f"Converged after {iter_count} iterations")
        break
    '''
    
    def get_decision_boundary(self, x1_points, x2_points):
        # Create mesh grid
        X1, X2 = np.meshgrid(x1_points, x2_points)
        # Combine coordinates
        grid_points = np.c_[X1.ravel(), X2.ravel()]
        X_norm = self._standardize_data(grid_points)
        X_with_bias = np.column_stack([np.ones(X_norm.shape[0]), X_norm])
        predictions = self._logistic_function(X_with_bias @ self.params)
        return X1, X2, predictions.reshape(X1.shape)
    

def main():
    labels = np.loadtxt('../data/Q3/logisticY.csv')
    features = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
    
    model = LogisticRegressor()
    final_params = model.fit(features, labels)
    print(f"Model parameters: {final_params}")
    
    # Create visualization
    plt.figure(figsize=(10, 8))
    class1_mask = labels == 1
<A NAME="1"></A><FONT color = #00FF00><A HREF="match179-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    class0_mask = labels == 0
    
    plt.scatter(features[class0_mask, 0], features[class0_mask, 1], 
               color='orange', marker='o', label='Class 0')
    plt.scatter(features[class1_mask, 0], features[class1_mask, 1], 
               color='red', marker='x', label='Class 1')
    
    # Add decision boundary
    x2_range = np.linspace(features[:, 1].min() - 0.5, features[:, 1].max() + 0.5, 100)
</FONT>    x1_range = np.linspace(features[:, 0].min() - 0.5, features[:, 0].max() + 0.5, 100)
    
    X1, X2, Z = model.get_decision_boundary(x1_range, x2_range)
    # Plot boundary
    plt.contour(X1, X2, Z, levels=[0.5], colors='blue', linestyles='--')
    plt.title('Binary Classification with Logistic Regression')
    plt.ylabel('Feature 2')
    plt.xlabel('Feature 1')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

if __name__ == "__main__":
    main()



import os
import matplotlib.pyplot as plt
import numpy as np

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.sigma = None
        self.sigma_1 = None
        self.sigma_0 = None
        self.mu_1 = None 
        self.mu_0 = None 
           
        
    def fit(self, X, y, assume_same_covariance=False):
        # Data preprocessing
        feature_stds = np.std(X, axis=0)
        feature_means = np.mean(X, axis=0)
        X_normalized = (X - feature_means) / feature_stds
        canada_samples = X_normalized[y == 1]
        alaska_samples = X_normalized[y == 0]
        self.mu_1 = np.mean(canada_samples, axis=0)
        self.mu_0 = np.mean(alaska_samples, axis=0)

        if assume_same_covariance:
            # Center the data for each class
            canada_centered = canada_samples - self.mu_1
            alaska_centered = alaska_samples - self.mu_0
            pooled_cov = np.dot(alaska_centered.T, alaska_centered)
            pooled_cov += np.dot(canada_centered.T, canada_centered)
            total_samples = len(alaska_samples) + len(canada_samples)
            self.sigma = pooled_cov / (total_samples - 2)  # Unbiased estimate
            return self.mu_0, self.mu_1, self.sigma
        else:
            # Calculate separate covariance matrices for each class
            canada_centered = canada_samples - self.mu_1
            alaska_centered = alaska_samples - self.mu_0
            # Unbiased estimates using (n-1)
            self.sigma_0 = np.dot(alaska_centered.T, alaska_centered) / (len(alaska_samples) - 1)
            self.sigma_1 = np.dot(canada_centered.T, canada_centered) / (len(canada_samples) - 1)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        if self.sigma is not None:
            # Linear discriminant
            inv_sigma = np.linalg.inv(self.sigma)
            canada_score = self._calc_linear_score(X_norm, self.mu_1, inv_sigma)
            alaska_score = self._calc_linear_score(X_norm, self.mu_0, inv_sigma)
        else:
            # Quadratic discriminant
            canada_score = self._calc_quadratic_score(X_norm, self.mu_1, self.sigma_1)
            alaska_score = self._calc_quadratic_score(X_norm, self.mu_0, self.sigma_0)
        return (canada_score &gt; alaska_score).astype(int)
    
    def _calc_linear_score(self, X, mu, inv_sigma):
        return np.dot(X, np.dot(inv_sigma, mu)) - 0.5 * np.dot(mu, np.dot(inv_sigma, mu))
    
    def _calc_quadratic_score(self, X, mu, sigma):     
        det_sigma = np.linalg.det(sigma)
        inv_sigma = np.linalg.inv(sigma)
        centered_X = X - mu
        quad_term = np.array([np.dot(np.dot(x, inv_sigma), x) for x in centered_X])      
        return -0.5 * np.log(det_sigma) - 0.5 * quad_term

def plot_data_and_boundaries(X, y, gda, title="GDA Classification"):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = gda.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(10, 8))
    plt.contour(xx, yy, Z, levels=[0.5], colors='k', linestyles='-')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Canada', marker='^')
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Alaska', marker='o')
    plt.ylabel('Growth ring diameter in marine water (x₂)')
    plt.xlabel('Growth ring diameter in freshwater (x₁)')
    
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_both_boundaries(X, y, title="GDA Classification Comparison"):
    # Create figure
    plt.figure(figsize=(12, 8))
    gda_linear = GaussianDiscriminantAnalysis()
    gda_quadratic = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma = gda_linear.fit(X, y, assume_same_covariance=True)
    mu_0, mu_1, sigma_0, sigma_1 = gda_quadratic.fit(X, y, assume_same_covariance=False)

    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5    
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    # predictions for both models
    Z_linear = gda_linear.predict(np.c_[xx.ravel(), yy.ravel()])
    Z_quad = gda_quadratic.predict(np.c_[xx.ravel(), yy.ravel()])
    
    Z_linear = Z_linear.reshape(xx.shape)
    Z_quad = Z_quad.reshape(xx.shape)
    
    # Plotting decision boundaries
    plt.contour(xx, yy, Z_linear, levels=[0.5], colors='r', linestyles='-', label='Linear Boundary')
    plt.contour(xx, yy, Z_quad, levels=[0.5], colors='b', linestyles='--', label='Quadratic Boundary')

    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Alaska', marker='o')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Canada', marker='^')
    plt.ylabel('Growth ring diameter in marine water (x₂)')
    plt.xlabel('Growth ring diameter in freshwater (x₁)')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

# Load data
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)

# Convert string labels to binary -- 0 Alaska, 1 Canada
y = (y == 'Canada').astype(int)

# Part 1 & 2:
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)

# Print Part 1 results
print("Part 1 Results:")
print("Mean for Alaska (μ₀):", mu_0)
print("Mean for Canada (μ₁):", mu_1)
print("Shared covariance matrix (Σ):\n", sigma)
# Plot the results
plot_data_and_boundaries(X, y, gda, "GDA Classification with Shared Covariance")

# Fit models and get parameters
gda_linear = GaussianDiscriminantAnalysis()
gda_quadratic = GaussianDiscriminantAnalysis()

# Get parameters for linear boundary (shared covariance)
mu_0, mu_1, sigma = gda_linear.fit(X, y, assume_same_covariance=True)
mu_0, mu_1, sigma_0, sigma_1 = gda_quadratic.fit(X, y, assume_same_covariance=False)
print("\nPart 4 Results (Quadratic GDA):")
print("Mean for Alaska (μ₀):", mu_0)
print("Mean for Canada (μ₁):", mu_1)
print("Covariance matrix for Alaska (Σ₀):\n", sigma_0)
print("Covariance matrix for Canada (Σ₁):\n", sigma_1)

print("\nPart 1 Results (Linear GDA):")
print("Mean for Alaska (μ₀):", mu_0)
print("Mean for Canada (μ₁):", mu_1)
print("Shared covariance matrix (Σ):\n", sigma)

plot_both_boundaries(X, y)

</PRE>
</PRE>
</BODY>
</HTML>
