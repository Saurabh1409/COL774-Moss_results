<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_7N54O.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_7N54O.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        
        self.param = None

    
    def fit(self, X, y, learning_rate=0.01):
        ''''''

        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        epsilon = 1e-6
        max_iter = 1000
        iter =0

        num_features = X.shape[1]
        n_samples = X.shape[0]

        bias = np.ones(shape=(n_samples,1))  ##Adding 1s in sample points
        X = np.column_stack((bias,X))
        y = y.reshape(n_samples,1)

        list_param = np.empty((0,num_features+1))
        parameters = np.zeros(shape = (num_features+1,1))  ##Initialising zero parameters

        output = np.dot(X,parameters)           ##Error with zero parameters
        error = y - output
        MSE = np.sum(error**2)/(2*n_samples)
        while True:
    
            gradient = np.dot(np.transpose(X),error)        ##Gradient Descent Update
            parameters+=(learning_rate*gradient)/n_samples
            iter+=1

            param = parameters.reshape(num_features+1,)
            list_param = np.vstack((list_param,param))

            output = np.dot(X,parameters)           ##Error for given parameters
            error = y - output
            new_MSE = np.sum(error**2)/(2*n_samples)
            if abs(new_MSE-MSE)&lt;epsilon or iter == max_iter:  ##Convergence Condition
                break
            MSE = new_MSE
        self.param = param
        return list_param


    

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples,1))     ##Inference
        X = np.column_stack((bias,X))
        y_pred = np.dot(X,self.param)
        return y_pred



'''
ML= LinearRegressor()
X= np.genfromtxt('../data/Q1/linearX.csv',delimiter='\n')
X = X.reshape(X.shape[0],1)
y = np.genfromtxt('../data/Q1/linearY.csv',delimiter='\n')
print(ML.fit(X,y,1))
print(ML.predict(X))
'''






# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LinearRegressor:
    def __init__(self):
        self.param = None

    def fit(self, X, y, learning_rate=0.01):
        """"""

        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        epsilon = 1e-6
        iter = 0
        num_features = X.shape[1]
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples, 1))
        X = np.column_stack((bias, X))
        y = y.reshape(n_samples, 1)
        list_param = np.empty((0, num_features + 1))
        parameters = np.zeros(shape=(num_features + 1, 1))
        output = np.dot(X, parameters)
        error = y - output
        MSE = np.sum(error**2) / (2 * n_samples)
        while True:
            print(iter, MSE)
            gradient = np.dot(np.transpose(X), error)
            parameters += (learning_rate * gradient) / n_samples
            iter += 1
            param = parameters.reshape(
                num_features + 1,
            )
            list_param = np.vstack((list_param, param))
            output = np.dot(X, parameters)
            error = y - output
            new_MSE = np.sum(error**2) / (2 * n_samples)
            if abs(new_MSE - MSE) &lt; epsilon or iter == 1000:
                break
            MSE = new_MSE
        self.param = param
        return list_param

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples, 1))
        X = np.column_stack((bias, X))
        y_pred = np.dot(X, self.param)
        return y_pred


def plot(X, y, param):
    plt.plot(
        X.reshape(
            X.shape[0],
        ),
        y,
        marker="o",
        color="red",
    )
    line_x = np.linspace(-2, 2, 100)
    line_y = (param[1] * line_x) + param[0]
    plt.plot(line_x, line_y, color="blue", linestyle="-")
    plt.xlabel("Acidity")
    plt.ylabel("Density")
    plt.title("Batch Gradient Descent")
    plt.grid(True)
    plt.show()


def plot_error_surface_3d(X, y, param_list, pause_time=0.2):
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection="3d")
    ax.set_xlabel("Intercept")
    ax.set_ylabel("Slope")
    ax.set_zlabel("Error")
    plt.title("Error Surface")
    intercept_range = np.linspace(param_list[0][0] - 4, param_list[-1][0] + 6, 100)
    slope_range = np.linspace(param_list[0][1] - 2, param_list[-1][1] + 20, 100)
    intercept_mesh, slope_mesh = np.meshgrid(intercept_range, slope_range)
    n_samples = X.shape[0]
    bias = np.ones((n_samples, 1))
    X = np.column_stack((bias, X))
    error_mesh = np.zeros(intercept_mesh.shape)
    for i in range(len(intercept_range)):
        for j in range(len(slope_range)):
            weights = np.array([intercept_mesh[i, j], slope_mesh[i, j]])
            h = np.dot(X, weights)
            error_mesh[i, j] = (1 / (2 * n_samples)) * np.sum((h - y) ** 2)
    surf = ax.plot_surface(
        intercept_mesh, slope_mesh, error_mesh, cmap="viridis", alpha=0.6
    )
    fig.colorbar(surf, label="Mean Square Error")
    for param in param_list:
        h = np.dot(X, param)
        error = (1 / (2 * n_samples)) * np.sum((h - y) ** 2)
        ax.scatter(param[0], param[1], error, color="red", s=50)
        plt.pause(pause_time)
    plt.show()


def plotting_contours(X, y, param_list, pause_time=0.2):
    fig = plt.figure(figsize=(12, 8))
    plt.xlabel("Intercept")
    plt.ylabel("Slope")
    plt.title("Contours of Error Function")
    intercept_range = np.linspace(param_list[0][0] - 1, param_list[-1][0] + 3, 100)
    slope_range = np.linspace(param_list[0][1] - 1, param_list[-1][1] + 15, 100)
    intercept_mesh, slope_mesh = np.meshgrid(intercept_range, slope_range)
    n_samples = X.shape[0]
    bias = np.ones((n_samples, 1))
    X = np.column_stack((bias, X))
    error_mesh = np.zeros(intercept_mesh.shape)
    for i in range(len(intercept_range)):
        for j in range(len(slope_range)):
            weights = np.array([intercept_mesh[i, j], slope_mesh[i, j]])
            h = np.dot(X, weights)
            error_mesh[i, j] = (1 / (2 * n_samples)) * np.sum((h - y) ** 2)

    for param in param_list[:50]:
        h = np.dot(X, param)
        error = (1 / (2 * n_samples)) * np.sum((h - y) ** 2)

        plt.contour(
            intercept_mesh, slope_mesh, error_mesh, levels=[error], colors=["orange"]
        )
        plt.scatter(param[0], param[1], color="red", s=50)
        plt.pause(pause_time)
    plt.grid()
    plt.show()


ML = LinearRegressor()
X = np.genfromtxt("../data/Q1/linearX.csv", delimiter="\n")
X = X.reshape(X.shape[0], 1)
y = np.genfromtxt("../data/Q1/linearY.csv", delimiter="\n")
param = ML.fit(X, y, 0.5)
plot(X, y, ML.param)
plot_error_surface_3d(X, y, param)
plotting_contours(X, y, param)
for lr in [0.1, 0.025, 0.001]:
    plotting_contours(X, y, ML.fit(X, y, lr))




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib as mpl
from mpl_toolkits.mplot3d import Axes3D
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match184-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
</FONT>    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0],input_sigma[0],(N,))
    x2 = np.random.normal(input_mean[1],input_sigma[1],(N,))
    X = np.column_stack((x1,x2))
    bias = np.ones(shape=(N,1))
    X_data = np.column_stack((bias,X))
<A NAME="2"></A><FONT color = #0000FF><A HREF="match184-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(0,noise_sigma,(N,))
    y = np.dot(X_data,theta) + noise
    return X,y

class StochasticLinearRegressor:
    def __init__(self):
        self.param = []   
</FONT>        self.train_error = [] 
        self.conv = []
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        epsilon = 5e-6
        max_iter1 = 3
        max_iter2 = 500
        max_iter3 = 5000
        max_iter4 = 20000
        k = 0.25
        
        
        num_features = X.shape[1]
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((bias,X))
        y = y.reshape(n_samples,1)
        ans = []
        for bs in [800000]:
            if bs==1:
                max_iter = max_iter1
            elif bs==80:
                max_iter = max_iter2
            elif bs == 8000:
                max_iter = max_iter3
            else:
                max_iter = max_iter4
            list_param = np.empty((0,num_features+1))
            parameters = np.zeros(shape = (num_features+1,1))
            iter =0
            batch_size = bs
            num_batches = n_samples//batch_size
            batch_X =[]
            batch_Y = []
            s = time.time()
            for i in range(num_batches):
                batch_X.append(X[i*batch_size:(i+1)*batch_size])
                batch_Y.append(y[i*batch_size:(i+1)*batch_size])
            print(time.time()-s)
            X_train = batch_X[0]
            y_train = batch_Y[0]
            start = time.time()
            output = np.dot(X_train,parameters)
            error = y_train - output
            
            MSE = np.sum(error**2)/(2*batch_size)
            mov_avg = MSE
            flag = False
            updates = 0
            while True:
                iter+=1
                for b in range(num_batches):
                    
                    gradient = np.dot(np.transpose(X_train),error)
                    parameters+=(learning_rate*gradient)/batch_size                   
                    updates+=1
                    X_train = batch_X[b]
                    y_train = batch_Y[b]
                    output = np.dot(X_train,parameters)
                    error = y_train - output
                    MSE = np.sum(error**2)/batch_size
                    new_avg = k*MSE + (1-k)*mov_avg
                    #print(new_avg,mov_avg)
                    if abs(new_avg-mov_avg)&lt;epsilon or iter == max_iter:
                        self.trainMSE = MSE
                        end = time.time()
                        time_taken = end -start
                        self.conv.append((time_taken,iter))
                        flag = True
                        break
                    mov_avg = new_avg
                    param = parameters.reshape(num_features+1,)
                    list_param = np.vstack((list_param,param))
                if flag:
                    break
            self.param.append(param)
            self.train_error.append(MSE) 
            ans.append(list_param)
        return ans
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((bias,X))
        ans = []
        for i in range(4):
            ans.append(np.dot(X,self.param[i]))
        return ans
    
    def normal_sol(self,X,y):
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((bias,X))
        res1 = np.dot(np.transpose(X),X)
        res2 = np.linalg.inv(res1)
        res3 =  np.dot(np.transpose(X),y)
        return np.dot(res2,res3)

def plotter(lst):
    fig = plt.figure(figsize=(12, 8))   
    ax = fig.add_subplot(111,projection='3d')
    for i in range(4):      
                
        x = lst[i][:,0]
        y = lst[i][:,1]
        z = lst[i][:,2]
        if i == 0:
            lab = '1'
        elif i== 1:
            lab = '80'
        elif i == 2:
            lab = '8000'
        else:
            lab = '800000'
        ax.plot(x, y, z, label=lab)
    ax.set_xlabel('X0')
    ax.set_ylabel('X1')    
    ax.set_zlabel('X2')
    plt.title('Movement of theta for different batch sizes')
    ax.legend()
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match184-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.show()

X,y =(generate(1000000,np.array([3,1,2]),np.array([3,-1]),np.array([2,2]),2**0.5))
</FONT>X_train = X[:800000]
X_test = X[800000:]
y_train = y[:800000]
y_test = y[800000:]
ML = StochasticLinearRegressor()

list_list_params = ML.fit(X_train,y_train,0.001)
print(ML.train_error)
print(ML.conv)
print(ML.param)
test_MSE = []
pred = ML.predict(X_test)
for i in range(4):    
    test_MSE.append(np.sum((pred[i]-y_test)**2)/400000)
print(test_MSE)
print(ML.normal_sol(X_train,y_train))
plotter(list_list_params)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.

    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.

    input_mean : numpy array of shape (2,)
        The mean of the input data.

    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.

    noise_sigma : float
        The standard deviation of the Gaussian noise.

    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.

    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(
        input_mean[0], input_sigma[0], (N,)
    )  ##Generating X1, X2 and Y
    x2 = np.random.normal(input_mean[1], input_sigma[1], (N,))
    X = np.column_stack((x1, x2))
    bias = np.ones(shape=(N, 1))
    X_data = np.column_stack((bias, X))
    noise = np.random.normal(0, noise_sigma, (N,))
    y = np.dot(X_data, theta) + noise
    return X, y


class StochasticLinearRegressor:
    def __init__(self):
        self.param = []

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        epsilon = 5e-6
        max_iter1 = 2
        max_iter2 = 500
        max_iter3 = 5000
        max_iter4 = 15000
        k = 0.25

        num_features = X.shape[1]
        n_samples = X.shape[0]

        bias = np.ones(shape=(n_samples, 1))  ##Adding 1s in the data
        X = np.column_stack((bias, X))
        y = y.reshape(n_samples, 1)

        ans = []
        for bs in [1, 80, 8000, 800000]:
            if (
                bs == 1
            ):  ##Defining bound on num_iterations, in case SGD doesn't converge
                max_iter = max_iter1
            elif bs == 80:
                max_iter = max_iter2
            elif bs == 8000:
                max_iter = max_iter3
            else:
                max_iter = max_iter4

            list_param = np.empty((0, num_features + 1))
            parameters = np.zeros(shape=(num_features + 1, 1))

            iter = 0
            batch_size = bs
            num_batches = n_samples // batch_size
            batch_X = []  ##Dividing data into batches
            batch_Y = []

            for i in range(num_batches):
                batch_X.append(X[i * batch_size : (i + 1) * batch_size])
                batch_Y.append(y[i * batch_size : (i + 1) * batch_size])

            X_train = batch_X[0]
            y_train = batch_Y[0]

            output = np.dot(X_train, parameters)  ##Computing Error on zero parameters
            error = y_train - output

            MSE = np.sum(error**2) / (2 * batch_size)
            mov_avg = MSE
            flag = False
            while True:
                for b in range(num_batches):  ##
                    gradient = np.dot(
                        np.transpose(X_train), error
                    )  ##Updating parameters based on Gradient Descent
                    parameters += (learning_rate * gradient) / batch_size
                    X_train = batch_X[b]
                    y_train = batch_Y[b]

                    output = np.dot(
                        X_train, parameters
                    )  ##Computing error on the parameters
                    error = y_train - output
                    MSE = np.sum(error**2) / batch_size
                    new_avg = k * MSE + (1 - k) * mov_avg

                    if (
                        abs(new_avg - mov_avg) &lt; epsilon or iter == max_iter
                    ):  ##Convergence Condition
                        flag = True
                        break
                    mov_avg = new_avg

                param = parameters.reshape(
                    num_features + 1,
                )  ##Adding parameters after each epoch
                list_param = np.vstack((list_param, param))
                iter += 1
                if flag:
                    break

            self.param.append(param)
            ans.append(list_param)
        return ans

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples, 1))  ##Adding 1s in the data
        X = np.column_stack((bias, X))
        ans = []
        for i in range(4):
            ans.append(np.dot(X, self.param[i]))  ##Inference for each batch size
        return ans

    def normal_sol(self, X, y):
        n_samples = X.shape[0]
        bias = np.ones(shape=(n_samples, 1))  ##Adding 1s in the data
        X = np.column_stack((bias, X))
        res1 = np.dot(np.transpose(X), X)
        res2 = np.linalg.inv(res1)
        res3 = np.dot(np.transpose(X), y)
        return np.dot(res2, res3)


"""
X, y = generate(
    1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), 2**0.5
)
X_train = X[:800000]
X_test = X[800000:]
y_train = y[:800000]
y_test = y[800000:]
ML = StochasticLinearRegressor()

list_list_params = ML.fit(X_train, y_train, 0.001)
test_MSE = []
pred = ML.predict(X_test)
for i in range(4):
    test_MSE.append(np.sum((pred[i] - y_test) ** 2) / 400000)
print(test_MSE)
print(ML.normal_sol(X_train, y_train))
"""




# Imports - you can add any other permitted libraries
import numpy as np
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.param = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        epsilon = 1e-6
        iter = 0
        num_features = X.shape[1]
        n_samples = X.shape[0]
        
        mean = np.mean(X,axis = 0)          ##Normalizing the data
        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev

        bias = np.ones(shape=(n_samples,1))     ##Adding 1s to each sample
        X = np.column_stack((bias,X))

        list_param = np.empty((0,num_features+1))       
        parameters = np.zeros(shape = (num_features+1,))       ##Initializing parameters
        LL = n_samples * np.log(0.5)                        
        output = np.dot(X,parameters)                       ##Prediction for zero parameters
        probability = 1.0/(1+np.exp(-output))
        while True:            
            error = y - probability                     
            gradient = np.dot(np.transpose(X),error)                ##Computing first order derivative
            
            sig_der = probability*(1-probability)                   ##Computing Hessian Matrix
            diagonal = np.diagflat(sig_der)
            hessian = np.dot(np.transpose(X),np.dot(diagonal,X))

            hessain_inverse = np.linalg.inv(hessian)            ##Computing Newton Update
            update = np.dot(hessain_inverse,gradient)
            parameters += update

            list_param = np.vstack((list_param,parameters))     
            iter+=1

            output = np.dot(X,parameters)           ##Log Likelihood with new set of parameters
            probability = 1.0/(1+np.exp(-output))


            log_term = np.log(1-probability)
            product_term = output*y
            new_LL = np.sum(product_term-log_term)

            
            abs_upd = np.sum(update**2)**0.5
            if abs_upd&lt;epsilon or iter == 1000:    ##Convergence condition
                break
            LL = new_LL

        self.param = parameters
        return list_param

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        n_samples = X.shape[0]

        mean = np.mean(X,axis = 0)          ##Normalization
        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev
        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((bias,X))

        output = np.dot(X,self.param)            ##Inference
        probability = 1.0/(1+np.exp(-output))
        y_pred = (probability&gt;0.5).astype(int)
        return y_pred
        
'''
X= np.genfromtxt('../data/Q3/logisticX.csv',delimiter=',')
y = np.genfromtxt('../data/Q3/logisticY.csv',delimiter='\n')
ML = LogisticRegressor()
params = ML.fit(X,y)
print(params)
print(ML.predict(X))'''




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.param = None
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        epsilon = 1e-6
        iter = 0
        num_features = X.shape[1]
        n_samples = X.shape[0]
        
        mean = np.mean(X,axis = 0)
        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev

        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((X,bias))
        #y = y.reshape(n_samples,1)

        list_param = np.empty((0,num_features+1))
        parameters = np.zeros(shape = (num_features+1,))     
        LL = n_samples * np.log(0.5)
        output = np.dot(X,parameters)
        probability = 1.0/(1+np.exp(-output))
        while True:            
            error = y - probability
            gradient = np.dot(np.transpose(X),error)
            sig_der = probability*(1-probability)
            diagonal = np.diagflat(sig_der)
            hessian = np.dot(np.transpose(X),np.dot(diagonal,X))
            hessain_inverse = np.linalg.inv(hessian)
            parameters += np.dot(hessain_inverse,gradient)
            list_param = np.vstack((list_param,parameters))
            iter+=1

            output = np.dot(X,parameters)
            probability = 1.0/(1+np.exp(-output))
            log_term = np.log(1-probability)
            product_term = output*y
            new_LL = np.sum(product_term-log_term)
            if abs(new_LL-LL)&lt;epsilon:
                break
            LL = new_LL

        self.param = parameters
        return list_param

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        n_samples = X.shape[0]
        mean = np.mean(X,axis = 0)
        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev
        bias = np.ones(shape=(n_samples,1))
        X = np.column_stack((X,bias))
        output = np.dot(X,self.param)
        probability = 1.0/(1+np.exp(-output))
        y_pred = (probability&gt;0.5).astype(int)
        return y_pred
        
def plotter(X,y,param):

    mean = np.mean(X,axis = 0)
    squared_mean = np.mean(X**2,axis=0)
    std_dev = (squared_mean - (mean**2))**0.5
    X = (X-mean)/std_dev
    X = np.column_stack((X,y))

    label0 = X[X[:,2]==0]
    label1 = X[X[:,2]==1]


<A NAME="5"></A><FONT color = #FF0000><A HREF="match184-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.scatter(label0[:,0],label0[:,1],marker='o',color = 'green',label ='Class 0')
    plt.scatter(label1[:,0],label1[:,1],marker='x',color = 'blue',label = 'Class 1')
    x2 = np.linspace(-2,2,100)
</FONT>    x1 = ((param[1]*x2) + param[2])/(-1*param[0])
    plt.plot(x1,x2,color = 'red',linestyle = '-')
    plt.title('Logistic Regression')
    plt.legend()
    plt.grid(True)
    plt.show()



X= np.genfromtxt('../data/Q3/logisticX.csv',delimiter=',')
y = np.genfromtxt('../data/Q3/logisticY.csv',delimiter='\n')
ML = LogisticRegressor()
params = ML.fit(X,y)
print(params)
plotter(X,y,ML.param)
print(ML.predict(X))





# Imports - you can add any other permitted libraries
import numpy as np
from math import log
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


<A NAME="1"></A><FONT color = #00FF00><A HREF="match184-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.same_cov = False
        self.bias = None               ##Coefficients in decision boundary
        self.first_order = None
        self.second_order = None

    
    def fit(self, X, y, assume_same_covariance=False):
</FONT>        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="0"></A><FONT color = #FF0000><A HREF="match184-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        n_samples = X.shape[0]
        
        mean = np.mean(X,axis = 0)                        ##Normalising the data
</FONT>        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev


        X = np.column_stack((X,y))        ##Partitoning the data for two classes
        label0 = X[X[:,2]==0][:,:2]
        label1 = X[X[:,2]==1][:,:2]


        sample0 = label0.shape[0]             ##Computing class priors
        sample1 = label1.shape[0]
        prob1 = label1.shape[0]/n_samples
        prob0 = 1-prob1

<A NAME="6"></A><FONT color = #00FF00><A HREF="match184-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        mu_1 = np.mean(label1,axis=0)   ##Computing mean of features for two classes
        mu_0 = np.mean(label0,axis=0)
        dev1 = label1 - mu_1
        dev0 = label0 - mu_0
</FONT>
        if assume_same_covariance:
            self.same_cov= True
            sigma1 = np.dot(np.transpose(dev1),dev1)   ##Computing co-variance matrix
            sigma0 = np.dot(np.transpose(dev0),dev0)
            sigma = (sigma0 + sigma1)/n_samples

            inv = np.linalg.inv(sigma)               ##Computing slope and intercept of decision boundary for prediction
            term1 = np.dot(inv,mu_1)
            term0 = np.dot(inv,mu_0)
            c1 = np.dot(np.transpose(mu_1),term1)
            c0 = np.dot(np.transpose(mu_0),term0)
            self.bias = log(prob1/prob0) - 0.5*(c1-c0)
            self.first_order = term1-term0

            return mu_0, mu_1, sigma
        else:
            sigma1 = np.dot(np.transpose(dev1),dev1)/sample1    ##Computing co-variance matrix
            sigma0 = np.dot(np.transpose(dev0),dev0)/sample0


            inv1 = np.linalg.inv(sigma1)                ##Computing decision boundary for prediction
            inv0 = np.linalg.inv(sigma0)
            term1 = np.dot(inv1,mu_1)
            term0 = np.dot(inv0,mu_0)
            c1 = np.dot(np.transpose(mu_1),term1)
            c0 = np.dot(np.transpose(mu_0),term0)
            c2 = log(np.linalg.det(inv0)/np.linalg.det(inv1))
            self.bias = log(prob1/prob0) + 0.5*(c2 - c1 + c0)
            self.first_order = term1-term0
            self.second_order = (inv1 - inv0)*(-0.5)


            return mu_0,mu_1,sigma0,sigma1


            
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        mean = np.mean(X,axis = 0)                  ##Normalizing the data
        squared_mean = np.mean(X**2,axis=0)
        std_dev = (squared_mean - (mean**2))**0.5
        X = (X-mean)/std_dev
        
        out_fo = np.dot(X,self.first_order)    ##Putting data in decision boundary
        output = out_fo + self.bias       
        if not(self.same_cov):
            out_so = np.diag(np.dot(np.dot(X,self.second_order),np.transpose(X)))
            output  = out_so + output

        

        y_pred = (output&gt;0).astype(int)
        return y_pred



# Imports - you can add any other permitted libraries
import numpy as np
from math import log
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.same_cov = False
        self.bias = None
        self.first_order = None
        self.second_order = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        n_samples = X.shape[0]

        mean = np.mean(X, axis=0)
        squared_mean = np.mean(X**2, axis=0)
        std_dev = (squared_mean - (mean**2)) ** 0.5
        X = (X - mean) / std_dev

        # bias = np.ones(shape=(n_samples,1))
        # X = np.column_stack((bias,X))
        X = np.column_stack((X, y))

        label0 = X[X[:, 2] == 0][:, :2]
        label1 = X[X[:, 2] == 1][:, :2]

        sample0 = label0.shape[0]
        sample1 = label1.shape[0]
        prob1 = label1.shape[0] / n_samples
        prob0 = 1 - prob1

        mu_1 = np.mean(label1, axis=0)
        mu_0 = np.mean(label0, axis=0)
        dev1 = label1 - mu_1
        dev0 = label0 - mu_0

        if assume_same_covariance:
            self.same_cov = True
            sigma1 = np.dot(np.transpose(dev1), dev1)
            sigma0 = np.dot(np.transpose(dev0), dev0)
            sigma = (sigma0 + sigma1) / n_samples

            inv = np.linalg.inv(sigma)
            term1 = np.dot(inv, mu_1)
            term0 = np.dot(inv, mu_0)
            c1 = np.dot(np.transpose(mu_1), term1)
            c0 = np.dot(np.transpose(mu_0), term0)
            self.bias = log(prob1 / prob0) - 0.5 * (c1 - c0)
            self.first_order = term1 - term0

            return mu_0, mu_1, sigma
        else:
            sigma1 = np.dot(np.transpose(dev1), dev1) / sample1
            sigma0 = np.dot(np.transpose(dev0), dev0) / sample0

            inv1 = np.linalg.inv(sigma1)
            inv0 = np.linalg.inv(sigma0)
            term1 = np.dot(inv1, mu_1)
            term0 = np.dot(inv0, mu_0)
            c1 = np.dot(np.transpose(mu_1), term1)
            c0 = np.dot(np.transpose(mu_0), term0)
            c2 = log(np.linalg.det(inv0) / np.linalg.det(inv1))
            self.bias = log(prob1 / prob0) + 0.5 * (c2 - c1 + c0)
            self.first_order = term1 - term0
            self.second_order = (inv1 - inv0) * (-0.5)

            return mu_0, mu_1, sigma0, sigma1

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        mean = np.mean(X, axis=0)
        squared_mean = np.mean(X**2, axis=0)
        std_dev = (squared_mean - (mean**2)) ** 0.5
        X = (X - mean) / std_dev

        # decision boundary
        out_fo = np.dot(X, self.first_order)
        output = out_fo + self.bias
        if not(self.same_cov):
            out_so = np.diag(np.dot(np.dot(X, self.second_order), np.transpose(X)))
            output = out_so + output

        y_pred = (output &gt; 0).astype(int)
        return y_pred


def plotter(GDA: GaussianDiscriminantAnalysis, X, Y):
    """
    Same_Cov = GaussianDiscriminantAnalysis()
    Same_Cov.fit(X,Y,True)
    x2 = np.linspace(-3,3,400)
    x1 = ((Same_Cov.first_order[1]*x2) + Same_Cov.bias)/(-1*Same_Cov.first_order[0])
    plt.plot(x1,x2,color = 'red',linestyle = '-')
    """

    mean = np.mean(X, axis=0)
    squared_mean = np.mean(X**2, axis=0)
    std_dev = (squared_mean - (mean**2)) ** 0.5
    X = (X - mean) / std_dev
    X = np.column_stack((X, Y))

    label0 = X[X[:, 2] == 0]
    label1 = X[X[:, 2] == 1]

    plt.scatter(label0[:, 0], label0[:, 1], marker="o", color="green", label="Alaska")
    plt.scatter(label1[:, 0], label1[:, 1], marker="x", color="blue", label="Canada")

    if GDA.same_cov:
        x2 = np.linspace(-2.5, 2.5, 100)
        x1 = ((GDA.first_order[1] * x2) + GDA.bias) / (-1 * GDA.first_order[0])
        plt.plot(x1, x2, color="red", linestyle="-")
        plt.title("GDA - Same Covariance")
        plt.grid(True)
        plt.legend()
        plt.show()
    else:
        x = np.linspace(-2.5, 2.5, 400)
        y = np.linspace(-3, 3, 400)
        x, y = np.meshgrid(x, y)
        a = GDA.second_order[0, 0]
        c = GDA.second_order[1, 1]
        b = 2 * GDA.second_order[1, 0]
        d = GDA.first_order[0]
        e = GDA.first_order[1]
        f = GDA.bias

        plt.contour(
            x,
            y,
            (a * x**2 + b * x * y + c * y**2 + d * x + e * y + f),
            [0],
            colors="violet",
        )

        plt.title("GDA")
        plt.grid(True)
        plt.legend()
        plt.show()

    # plt.savefig('Q4.png')


def str_to_bin(label):
    if label == "Alaska":
        return 0
    else:
        return 1


GDA = GaussianDiscriminantAnalysis()
X = np.loadtxt("../data/Q4/q4x.dat")
y_str = np.loadtxt("../data/Q4/q4y.dat", dtype=str)
y = np.vectorize(str_to_bin)(y_str)
print(GDA.fit(X, y))
print(GDA.predict(X))
print(GDA.second_order, GDA.first_order, GDA.bias)
plotter(GDA, X, y)


</PRE>
</PRE>
</BODY>
</HTML>
