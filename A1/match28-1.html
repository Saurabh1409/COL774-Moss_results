<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_71W6W.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_UPXZI.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.J_history = []
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.J_history = []
        m, n = X.shape
        delta = 2*1e-6
        X = np.c_[np.ones((m)), X]
        self.theta = np.zeros(n+1)
        theta_history = [self.theta.copy()]
        iter = 0
        while True:
            new_theta = self.cal_theta(X, y, learning_rate)
            theta_history.append(new_theta.copy())
            self.J_history.append(self.cal_cost(X, y, self.theta))
            
            if iter &gt; 0 and np.linalg.norm(self.theta - new_theta) &lt; delta:
                # print(f"Converged at iteration {iter}")
                break
            iter += 1
            self.theta = new_theta
        return np.array(theta_history)
    
    def cal_theta(self, X, y, learning_rate):
        h = np.dot(X, self.theta)
        error = (h - y)
        m = X.shape[0]
        gradient = (np.dot(X.T, error))/m
        theta = self.theta - (gradient * learning_rate)
        return theta
    
    def cal_cost(self, X, y, theta):
        h = np.dot(X, theta)
        error = (h - y)
        m = X.shape[0]
        return np.sum(error**2)/(2*m)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.c_[np.ones(m), X]
        return np.dot(X, self.theta)
    
    def plot_hypo(self, X, y):
        "Plot hypothesis"
        plt.scatter(X, y, color='red', label='Data Points')
        mean = np.mean(X)
        x_cord = np.linspace(np.min(X[:,0])-mean, np.max(X[:,0])+mean, 100)
        y_cord = self.theta[0] + self.theta[1] * x_cord
        plt.plot(x_cord, y_cord, color='black')
        plt.xlabel('Acidity of wine')
        plt.ylabel('Density of wine')
        plt.title('Linear Regression fit')
        plt.legend()
        plt.show()

    def plot_error_3d(self, X, y, theta_history, J_history):
        "Plot for error in 3D"
        mean1 = np.mean(theta_history[0])
        mean2 = np.mean(theta_history[1])
        th_0 = np.linspace(np.min(theta_history[:, 0]) - mean1, np.max(theta_history[:, 0]) + mean1, 100)
        th_1 = np.linspace(np.min(theta_history[:,1]) - mean2, np.max(theta_history[:,1]) + mean2, 100)
        TH_0, TH_1 = np.meshgrid(th_0, th_1)
        m = X.shape[0]
        X = np.c_[np.ones(m), X]
        J = np.zeros_like(TH_0)
        for i in range(th_0.size):
            for j in range(th_1.size):
                theta = np.array([th_0[i], th_1[j]])
                J[i, j] = self.cal_cost(X, y, theta)
                
        theta_history = np.array(theta_history)
        J_history = [self.cal_cost(X, y, theta_history[0])] + J_history
        fig = plt.figure(figsize=(9, 6))
        axes = fig.add_subplot(111, projection='3d')
        axes.set_xlabel('\nTheta 0')
        axes.set_ylabel('\nTheta 1')
        axes.set_zlabel('\nJ(theta)')
        axes.set_title('3D Error Function')
        sur = axes.plot_surface(TH_0, TH_1, J, cmap='viridis', edgecolor='none', alpha=0.5)
        fig.colorbar(sur, ax=axes, shrink=0.5, aspect=10)
        # axes.plot(theta_history[:,0], theta_history[:,1], J_history, color='red', marker='o', markersize=5, label='Gradient Descent Path', alpha=1.0, linewidth=2)
        try: 
            for i in range(len(theta_history)):
                if not plt.fignum_exists(fig.number):
                    # print("Plot closed")
                    break
                axes.scatter(theta_history[i, 0], theta_history[i, 1], J_history[i], color='red', marker='o', s=10, label='Gradient Descent Path')
                plt.pause(0.2)
        except:
            # print("Error in plotting"
            pass
        # print("data points plotted in 3d surface")
        plt.show()
        
    
    def plot_contour(self, X, y, theta_history):
        "Plot contours for cost function"
        mean1 = np.mean(theta_history[:,0])
        mean2 = np.mean(theta_history[:,1])
        th_0 = np.linspace(np.min(theta_history[:, 0]) - (3*mean1), np.max(theta_history[:, 0]) + (3*mean1), 100)
        th_1 = np.linspace(np.min(theta_history[:, 1]) - (3*mean2), np.max(theta_history[:, 1]) + (3*mean2), 100)
        X = np.c_[np.ones(X.shape[0]), X]
        TH_0, TH_1 = np.meshgrid(th_0, th_1)
        J_theta = np.zeros(TH_0.shape)
        for i, theta0 in enumerate(th_0):
            for j, theta1 in enumerate(th_1):
                J_theta[i,j] = self.cal_cost(X, y, np.array([theta0, theta1]))
        fig = plt.figure(figsize=(10, 7))
        plt.contour(TH_0, TH_1, J_theta, levels=np.logspace(-2, 3, 20), cmap='coolwarm')
        # plt.plot(theta_history[:,0], theta_history[:,1], color='blue', marker='o', markersize=4)
        plt.xlabel('Theta 0')
        plt.ylabel('Theta 1')
        plt.title('Contour Plot of Cost Function')
        
        try: 
            for theta in theta_history:
                if(not plt.fignum_exists(fig.number)):
                    # print("Plot closed")
                    break
                plt.scatter(theta[0], theta[1], color='blue', s=10)
                plt.pause(0.2)
        except:
            # print("Error in plotting contours")
            pass
        # print("contours plotted")
        plt.show()
        
        
# Load dataset
dir = os.path.dirname(os.path.abspath(__file__))
x_path = os.path.join(dir, '../data/Q1/linearX.csv')
y_path = os.path.join(dir, '../data/Q1/linearY.csv')
X = pd.read_csv(x_path, header=None).values
y = pd.read_csv(y_path, header=None).values.flatten()
regressor = LinearRegressor()
theta_history = regressor.fit(X, y)
# print(f"Final Parameter: {regressor.theta}")
# print(f"Cost: {regressor.J_history[-1]}")
regressor.plot_hypo(X, y)
regressor.plot_error_3d(X, y, theta_history=theta_history, J_history=regressor.J_history)
regressor.plot_contour(X, y, theta_history)

learning_rate = [0.001, 0.025, 0.1]
for rate in learning_rate:
    regressor = LinearRegressor()
    theta_history = regressor.fit(X, y, learning_rate=rate)
    regressor.plot_contour(X, y, theta_history)



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
<A NAME="7"></A><FONT color = #0000FF><A HREF="match28-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    np.random.seed(97)
    X = np.random.normal(input_mean, input_sigma, (N, 2))
</FONT>    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch_size = [1,80,8000,800000]
        self.max_epochs = 30000
        
    def set_batch_size(self, batch_size):
        self.batch_size = batch_size
    
    def set_max_epochs(self, max_epochs):
        self.max_epochs = max_epochs
        
    def cal_theta(self, X, y, learning_rate, theta, size):
        h = np.dot(X, theta)
        error = (h - y)
        gradient = (np.dot(X.T, error))/size
        theta = theta - (gradient * learning_rate)
        return theta
    
    def cal_cost(self, X, y, theta):
        h = np.dot(X, theta)
        error = (h - y)
        m = X.shape[0]
        return np.sum(error**2)/(2*m)
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        permutation = np.random.permutation(m)
        X = X[permutation]
        y = y[permutation]
        X = np.c_[np.ones((m, 1)), X]
        thetas = {}
        delta = 1e-6
        for batch in self.batch_size:
            self.theta = np.zeros(n+1)
            theta_history = [self.theta.copy()]
            prev_cost = float('inf')
            # print(f"Batch Size: {batch}")
            start_time = time.time()
            for epoch in range(self.max_epochs):
                avg_cost = 0
                cost = self.cal_cost(X, y, self.theta)
                for i in range(0, m, batch):
                    avg_cost += self.cal_cost(X[i:i+batch], y[i:i+batch], self.theta)
                    self.theta = self.cal_theta(X[i:i+batch], y[i:i+batch], learning_rate, self.theta, batch)
                    theta_history.append(self.theta.copy())
                avg_cost /= (m//batch)
                if epoch&gt;0 and abs(prev_cost - avg_cost) &lt; delta:
                    # print(f"Theta {self.theta}")
                    # print(f"Converged at epoch {epoch}")
                    break
                prev_cost = avg_cost
            time_taken = time.time() - start_time
            minutes, second = divmod(time_taken, 60)
            # print(f"Time taken for batch size {batch}: {minutes}m{int(second)}sec")
            thetas[batch] = theta_history

        return thetas
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.c_[np.ones((m, 1)), X]
        return np.dot(X, self.theta)
    
    def closed_form(self, X, y):
        X = np.c_[np.ones((X.shape[0], 1)), X]
        return np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)
    
    def mse(self, X, y):
        y_pred = self.predict(X)
        return np.mean((y_pred - y)**2)
    
    def plot_batches(self, history):
        for batch in self.batch_size:
            fig = plt.figure(figsize=(10, 6))
            axes = fig.add_subplot(111, projection='3d')
            theta_history = np.array(history[batch])
            axes.plot(theta_history[:,0], theta_history[:,1], theta_history[:,2], marker='x', color='r')
            axes.set_xlabel('\nTheta0')
            axes.set_ylabel('\nTheta1')
            axes.set_zlabel('\nTheta2')
            axes.set_title('Trajectory of Theta values, Batch Size = {}'.format(batch))
            plt.show()
        
# Solution 1
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match28-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X,y = generate(1000000, [3,1,2], [3,-1], [2,2], np.sqrt(2))
data_margin = int(0.8*X.shape[0])
</FONT>x_train, x_test  = X[:data_margin], X[data_margin:]
y_train, y_test = y[:data_margin], y[data_margin:]
# print('x_train shape: ', x_train.shape)
# print('x_test shape: ', x_test.shape)
# print('y_train shape: ', y_train.shape)
# print('y_test shape: ', y_test.shape)


# Solution 2
regressor = StochasticLinearRegressor()
thetas_history = regressor.fit(x_train, y_train)
# print("Theta: ", regressor.theta)

# Solution 3
closed_form_sol = regressor.closed_form(x_train, y_train)
# print("Closed Form Solution: ", closed_form_sol)

# Solution 4
train_error = regressor.mse(x_train, y_train)
test_error = regressor.mse(x_test, y_test)
# print("Train Error: ", train_error)
# print("Test Error: ", test_error)

# Solution 5
regressor.plot_batches(thetas_history)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match28-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.theta = None
        
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def log_likelihood(self, X, y, theta):
        z = np.dot(X, theta)
</FONT><A NAME="5"></A><FONT color = #FF0000><A HREF="match28-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        h = self.sigmoid(z)
        return np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    
    def hesse_matrix(self, X, theta):
        z = np.dot(X, theta)
</FONT>        h = self.sigmoid(z)
        return np.dot(X.T, np.dot(np.diag(h*(1 - h)), X))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match28-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Newton's Method.
        """
        m,n = X.shape
        X = np.c_[np.ones((m, 1)), X]
</FONT>        self.theta = np.zeros(n+1)
        theta_history = [self.theta.copy()]
        iter = 0
        delta = 1e-6
        prev_ll = self.log_likelihood(X, y, self.theta)
        while True:
            gradient = np.dot(X.T, self.sigmoid(np.dot(X, self.theta)) - y)
            hessian = self.hesse_matrix(X, self.theta)
            self.theta = self.theta - np.dot(np.linalg.inv(hessian), gradient)
            theta_history.append(self.theta.copy())
            curr_ll = self.log_likelihood(X, y, self.theta)
            if iter &gt; 0 and np.linalg.norm(prev_ll - curr_ll) &lt; delta:
                # print(f"Converged at iteration {iter}")
                break
            prev_ll = curr_ll
            iter += 1
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = np.c_[np.ones((X.shape[0], 1)), X]
        prob = self.sigmoid(np.dot(X, self.theta))
        prob = [1 if p &gt;= 0.5 else 0 for p in prob]
        return np.array(prob)
    
    def plot_boundary(self, X, y):
        plt.figure(figsize=(10, 6))
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='0', marker='x')
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='1', marker='o')
        x1_min, x1_max = X[:, 0].min() - 1, X[:,0].max() + 1
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match28-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x2_min, x2_max = X[:, 1].min()-1, X[:,1].max()+1
        x1_vals, x2_vals = np.meshgrid(np.arange(x1_min, x1_max, 0.01), np.arange(x2_min, x2_max, 0.01))
        J = self.sigmoid(self.theta[0] + self.theta[1]*x1_vals + self.theta[2]*x2_vals)
</FONT><A NAME="6"></A><FONT color = #00FF00><A HREF="match28-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.contour(x1_vals, x2_vals, J, levels=[0.5], colors='green')
        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.title('Decision Boundary')
        plt.legend()
        plt.grid(True)
</FONT>        plt.show()
        
# Load Data
dir = os.path.dirname(os.path.abspath(__file__))
x_path = os.path.join(dir, '../data/Q3/logisticX.csv')
y_path = os.path.join(dir, '../data/Q3/logisticY.csv')
X = pd.read_csv(x_path,header= None).values
y = pd.read_csv(y_path,header= None).values
<A NAME="10"></A><FONT color = #FF0000><A HREF="match28-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y = y.flatten()
mean = np.mean(X, axis=0)
std = np.std(X, axis=0)
X = (X - mean) / std
</FONT>
regressor = LogisticRegressor()
# Question 1 
history = regressor.fit(X, y)
# print(f"Theta history: {history}")

# Question 2
regressor.plot_boundary(X, y)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
# from scipy.stats import multivariate_normal

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.phi = None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        m, n = X.shape
<A NAME="2"></A><FONT color = #0000FF><A HREF="match28-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x0 = X[y==0]
        x1 = X[y==1]
        self.phi = np.mean(y)
        self.mu0 = np.mean(x0, axis=0)
        self.mu1 = np.mean(x1, axis=0)
        
        if assume_same_covariance:
</FONT>            diff0 = x0 - self.mu0
            diff1 = x1 - self.mu1
            self.sigma = (diff0.T @ diff0 + diff1.T @ diff1) / m
            # self.sigma = np.cov(X.T, bias=True)
            return self.mu0, self.mu1, self.sigma
        else:
            self.sigma0 = np.cov(x0.T, bias=True)
            self.sigma1 = np.cov(x1.T, bias=True)
            return self.mu0, self.mu1, self.sigma0, self.sigma1
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        inv_sigma = np.linalg.inv(self.sigma)
        log_odds = (X @ inv_sigma @ (self.mu1 - self.mu0).T 
                    - 0.5 * self.mu1.T @ inv_sigma @ self.mu1
                    + 0.5 * self.mu0.T @ inv_sigma @ self.mu0
                    + np.log(self.phi / (1 - self.phi)))
        log_odds = [1 if i &gt; 0 else 0 for i in log_odds]
        return np.array(log_odds)
    
    def plot_data(self, X, y):
        plt.figure(figsize=(10, 6))
<A NAME="0"></A><FONT color = #FF0000><A HREF="match28-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Canada', alpha=0.5)
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Alaska', alpha=0.5)
        self.plot_gda_linear(X, y)
</FONT>        self.plot_gda_quadratic(X, y)
        plt.xlabel('X1')
        plt.ylabel('X2')
        plt.title('Gaussian Discriminant Analysis')
        plt.legend()
        plt.title('Alaska vs Canada')
        plt.show()
        
    def plot_gda_linear(self, X, y):
        sig_inv = np.linalg.inv(self.sigma)
        intercept = 0.5 * (self.mu0.T @ sig_inv @ self.mu0 - self.mu1.T @ sig_inv @ self.mu1) + np.log(self.phi / (1 - self.phi))
        mu_diff = self.mu1 - self.mu0
        slope = mu_diff @ sig_inv
        x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
        x2_vals = -1 * (intercept + slope[0] * x1_vals) / slope[1]
        plt.plot(x1_vals, x2_vals, 'k--', label='Linear Decision Boundary')
    
    def plot_gda_quadratic(self, X, y):
        x1_min, x1_max = X[:, 0].min() - 1, X[:,0].max() + 1
        x2_min, x2_max = X[:, 1].min()-1, X[:,1].max()+1
        x1_vals, x2_vals = np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100)
        x1_mesh, x2_mesh = np.meshgrid(x1_vals, x2_vals)
        Z = np.zeros(x1_mesh.shape)
        
        sig_inv0 = np.linalg.inv(self.sigma0)
        sig_inv1 = np.linalg.inv(self.sigma1)
        
        for i in range(x1_mesh.shape[0]):
            for j in range(x1_mesh.shape[1]):
                x = np.array([x1_mesh[i, j], x2_mesh[i, j]])
                p_0 = -0.5 * ((x - self.mu0).T @ sig_inv0 @ (x - self.mu0)) - 0.5 * np.log(np.linalg.det(self.sigma0))
                p_1 = -0.5 * ((x - self.mu1).T @ sig_inv1 @ (x - self.mu1)) - 0.5 * np.log(np.linalg.det(self.sigma1))
                Z[i, j] = p_1 - p_0 + np.log(self.phi / (1 - self.phi))
        
        plt.contour(x1_mesh, x2_mesh, Z, levels=[0], colors='green')
        
        
# Load Data
dir = os.path.dirname(os.path.abspath(__file__))
x_path = os.path.join(dir, '../data/Q4/q4x.dat')
y_path = os.path.join(dir, '../data/Q4/q4y.dat')
<A NAME="1"></A><FONT color = #00FF00><A HREF="match28-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = pd.read_csv(x_path ,sep='\\s+', header=None)
y = pd.read_csv(y_path ,sep='\\s+', header=None)
X = np.array(X)
y = np.array(y)
y = np.where(y == 'Alaska', 1, 0)
y = y.reshape((y.shape[0],))
</FONT>mean = np.mean(X, axis=0)
std = np.std(X, axis=0)
X = (X - mean) / std

gda = GaussianDiscriminantAnalysis()
res1 = gda.fit(X, y, True)
# print("Parameters 1: ", res1)
# print("Predicted output: ",gda.predict(X))
res2 = gda.fit(X, y, False)
# print("Parameters 2: ", res2)
gda.plot_data(X, y)


</PRE>
</PRE>
</BODY>
</HTML>
