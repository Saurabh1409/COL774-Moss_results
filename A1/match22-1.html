<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_UHU23.py<p><PRE>


#Data points and the predicted line

from linear_regression import LinearRegressor
import numpy as np
import os

# Initialize the Linear Regressor
model = LinearRegressor()

# Load the data
X = np.loadtxt("data/Q1/linearX.csv", delimiter=',')
y = np.loadtxt("data/Q1/linearY.csv", delimiter=',')

print("Data loaded successfully!")

# Fit the model
learning_rate = 0.01
model.fit(X, y, learning_rate=learning_rate)

# Predict values (for demonstration, we predict on the same training data)
y_pred = model.predict(X)
y_pred = y_pred.tolist()
# Print predicted values
print("Predicted Values:", y_pred)

import matplotlib.pyplot as plt

# Ensure the Q1 directory exists
os.makedirs("Q1", exist_ok=True)

<A NAME="8"></A><FONT color = #00FFFF><A HREF="match22-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X, y_pred, color='red', label='Fitted Line')
plt.xlabel('X (Feature)')
plt.ylabel('y (Target)')
plt.title('Linear Regression Fit')
</FONT>plt.legend()

# Save the plot as an image
plt.savefig("Q1/linear_regression_plot.png", dpi=300)



# 3D visualisation of points and the error function

from linear_regression import LinearRegressor
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Initialize the Linear Regressor
model = LinearRegressor()

# Load the data
X = np.loadtxt("data/Q1/linearX.csv", delimiter=',')
y = np.loadtxt("data/Q1/linearY.csv", delimiter=',')

# Verify normalization
print(f"X mean: {np.mean(X):.2f}, X std: {np.std(X):.2f}")

# Initialize variables for plotting
theta_history = []  # To store theta values at each iteration
error_history = []  # To store error values at each iteration

# Modify the fit method to store theta and error history
def fit_with_visualization(self, X, y, learning_rate=0.01):
    m = len(X)
    theta = np.array([0.0, 0.0])  # Initial parameters [theta_0, theta_1]
    new_theta = np.array([1.0, 1.0])  # To track parameter updates
    iter_count = 1

    while True:
        sum = np.zeros(2)
        for i in range(m):
            sum[0] += (y[i] - (theta[0] + theta[1] * X[i]))
            sum[1] += (y[i] - (theta[0] + theta[1] * X[i])) * X[i]
        sum /= m

        new_theta[0] = theta[0] + learning_rate * sum[0]
        new_theta[1] = theta[1] + learning_rate * sum[1]

        # Save history for visualization
        theta_history.append(new_theta.copy())
        error_history.append(np.sum((y - (new_theta[0] + new_theta[1] * X)) ** 2) / (2 * m))
        # Check for convergence
        if self.has_converged(X,y,theta, new_theta, tol=1e-6):
            break

        theta = new_theta.copy()
        iter_count += 1

    self.theta = new_theta
    return new_theta

# Replace the original fit method
LinearRegressor.fit = fit_with_visualization

# Fit the model
learning_rate = 0.01
model.fit(X, y, learning_rate=learning_rate)

# Create a mesh grid for the error function
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match22-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta_0_vals = np.linspace(-10, 20, 100)  # Centered around ideal theta_0
theta_1_vals = np.linspace(10, 50, 100)  # Centered around ideal theta_1
J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

# Compute the error function J(theta) for each pair of (theta_0, theta_1)
for i, theta_0 in enumerate(theta_0_vals):
    for j, theta_1 in enumerate(theta_1_vals):
</FONT>        predictions = theta_0 + theta_1 * X  # Linear model
        J_vals[i, j] = np.sum((y - predictions) ** 2) / (2 * len(y))

# Plot the 3D mesh and gradient descent steps
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)

# Surface plot of the error function
surf = ax.plot_surface(theta_0_vals, theta_1_vals, J_vals.T, cmap='viridis', alpha=0.8)

# Gradient descent path
theta_history = np.array(theta_history)
error_history = np.array(error_history)

# Plot gradient descent steps
for i in range(len(theta_history)):
    ax.scatter(theta_history[i, 0], theta_history[i, 1], error_history[i], color='red', s=20)
    plt.pause(0.2)  # Pause to visualize the updates

# Final plot details
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('Error Function with Gradient Descent Path')
plt.show()




# 2d visualization of the error function and gradient descent steps, along contours of the error function

from linear_regression import LinearRegressor
import numpy as np
import matplotlib.pyplot as plt
import time

# Initialize the Linear Regressor
model = LinearRegressor()

# Load the data
X = np.loadtxt("data/Q1/linearX.csv", delimiter=',')
y = np.loadtxt("data/Q1/linearY.csv", delimiter=',')

# Verify normalization
print(f"X mean: {np.mean(X):.2f}, X std: {np.std(X):.2f}")

# Initialize variables for plotting
theta_history = []  # To store theta values at each iteration
error_history = []  # To store error values at each iteration

# Modify the fit method to store theta and error history
def fit_with_visualization(self, X, y, learning_rate):
    m = len(X)
    theta = np.array([0.0, 0.0])  # Initial parameters [theta_0, theta_1]
    new_theta = np.array([1.0, 1.0])  # To track parameter updates
    iter_count = 1

    while True:
        gradient = np.zeros(2)
        for i in range(m):
            error = y[i] - (theta[0] + theta[1] * X[i])
            gradient[0] += error
            gradient[1] += error * X[i]
        gradient /= m

        new_theta[0] = theta[0] + learning_rate * gradient[0]
        new_theta[1] = theta[1] + learning_rate * gradient[1]

        # Save history for visualization
        theta_history.append(new_theta.copy())
        error_history.append(np.sum((y - (new_theta[0] + new_theta[1] * X)) ** 2) / (2 * m))

        # Check for convergence
        if self.has_converged(X,y,theta, new_theta, tol=1e-6):
            break

        theta = new_theta.copy()
        iter_count += 1

    self.theta = new_theta
    return new_theta

# Replace the original fit method
LinearRegressor.fit = fit_with_visualization

# Fit the model
learning_rate = 0.001
model.fit(X, y, learning_rate=learning_rate)

# Create a mesh grid for the error function
theta_0_vals = np.linspace(-10, 20, 100)  # Centered around ideal theta_0
<A NAME="7"></A><FONT color = #0000FF><A HREF="match22-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta_1_vals = np.linspace(0, 50, 100)  # Centered around ideal theta_1
J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

# Compute the error function J(theta) for each pair of (theta_0, theta_1)
for i, theta_0 in enumerate(theta_0_vals):
    for j, theta_1 in enumerate(theta_1_vals):
</FONT>        predictions = theta_0 + theta_1 * X  # Linear model
        J_vals[i, j] = np.sum((y - predictions) ** 2) / (2 * len(y))

# Plot the contours of the error function and gradient descent steps
fig, ax = plt.subplots(figsize=(8, 6))

# Create contour plot for the error function
theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
cp = ax.contourf(theta_0_vals, theta_1_vals, J_vals.T, levels=50, cmap='viridis')

# Gradient descent path
theta_history = np.array(theta_history)
error_history = np.array(error_history)

# Plot gradient descent steps
for i in range(len(theta_history)):
    ax.scatter(theta_history[i, 0], theta_history[i, 1], color='red', s=50)
    plt.pause(0.2)  # Pause to visualize the updates

# Final plot details
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_title('Contours of $J(\\theta)$ with Gradient Descent Path')
fig.colorbar(cp)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match22-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()




# Final 2d plot(static) of the error function and gradient descent steps

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Import the Linear Regression class
from linear_regression import LinearRegressor

# Load the data
X = np.loadtxt("data/Q1/linearX.csv", delimiter=',')
y = np.loadtxt("data/Q1/linearY.csv", delimiter=',')
</FONT>
# Initialize the model
model = LinearRegressor()
learning_rate = 0.01

# Fit the model and get the history of theta values
theta_history = model.fit(X, y, learning_rate=learning_rate)
theta_optimal=theta_history[-1]
print("Optimal theta values:", theta_optimal)
print("Theta history :", theta_history)

# Now, create the mesh grid and error function J(theta)
theta_0_vals = np.linspace(-10, 20, 100)
theta_1_vals = np.linspace(0, 50, 100)
J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

# Calculate the error function J(theta) for each combination of theta_0 and theta_1
for i, theta_0 in enumerate(theta_0_vals):
    for j, theta_1 in enumerate(theta_1_vals):
        predictions = theta_0 + theta_1 * X
        J_vals[i, j] = np.sum((y - predictions) ** 2) / (2 * len(y))

# Create the contour plot for the error function
fig, ax = plt.subplots(figsize=(8, 6))

# Create the filled contour plot of J(theta)
theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
cp = ax.contourf(theta_0_vals, theta_1_vals, J_vals.T, levels=50, cmap='viridis')

# Plot the final gradient descent path (no pauses, just the final positions)
<A NAME="10"></A><FONT color = #FF0000><A HREF="match22-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta_history = np.array(theta_history)

# Plot the red dots for each gradient descent step
ax.scatter(theta_history[:, 0], theta_history[:, 1], color='red', s=5)

# Add labels and title
ax.set_xlabel(r'$\theta_0$')
</FONT>ax.set_ylabel(r'$\theta_1$')
ax.set_title(f'Contours of $J(\\theta)$ with Gradient Descent Path, learning rate={learning_rate}')

# Show the colorbar to visualize the error values
fig.colorbar(cp)

# Finally, display the plot
plt.show()




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def compute_cost(self, X, y, theta):
        """
        Compute the cost function J(theta) for given theta.
        """
        m = len(y)
        predictions = X * theta[1] + theta[0]  # Linear hypothesis
        error = predictions - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost

    def has_converged(self, X, y, theta_old, theta_new, tol=1e-3):
        """
        True if the gradient descent has converged, False otherwise.
        """
        # error= 0
        # for i in range(len(theta_new)):
        #     error += (theta_new[i]-theta_old[i])**2
        # print("Error: ",error)
        # if error&lt;tol:
        #     print("Converged")
        #     print("Theta: ",theta_new)       
        # return error&lt;tol
        old_cost=self.compute_cost(X,y,theta_old)
        new_cost=self.compute_cost(X,y,theta_new)
        return abs(new_cost-old_cost)&lt;tol

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        print("Here!")
        theta_history=[]
        iter=1
        m =len(X)
        theta=[0,0]
        new_theta=[1,1]
        while (True):
            sum0=0
            sum1=0
            for i in range(m):
                sum0 += (y[i]-(X[i]*theta[1]+1*theta[0]))*1
                sum1 += (y[i]-(X[i]*theta[1]+1*theta[0]))*X[i]
            sum0/=m
            sum1/=m
        
            new_theta[0]=theta[0]+learning_rate*sum0
            new_theta[1]=theta[1]+learning_rate*sum1
            theta_history.append(np.array(new_theta))
            iter+=1
            print("Iteration: ",iter)
            if self.has_converged(X,y,theta,new_theta,1e-6):
                break
            
            theta=new_theta.copy()
        self.theta=theta
        return theta_history
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        predictions=[]
        for i in range(len(X)):
            predictions.append(X[i]*self.theta[1]+1*self.theta[0])
        return np.array(predictions)



import numpy as np
from sampling_sgd import generate,closed_form_solution, StochasticLinearRegressor
import unittest
import math

def test_data_generation():
    # Test for generating synthetic data
    N = 1000000
    theta_true = np.array([3, 1, 2])  # True parameters [θ0, θ1, θ2]
    input_mean = np.array([3, -1])    # Mean for x1 and x2
    input_sigma = np.array([2, 2])    # Standard deviation for x1 and x2
    noise_sigma = math.sqrt(2)                   # Noise variance
    
    # Generate the data
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

    # Shuffle the data
    data = np.hstack((X, y.reshape(-1, 1)))  # Combine X and y for shuffling
    np.random.shuffle(data)                  # Shuffle the rows randomly

    # Split into train and test sets
    train_size = int(0.8 * len(data))        # 80% training data
    train_data = data[:train_size]
    test_data = data[train_size:]

    # Save to files
    np.savetxt("train_data.csv", train_data, delimiter=',', header="x1,x2,y", comments='')
    np.savetxt("test_data.csv", test_data, delimiter=',', header="x1,x2,y", comments='')

    print("Data has been generated, shuffled, and split into train and test sets.")

    # Check if the generated data is of the correct shape
    
    # assertEqual(X.shape, (N, 2))
    # assertEqual(y.shape, (N,))

    # Check if the target values roughly follow the true model with added noise
    predictions = X @ theta_true[1:] + theta_true[0]
    error = np.mean((y - predictions)**2)
    print("Error: ",error)
    print("Generated data !!!_")

def test_sgd_convergence():
    # Test Stochastic Gradient Descent with different batch sizes

    theta_true = np.array([3, 1, 2])  # True theta for comparison
    
    # Read data from the train.csv file
    train_data = np.loadtxt("train_data.csv", delimiter=",", skiprows=1)
    X = train_data[:, :-1]  # Extract input features (x1, x2)
    y = train_data[:, -1]   # Extract target values (y)

    # Initialize the model
    model = StochasticLinearRegressor()
    
    # batch_sizes = [1,80,8000,800000]

    history = model.fit(X, y, learning_rate=0.001)

    print("Done!!!")
    # Test SGD for different batch sizes
    # for batch_size in batch_sizes:
    #     print(f"\nTesting SGD with batch size: {batch_size}")

    #     # Fit the model using the training data
    #     history = model.fit(X, y, learning_rate=0.001, batch_size=batch_size, tol=1e-5)

    #     # Check if the model converged (the learned theta should be close to the true theta)
    #     learned_theta = model.theta
    #     print(f"Learned theta: {learned_theta}")

        # Assert that learned parameters are close to the true ones
        # self.assertTrue(np.allclose(learned_theta, theta_true, atol=0.5), f"Failed for batch size {batch_size}")

# def test_predict(self):
#     # Test prediction after training the model
#     N = 10000  # Smaller size for test purposes
#     theta_true = np.array([3, 1, 2])
#     input_mean = np.array([3, -1])
#     input_sigma = np.array([2, 2])
#     noise_sigma = 2
    
#     # Generate the data
#     X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

#     # Initialize the model
#     model = StochasticLinearRegressor()

#     # Train the model
#     model.fit(X, y, learning_rate=0.001, batch_size=80, tol=1e-6)

#     # Make predictions
#     X_test = np.random.randn(10, 2)  # Some random test data
#     y_pred = model.predict(X_test)

#     # Check if the predictions are made
#     self.assertEqual(y_pred.shape, (10,))
#     print(f"Predictions: {y_pred}")


# def test_large_data(self):
#     # Test large dataset with 1 million points
#     N = 1000000
#     theta_true = np.array([3, 1, 2])
#     input_mean = np.array([3, -1])
#     input_sigma = np.array([2, 2])
#     noise_sigma = 2
    
#     # Generate the data
#     X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

#     # Initialize the model
#     model = StochasticLinearRegressor()

#     # Fit the model using large data
#     model.fit(X, y, learning_rate=0.001, batch_size=8000, tol=1e-6)

#     # Check if the learned parameters are close to the true ones
#     learned_theta = model.theta
#     print(f"Learned theta: {learned_theta}")
#     # Assert that learned parameters are close to the true ones
#     self.assertTrue(np.allclose(learned_theta, theta_true, atol=0.5))

if __name__ == "__main__":    
    # Test SGD convergence
    # test_data_generation()
    
    # test_sgd_convergence()
    closed_form_theta = closed_form_solution()




import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor, generate
<A NAME="11"></A><FONT color = #00FF00><A HREF="match22-0.html#11" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import math

# Data Generation Parameters
N = 1000000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
</FONT>noise_sigma = math.sqrt(2)

# Generate dataset
X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Shuffle dataset
data = np.hstack((X, y.reshape(-1, 1)))
np.random.shuffle(data)

# Split into training (80%) and testing (20%)
train_size = int(0.8 * len(data))
train_data = data[:train_size]
test_data = data[train_size:]

# Save to CSV
np.savetxt("train_data.csv", train_data, delimiter=',', header="x1,x2,y", comments='')
np.savetxt("test_data.csv", test_data, delimiter=',', header="x1,x2,y", comments='')

print("Data has been generated, shuffled, and split into train and test sets.")

# Extract train set for model training
X_train = train_data[:, :2]
y_train = train_data[:, 2]

# Initialize model
model = StochasticLinearRegressor()

# Batch sizes to test
batch_sizes = [1, 80, 8000, 800000]

for batch_size in batch_sizes:
    print(f"Training with batch_size={batch_size}")
    
    history = model.fit(X_train, y_train, learning_rate=0.001, batch_size=batch_size)

    # Convert history to numpy array
    history_np = np.array(history)

    # 3D Plot
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    # Scatter plot with smaller dots
    ax.scatter(history_np[:, 0], history_np[:, 1], history_np[:, 2], c='r', marker='.', s=10, label='SGD Steps')

    # Line plot to connect points
    ax.plot(history_np[:, 0], history_np[:, 1], history_np[:, 2], color='blue', linewidth=1)

    ax.set_xlabel("Theta 0 (Bias)")
    ax.set_ylabel("Theta 1 (Weight 1)")
    ax.set_zlabel("Theta 2 (Weight 2)")
    ax.set_title(f"SGD Parameter Movement (batch_size={batch_size})")
    ax.legend()

    # Save plot
    plt.savefig(f"sgd_movement_batch_{batch_size}.png")
    plt.close()

print("All plots saved successfully!")




import numpy as np
from sampling_sgd import StochasticLinearRegressor, generate

# Load training and test data
train_data = np.loadtxt("train_data.csv", delimiter=',', skiprows=1)
test_data = np.loadtxt("test_data.csv", delimiter=',', skiprows=1)

# Extract features and labels
X_train, y_train = train_data[:, :2], train_data[:, 2]
X_test, y_test = test_data[:, :2], test_data[:, 2]

# Initialize the model
model = StochasticLinearRegressor()

# Train the model on training data
print("Training the model...")
model.fit(X_train, y_train, learning_rate=0.001)

# Add bias term to the feature matrices
X_train_bias = model.add_bias_term(X_train)
X_test_bias = model.add_bias_term(X_test)

# Predict on training data using the final theta values from training
train_predictions = [
    np.dot(X_train_bias, model.theta1),
    np.dot(X_train_bias, model.theta2),
    np.dot(X_train_bias, model.theta3),
    np.dot(X_train_bias, model.theta4),
]

# Predict on test data using the final theta values from training
test_predictions = [
    np.dot(X_test_bias, model.theta1),
    np.dot(X_test_bias, model.theta2),
    np.dot(X_test_bias, model.theta3),
    np.dot(X_test_bias, model.theta4),
]

# Compute MSE for training & test data
train_mse = 0.5*np.mean((train_predictions - y_train) ** 2, axis=1)
test_mse = 0.5*np.mean((test_predictions - y_test) ** 2, axis=1)

# Print results
batch_sizes = [1, 80, 8000, 800000]
for i, batch_size in enumerate(batch_sizes):
    print(f"Batch Size: {batch_size}")
    print(f"Training MSE: {train_mse[i]}")
    print(f"Test MSE: {test_mse[i]}")
    print("-" * 40)




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    # Generate x1 and x2 from normal distributions
    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)
    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)

    # Combine x1 and x2 into a single input matrix
    X = np.column_stack((x1, x2))
    
    # Add Gaussian noise to the output
<A NAME="12"></A><FONT color = #0000FF><A HREF="match22-0.html#12" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    
    # Calculate the target values
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
</FONT>    print("Generated data !!!")
    
    return X, y


def closed_form_solution():
    # Read data from the train.csv file
    train_data = np.loadtxt("train_data.csv", delimiter=",", skiprows=1)
    X = train_data[:, :-1]  # Extract input features (x1, x2)
    y = train_data[:, -1]   # Extract target values (y)

    # Step 1: Add a column of ones to X for the intercept term (bias)
    ones_column = np.ones((X.shape[0], 1))
    X_with_bias = np.hstack((ones_column, X))

    # Step 2: Compute the transpose of X_with_bias
    X_transpose = X_with_bias.T

    # Step 3: Compute the product of X_transpose and X_with_bias
    XTX = np.dot(X_transpose, X_with_bias)

    # Step 4: Compute the inverse of (X_transpose @ X_with_bias)
    XTX_inv = np.linalg.inv(XTX)

    # Step 5: Compute the product of X_transpose and y
    XTy = np.dot(X_transpose, y)

    # Step 6: Compute the final theta values
    theta_closed_form = np.dot(XTX_inv, XTy)

    print(f"Closed-form solution: {theta_closed_form}")

    return theta_closed_form

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta1 = None
        self.theta2 = None
        self.theta3 = None
        self.theta4 = None
        
    def compute_cost(self, X, y, theta):
        """
        Compute Mean Squared Error (MSE) cost function.
        """
        m = len(y)
        predictions = X @ theta
        total_cost = np.sum((predictions - y) ** 2) / (2 * m)
        return total_cost

    def has_converged(self, X, y, theta_old, theta_new, tol=1e-6):
        """
        Check if the cost function difference is below tolerance.
        """
        old_cost = self.compute_cost(X, y, theta_old)
        new_cost = self.compute_cost(X, y, theta_new)
        # print(f"Cost Old: {old_cost}, Cost New: {new_cost}, Difference: {abs(new_cost - old_cost)}")
        return abs(new_cost - old_cost) &lt; tol

    def add_bias_term(self, X):
        """
        Add a bias term (column of ones) to the feature matrix.
        """
        return np.hstack((np.ones((X.shape[0], 1)), X))

    def _predict_raw(self, X):
        """
        Internal function to compute predictions given input features.
        """
        return np.dot(X, self.theta)

    def my_fit(self, X, y, learning_rate, batch_size, tol):
        """
        Train the model using Stochastic Gradient Descent (SGD).
        """
        m, n = X.shape  # Get dataset dimensions
        X = self.add_bias_term(X)
        max_epochs=50000

        # Initialize theta
        self.theta = np.zeros(n + 1)

        theta_old = self.theta.copy()
        history = []

        # Shuffle dataset **once**
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

        for epoch in range(max_epochs):
            for i in range(0, m, batch_size):  # Loop over batches
                history.append(self.theta.copy())  # Save history for debugging
                end = i + batch_size
                if end&gt;m:
                    end=m
                X_batch = X_shuffled[i:end]
                y_batch = y_shuffled[i:end]

                # Compute predictions and error
                predictions = self._predict_raw(X_batch)
                error = predictions - y_batch

                # Compute gradient and update parameters
                individual_gradients = X_batch.T @ error
                mean_gradient = individual_gradients / batch_size
                self.theta -= learning_rate * mean_gradient

            # Check convergence
            if self.has_converged(X, y, theta_old, self.theta, tol):
                print(f"Converged in {epoch+1} epochs for batch size {batch_size}.")
                print("Final theta:", self.theta)
                break

            theta_old = self.theta.copy()
        return history    

    def fit(self, X, y, learning_rate=0.001):
            batch_sizes = [1, 80, 8000, 800000]
            
            histories = []
            i=1
            for batch_size in batch_sizes:
                history = self.my_fit(X, y, learning_rate, batch_size, tol=1e-6)
                print("Batch size: ",batch_size)
                print("Theta: ",self.theta)
                if (i==1):
                    self.theta1 = self.theta
                elif (i==2):
                    self.theta2 = self.theta
                elif (i==3):
                    self.theta3 = self.theta
                elif (i==4):
                    self.theta4 = self.theta
                i+=1
                histories.append(np.array(history))
            # print(self.theta1,"|",self.theta2,"|",self.theta3,"|",self.theta4)
            return histories

    def predict(self, X):
        """
        Predict using the trained model.
        """
        X = self.add_bias_term(X)
        self.theta=self.theta1.copy()
        a1=self._predict_raw(X)
        self.theta=self.theta2.copy()
        a2=self._predict_raw(X)
        self.theta=self.theta3.copy()
        a3=self._predict_raw(X)
        self.theta=self.theta4.copy()
        a4=self._predict_raw(X)
        # print(a1,a2,a3,a4)
        return [a1,a2,a3,a4]
    




import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use a non-GUI backend
import matplotlib.pyplot as plt
import csv
<A NAME="6"></A><FONT color = #00FF00><A HREF="match22-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

from logistic_regression import LogisticRegressor

def load_csv(filename):
    data = []
    with open(filename, 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            data.append([float(x) for x in row])
</FONT>    return np.array(data)

def plot_decision_boundary(model, X, y):
    plt.figure(figsize=(8, 6))
    
    # Plot data points based on logisticY.csv
    class_0 = (y == 0)
    class_1 = (y == 1)
    plt.scatter(X[class_0][:, 0], X[class_0][:, 1], color='red', marker='o', label='Class 0')
    plt.scatter(X[class_1][:, 0], X[class_1][:, 1], color='blue', marker='x', label='Class 1')
    
    # Compute decision boundary line
    x1_min, x1_max = min(X[:, 0]), max(X[:, 0])
    x1_values = np.linspace(x1_min, x1_max, 100)
    
    intercept, theta1, theta2 = model.theta
    if abs(theta2) &gt; 1e-6:  # Prevent division by zero
        x2_values = -(intercept + theta1 * x1_values) / theta2
        plt.plot(x1_values, x2_values, color='green', label='Decision Boundary')
    
    # Plot settings
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.title('Logistic Regression Decision Boundary')
    plt.savefig('logistic_regression.png')

def main():
    X = load_csv('data/Q3/logisticX.csv')
    y = load_csv('data/Q3/logisticY.csv').flatten()
    
    model = LogisticRegressor()
    theta = model.fit(X, y)
    # theta = theta.tolist()
    
    print("Optimized Theta:", theta)
    
    plot_decision_boundary(model, X, y)

if __name__ == "__main__":
    main()



import unittest
import numpy as np
from autograder import main, load_csv
from logistic_regression import LogisticRegressor

class TestAutograder(unittest.TestCase):
    def test_main(self):
        # Run the main function to ensure it executes without errors
        main()
    
    def test_logistic_regressor_predict(self):
        # Load data
        X = load_csv('data/Q3/logisticX.csv')
        y = load_csv('data/Q3/logisticY.csv').flatten()
        
        # Train model
        model = LogisticRegressor()
        model.fit(X, y)
        
        # Predict
        predictions = model.predict(X)
        
        # Calculate accuracy
        accuracy = np.mean(predictions == y)
        print("Accuracy:", accuracy)
        
        # Check if accuracy is reasonable (e.g., above 70%)
        self.assertGreater(accuracy, 0.7)

if __name__ == "__main__":
    unittest.main()



import numpy as np

class LogisticRegressor:
    def __init__(self):
        self.theta = None  # Model parameters
    
    def sigmoid(self, z):
        """Computes the sigmoid function."""
        return 1 / (1 + np.exp(-z))
    
    def compute_mean_std(self, X):
        """Computes the mean and standard deviation for normalization."""
        mean = [sum(col) / len(col) for col in zip(*X)]
        std = [
            (sum((x - m) ** 2 for x in col) / len(col)) ** 0.5 if len(set(col)) &gt; 1 else 1 
            for col, m in zip(zip(*X), mean)
        ]
        return mean, std
    
    def normalize(self, X):
        """Normalizes the feature matrix X using mean and standard deviation."""
        mean, std = self.compute_mean_std(X)
        X_normalized = []
        for i in range(len(X)):
            row = []
            for j in range(len(X[i])):
                row.append((X[i][j] - mean[j]) / std[j])
            X_normalized.append(row)
        return X_normalized
    
    def add_bias_term(self, X):
        """Adds a bias (intercept) term to the feature matrix."""
        return [[1] + row for row in X]
    
    def compute_gradient(self, X, y, h):
        """Computes the gradient vector for Newton's method."""
        gradient = [0] * len(X[0])
        for j in range(len(X[0])):
            sum_error = 0
            for i in range(len(X)):
                sum_error += (h[i] - y[i]) * X[i][j]
            gradient[j] = sum_error / len(y)
        return gradient

    def compute_hessian(self, X, h):
        """Computes the Hessian matrix for Newton's method."""
        n_features = len(X[0])
        hessian = [[0] * n_features for _ in range(n_features)]
        for j1 in range(n_features):
            for j2 in range(n_features):
                sum_value = 0
                for i in range(len(X)):
                    sum_value += X[i][j1] * X[i][j2] * h[i] * (1 - h[i])
                hessian[j1][j2] = sum_value / len(X)
        return hessian

    def fit(self, X, y, learning_rate=0.01, tol=1e-6, max_iter=10000):
        """Fits the logistic regression model using Newton's method."""
        X = self.normalize(X)
        X = self.add_bias_term(X)
        n_features = len(X[0])
        self.theta = [0] * n_features  # Initialize parameters
        param_history = []
        
        for _ in range(max_iter):
            h = [self.sigmoid(sum(self.theta[j] * X[i][j] for j in range(n_features))) for i in range(len(X))]
            gradient = self.compute_gradient(X, y, h)
            hessian = self.compute_hessian(X, h)
            
            try:
                hessian_inv = np.linalg.inv(hessian)
            except np.linalg.LinAlgError:
                print("Hessian matrix is singular. Stopping optimization.")
                break
            
            delta = [0] * n_features
            for j in range(n_features):
                sum_value = 0
                for k in range(n_features):
                    sum_value += hessian_inv[j][k] * gradient[k]
                delta[j] = sum_value
            self.theta = [self.theta[j] - learning_rate * delta[j] for j in range(n_features)]
            param_history.append(self.theta[:])
            
            if sum(abs(delta[j]) for j in range(n_features)) &lt; tol:
                break
        param_history = [np.array(params) for params in param_history]
        return param_history
    
    def predict(self, X):
        """Predicts binary labels for input data."""
        X = self.normalize(X)
        X = self.add_bias_term(X)
        predictions = []
        for i in range(len(X)):
            prob = self.sigmoid(sum(self.theta[j] * X[i][j] for j in range(len(X[i]))))
            predictions.append(1 if prob &gt;= 0.5 else 0)
        predictions = np.array(predictions)
        return predictions



# autograder.py

import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load the data
<A NAME="1"></A><FONT color = #00FF00><A HREF="match22-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = np.loadtxt("data/Q4/q4x.dat")
y = np.loadtxt("data/Q4/q4y.dat", dtype=str)

# Convert labels to numerical values (Alaska -&gt; 0, Canada -&gt; 1)
y = np.where(y == "Alaska", 0, 1)

# Normalize the data
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Instantiate and train the GDA model (same covariance assumption)
gda = GaussianDiscriminantAnalysis()
</FONT>mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)

# Print learned parameters
print("Mean for Alaska (Class 0):", mu_0)
print("Mean for Canada (Class 1):", mu_1)
print("Shared Covariance Matrix (Sigma):\n", sigma)

# Plot the training data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Alaska")
<A NAME="5"></A><FONT color = #FF0000><A HREF="match22-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Canada")
plt.xlabel("Feature x1")
plt.ylabel("Feature x2")
plt.legend()
</FONT>plt.title("GDA: Training Data")

# Save plot
plt.savefig("gda_training_data.png")
plt.close()

# Compute and plot the linear decision boundary
<A NAME="0"></A><FONT color = #FF0000><A HREF="match22-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

sigma_inv = np.linalg.inv(sigma)
w = np.dot(sigma_inv, (mu_1 - mu_0))
b = -0.5 * np.dot(mu_1.T, np.dot(sigma_inv, mu_1)) + 0.5 * np.dot(mu_0.T, np.dot(sigma_inv, mu_0))

x_vals = np.linspace(-2, 2, 100)
</FONT><A NAME="2"></A><FONT color = #0000FF><A HREF="match22-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

y_vals = (-w[0] * x_vals - b) / w[1]

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Alaska")
</FONT>plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Canada")
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match22-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.plot(x_vals, y_vals, label="Linear Decision Boundary", color='red')
plt.xlabel("Feature x1")
plt.ylabel("Feature x2")
plt.legend()
plt.title("GDA: Linear Decision Boundary")

# Save plot
plt.savefig("gda_linear_boundary.png")
</FONT>plt.close()

# Train the GDA model (different covariance matrices)
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)

# Print parameters for separate covariance case
print("\nWith Different Covariance Matrices:")
print("Sigma_0 (Alaska):\n", sigma_0)
print("Sigma_1 (Canada):\n", sigma_1)

# Compute and plot the quadratic decision boundary
xx, yy = np.meshgrid(np.linspace(-2, 2, 500), np.linspace(-2, 2, 500))
grid_points = np.c_[xx.ravel(), yy.ravel()]

# Compute continuous decision boundary
decision_values = gda.predict(grid_points).astype(float).reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label="Alaska")
<A NAME="13"></A><FONT color = #00FFFF><A HREF="match22-0.html#13" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label="Canada")

# Use contourf for smooth shading
contour = plt.contour(xx, yy, decision_values, levels=[0.5], colors='blue', linewidths=2)
</FONT>plt.clabel(contour, fmt={0.5: 'Decision Boundary'}, inline=True, fontsize=10)

plt.xlabel("Feature x1")
plt.ylabel("Feature x2")
plt.legend()
plt.title("GDA: Smooth Quadratic Decision Boundary")

# Save the smooth plot
plt.savefig("gda_smooth_quadratic_boundary.png")
plt.close()


print("Plots saved as 'gda_training_data.png', 'gda_linear_boundary.png', and 'gda_quadratic_boundary.png'")




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
    
    def _normalize_data(self, X):
        """Normalize the input data."""
        mean = np.mean(X, axis=0)
        std_dev = np.std(X, axis=0)
        return (X - mean) / std_dev
    
    def _separate_data_by_class(self, X, y):
        """Separate the data based on class labels."""
        X_0 = []
        X_1 = []
        
        for i in range(len(y)):
            if y[i] == 0:
                X_0.append(X[i])
            else:
                X_1.append(X[i])
        
        return np.array(X_0), np.array(X_1)
    
    def _compute_means(self, X_0, X_1):
        """Compute the means for each class."""
        mu_0 = np.sum(X_0, axis=0) / len(X_0)
        mu_1 = np.sum(X_1, axis=0) / len(X_1)
        return mu_0, mu_1
    
    def _compute_shared_covariance(self, X_0, X_1, mu_0, mu_1, total_samples):
        """Compute the shared covariance matrix."""
        total = np.concatenate((X_0 - mu_0, X_1 - mu_1), axis=0)
        sigma = np.zeros((X_0.shape[1], X_0.shape[1]))
        
        for i in range(len(total)):
            for j in range(X_0.shape[1]):
                for k in range(X_0.shape[1]):
                    sigma[j][k] += total[i][j] * total[i][k]
        
        # Average the covariance matrix
        for i in range(len(sigma)):
            for j in range(len(sigma[i])):
                sigma[i][j] /= total_samples
        
        return sigma
    
    def _compute_separate_covariances(self, X_0, X_1, mu_0, mu_1):
        """Compute separate covariance matrices for each class."""
        sigma_0 = np.zeros((X_0.shape[1], X_0.shape[1]))
        sigma_1 = np.zeros((X_1.shape[1], X_1.shape[1]))
        
        for x in X_0:
            for i in range(len(x)):
                for j in range(len(x)):
                    sigma_0[i][j] += (x[i] - mu_0[i]) * (x[j] - mu_0[j])
        
        for x in X_1:
            for i in range(len(x)):
                for j in range(len(x)):
                    sigma_1[i][j] += (x[i] - mu_1[i]) * (x[j] - mu_1[j])
        
        # Average the covariance matrices
        for i in range(len(sigma_0)):
            for j in range(len(sigma_0[i])):
                sigma_0[i][j] /= len(X_0)
        
        for i in range(len(sigma_1)):
            for j in range(len(sigma_1[i])):
                sigma_1[i][j] /= len(X_1)
        
        return sigma_0, sigma_1
    



    def fit(self, X, y, assume_same_covariance=False):
        # Normalize the data
        X = self._normalize_data(X)
        
        # Separate data based on class labels
        X_0, X_1 = self._separate_data_by_class(X, y)
        
        # Compute means
        self.mu_0, self.mu_1 = self._compute_means(X_0, X_1)
        
        if assume_same_covariance:
            # Compute shared covariance matrix
            self.sigma = self._compute_shared_covariance(X_0, X_1, self.mu_0, self.mu_1, len(X))
            return self.mu_0, self.mu_1, self.sigma
        else:
            # Compute separate covariance matrices
            self.sigma_0, self.sigma_1 = self._compute_separate_covariances(X_0, X_1, self.mu_0, self.mu_1)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1


    
    def _compute_matrix_inverse(self, matrix):
        return np.linalg.inv(matrix)


    def _compute_quadratic_term(self, X, sigma_inv):
        n_samples, n_features = X.shape
        quadratic_term = np.zeros(n_samples)
        
        for i in range(n_samples):
            temp = 0.0
            for j in range(n_features):
                for k in range(n_features):
                    temp += X[i][j] * sigma_inv[j][k] * X[i][k]
            quadratic_term[i] = -0.5 * temp
        
        return quadratic_term

    def _compute_linear_term(self, X, sigma_inv, mu):
        n_samples, n_features = X.shape
        linear_term = np.zeros(n_samples)
        
        for i in range(n_samples):
            temp = 0.0
            for j in range(n_features):
                for k in range(n_features):
                    temp += X[i][j] * sigma_inv[j][k] * mu[k]
            linear_term[i] = temp
        
        return linear_term

    def _compute_constant_term(self, mu, sigma_inv, sigma):
        # Compute mu^T * sigma_inv * mu
        temp1 = 0.0
        for i in range(len(mu)):
            for j in range(len(mu)):
                temp1 += mu[i] * sigma_inv[i][j] * mu[j]
        
        # Compute log determinant of sigma
        log_det_sigma = np.log(np.linalg.det(sigma))
        
        # Combine terms
        constant_term = -0.5 * temp1 - 0.5 * log_det_sigma
        
        return constant_term

    def predict(self, X):
        # Normalize the data
        X = self._normalize_data(X)
        
        # Compute inverse of covariance matrices
        sigma_0_inv = np.linalg.inv(self.sigma_0)
        sigma_1_inv = np.linalg.inv(self.sigma_1)
        
        # Compute quadratic terms
        quad_term_0 = self._compute_quadratic_term(X, sigma_0_inv)
        quad_term_1 = self._compute_quadratic_term(X, sigma_1_inv)
        
        # Compute linear terms
        linear_term_0 = self._compute_linear_term(X, sigma_0_inv, self.mu_0)
        linear_term_1 = self._compute_linear_term(X, sigma_1_inv, self.mu_1)
        
        # Compute constant terms
        const_term_0 = self._compute_constant_term(self.mu_0, sigma_0_inv, self.sigma_0)
        const_term_1 = self._compute_constant_term(self.mu_1, sigma_1_inv, self.sigma_1)
        
        # Compute decision values
        decision_values = (quad_term_1 - quad_term_0) + (linear_term_1 - linear_term_0) + (const_term_1 - const_term_0)
        print("Decision values:", decision_values)
        print("Type of decision values:", type(decision_values))
        result = (decision_values &gt; 0).astype(int)
        return result # Class 1 if positive, else Class 0

</PRE>
</PRE>
</BODY>
</HTML>
