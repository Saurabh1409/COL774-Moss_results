<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_1HEUC.py<p><PRE>



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time





class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.theta_history = []

    def compute_cost(self, X, y, theta):
        """Compute the cost function J(θ)."""
        m = len(y)
        error = X.dot(theta) - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        """Fit the linear regression model using Batch Gradient Descent."""
        epsilon=1e-6
        max_iterations=100000
        m, n = X.shape
        X = np.c_[np.ones(m), X]  
        self.theta = np.zeros(n + 1)  
        
        for i in range(max_iterations):
            h = X.dot(self.theta)
            gradient = (1/m) * X.T.dot(h - y)
            self.theta -= learning_rate * gradient
            
            cost = self.compute_cost(X, y, self.theta)
            self.cost_history.append(cost)
            self.theta_history.append(self.theta.copy())
            print(cost)
            
            if i &gt; 0 and abs(self.cost_history[-1] - self.cost_history[-2]) &lt; epsilon:
                print(f"Convergence reached at iteration {i}.")
                break
        return self.theta_history
    
    def predict(self, X):
        """Predict the target values for the input data."""
        X = np.c_[np.ones(X.shape[0]), X]  
        print(X.dot(self.theta).shape)
        return X.dot(self.theta)
    
    def plot_data_and_hypothesis(self, X, y):
        """Plot the data and the learned hypothesis function."""
        plt.figure(figsize=(10, 6))
        plt.scatter(X, y, color='b', label='Data points')
        plt.plot(X, self.predict(X), color='r', label='Hypothesis function')
        plt.xlabel('Acidity')
        plt.ylabel('Density')
        plt.title('Linear Regression: Data and Hypothesis Function')
        plt.legend()
        plt.show()
    
    def plot_cost_3d(self, X, y):
        """Plot the 3D cost function with a color gradient and a color bar."""
        theta0_vals = np.linspace(-100, 100, 200)
        theta1_vals = np.linspace(-100, 100, 200)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
        
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
        
        
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")

        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

        plt.show()


    def plot_cost_3d_animated(self, X, y, speed_factor=1):
        """Plot the 3D cost function and animate gradient descent steps."""
        theta_history = self.theta_history
        cost_history = self.cost_history

        
        theta0_vals = np.linspace(-10, 30, 40)
        theta1_vals = np.linspace(0, 40, 40)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

        
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')

        
        surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")

        
        ax.set_xlabel(r'$\theta_0$')
        ax.set_ylabel(r'$\theta_1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Mesh Plot of Cost Function with Gradient Descent Path')

        
        scatter = ax.scatter([], [], [], color='red', s=50, label="Current Position")
        path, = ax.plot([], [], [], color='red', linestyle='-', linewidth=2, label="Gradient Descent Path")
        persistent_scatter = ax.scatter([], [], [], color='black', s=30)  

        
        theta_0_vals = []
        theta_1_vals = []
        cost_vals = []

        
        for i in range(0, len(theta_history), speed_factor):
            theta_0, theta_1 = theta_history[i]
            cost = cost_history[i]

            
            theta_0_vals.append(theta_0)
            theta_1_vals.append(theta_1)
            cost_vals.append(cost)

            
            scatter._offsets3d = (np.array([theta_0]), np.array([theta_1]), np.array([cost]))

            
            path.set_data(theta_0_vals, theta_1_vals)
            path.set_3d_properties(cost_vals)

            
            persistent_scatter._offsets3d = (np.array(theta_0_vals), np.array(theta_1_vals), np.array(cost_vals))

            plt.legend()
            plt.pause(0.01)  

        
        plt.close(fig)




    def plot_contours(self, X, y):
        """Plot the contours of the cost function and animate gradient descent updates."""
        theta0_vals = np.linspace(-100, 100, 200)
        theta1_vals = np.linspace(-100, 100, 200)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
<A NAME="5"></A><FONT color = #FF0000><A HREF="match9-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        
        plt.figure(figsize=(10, 6))
        
        
        plt.contour(theta0_vals, theta1_vals, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='jet')
        plt.xlabel(r'$\theta_0$')
</FONT>        plt.ylabel(r'$\theta_1$')
        plt.title('Contour Plot of Cost Function')

        
        theta_history = np.array(self.theta_history)
        path, = plt.plot([], [], 'r-x', label='Gradient Descent Path')
        
        plt.legend()

        
        for i in range(len(theta_history)):
            path.set_data(theta_history[:i+1, 0], theta_history[:i+1, 1])  
            plt.pause(0.2)  

        plt.show()


    def plot_contours_multiple_lrs(self, X, y, learning_rates):
        """Plot the contours of the cost function and compare different learning rates."""
        theta0_vals = np.linspace(-2, 8, 200)   
        theta1_vals = np.linspace(-2, 30, 200)  
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                t = np.array([theta0_vals[i], theta1_vals[j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones(len(X)), X], y, t)

        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))  
        axes = axes.flatten()  

        for idx, eta in enumerate(learning_rates):
            
            self.theta = np.zeros(2)  
            self.theta_history = []  

            
            self.fit(X, y, eta)  
            theta_history = np.array(self.theta_history)

            ax = axes[idx]
            CS = ax.contour(theta0_vals, theta1_vals, J_vals.T, levels=np.logspace(-2, 3, 20), cmap='viridis')
            ax.set_xlabel(r'$\theta_0$ (Intercept)')
            ax.set_ylabel(r'$\theta_1$ (Slope)')
            ax.set_title(f'Contour Plot with Gradient Descent Path (η={eta})')
            fig.colorbar(CS, ax=ax, label=r'Cost $J(\theta)$')

            
            ax.scatter(theta_history[:, 0], theta_history[:, 1], color='red', s=40)  
            ax.plot(theta_history[:, 0], theta_history[:, 1], 'r-', linewidth=2)  

        plt.tight_layout()
        plt.show()


    
    def compare_learning_rates(self, X, y, learning_rates=[0.001, 0.025, 0.1]):
        """Compare contour plots for different learning rates."""
        for alpha in learning_rates:
            print(f"Learning rate: {alpha}")
            self.fit(X, y, learning_rate=alpha)
            self.plot_contours_multiple_lrs(X, y, [0.001, 0.025, 0.1])


X = np.loadtxt('data/Q1/linearX.csv', delimiter=',')
y = np.loadtxt('data/Q1/linearY.csv', delimiter=',')
X=X.reshape(-1,1)

model = LinearRegressor()
model.fit(X, y)
model.predict(X)









model.compare_learning_rates(X, y, learning_rates=[0.1, 0.025, 0.001])




import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from linear_regression import LinearRegressor

# Load dataset
X = pd.read_csv('../data/Q1/linearX.csv').to_numpy()
y = pd.read_csv('../data/Q1/linearY.csv').to_numpy().reshape(-1, 1)

# Get dimensions
m, n = X.shape

# Generate 2D contour grid for error function J(θ)
theta_1_vals = np.linspace(-50, 50, 100)
theta_2_vals = np.linspace(-50, 50, 100)
Theta_1, Theta_2 = np.meshgrid(theta_1_vals, theta_2_vals)
J_vals = np.zeros(Theta_1.shape)

# Compute J(θ) over the entire mesh grid
for i in range(Theta_1.shape[0]):
    for j in range(Theta_1.shape[1]):
        theta_1 = np.array(Theta_1[i, j]).reshape(1, n)  # Ensure correct shape (1, n)
        theta_2 = Theta_2[i, j] * np.ones((m, 1))  # Broadcast bias term
        predictions = X @ theta_1.T + theta_2  # Compute hypothesis
        J_vals[i, j] = (1 / (2 * m)) * np.sum((predictions - y) ** 2)

# Train model
model = LinearRegressor()
param_history = model.fit(X, y, learning_rate=0.025, tolerance=1e-4, max_iters=10000)
param_history = np.array(param_history)

# Compute loss J(theta) dynamically for each step in param_history
J_history = []
Theta_1_path = []
Theta_2_path = []
for i in range(len(param_history)):
    theta_1 = param_history[i][:-1].reshape(1, n)  # Ensure shape (1, n)
    theta_2 = param_history[i][-1]  # Extract bias term
    predictions = X @ theta_1.T + theta_2 * np.ones((m, 1))  # Compute hypothesis
    J_history.append((1 / (2 * m)) * np.sum((predictions - y) ** 2))
    Theta_1_path.append(theta_1.flatten()[0])  # Store for plotting
    Theta_2_path.append(theta_2)  # Store for plotting
J_history = np.array(J_history)
Theta_1_path = np.array(Theta_1_path)
Theta_2_path = np.array(Theta_2_path)

# Plot 2D contour of error function
fig, ax = plt.subplots()
contour = ax.contourf(Theta_1, Theta_2, J_vals, levels=50, cmap='viridis')
fig.colorbar(contour, ax=ax, shrink=0.5, aspect=5)
ax.set_xlabel("Theta 1")
ax.set_ylabel("Theta 2 (Bias)")
ax.set_title("Contour Plot of Error Function with Gradient Descent Path")

# Overlay gradient descent path
ax.scatter(Theta_1_path, Theta_2_path, color='cyan', marker='.', label='Gradient Descent Path')
plt.legend()
plt.show()




import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    learning_rate=0.001
    regressor = LinearRegressor()
    param_history = regressor.fit(x, y, learning_rate)
    param_history = np.array(param_history)
    m = len(y)

    def compute_error(weight, bias):
        predictions = weight * x + bias
        errors = predictions - y
            
        return (1/(2*m)) * np.sum(errors ** 2)
    
    w_min = np.min(param_history[:, 0]) - 50
    w_max = np.max(param_history[:, 0]) + 50
    b_min = np.min(param_history[:, 1]) - 20
    b_max = np.max(param_history[:, 1]) + 20

    weights = np.linspace(w_min, w_max, 200)
    biases = np.linspace(b_min, b_max, 200)
    W, B = np.meshgrid(weights, biases)

    Z = np.zeros(W.shape)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            Z[i, j] = compute_error(W[i, j], B[i, j])

    fig, ax = plt.subplots(figsize=(10, 8))
    contour_levels = np.logspace(-2, 3, 50)
    contf = ax.contourf(W, B, Z, levels=contour_levels, cmap='viridis', alpha=0.7)
    cont = ax.contour(W, B, Z, levels=contour_levels, colors='black', linewidths=0.5)
    fig.colorbar(contf, label='Error J')

    ax.set_xlabel('Weight (w)')
    ax.set_ylabel('Bias (b)')
    ax.set_title('Contour Plot of Error Function with Gradient Descent Trajectory')

    path_w = []
    path_b = []
    
    trajectory, = ax.plot([], [], 'b-o', linewidth=2, markersize=4, label='Trajectory')
    current_point, = ax.plot([], [], 'ro', markersize=8, label='Current Point')
    ax.legend()

    for i in range(len(param_history)):
        w_curr = param_history[i, 0]
        b_curr = param_history[i, 1]
        
        path_w.append(w_curr)
        path_b.append(b_curr)
        
        trajectory.set_data(path_w, path_b)
        current_point.set_data([w_curr], [b_curr])
        
        plt.draw()
        plt.pause(0.2)

    plt.show()
    # plt.savefig('plots/Q1/contour_graph.png')





import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from linear_regression import LinearRegressor

# Load dataset
X = pd.read_csv('../data/Q1/linearX.csv').to_numpy()
y = pd.read_csv('../data/Q1/linearY.csv').to_numpy().reshape(-1, 1)

# Get dimensions
m, n = X.shape

# Train model
model = LinearRegressor()
model.fit(X, y, learning_rate=0.01, tolerance=1e-4, max_iters=10000)

# Generate points uniformly in the range of X for hypothesis function
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
y_preds = model.predict(X_range)  # Predict corresponding y values

# Plot data points and hypothesis function
plt.figure()
plt.scatter(X, y, color='red', marker='o', label='Data Points')  # Scatter plot for data
plt.plot(X_range, y_preds, color='blue', linestyle='-', label='Hypothesis Function')  # Line plot for hypothesis
plt.xlabel("X")
plt.ylabel("y")
plt.title("Data Points and Hypothesis Function")
plt.legend()
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from linear_regression import *

# Load data
x = np.loadtxt("data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("data/Q1/linearY.csv", delimiter=",")
m = len(x)
shuffled = np.random.permutation(m)
x = x[shuffled]
y = y[shuffled]
# Train model
lr = LinearRegressor()
learning_rates = [0.001,0.025,0.1]
idx = 0
for l_rate in learning_rates:
    idx+=1
    theta_list = lr.fit(x, y, l_rate)
    print(f"theta for learning rate {l_rate} :\n {lr.theta}")

    # Plot data and regression line
    plt.scatter(x, y, c='red', marker='x')
    x_vals = np.linspace(min(x), max(x), 100)
    plt.plot(x_vals, lr.predict(x_vals), 'b-')
    plt.title(f"Linear Regression Fit for learning_rate {l_rate}")
    # plt.savefig(f"q1_line{idx}.png")
    plt.show()
    plt.close()

    # 3D Surface plot
    X_bias = np.c_[np.ones((len(x), 1)), x]
    theta0_vals = np.linspace(-2, 2, 100)
    theta1_vals = np.linspace(-1, 1, 100)
<A NAME="7"></A><FONT color = #0000FF><A HREF="match9-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

    for i, t0 in enumerate(theta0_vals):
        for j, t1 in enumerate(theta1_vals):
            J_vals[i, j] = lr.loss_fxn(X_bias, y, np.array([[t0], [t1]]))
</FONT>
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
    ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
    # theta_hist = np.array(lr.theta_history)
    # ax.plot(theta_hist[:, 0], theta_hist[:, 1], lr.cost_history, 'r.-')
    # cost_history = np.array(lr.cost_history).flatten()
    # ax.plot(theta_hist[:-1, 0], theta_hist[:-1, 1], lr.cost_history, 'r.-')

    # Convert lists to NumPy arrays
    theta_hist = np.array(lr.theta_list).squeeze()  # Removes extra dimension
    loss_history = np.array(lr.loss_list).reshape(-1)  # Ensure 1D shape

    # print(f"theta_hist shape after squeeze: {theta_hist.shape}")  # Should be (1143, 2)
    # print(f"cost_history shape: {loss_history.shape}")  # Should be (1142,)

    # Ensure correct slicing for matching dimensions
    ax.plot(theta_hist[:len(loss_history), 0],  # First column
            theta_hist[:len(loss_history), 1],  # Second column
            loss_history, 'r.-')

    plt.title(f"Cost Function Surface for learning Rate {l_rate}")
    plt.show()
    # plt.savefig(f"q1_3d_{idx}.png")
    plt.close()

    # Contour plots
    # plt.contour(T0, T1, J_vals.T, levels=np.logspace(-2, 3, 20))
    # plt.plot(theta_hist[:, 0], theta_hist[:, 1], 'r.-')
    # plt.title("Contour Plot")
    # plt.savefig("q1_contour.png")
    # plt.close()
    import matplotlib.patches as patches

    plt.figure()
    contour = plt.contour(T0, T1, J_vals.T, levels=np.logspace(-2, 3, 20))
    plt.plot(theta_hist[:, 0], theta_hist[:, 1], 'r.-')

    # Compute covariance for reference ellipses
    cov_matrix = np.linalg.inv(X_bias.T @ X_bias)  # Approximation of cost function curvature
    eigvals, eigvecs = np.linalg.eigh(cov_matrix)

    # Scale factor to make ellipses visible
    scale_factor = 100  # Adjust this value as needed

    # Draw reference ellipses
    for scale in [1, 2, 3]:  # Different scale levels to show shape
        width, height = 2 * scale * np.sqrt(eigvals) * scale_factor  # Scale up ellipse dimensions
        angle = np.degrees(np.arctan2(eigvecs[1, 0], eigvecs[0, 0]))  # Rotation angle
        
        ellipse = patches.Ellipse(
            xy=lr.theta.ravel(),  # Center of ellipse
            width=width,
            height=height,
            angle=angle,  # Use keyword argument explicitly
            edgecolor='g',
            facecolor='none',
            linestyle="--",
            alpha=0.7
        )
        plt.gca().add_patch(ellipse)

    plt.title(f"Contour Plot with Reference Ellipses for learning rate = {l_rate}")
    plt.xlabel(r"$\theta_0$")
    plt.ylabel(r"$\theta_1$")
    # plt.savefig(f"q1_contour_ellipses_{idx}.png")
    plt.show()
    plt.close()



import numpy as np
import matplotlib.pyplot as plt

import time
class LinearRegressor:
    def __init__(self):
        self.theta = None  # Row vector (1 x n)
        self.bias = None   # Scalar θ_N+1
        self.called = 0    # Track number of times fit() is called

    def fit(self, X, y, learning_rate=0.01, tolerance=1e-4, max_iters=100000):
        """
        Fit the linear regression model to the data using Gradient Descent until convergence.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
        
        learning_rate : float, default=0.01
            The learning rate for gradient descent.

        tolerance : float, default=1e-4
            Training stops when the absolute change in loss is smaller than this value.

        max_iters : int, default=10000
            Safety limit for the maximum number of iterations.
        
        Returns
        -------
        param_history : numpy array of shape (actual_iters, n_features + 1)
            The list of parameters obtained at each iteration.
        """
        # Reshape y to (m, 1) for matrix operations
        y = y.reshape(-1, 1)  # Ensures y has shape (m, 1)

        # Get dimensions
        m, n = X.shape

        # Initialize parameters only if fit() is called for the first time
        if self.called == 0:
            self.theta = np.zeros((1, n))  # (1 x n)
            self.bias = 0  # Scalar θ_N+1

        # Increment self.called outside the if block to track calls correctly
        self.called += 1

        # Store parameter history dynamically
        param_history = []
        time_history = []

        # Store loss history for plotting
        loss_history = []
        prev_loss = float("inf")  # Start with an infinite previous loss

        # Prevent extra figure logs
        # plt.ioff()  
        # plt.close('all')  # Close any previous figures

        # # Setup live plot
        # plt.ion()  # Interactive mode on
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iterations")
        # ax.set_ylabel("Loss")
        # ax.set_title("Live Loss Plot")
        # line, = ax.plot([], [], 'r-')  # Blue line

        # Gradient Descent
        t_init = time.time()
        for i in range(max_iters):
            # Compute predictions (broadcast bias explicitly)
            y_pred = np.dot(X, self.theta.T) + self.bias * np.ones((m, 1))  # Bias broadcasting

            # Compute loss function
            loss = (1 / (2 * m)) * np.dot((y_pred - y).T, (y_pred - y)).flatten()[0]
            loss_history.append(loss)

            # Check for convergence (Stop if loss change &lt; tolerance)
            if abs(prev_loss - loss) &lt; tolerance:
                print(f"\n[EARLY STOPPING] Converged at iteration {i} with loss = {loss:.6f}")
                break
            prev_loss = loss  # Update previous loss

            # Compute gradients (vectorized)
            grad_theta = (1/m) * np.dot((y_pred - y).T, X)  # (1 x n)
            grad_bias = self.bias + (1/m) * np.sum(y_pred - y)  # Corrected gradient for bias

            # Update parameters using learning_rate
            self.theta -= learning_rate * grad_theta
            self.bias -= learning_rate * grad_bias

            # Store parameters dynamically
            param_history.append(np.hstack((self.theta.flatten(), self.bias)))  # (n_features + 1,)

            # Live update of the loss plot
        #     if i % 10 == 0:  # Update every 10 iterations to avoid flickering
        #         line.set_xdata(range(len(loss_history)))
        #         line.set_ydata(loss_history)
        #         ax.relim()
        #         ax.autoscale_view()
        #         fig.canvas.flush_events()  # Properly update the live plot
        #         plt.pause(0.001)

        #     # Clean logging with '\r' to overwrite output
        #     if i % 10 == 0:  # Print less frequently
        #         print(f"\rIteration {i}: Loss = {loss:.6f}", end="")

        # plt.ioff()  # Turn off interactive mode
        # plt.show()  # Show final plot
        
        return np.array(param_history)  # Convert list to NumPy array and return
        # return np.array(time_history)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples, 1)
            The predicted target values.
        """
        m = X.shape[0]  # Get number of samples
        y_pred = np.dot(X, self.theta.T) + self.bias * np.ones((m, 1))  # Explicit bias broadcasting
        return y_pred



<A NAME="12"></A><FONT color = #0000FF><A HREF="match9-1.html#12" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from linear_regression import LinearRegressor

def plot_regression_line(X, y, param_history):
</FONT>    theta = param_history[-1]
    plt.scatter(X, y, color='blue', s=5, label="Data points")
    x_range = np.linspace(X.min(), X.max(), 100)
    y_pred = theta[0] + theta[1] * x_range
    plt.plot(x_range, y_pred, color='red', label="Learned hypothesis")
    plt.xlabel("X")
    plt.ylabel("y")
    plt.legend()
    plt.title("Regression Line")
    plt.show()

def plot_error_surface(X, y, param_history):
    """Plots a 3D surface of the cost function and the gradient descent path."""
    theta0_vals = np.linspace(-10, 20, 500)
    theta1_vals = np.linspace(15, 45, 500)
    J_vals = np.zeros((len(theta1_vals), len(theta0_vals)))  

    # Compute J(theta) over a grid of theta0, theta1 values
    for i, theta1 in enumerate(theta1_vals): 
        for j, theta0 in enumerate(theta0_vals):                
            theta = np.array([theta0, theta1] + [0] * (X.shape[1] - 1))  # Extend for higher dims
            predictions = np.column_stack((np.ones(X.shape[0]), X)) @ theta
            J_vals[i, j] = np.mean((y - predictions) ** 2)

    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta0_grid, theta1_grid, J_vals, cmap="viridis", alpha=0.7)

    # Compute cost for each iteration in param_history
    param_history = np.array(param_history)
    cost_values = np.array([
        np.mean((y - (np.column_stack((np.ones(X.shape[0]), X)) @ theta)) ** 2)
        for theta in param_history
    ])

    # Initialize the scatter plot
    scatter = ax.scatter([], [], [], color='red', marker='o')

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Cost Function J(θ)")
    plt.title("Error Surface and Gradient Descent Path")

    # Animation function
    def update(frame):
        scatter._offsets3d = (
            param_history[:frame, 0],  # Theta0 values up to current frame
            param_history[:frame, 1],  # Theta1 values up to current frame
            cost_values[:frame]        # Cost values up to current frame
        )

    ani = animation.FuncAnimation(fig, update, frames=len(param_history), interval=200)  # 0.2 sec intervals
    plt.show()

def plot_contour(X, y, param_history):
    theta0_vals = np.linspace(0, 40, 500)
    theta1_vals = np.linspace(0, 40, 500)
    J_vals = np.zeros((len(theta1_vals), len(theta0_vals))) 

<A NAME="13"></A><FONT color = #00FFFF><A HREF="match9-1.html#13" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for i, theta1 in enumerate(theta1_vals): 
        for j, theta0 in enumerate(theta0_vals):
            theta = np.array([theta0, theta1])
            predictions = np.column_stack((np.ones(X.shape[0]), X)) @ theta
</FONT>            J_vals[i, j] = np.mean((y - predictions) ** 2)

    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    fig, ax = plt.subplots()
    contour = ax.contour(theta0_grid, theta1_grid, J_vals, levels=np.logspace(-2, 3, 20), cmap="viridis")

    param_history = np.array(param_history)
    line, = ax.plot([], [], 'ro-', markersize=5)

    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_title("Cost Function Contour and Gradient Descent Path")

    # Animation function
    def update(frame):
        line.set_data(param_history[:frame, 0], param_history[:frame, 1])

    ani = animation.FuncAnimation(fig, update, frames=len(param_history), interval=200)  # 0.2 sec intervals
    plt.show()


# Load X and ensure it is 2D
X = np.loadtxt("data\Q1\linearX.csv", delimiter=",")
if X.ndim == 1:
    X = X.reshape(-1, 1)  # Convert (n_samples,) to (n_samples, 1)

# Load y and ensure it is 1D
y = np.loadtxt("data\Q1\linearY.csv", delimiter=",").flatten()

# Initialize and train the model
model = LinearRegressor()
param_history = model.fit(X, y, learning_rate=0.001)

# Print final parameters
print("Final Parameters:", param_history[-1])
print("No of iterations:", len(param_history))

# Make predictions on training data
y_pred = model.predict(X)

# Mean relative error
mre = np.mean(np.abs((y - y_pred) / y)) * 100

print(f"Mean Relative Error: {mre}%")

# plot_regression_line(X, y, param_history)
# plot_error_surface(X, y, param_history)
plot_contour(X, y, param_history)





import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    regressor = LinearRegressor()
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    weights = regressor.fit(x, y, learning_rate=0.01)
    weights = np.array(weights)
    m = y.shape[0]

    def compute_error(weight, bias):
        predictions = weight * x + bias
        errors = predictions - y
            
        return (1/(2*m)) * np.sum(errors ** 2)

    w_min = np.min(weights[:, 0]) - 10
    w_max = np.max(weights[:, 0]) + 10
    b_min = np.min(weights[:, 1]) - 10
    b_max = np.max(weights[:, 1]) + 10

    weight_vals = np.linspace(w_min, w_max, 200)
    bias_vals = np.linspace(b_min, b_max, 200)
    W, B = np.meshgrid(weight_vals, bias_vals)

    J_vals = np.zeros(W.shape)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            J_vals[i, j] = compute_error(W[i, j], B[i, j])

    fig = plt.figure(figsize=(10,8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(W, B, J_vals, cmap='viridis', alpha=0.7,edgecolor=None)
    fig.colorbar(surf, shrink=0.5, aspect=5, label='Error J(w, b)')
    ax.set_xlabel('Weight (w)')
    ax.set_ylabel('Bias (b)')
    ax.set_zlabel('Cost J')
    ax.set_title('Cost Function Surface and Gradient Descent Path')

    path_weights = weights[:, 0]
    path_biases = weights[:, 1]
    path_costs = np.array([compute_error(w, b) for w, b in zip(path_weights, path_biases)])

    for i in range(len(weights)):
        current_w = weights[i, 0]
        current_b = weights[i, 1]
        current_cost = compute_error(current_w, current_b)
        current_point = ax.scatter(current_w, current_b, current_cost, color='k', s=50)
        plt.draw()
        plt.pause(0.2)
    plt.show()
    # plt.savefig('plots/Q1/error_curve.png')




import numpy as np
import csv
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from linear_regression import LinearRegressor

def load_csv_files(x_file='linearX.csv',y_file='linearY.csv'):
    X=np.genfromtxt(x_file,delimiter=',').reshape(-1, 1)
    y=np.genfromtxt(y_file,delimiter=',')
    return X,y

def plot_and_fit(X,y,regressor):
    plt.figure(figsize=(10, 6))
<A NAME="0"></A><FONT color = #FF0000><A HREF="match9-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.scatter(X, y, color='blue', alpha=0.5, label='Training Data')
    
    X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_pred = regressor.predict(X_test)
    
    plt.plot(X_test, y_pred, color='red', linewidth=2, label='Hypothesis Function')
    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Wine Density vs Acidity with Fitted Line')
    plt.legend()
    plt.grid(True, alpha=0.3)
</FONT>    
    plt.savefig('regression_fit.png', dpi=300, bbox_inches='tight')
    plt.show()


def plot_error_surface_3d(X, y, regressor, pause_time=0.2):
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    margin = 2
    theta0_range = np.linspace(regressor.theta_list[0][0]-margin, 
                              regressor.theta_list[-1][0]+margin, 100)
    theta1_range = np.linspace(regressor.theta_list[0][1]-margin, 
                              regressor.theta_list[-1][1]+margin, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    J_mesh = np.zeros(theta0_mesh.shape)
    for i in range(len(theta0_range)):
        for j in range(len(theta1_range)):
            theta = np.array([theta0_mesh[i,j], theta1_mesh[i,j]])
            h = X_b @ theta
            J_mesh[i,j] = (1/(2*len(y))) * np.sum((h - y)**2)
    
    surf = ax.plot_surface(theta0_mesh, theta1_mesh, J_mesh, 
                          cmap='viridis', alpha=0.6)
    fig.colorbar(surf, label='Cost J(θ)')
    ii=0
    for theta in regressor.theta_list:
        h = X_b @ theta
        cost = (1/(2*len(y))) * np.sum((h - y)**2)
        ax.scatter(theta[0], theta[1], cost, color='red', s=50)
        if pause_time &gt; 0:
            plt.pause(pause_time)
        if ii%50==0:
            print(ii)
        ii=ii+1
    
    ax.set_xlabel('θ₀ (Intercept)')
    ax.set_ylabel('θ₁ (Slope)')
    ax.set_zlabel('Cost J(θ)')
    plt.title('Error Surface with Gradient Descent Path')
    
    plt.savefig('error_surface_3d.png', dpi=300, bbox_inches='tight')
    plt.show()


def plot_contours(X, y, regressor, learning_rate, pause_time=0.2):
    """Plot contours of the error function with gradient descent iterations"""
    plt.figure(figsize=(10, 6))
    
    # Create mesh grid for theta values
    margin = 2
    theta0_range = np.linspace(regressor.theta_list[0][0]-2, 
                              regressor.theta_list[-1][0]+2, 100)
    theta1_range = np.linspace(regressor.theta_list[0][1]-2, 
                              regressor.theta_list[-1][1]+2, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    
    # Compute cost for each theta combination
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    J_mesh = np.zeros(theta0_mesh.shape)
    for i in range(len(theta0_range)):
        for j in range(len(theta1_range)):
            theta = np.array([theta0_mesh[i,j], theta1_mesh[i,j]])
            h = X_b @ theta
            J_mesh[i,j] = (1/(2*len(y))) * np.sum((h - y)**2)
    
    # Plot contours
    contours = plt.contour(theta0_mesh, theta1_mesh, J_mesh, levels=50)
    plt.colorbar(contours, label='Cost J(θ)')
    
    # Animate gradient descent path
    path_x = [theta[0] for theta in regressor.theta_list]
    path_y = [theta[1] for theta in regressor.theta_list]
    
    # Plot path with animation
    for i in range(len(path_x)):
        plt.scatter(path_x[i], path_y[i], color='red', s=50)
        if i &lt; len(path_x)-1:
            plt.plot([path_x[i], path_x[i+1]], [path_y[i], path_y[i+1]], 
                     'r--', alpha=0.3)
        if pause_time &gt; 0:
            plt.pause(pause_time)
    
    plt.xlabel('θ₀ (Intercept)')
    plt.ylabel('θ₁ (Slope)')
    plt.title(f'Contour Plot with Gradient Descent Path (η={learning_rate})')
    plt.grid(True, alpha=0.3)
    

    plt.savefig(f'contour_plot_lr_{learning_rate}.png', dpi=300, bbox_inches='tight')
    plt.show()


def compare_learning_rates(X, y, learning_rates=[0.01]):
    """Compare the convergence for different learning rates"""
    results = []
    
    for lr in learning_rates:
        regressor = LinearRegressor()
        theta_list = regressor.fit(X, y, learning_rate=lr)
        
        # Plot contours for this learning rate
        plot_contours(X, y, regressor, lr)
        results.append({
            'learning_rate': lr,
            'final_theta': regressor.theta,
            'n_iterations': len(regressor.theta_list),
            'final_cost': regressor.error_list[-1]
        })
        
        print(f"\nLearning Rate: {lr}")
        print(f"Final parameters: θ₀={regressor.theta[0]:.4f}, θ₁={regressor.theta[1]:.4f}")
        print(f"Number of iterations: {len(regressor.theta_list)}")
        print(f"Final cost: {regressor.error_list[-1]:.8f}")
    
    return results


if __name__ == "__main__":

    
    X, y = load_csv_files()
    regressor = LinearRegressor()
    theta_list = regressor.fit(X, y, learning_rate=0.025)
    
    plot_and_fit(X, y, regressor)
    # plot_error_surface_3d(X, y, regressor)
    
    results = compare_learning_rates(X, y)



import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
import os
if __name__=='__main__':
    regressor = LinearRegressor()
    x = np.genfromtxt(os.path.abspath('data/Q1/linearX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q1/linearY.csv'), delimiter=',')
    weights = regressor.fit(x, y, learning_rate=0.01)

    print("Final parameters:")
    print("weight (w):", regressor.w)
    print("Intercept (b):", regressor.b)

    plt.scatter(x, y, color='blue', label='Data Points')

    x_vals = np.linspace(min(x), max(x), 100)

    y_vals = regressor.w * x_vals + regressor.b

    plt.plot(x_vals, y_vals, color='red', label='Learned Hypothesis')

    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Linear Regression')
    plt.legend()

    plt.show()
    # plt.savefig('plots/Q1/datapoints.png')



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pickle
from linear_regression import LinearRegressor
X = np.loadtxt("../data/Q1/linearX.csv")
y = np.loadtxt("../data/Q1/linearY.csv")
if X.ndim == 1:
    X = X.reshape(-1, 1)



def find_learning_rate():

    learning_rates = [0.0001, 0.001, 0.01, 0.1]
    cost_histories = []
  

    for lr in learning_rates:
        model = LinearRegressor()
        model.fit(X, y, learning_rate=lr)
        costs = [cost for _, cost in model.history]
        cost_histories.append(costs)

    for lr, costs in zip(learning_rates, cost_histories):
        plt.plot(costs, label=f"LR = {lr}")
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    plt.legend()
    plt.savefig("learning_rate_analysis.png")

def gradient(X, y, ax,learning_rate=0.01):
    
    max_iter=1000
    tol=1e-6
    
    n_samples, n_features = X.shape
    X = np.hstack((np.ones((n_samples, 1)), X))
    
    theta = np.zeros(X.shape[1])
    
    for iteration in range(max_iter):
        y_pred = np.dot(X, theta)
        
        cost = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)
        
        gradients = -(1 / n_samples) * np.dot(X.T, (y - y_pred))
        
        theta -= learning_rate * gradients
        ax.scatter(theta[0], theta[1], cost, color='red', s=50, label="Current Step" if iteration == 0 else "")
        
        plt.pause(0.2)

        if iteration &gt; 0 and abs(cost - compute_cost(X, y, theta)) &lt; tol:
            break
        
    
    
def compute_cost(X, y,theta):
    
    n_samples = len(X)
    predictions = np.dot(X, theta)
    errors = predictions - y
    return (1 / (2 * n_samples)) * np.sum(errors**2)



def plot_2d(X,y,theta):
    X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)  # Dense X values for a smooth line
    X_plot_with_intercept = np.hstack((np.ones((X_plot.shape[0], 1)), X_plot))  # Add intercept term
    y_pred = np.dot(X_plot_with_intercept, theta)  # Hypothesis

    plt.scatter(X, y, color='blue', label='Data Points')

    plt.plot(X_plot, y_pred, color='red', label='Learned Hypothesis (h_theta(x))')

    plt.xlabel('X (Feature)')
    plt.ylabel('y (Target)')
    plt.title('Data Points and Hypothesis Function')
    plt.legend()
    plt.savefig("2d_plot.png")
def plot_3d_mesh(X,y):
    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 5, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)
    

    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            new_X = np.hstack((np.ones((X.shape[0], 1)), X))
            J_values[i, j] = compute_cost(new_X, y, theta)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta_0, theta_1, J_values, cmap='viridis', alpha=0.8)
    ax.set_xlabel(r'$\theta_0$ (Intercept)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_zlabel(r'$J(\theta)$ (Cost)')
    ax.set_title('3D Mesh of Cost Function with Gradient Descent')

    gradient(X, y, ax, learning_rate=0.01)

    plt.savefig("3d_plot.png")
def plot_contour_with_gradient_descent( X, y, learning_rate=0.01):
    """Plots the error function's contour and visualizes gradient descent."""
    
    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 30, 100)

    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)
    X = np.hstack((np.ones((X.shape[0], 1)),X))
    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            J_values[i, j] = compute_cost(X, y, theta)

    fig, ax = plt.subplots(figsize=(8, 6))

    ax.contour(theta_0, theta_1, J_values, levels=np.logspace(-2, 3, 20), cmap="viridis")
    ax.set_xlabel(r'$\theta_0$ (Intercept)')
    ax.set_ylabel(r'$\theta_1$ (Slope)')
    ax.set_title("Contour of Cost Function with Gradient Descent")

    max_iter = 1000000
    tol = 1e-6
    n_samples = len(X)
    theta = np.zeros(X.shape[1])

    for iteration in range(max_iter):
        y_pred = np.dot(X, theta)
        cost = compute_cost(X, y, theta)

       

        gradients = -(1 / n_samples) * np.dot(X.T, (y - y_pred))
        theta -= learning_rate * gradients

        ax.scatter(theta[0], theta[1], color='red', s=50, marker='o')

        plt.pause(0.2)  

        if iteration &gt; 0 and abs(cost - compute_cost(X, y, theta)) &lt; tol:
            break

    plt.savefig("contour_gradient_descent.png")
    plt.show()


def plot_contour_learning_rates(X, y, learning_rates=[0.001, 0.025, 0.1]):
    """Plots the error function's contour and visualizes gradient descent for different learning rates."""

    theta_0_range = np.linspace(-10, 20, 100)
    theta_1_range = np.linspace(-5, 30, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_range, theta_1_range)

    X = np.hstack((np.ones((X.shape[0], 1)), X))

    J_values = np.zeros_like(theta_0)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            J_values[i, j] = compute_cost(X, y, theta)

    for eta in learning_rates:
        fig, ax = plt.subplots(figsize=(8, 6))

        contour_levels = np.logspace(
            np.log10(np.min(J_values)), np.log10(np.max(J_values)), 20
        )
        ax.contour(theta_0, theta_1, J_values, levels=contour_levels, cmap="viridis")
        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope)')
        ax.set_title(f"Contour of Cost Function with Gradient Descent (η={eta})")

        theta = np.zeros(X.shape[1])  
        theta_history = []
        prev_cost = compute_cost(X, y, theta)
        max_iter = 100000
        tol = 1e-6

        for iteration in range(max_iter):
            y_pred = np.dot(X, theta)
            gradients = -(1 / len(X)) * np.dot(X.T, (y - y_pred))
            theta -= eta * gradients  
            theta_history.append(theta.copy())

            ax.scatter(theta[0], theta[1], color="red", s=20, marker="o")

            plt.pause(0.2)  

            cost = compute_cost(X, y, theta)
            if abs(cost - prev_cost) &lt; tol:
                break
            prev_cost = cost

        theta_history = np.array(theta_history)

        ax.plot(
            theta_history[:, 0],
            theta_history[:, 1],
            color="blue",
            lw=2,
            label="Actual Path"
        )

        ax.legend()

        plt.savefig(f"contour_gradient_descent_eta_{eta}.png")

X = np.loadtxt("../data/Q1/linearX.csv")
y = np.loadtxt("../data/Q1/linearY.csv")
if X.ndim == 1:
    X = X.reshape(-1, 1)
find_learning_rate(X,y)
model=LinearRegressor()
model.fit(X,y)
model.save_model('linear_reg.pkl')
with open('linear_reg.pkl', 'rb') as f:
    model_data = pickle.load(f)

theta = model_data['theta']
theta_history = model_data['theta_history']
cost_history = model_data['cost_history']



plot_2d(X,y,theta)

plot_3d_mesh(X,y)
plot_contour_with_gradient_descent(X,y)
plot_contour_learning_rates(X,y)




#!/usr/bin/env python
# coding: utf-8

# In[1]:


from linear_regression import LinearRegressor
import pandas as pd
import numpy as np


# In[2]:


X = pd.read_csv('../data/Q1/linearX.csv')
Y = pd.read_csv('../data/Q1/linearY.csv')

X = X.to_numpy()
Y = Y.to_numpy()
Y = Y.reshape(-1)


# In[3]:


model = LinearRegressor()
model.fit(X, Y)


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:








import numpy as np
import pandas as pd
from sampling_sgd import StochasticLinearRegressor  # Import SGD model

# **Load Sampled Data from Part A**
X_train = pd.read_csv('X_train.csv').to_numpy()  # (m, n)
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)  # (m, 1)

# **True Parameters Used for Sampling**
true_theta = np.array([[3], [1], [2]])  # Given in problem

# **Compute Closed-Form Solution for Linear Regression**
X_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))  # Add bias column
closed_form_theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ Y_train  # (X^T X)^(-1) X^T Y

# **Train SGD Model**
sgd_model = StochasticLinearRegressor()
sgd_theta_history = sgd_model.fit(X_train, Y_train, learning_rate=0.001, batch_size=8000, tolerance=1e-5, max_iters=100000)
sgd_theta = sgd_theta_history[-1]  # Extract final parameters from SGD

# **Print Comparison**
print("\nComparison of True Parameters, Closed-Form Solution, and SGD Learned Parameters:")
print("--------------------------------------------------------------------")
print(f"True Parameters: \n{true_theta}")
print(f"Closed-Form Solution: \n{closed_form_theta}")
print(f"SGD Learned Parameters: \n{sgd_theta}")
print("--------------------------------------------------------------------")

# **Analyze Differences**
theta_diff_closed = np.abs(true_theta - closed_form_theta)
theta_diff_sgd = np.abs(true_theta - sgd_theta.reshape(-1, 1))  # Reshape SGD theta for comparison

print(f"Difference (True - Closed-Form): \n{theta_diff_closed}")
print(f"Difference (True - SGD): \n{theta_diff_sgd}")

# **Observations**
print("\nObservations:")
if np.allclose(true_theta, closed_form_theta, atol=1e-5):
    print("- Closed-form solution accurately recovers the true parameters.")
else:
    print("- Closed-form solution slightly deviates due to numerical precision.")

if np.allclose(true_theta, sgd_theta.reshape(-1, 1), atol=1e-2):
    print("- SGD solution is close but might have small deviations due to stochastic updates.")
else:
    print("- SGD has larger deviations, likely due to noise in gradient updates.")

# **Store results for LaTeX report**
results = pd.DataFrame({
    "Method": ["True Parameters", "Closed-Form Solution", "SGD Solution"],
    "Theta_0": [true_theta[0,0], closed_form_theta[0,0], sgd_theta[0]],
    "Theta_1": [true_theta[1,0], closed_form_theta[1,0], sgd_theta[1]],
    "Theta_2": [true_theta[2,0], closed_form_theta[2,0], sgd_theta[2]]
})

# **Save Results as CSV for Report**
results.to_csv("theta_comparison.csv", index=False)



#!/usr/bin/env python
# coding: utf-8

# In[10]:


import numpy as np
import pandas as pd
from sampling_sgd import generate

N = 1000000 # a million samples
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.array([np.sqrt(2)])


# In[11]:


X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)
# we will do a 80-20 split of the data and save it as csv file
# i want to name the headers of features x1 and x2
X = pd.DataFrame(X, columns=['x1', 'x2'])
y = pd.DataFrame(y, columns=['y'])
split = int(0.8 * N)

# Split the data
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Save to csv files
X_train.to_csv("X_train.csv", index=False)
X_test.to_csv("X_test.csv", index=False)
y_train.to_csv("y_train.csv", index=False)
y_test.to_csv("y_test.csv", index=False)
#


# In[ ]:





# In[12]:


X_train = pd.read_csv("X_train.csv")
Y_train = pd.read_csv("y_train.csv")
X_test = pd.read_csv("X_test.csv")
Y_test = pd.read_csv("y_test.csv")


# In[13]:


X_train.shape


# In[14]:


X_train.head


# In[15]:


Y_train


# In[ ]:








from matplotlib import pyplot as plt
from sampling_sgd import *
import numpy as np

np.random.seed(43)
N = 1000000
theta_true = np.array([3, 1, 2])
mean_x = np.array([3, -1])
delta_x = np.array([2, 2])
delta_y = np.sqrt(2)

X, y = generate(N, theta_true, mean_x, delta_x, delta_y)

size_train = int(N * 0.8)
X_train = X[:size_train]
y_train = y[:size_train]
X_test = X[size_train:]
y_test = y[size_train:]

# batch_sizes = [1, 800, 8000, 80000]
batch_sizes = [800,8000,80000]

# Closed-form solution
m, n = X_train.shape
X_train_closed = np.c_[np.ones((m, 1)), X_train]
y_train = y_train.reshape(-1, 1)
theta_closed = np.linalg.inv(X_train_closed.T @ X_train_closed) @ X_train_closed.T @ y_train
print(f"Closed form Theta : {theta_closed.flatten()}")

# Plotting separate graphs for each batch size
for batch_size in batch_sizes:
    sgd = StochasticLinearRegressor()
    sgd.batch_size = batch_size
    sgd.fit(X_train, y_train, learning_rate=0.001)
    print(sgd.theta)

    theta_history = np.array(sgd.theta_list)  # Convert list to numpy array

    # Create a 3D plot for the current batch size
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match9-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # Extract each parameter's trajectory
    theta_0 = theta_history[:, 0]
    theta_1 = theta_history[:, 1]
    theta_2 = theta_history[:, 2]

    # Plot trajectory
    ax.plot(theta_0, theta_1, theta_2, marker='o', markersize=2, label=f"SGD Trajectory (Batch Size {batch_size})", color='b')
</FONT>
    # Mark the start and end points
    ax.scatter(theta_0[0], theta_1[0], theta_2[0], color='g', marker='x', s=100, label=f"Start (Batch {batch_size})")
    ax.scatter(theta_0[-1], theta_1[-1], theta_2[-1], color='r', marker='o', s=100, label=f"End (Batch {batch_size})")

    # Mark the closed-form solution
    ax.scatter(theta_closed[0], theta_closed[1], theta_closed[2], color='k', marker='*', s=200, label="Closed-form Solution")

    # Labels and legend
    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Theta 2")
    ax.set_title(f"SGD Parameter Convergence (Batch Size {batch_size})")
    ax.legend()
    plt.show()



import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor

# **Load Training and Test Data**
X_train = pd.read_csv('X_train.csv').to_numpy()
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)  # Ensure column vector

X_test = pd.read_csv('X_test.csv').to_numpy()
Y_test = pd.read_csv('Y_test.csv').to_numpy().reshape(-1, 1)  # Ensure column vector

# **Train SGD Model**
sgd_model = StochasticLinearRegressor()
sgd_theta_history = sgd_model.fit(X_train, Y_train, learning_rate=0.001, batch_size=8000, tolerance=1e-5, max_iters=100000)
sgd_theta = sgd_theta_history[-1]  # Extract final parameters from SGD

# **Function to Compute Mean Squared Error (MSE)**
# **Function to Compute Mean Squared Error (MSE)**
def compute_mse(X, Y, theta):
    """Compute Mean Squared Error (MSE) correctly"""
    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))  # Add bias column
    Y_pred = np.dot(X_bias, theta)  # Predictions
    mse = np.mean((Y_pred - Y) ** 2)  # Corrected MSE formula
    return mse

# **Compute Training and Test MSE**
train_mse = compute_mse(X_train, Y_train, sgd_theta)
test_mse = compute_mse(X_test, Y_test, sgd_theta)

# **Print and Compare Errors**
print("\nTraining vs. Test MSE Comparison:")
print("--------------------------------------")
print(f"Training MSE: {train_mse:.6f}")
print(f"Test MSE: {test_mse:.6f}")
print("--------------------------------------")

# **Plot Training vs Test Error**
plt.figure(figsize=(6, 5))
plt.bar(["Training MSE", "Test MSE"], [train_mse, test_mse], color=['blue', 'red'])
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Training vs Test MSE Comparison")
plt.show()



import numpy as np
from sampling_sgd import StochasticLinearRegressor


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    # Generate normally distributed input features
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))

    # Generate noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute target values: y = θ0 + θ1 * x1 + θ2 * x2 + ϵ
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise

    return X, y


def closed_form_solution(X, y):
    # Computes theta using the closed-form solution: theta = (X^T X)^(-1) X^T y

    X = np.column_stack((np.ones(X.shape[0]), X))  # Add bias term (column of ones)
    theta = np.linalg.inv(X.T @ X) @ X.T @ y
    return theta


# Set parameters
N = 1000000  # Number of samples
theta_true = np.array([3, 1, 2])  # True parameters (θ0, θ1, θ2)
input_mean = np.array([3, -1])  # Mean for x1, x2
input_sigma = np.array([4, 4])  # Standard deviation for x1, x2
noise_sigma = 2  # Noise standard deviation

# Generate dataset
X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train the model
sgd_regressor = StochasticLinearRegressor()
param_history = sgd_regressor.fit(X[:800000], y[:800000], learning_rate=0.001)

# Print final learned parameters
print("Final Parameters:", sgd_regressor.theta)

# Predict on training data
y_pred_train = sgd_regressor.predict(X[:800000])
mse = np.mean((y[:800000] - y_pred_train) ** 2) 
print(f"Mean Squared Error (Training): {mse}")

# Predict on test data
y_pred_test = sgd_regressor.predict(X[800000:])
mse = np.mean((y[800000:] - y_pred_test) ** 2) 
print(f"Mean Squared Error (Test): {mse}")


# Theta (closed-form solution): [3.00005329 1.00111048 2.00072717]
theta_closed_form = closed_form_solution(X[:800000], y[:800000])
print("Theta (closed-form solution):", theta_closed_form)





import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor

# Load training data
X_train = pd.read_csv('X_train.csv').to_numpy()
Y_train = pd.read_csv('Y_train.csv').to_numpy().reshape(-1, 1)

# Batch sizes to test
batch_sizes = [8000, 1]
learning_rate = 0.001
colors = ['b', 'r']  # Different color for each batch size

# Create a 3D figure
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')

# Store results for analysis
results = []

# Iterate over different batch sizes
for idx, batch_size in enumerate(batch_sizes):
    print(f"\nTraining with batch size r = {batch_size}...")
    
    # Initialize model
    model = StochasticLinearRegressor()
    
    # Train model and get parameter history
    start_time = time.time()
    param_history = model.fit(X_train, Y_train, 
                            learning_rate=learning_rate,
                            batch_size=batch_size, 
                            tolerance=1e-4,
                            max_iters=100000)
    end_time = time.time()
    
    # Convert parameter history to numpy array for easier plotting
    param_history = np.array(param_history)
    
    # Plot the parameter trajectory in 3D
    ax.plot(param_history[:, 0],  # theta0
            param_history[:, 1],  # theta1
            param_history[:, 2],  # theta2
            color=colors[idx],
            label=f'Batch Size = {batch_size}',
            linewidth=2,
            alpha=0.7)
    
    # Add markers for start and end points
    ax.scatter(param_history[0, 0],  # theta0 start
              param_history[0, 1],  # theta1 start
              param_history[0, 2],  # theta2 start
              color=colors[idx],
              marker='o',
              s=100,
              label=f'Start (r={batch_size})')
    
    ax.scatter(param_history[-1, 0],  # theta0 end
              param_history[-1, 1],  # theta1 end
              param_history[-1, 2],  # theta2 end
              color=colors[idx],
              marker='*',
              s=200,
              label=f'End (r={batch_size})')
    
    # Calculate final loss
    final_theta = param_history[-1]
    final_loss = (1 / (2 * len(Y_train))) * np.sum((model.predict(X_train) - Y_train) ** 2)
    elapsed_time = end_time - start_time
    
    # Store results
    results.append((batch_size, final_theta, final_loss, elapsed_time))
    print(f"Final θ values for batch size {batch_size}:")
    print(f"θ₀: {final_theta[0]:.6f}")
    print(f"θ₁: {final_theta[1]:.6f}")
    print(f"θ₂: {final_theta[2]:.6f}")
    print(f"Final loss: {final_loss:.6f}, Time taken: {elapsed_time:.2f} seconds")

# Customize the 3D plot
ax.set_xlabel('θ₀')
ax.set_ylabel('θ₁')
ax.set_zlabel('θ₂')
ax.set_title('Parameter Movement in 3D Space (θ₀, θ₁, θ₂) for Different Batch Sizes')

# Add legend
ax.legend(bbox_to_anchor=(1.15, 1), loc='upper right')

# Adjust the view angle for better visualization
ax.view_init(elev=20, azim=45)

# Add grid
ax.grid(True)

# Adjust layout to prevent label cutoff
plt.tight_layout()
plt.show()

# Create DataFrame for additional analysis
df_results = pd.DataFrame(results, columns=["Batch Size", "Final Theta", "Final Loss", "Time Taken"])

# Plot convergence metrics



#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import numpy as np
import pandas as pd


# In[ ]:


X_train = pd.read_csv('X_train.csv')
Y_train = pd.read_csv('Y_train.csv')





import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor, generate


N = 1000000  
theta_true = np.array([3, 1, 2])  
input_mean = np.array([3, -1])  
input_sigma = np.array([2, 2]) 
noise_sigma = np.sqrt(2)

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
train_size = int(0.8 * N)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]


def run_experiment( learning_rate=0.001):
    results = {}
    x=True
    model = StochasticLinearRegressor()
    theta_history = model.fit(X_train, y_train,
                            learning_rate=learning_rate)
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    batch_size={0:1,1:80,2:8000,3:800000}
    for i in range(4):
        train_mse = np.mean((train_pred[i] - y_train) ** 2)
        test_mse = np.mean((test_pred[i] - y_test) ** 2)
        
        results[batch_size[i]] = {
            'final_theta': model.ls[i][-1],
            'theta_history': theta_history[i],
            'train_mse': train_mse,
            'test_mse': test_mse
        }
        
        print(f"Final theta: {model.ls[i][-1]}")
        print(f"Training MSE: {train_mse:.4f}")
        print(f"Test MSE: {test_mse:.4f}")
    
    return results

def plot_parameter_trajectories(results):
    """Plot 3D trajectories of parameter updates."""
    fig = plt.figure(figsize=(15, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    colors = ['b', 'g', 'r', 'c']
    for (batch_size, result), color in zip(results.items(), colors):
        theta_history = np.array(result['theta_history'])
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2],
                label=f'Batch size={batch_size}', color=color)
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2],
                  color=color, marker='*', s=100)
    
    ax.set_xlabel('θ₀')
    ax.set_ylabel('θ₁')
    ax.set_zlabel('θ₂')
    ax.legend()
    plt.title('Parameter Trajectories for Different Batch Sizes')
    plt.savefig('plt2.png')
    plt.show()


batch_sizes = [1,80,8000,800000]
results = run_experiment()

# Compute closed-form solution
model = StochasticLinearRegressor()
theta_closed_form = model.closed_form_solution(X_train, y_train)
print("\nClosed-form solution:")
print(f"Theta: {theta_closed_form}")
plot_parameter_trajectories(results)



import pickle
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor

import time

import numpy as np
from sampling_sgd import generate




def compare_parameters(X,y,theta):
    model1=StochasticLinearRegressor()
    param1=theta
    param2=model1.closed_form(X,y)
    model2=StochasticLinearRegressor()
    param3=model2.fit(X,y,0.01,8000)
    print("original : ",param1)
    print("closed form: ",param2)
    print("sgd: ",param3[2][-1])
def run_and_plot_sgd(X, y, batch_sizes=[1,80,8000,800000], eta=0.001, tolerance=1e-6,window_size=10):
    """
    Runs SGD for multiple batch sizes, tracks time, and plots cost and theta trajectories.
    
    Parameters:
        sgd_function (function): The SGD function that takes (X, y, batch_size, eta, tolerance) as input 
                                and returns (theta, cost_history).
        X (numpy.ndarray): Input data of shape (N, 2).
        y (numpy.ndarray): Target values of shape (N,).
        batch_sizes (list): List of batch sizes to evaluate.
        eta (float): Learning rate for SGD. Default is 0.001.
        tolerance (float): Convergence tolerance. Default is 1e-6.
    """
    results = {}
    model=StochasticLinearRegressor()

    

    model.fit(X, y, eta,window_size=window_size)

    model.save_model('sgd.pkl')
    with open('sgd.pkl', 'rb') as f:
            model_data = pickle.load(f)
    for batch_size in batch_sizes:
        
        print(f"\nRunning SGD for batch size {batch_size}...")

        theta = model_data['theta']
        theta_history = model_data['theta_history']
        cost_history = model_data['cost_history']
       
        elapsed_time=model_data['time_elapsed']
        time_taken=elapsed_time[batch_size]
        results[batch_size] = {
            "theta": theta[batch_size],
            "cost_history": cost_history[batch_size],
            "time": elapsed_time[batch_size],
            "theta_history":theta_history[batch_size]
           
        }
        print(f"Batch size {batch_size}:")
        print(f"  Learned theta: {theta}")
        print(f"  Time taken: {time_taken:.2f} seconds")
    
   
        plt.figure(figsize=(10, 6))
        
        plt.plot(results[batch_size]["cost_history"])
        plt.xlabel("Iterations")
        plt.ylabel("Cost")
        plt.title(f"Cost vs Iterations for Batch Sizes {batch_size}: window_size {window_size}")
        plt.legend()
        plt.grid(True)
        plt.savefig(f"cost_vs_iter_b{batch_size}_w{window_size}.png")
    
   
    
    
    print("\nTime for convergence (seconds):")
    for batch_size, result in results.items():
        print(f"  Batch size {batch_size} window_size {window_size}: {result['time']:.2f} seconds ")
   
    
    

def plot_theta_trajectory(theta_history, batch_size):
    # theta_history = np.array(theta_history)
    fig = plt.figure(figsize=(20, 17))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label=f'Batch Size {batch_size}')


    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    ax.set_title('Theta Trajectory in Parameter Space with Projections')
    ax.legend()
    plt.savefig(f"3_d_{batch_size}")

def compute_convergence_criteria(X_train,y_train):
    for window_size in [1,10,1000]:
        run_and_plot_sgd(X_train,y_train,window_size=window_size)


N=1000000
theta=[3, 1, 2] 
input_mean=3
input_sigma=2
noise_sigma=np.sqrt(2)
X,y=generate(N, theta, input_mean, input_sigma, noise_sigma)
indices = np.arange(N)
np.random.shuffle(indices)
X = X[indices]
y = y[indices]

train_size = int(0.8 * N)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

compare_parameters(X_train,y_train,theta)
# compute_convergence_criteria(X_train,y_train)


# run_and_plot_sgd(X_train,y_train)


# model=StochasticLinearRegressor()

# theta_history = model.fit(X_train, y_train, 0.001)
# for batch_size in [1, 80, 8000,800000]:
#     if batch_size==1:
#         index=0
#     elif batch_size==80:
#         index=1
#     elif batch_size==8000:
#         index=2
#     elif batch_size==800000:
#         index=3


#     plot_theta_trajectory(theta_history[index], batch_size)
#     y_train_pred=model.predict(X_train)
#     y_test_pred=model.predict(X_test)
#     train_mse=model.mse(y_train,y_train_pred)
#     test_mse=model.mse(y_test,y_test_pred)
#     print(f"batch size: {batch_size}")
#     print("training error: ",train_mse)
#     print("test error: ",test_mse)






import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from mpl_toolkits.mplot3d import Axes3D
from sampling_sgd import StochasticLinearRegressor as LinearRegressor

# Define learning rates to test
# learning_rates = [10, 1, 0.1, 0.01, 0.001]
learning_rates = [10,1 ,0.1, 0.01, 0.001, 0.0005, 0.00001]

n_iters_list = []
final_losses = []
time_taken_list = []

X = pd.read_csv('../data/Q1/linearX.csv')
Y = pd.read_csv('../data/Q1/linearY.csv')

X = X.to_numpy()
Y = Y.to_numpy()
y = Y.reshape(-1)

# Run experiments for each learning rate
for eta in learning_rates:
    print(f"Running gradient descent with learning rate {eta}...")
    model = LinearRegressor()
    start_time = time.time()
    param_history = model.fit(X, y, learning_rate=eta, tolerance=1e-4, max_iters=10000)
    # Extract the final parameters
    final_params = param_history[-1]  # Last row contains the final parameters
    theta_values = final_params[:-1]  # All but last value are theta
    bias_value = final_params[-1]  # Last value is bias
    print(f"Final parameters for learning rate {eta}:")
    print(f"Theta (θ): {theta_values}")
    print(f"Bias (θ₀): {bias_value}\n")
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # Track results
    n_iters = param_history.shape[0]  # Number of iterations taken to stop
    final_loss = (1 / (2 * len(y))) * np.sum((model.predict(X) - y.reshape(-1, 1)) ** 2)
    
    n_iters_list.append(n_iters)
    final_losses.append(final_loss)
    time_taken_list.append(elapsed_time)
    print(f"Learning rate {eta}: Converged in {n_iters} iterations with final loss {final_loss:.6f} in {elapsed_time:.2f} seconds\n")

# Convert learning rates to log scale
log_learning_rates = np.log10(learning_rates)

# Plot 1: n_iters vs. learning rate
plt.figure()
plt.plot(learning_rates, n_iters_list, marker='o', linestyle='-')
plt.xlabel("Learning Rate")
plt.ylabel("Iterations Until Convergence")
plt.xscale("log")
plt.title("Iterations Until Convergence vs. Learning Rate")
plt.grid(True)
plt.show()

# Plot 2: Final loss vs. learning rate
plt.figure()
plt.plot(learning_rates, final_losses, marker='o', linestyle='-')
plt.xlabel("Learning Rate")
plt.ylabel("Final Loss at Convergence")
plt.xscale("log")
plt.title("Final Loss vs. Learning Rate")
plt.grid(True)
plt.show()

# Plot 3: n_iters and final loss vs. log(learning rate)
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel("Log(Learning Rate)")
ax1.set_ylabel("Iterations Until Convergence", color=color)
ax1.plot(log_learning_rates, n_iters_list, marker='o', linestyle='-', color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.grid(True)

ax2 = ax1.twinx()  # Create a secondary y-axis
color = 'tab:blue'
ax2.set_ylabel("Final Loss at Convergence", color=color)
ax2.plot(log_learning_rates, final_losses, marker='s', linestyle='--', color=color)
ax2.tick_params(axis='y', labelcolor=color)

fig.suptitle("Iterations & Final Loss vs. Log(η)")
plt.show()

# 3D Plot: Learning Rate (η) vs. Time Taken vs. Final Loss
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(learning_rates, time_taken_list, final_losses, marker='o', color='b')
ax.set_xlabel("η (Learning Rate)")
ax.set_ylabel("Time Taken to Converge (s)")
ax.set_zlabel("Final Loss at Convergence")
ax.set_xscale("log")
ax.set_title("3D Plot: Learning Rate vs. Time vs. Final Loss")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import time
import psutil

import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    Note that we have 2 input features.

    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    # Ensure inputs are numpy arrays (for safety)
    theta = np.array(theta)
    input_mean = np.array(input_mean)
    input_sigma = np.array(input_sigma)

    # Generate input fea
    # ures x1 and x2 from normal distributions
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    
    # Generate Gaussian noise epsilon
    epsilon = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute y using y = theta_0 + theta_1*x1 + theta_2*x2 + epsilon
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + epsilon

    # Compute mean and standard deviation of y
    y_mean = np.mean(y)
    y_std = np.std(y)

    # Logging shapes and statistics for debugging
    print(f"Generated Data Shapes:")
    print(f"  X shape: {X.shape}  (should be {N, 2})")
    print(f"  y shape: {y.shape}  (should be {N,})")
    print(f"  epsilon shape: {epsilon.shape}  (should be {N,})")
    print(f"\nStatistics of y:")
    print(f"  Mean of y: {y_mean:.4f}")
    print(f"  Standard deviation of y: {y_std:.4f}")

    return X, y



class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None  # Parameter vector (n+1, 1)
        self.called = 0  # Track number of calls to fit()

    def fit(self, X, y, learning_rate=0.01, batch_size=32, tolerance=1e-4, max_iters=100000):
        """
        Fit the linear regression model using Stochastic Gradient Descent (SGD)
        with live batch loss plotting.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input feature matrix (without bias column).
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float, default=0.01
            Learning rate for gradient descent.
        batch_size : int, default=32
            Batch size for mini-batch updates.
        tolerance : float, default=1e-4
            Stopping criterion based on epoch loss change.
        max_iters : int, default=100000
            Maximum number of iterations.

        Returns
        -------
        param_history : numpy array of shape (n_iter, n+1)
            The list of parameter values at each iteration.
        """
        # Get dimensions
        m, n = X.shape

        # Prepend a column of ones to X for bias term
        X = np.hstack((np.ones((m, 1)), X))  # X is now (m, n+1)
        y = y.reshape(-1, 1)  # Ensure y is (m, 1)

        # Initialize parameters
        if self.called == 0:
            self.theta = np.zeros((n + 1, 1))  # (n+1, 1)

        self.called += 1  # Track number of calls

        num_batches = m // batch_size + int(m % batch_size != 0)  # Ensure last batch is counted
        prev_epoch_loss = 0  # **Updated Fix: Start from 0**
        
        param_history = []  # Store parameter history
        batch_loss_history = []  # Store batch loss for live plotting

        # **Initialize Live Plot**
        # plt.ion()  # Enable interactive mode
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iteration")
        # ax.set_ylabel("Batch Loss")
        # ax.set_title("Live Batch Loss Plot")
        # line, = ax.plot([], [], 'r-', label="Batch Loss")  # Line for updating loss
        # plt.legend()
        
        iter_count = 0  # Track number of iterations

        # **Training Loop Over Epochs**
        for epoch in range(max_iters // num_batches):
            epoch_loss = 0  # Initialize loss for this epoch

            # **Iterate Over Mini-Batches**
            for batch_no in range(num_batches):
                batch_start = batch_no * batch_size
                batch_end = min((batch_no + 1) * batch_size, m)
                X_b = X[batch_start:batch_end]
                y_b = y[batch_start:batch_end]
                actual_batch_size = X_b.shape[0]

                # Compute Predictions
                y_pred = np.dot(X_b, self.theta)  # (batch_size, 1)

                # Compute Batch Loss
                batch_loss = (1 / (2 * actual_batch_size)) * np.sum((y_pred - y_b) ** 2)
                epoch_loss += actual_batch_size * batch_loss  # Accumulate batch loss weighted by size

                # Compute Gradient and Update Parameters
                grad_theta = (1 / actual_batch_size) * np.dot(X_b.T, (y_pred - y_b))  # (n+1, 1)
                self.theta -= learning_rate * grad_theta  # Update parameters

                # Store parameter history at this iteration
                param_history.append(self.theta.flatten().copy())  # Ensure independent storage

                # **Update Live Plot**
                batch_loss_history.append(batch_loss)
                # line.set_xdata(range(len(batch_loss_history)))
                # line.set_ydata(batch_loss_history)
                # ax.relim()
                # ax.autoscale_view()
                # fig.canvas.flush_events()  # Refresh the plot

                iter_count += 1  # Increment iteration count

            # **Normalize Epoch Loss**
            epoch_loss /= m

            print(f"Epoch {epoch + 1}: Loss = {epoch_loss:.6f}, self.theta = {self.theta }" , end = '\r')
            # **Check Convergence**
            if abs(epoch_loss - prev_epoch_loss) &lt; tolerance:
                print(f"[EARLY STOPPING] Converged at epoch {epoch} with loss = {epoch_loss:.6f}")
                break

            prev_epoch_loss = epoch_loss  # Update for next epoch tracking

        # plt.ioff()  # Turn off interactive mode
        # plt.show()  # Display final plot

        return np.array(param_history)  # Convert list to numpy array (n_iter, n+1)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input feature matrix.

        Returns
        -------
        y_pred : numpy array of shape (n_samples, 1)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.hstack((np.ones((m, 1)), X))  # Add bias column
        return np.dot(X, self.theta)  # Compute predictions



#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:


plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:


def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
plt.show()






import matplotlib.pyplot as plt
from logistic_regression import *
# Load data
X = np.loadtxt("data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("data/Q3/logisticY.csv", delimiter=",")

# Train logistic regression model
lr = LogisticRegressor()
lr.fit(X, y, learning_rate=0.01)
theta = lr.theta
print(f"theta for logistic regression :\n{theta}")
# print(lr.predict(X))

# Plot decision boundary
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='x', label="Class 0")
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='o', label="Class 1")

# Decision boundary line
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match9-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]
plt.plot(x1_vals, x2_vals, 'g-', label="Decision Boundary")

plt.xlabel("Feature 1 (x1)")
plt.ylabel("Feature 2 (x2)")
plt.title("Logistic Regression Decision Boundary")
plt.legend()
# plt.savefig("q3_decision_boundary.png")
plt.show()  
</FONT>


import numpy as np
import matplotlib.pyplot as plt
import time

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.called = 0  # Track number of times fit() is called
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, max_iters=100, tolerance=1e-4):
        """
        Fit the logistic regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        max_iters : int, default=100
            Maximum number of iterations for Newton's Method.
        
        tolerance : float, default=1e-4
            The stopping criterion based on function value change.
        
        Returns
        -------
        param_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Add intercept term (convert X from m x n to m x (n+1))
        m, n = X.shape
        X = np.hstack((X, np.ones((m, 1))))  # Append a column of ones
        
        # Initialize parameters only if fit() is called for the first time
        if self.called == 0:
            self.theta = np.zeros((n + 1, 1))  # (n+1) x 1
        self.called += 1  # Increment call count
        
        y = y.reshape(-1, 1)  # Ensure y is a column vector
        param_history = []
        prev_log_likelihood = -np.inf
        loss_history = []
        
        # Setup live plot
        # plt.ion()
        # fig, ax = plt.subplots()
        # ax.set_xlabel("Iterations")
        # ax.set_ylabel("Log-Likelihood")
        # ax.set_title("Live Loss Plot")
        # line, = ax.plot([], [], 'r-')
        
        for i in range(max_iters):
            # Compute predictions
            h = self.sigmoid(X @ self.theta)
            
            # Compute log-likelihood
            log_likelihood = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
            loss_history.append(log_likelihood)
            
            # Compute gradient
            gradient = X.T @ (y - h)  # (n+1) x 1
            
            # Compute Hessian
            R = np.diagflat(h * (1 - h))  # m x m
            H = -(X.T @ R @ X)  # (n+1) x (n+1)
            
            # Newton's update step
            theta_update = np.linalg.inv(H) @ gradient  # (n+1) x 1
            self.theta -= theta_update
            param_history.append(self.theta.flatten())
            
            # Update live loss plot
            # line.set_xdata(range(len(loss_history)))
            # line.set_ydata(loss_history)
            # ax.relim()
            # ax.autoscale_view()
            # fig.canvas.flush_events()
            # plt.pause(0.001)
            
            # Check stopping criterion (change in function value)
            if abs(log_likelihood - prev_log_likelihood) &lt; tolerance:
                print(f"Converged after {i+1} iterations.")
                break
            prev_log_likelihood = log_likelihood
        
        # plt.ioff() plt.show()
        
        return np.array(param_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Add intercept term
        m, _ = X.shape
        X = np.hstack((X, np.ones((m, 1))))
        
        # Compute probabilities
        probs = self.sigmoid(X @ self.theta)
        
        # Convert to binary predictions
        return (probs &gt;= 0.5).astype(int).flatten()



import numpy as np
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

# Load data
X = np.loadtxt("data\Q3\logisticX.csv", delimiter=",")
y = np.loadtxt("data\Q3\logisticY.csv", delimiter=",").reshape(-1)  # Ensure y is a 1D array

train_X = X[:80]
train_y = y[:80]
test_X = X[80:]
test_y = y[80:]

# Train and predict
model = LogisticRegressor()
param_history = model.fit(train_X, train_y)
train_y_pred = model.predict(train_X)
test_y_pred = model.predict(test_X)

# Compute accuracy
train_accuracy = np.mean(train_y_pred == train_y) * 100  
test_accuracy = np.mean(test_y_pred == test_y) * 100  

# Output results
print("No of iterations:", param_history.shape[0])
print("Parameter list:", param_history)
print("Final Parameters:", model.theta)
print(f"Train Accuracy: {train_accuracy:.2f}%")
print(f"Test Accuracy: {test_accuracy:.2f}%")

# Plot training data points
X = train_X
y = train_y

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label="Label 0", edgecolors='k', facecolors='none')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label="Label 1")

# Plot decision boundary
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_vals = -(model.theta[0] + model.theta[1] * x1_vals) / model.theta[2]

plt.plot(x1_vals, x2_vals, 'r-', label="Decision Boundary")
plt.xlabel("x1 (Normalized)")
plt.ylabel("x2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.grid()
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import os
from logistic_regression import LogisticRegressor

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q3/logisticX.csv'), delimiter=',')
    y = np.genfromtxt(os.path.abspath('data/Q3/logisticY.csv'), delimiter=',')

    regressor = LogisticRegressor()
    theta = regressor.fit(X, y, learning_rate=1)
    print("Learned coefficients (theta):", regressor.theta)

    plt.figure(figsize=(8, 6))
    idx0 = np.where(y == 0)[0]
    idx1 = np.where(y == 1)[0]
    
    plt.scatter(X[idx0, 0], X[idx0, 1], marker='o', color='blue', label='Class 0')
    plt.scatter(X[idx1, 0], X[idx1, 1], marker='x', color='red', label='Class 1')
    
    if regressor.theta[2] != 0:
        x1_vals = np.linspace(np.min(X[:, 0]) - 1, np.max(X[:, 0]) + 1, 100)
        mu1, mu2 = regressor.mean[0], regressor.mean[1]
        sigma1, sigma2 = regressor.std[0], regressor.std[1]
        x2_vals = mu2 - (sigma2 / regressor.theta[2]) * (regressor.theta[0] + regressor.theta[1] * ((x1_vals - mu1) / sigma1))
        plt.plot(x1_vals, x2_vals, 'k-', linewidth=2, label='Decision Boundary')
    else:
        mu1, sigma1 = regressor.mean[0], regressor.std[0]
        if regressor.theta[1] != 0:
            x1_const = mu1 - sigma1 * (regressor.theta[0] / regressor.theta[1])
            plt.axvline(x=x1_const, color='k', linewidth=2, label='Decision Boundary')
        else:
            print("Both theta[1] and theta[2] are zero; decision boundary cannot be determined.")

    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.title("Logistic Regression: Training Data and Decision Boundary")
    plt.legend()
    # plt.savefig('plots/Q3/plot.png')
    plt.show()




#!/usr/bin/env python
# coding: utf-8

# In[23]:


import pickle
import matplotlib.pyplot as plt
import importlib
import logistic_regression
importlib.reload(logistic_regression)
from logistic_regression import LogisticRegressor

import time

import numpy as np



# In[30]:


def plot_data(X, y):
    
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match9-1.html#8" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    positive = y == 1
    negative = y == 0

    
    plt.scatter(X[positive, 0], X[positive, 1], c='b', marker='o', label='Label 1')
    plt.scatter(X[negative, 0], X[negative, 1], c='r', marker='x', label='Label 0')
</FONT>

def plot_decision_boundary(theta, X):

    x1_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]

    plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

def plot(X,y,theta):
    plt.figure(figsize=(20, 15))
    plot_data(X, y)
    plot_decision_boundary(theta, X)

    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.legend()
    plt.title('Training Data and Decision Boundary')
    plt.grid()
    plt.savefig("logistic_decision_boundary.png")


# In[ ]:


X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=',')
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=',')

model=LogisticRegressor()
model.fit(X,y)
model.save_model('logistic_reg.pkl')
with open('logistic_reg.pkl', 'rb') as f:
    model_data = pickle.load(f)

theta = model_data['theta']
print(theta)


# In[32]:


plot(X,y,theta)





#!/usr/bin/env python
# coding: utf-8

# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[2]:


import numpy as np
import matplotlib.pyplot as plt
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match9-1.html#9" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import pandas as pd


# In[6]:


X = pd.read_csv('../data/Q3/logisticX.csv', header=None).to_numpy()
Y = pd.read_csv('../data/Q3/logisticY.csv', header=None).to_numpy().reshape(-1,1)
</FONT>

# In[7]:


X.shape
Y.shape


# 

# In[ ]:


X_mean = np.mean(X, axis= 0)
X_std = np.std(X, axis= 0)
print(X_mean.shape)
print(X_std.shape)


# In[ ]:


X_norm = (X - X_mean)/X_std
print(X_norm)


# In[23]:


from logistic_regression import LogisticRegressor

logistic_regressor = LogisticRegressor()
params = logistic_regressor.fit(X_norm, Y)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:


plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:


def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
plt.show()





import numpy as np

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = True
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        assume_same_covariance : bool, default=False
            Whether to assume a shared covariance matrix across both classes.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays (mu_0, mu_1, sigma) 
            If assume_same_covariance = False - 4-tuple of numpy arrays (mu_0, mu_1, sigma_0, sigma_1)
            The parameters learned by the model.
        """
        # Store assumption
        self.assume_same_covariance = assume_same_covariance
        
        # Compute class priors
        self.phi = np.mean(y)  # P(y=1)
        
        # Compute class means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        if assume_same_covariance:
            # Compute shared covariance matrix
            m = X.shape[0]
            self.sigma = (1/m) * np.sum([(X[i] - (self.mu_1 if y[i] else self.mu_0)).reshape(-1, 1) @
                                         (X[i] - (self.mu_1 if y[i] else self.mu_0)).reshape(1, -1)
                                         for i in range(m)], axis=0)
            return self.mu_0, self.mu_1, self.sigma
        else:
            # Compute class-specific covariance matrices
            self.sigma_0 = np.cov(X[y == 0].T, bias=True)
            self.sigma_1 = np.cov(X[y == 1].T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data using Bayes' theorem.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        def gaussian_pdf(x, mean, cov):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match9-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            d = x.shape[0]
            det_cov = np.linalg.det(cov)
            inv_cov = np.linalg.inv(cov)
            norm_factor = 1 / (np.sqrt((2 * np.pi) ** d * det_cov))
            exp_term = np.exp(-0.5 * (x - mean).T @ inv_cov @ (x - mean))
</FONT>            return norm_factor * exp_term
        
        # Compute likelihoods using Gaussian PDF
        p_x_given_y0 = np.array([gaussian_pdf(x, self.mu_0, self.sigma if self.assume_same_covariance else self.sigma_0) for x in X])
        p_x_given_y1 = np.array([gaussian_pdf(x, self.mu_1, self.sigma if self.assume_same_covariance else self.sigma_1) for x in X])
        
        # Compute posterior probabilities using Bayes' theorem
        p_y1_given_x = (self.phi * p_x_given_y1) / (self.phi * p_x_given_y1 + (1 - self.phi) * p_x_given_y0)
        
        # Assign class labels based on probability threshold
        return (p_y1_given_x &gt;= 0.5).astype(int)



import numpy as np
import matplotlib.pyplot as plt
from gda import *

# Load data
X = np.loadtxt("data/Q4/q4x.dat")
y = np.loadtxt("data/Q4/q4y.dat", dtype=str)
y = (y == "Canada").astype(int)  # Convert labels to 0 (Alaska) and 1 (Canada)

# Train GDA model with shared covariance
gda = GaussianDiscriminantAnalysis()
mu0, mu1, sigma = gda.fit(X, y, assume_same_covariance=True)
print(f" mean0 : {mu0}\n mean1 : {mu1}\n sigma : {sigma}\n")
# print(gda.predict(X))

# Plot data points
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='x', label="Alaska")
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='o', label="Canada")

# Plot decision boundary (linear)
# x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
# # x2_vals = -(sigma[0, 0] * x1_vals + (mu0[0] - mu1[0])) / sigma[1, 1]
# sigma_inv = np.linalg.inv(sigma)
# w = sigma_inv @ (mu1 - mu0)
# w0 = -0.5 * (mu1.T @ sigma_inv @ mu1 - mu0.T @ sigma_inv @ mu0)
# x2_vals = -(w[0] * x1_vals + w0) / w[1]
# After fitting the model:
sigma_inv = np.linalg.inv(sigma)
w_norm = sigma_inv @ (mu1 - mu0)
w0_norm = -0.5 * (mu1.T @ sigma_inv @ mu1 - mu0.T @ sigma_inv @ mu0)

# Transform coefficients to original space
A = w_norm[0] * gda.X_std[1]
B = w_norm[1] * gda.X_std[0]
C = - (w_norm[0] * gda.X_std[1] * gda.X_mean[0] + w_norm[1] * gda.X_std[0] * gda.X_mean[1]) + w0_norm * gda.X_std[0] * gda.X_std[1]

# Decision boundary in original coordinates: A*x1 + B*x2 + C = 0
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_vals = (-A * x1_vals - C) / B  # Solve for x2

plt.plot(x1_vals, x2_vals, 'g-', label="Decision Boundary")

plt.xlabel("Feature 1 (x1)")
plt.ylabel("Feature 2 (x2)")
plt.title("GDA with Shared Covariance (Linear Boundary)")
plt.legend()
# plt.savefig("q4_linear_boundary.png")
plt.show()




# Load data
X = np.loadtxt("data/Q4/q4x.dat")
y = np.loadtxt("data/Q4/q4y.dat", dtype=str)
y = (y == "Canada").astype(int)  # Convert labels to 0 (Alaska) and 1 (Canada)

# Train GDA model with separate covariances
gda = GaussianDiscriminantAnalysis()
mu0, mu1, sigma0, sigma1 = gda.fit(X, y, assume_same_covariance=False)
print("\n Not assuming Same Covariance\n")
print(f" mean0 : {mu0}\n mean1 : {mu1}\n sigma0 : {sigma0}\n sigma1 : {sigma1}\n")

# Plot data points
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', marker='x', label="Alaska")
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='o', label="Canada")

# Plot quadratic decision boundary
<A NAME="2"></A><FONT color = #0000FF><A HREF="match9-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)
X1, X2 = np.meshgrid(x1_vals, x2_vals)
Z = np.zeros_like(X1)
</FONT>
# Compute decision boundary
for i in range(len(x1_vals)):
    for j in range(len(x2_vals)):
<A NAME="10"></A><FONT color = #FF0000><A HREF="match9-1.html#10" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x = np.array([X1[i, j], X2[i, j]])
        delta0 = (x - mu0) @ np.linalg.inv(sigma0) @ (x - mu0).T
</FONT><A NAME="1"></A><FONT color = #00FF00><A HREF="match9-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        delta1 = (x - mu1) @ np.linalg.inv(sigma1) @ (x - mu1).T
        Z[i, j] = delta1 - delta0

# Plot contour at Z = 0 (decision boundary)
plt.contour(X1, X2, Z, levels=[0], colors='green', label="Quadratic Boundary")

plt.xlabel("Feature 1 (x1)")
plt.ylabel("Feature 2 (x2)")
plt.title("GDA with Separate Covariances (Quadratic Boundary)")
plt.legend()
# plt.savefig("q4_quadratic_boundary.png")
plt.show()
</FONT>



import numpy as np
import os
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
    y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)
    
    gda = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda.fit(X, y, assume_same_covariance=True)
    normalized_graph=False
    
    print("Normalization parameters:")
    print("Estimated class means in normalized space:")
    print("mu0 (Alaska):", mu0)
    print("mu1 (Canada):", mu1)
    print("Pooled covariance matrix (in normalized space):")
    print(sigma)
    print()

    # linear separator
    A,B,C = gda.w[0], gda.w[1], gda.b

    if normalized_graph:
        X=(X-gda.mean)/gda.std
    else:
        A /= gda.std[0]
        B /= gda.std[1]
        C -= A*gda.mean[0] + B*gda.mean[1]

    alaska_indices = X[y == 'Alaska']
    canada_indices = X[y == 'Canada']
    plt.figure(figsize=(8, 6))
    plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
    plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')    
    

    if np.abs(B) &gt; 1e-10:
        x_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)
        y_vals = -(A / B) * x_vals - (C / B)
        plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')
    elif np.abs(A) &gt; 1e-10:
        x_val = -C / A
        plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')
    else:
        print("Invalid decision boundary: both A and B are zero.")

    plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
    plt.ylabel('x2 (Marine Growth Ring Diameter)')
    plt.title('GDA Training Data: Alaska vs. Canada')
    plt.legend()

    plt.savefig('plots/Q4/linear_separator_notnormal.png')
    # plt.show()



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis
from plot import plot_data, plot_linear_boundary, plot_quadratic_boundary, plot_combined_boundaries
def normalize_data(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X - mean) / std, mean, std

# Load data
X_nn = np.loadtxt('q4x.dat')
X,std,mean = normalize_data(X_nn)
y_original = np.loadtxt('q4y.dat',dtype=str)
y = np.where(y_original == 'Alaska', 0, 1)
# Fit linear model (shared covariance)
gda_linear = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda_linear.fit(X_nn, y, assume_same_covariance=True)
print(f"mu_0 : {mu_0}")
print(f"mu_1 : {mu_1}")
print(f"sigma : {sigma}")

# Fit quadratic model (separate covariances)
gda_quadratic = GaussianDiscriminantAnalysis()
mu_0_q, mu_1_q, sigma_0_q, sigma_1_q = gda_quadratic.fit(X_nn, y, assume_same_covariance=False)

print(f"mu_0 : {mu_0_q}")
print(f"mu_1 : {mu_1_q}")
print(f"sigma_0 : {sigma_0_q}")
print(f"sigma_1 : {sigma_1_q}")
# Create visualizations
plot_data(X_nn, y)

plot_linear_boundary(X_nn,X, y ,mu_0,mu_1,sigma,np.mean(y))

plot_quadratic_boundary(X_nn,X,y,mu_0_q,mu_1_q,sigma_0_q,sigma_1_q)

plot_combined_boundaries(X_nn,X,y,mu_0,mu_1,sigma,np.mean(y),sigma_0_q,sigma_1_q)
# plot_comparison(X, y, gda_linear, gda_quadratic)

# plot_together(X,y,gda_linear,gda_quadratic)




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train GDA with shared covariance (Linear Boundary)
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)

# Compute posterior probabilities for Linear Boundary
sigma_inv = np.linalg.inv(sigma)
w = np.dot(sigma_inv, (mu_1 - mu_0))  # Weight vector
b = -0.5 * np.dot(mu_1.T, np.dot(sigma_inv, mu_1)) + 0.5 * np.dot(mu_0.T, np.dot(sigma_inv, mu_0))

linear_decision = np.dot(X_norm, w) + b
y_pred_linear = (linear_decision &gt; 0).astype(int)  # Convert to class labels

# Train GDA with separate covariance (Quadratic Boundary)
mu_0_q, mu_1_q, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)

# Compute posterior probabilities for Quadratic Boundary
sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)

A = 0.5 * (sigma_0_inv - sigma_1_inv)
B = np.dot(sigma_1_inv, mu_1_q) - np.dot(sigma_0_inv, mu_0_q)
C = (
    -0.5 * np.dot(mu_1_q.T, np.dot(sigma_1_inv, mu_1_q))
    + 0.5 * np.dot(mu_0_q.T, np.dot(sigma_0_inv, mu_0_q))
    + 0.5 * np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
)

quadratic_decision = np.einsum("ij,jk,ik-&gt;i", X_norm, A, X_norm) + np.dot(X_norm, B) + C
y_pred_quadratic = (quadratic_decision &gt; 0).astype(int)  # Convert to class labels

# Compute Misclassification Metrics
misclassified_linear = (y_pred_linear != y)
misclassified_quadratic = (y_pred_quadratic != y)

num_misclass_linear = np.sum(misclassified_linear)
num_misclass_quadratic = np.sum(misclassified_quadratic)

misclass_rate_linear = num_misclass_linear / len(y)
misclass_rate_quadratic = num_misclass_quadratic / len(y)

# Output the misclassification statistics
print(f"Misclassification Rate (Linear Boundary): {misclass_rate_linear:.4f} ({num_misclass_linear} points misclassified)")
print(f"Misclassification Rate (Quadratic Boundary): {misclass_rate_quadratic:.4f} ({num_misclass_quadratic} points misclassified)")

# Unnormalize for plotting
X1_unnorm = X[:, 0]
X2_unnorm = X[:, 1]

# Plot the data with misclassified points highlighted
plt.figure(figsize=(8, 6))
plt.scatter(X1_unnorm[y == 0], X2_unnorm[y == 0], color='red', label="Alaska (Class 0)", alpha=0.6)
plt.scatter(X1_unnorm[y == 1], X2_unnorm[y == 1], color='blue', label="Canada (Class 1)", alpha=0.6)

# Mark misclassified points separately for each model
plt.scatter(X1_unnorm[misclassified_linear], X2_unnorm[misclassified_linear], edgecolors='black', facecolors='none', s=100, label="Misclassified (Linear)")
plt.scatter(X1_unnorm[misclassified_quadratic], X2_unnorm[misclassified_quadratic], edgecolors='green', facecolors='none', s=100, label="Misclassified (Quadratic)")

# Labels and Legends
plt.xlabel("Freshwater Growth Ring Diameter (X1)")
plt.ylabel("Marine Water Growth Ring Diameter (X2)")
plt.title("Misclassified Points in GDA")
plt.legend()
plt.grid()
plt.savefig("../fig/gda_misclassification.png")
plt.show()




import pandas as pd
import numpy as np
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# Print computed parameters
print(f"μ_0 (Mean for Class 0 - Alaska):\n{mu_0}")
print(f"μ_1 (Mean for Class 1 - Canada):\n{mu_1}")
print(f"Σ (Shared Covariance Matrix):\n{sigma}")



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# **Plot Training Data**
plt.figure(figsize=(8, 6))

# Scatter plot for each class
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska (Class 0)")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada (Class 1)")

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (x1)")
plt.ylabel("Marine Water Growth Ring Diameter (x2)")
plt.title("Scatter Plot of Training Data")
plt.legend()
plt.grid()
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()  # Read each line as a string
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Compute shared covariance

# Compute Decision Boundary
sigma_inv = np.linalg.inv(sigma)  # Compute Σ⁻¹
w = np.dot(sigma_inv, (mu_1 - mu_0))  # Compute w
b = -0.5 * np.dot((mu_1 + mu_0).T, np.dot(sigma_inv, (mu_1 - mu_0)))  # Compute b

# Convert w and b back to unnormalized space
w_unnorm = w / X_std  # Adjust w using standard deviation
b_unnorm = b - np.dot(w_unnorm, X_mean)  # Adjust b using mean

# Generate decision boundary (Unnormalized)
x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)  # Unnormalized x1 values
x2_vals = -(w_unnorm[0] / w_unnorm[1]) * x1_vals - (b_unnorm / w_unnorm[1])  # Unnormalized decision boundary

# Plot GDA Decision Boundary
plt.figure(figsize=(8, 6))

# Scatter plot of original (unnormalized) data
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska (Class 0)")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada (Class 1)")

# Corrected decision boundary using unnormalized values
plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (x1)")
plt.ylabel("Marine Water Growth Ring Diameter (x2)")
plt.title("GDA Decision Boundary (Fixed)")
plt.legend()
plt.grid()

# Save & Show the Figure
plt.savefig("../fig/gda_decision_boundary_fixed.png")
plt.show()




import numpy as np
import pandas as pd
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train Gaussian Discriminant Analysis Model with separate covariances
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)  # Compute separate covariance matrices

# Print computed parameters
print(f"μ_0 (Mean for Class 0 - Alaska):\n{mu_0}")
print(f"μ_1 (Mean for Class 1 - Canada):\n{mu_1}")
print(f"Σ_0 (Covariance Matrix for Alaska):\n{sigma_0}")
print(f"Σ_1 (Covariance Matrix for Canada):\n{sigma_1}")




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load feature dataset (X) - values are space-separated
X = pd.read_csv("../data/Q4/q4x.dat", sep='\s+', header=None).to_numpy()

# Load labels (y) and convert "Alaska" → 0, "Canada" → 1
with open("../data/Q4/q4y.dat", "r") as f:
    y_raw = f.read().splitlines()
y = np.array([0 if label.strip() == "Alaska" else 1 for label in y_raw])  # Convert to numerical labels

# Normalize features (column-wise)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_norm = (X - X_mean) / X_std  # Normalized X

# Train GDA Model (for both linear and quadratic cases)
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X_norm, y, assume_same_covariance=True)  # Linear boundary
mu_0_q, mu_1_q, sigma_0, sigma_1 = gda.fit(X_norm, y, assume_same_covariance=False)  # Quadratic boundary

# Unnormalize means for plotting
mu_0 = mu_0 * X_std + X_mean
mu_1 = mu_1 * X_std + X_mean

# Compute Linear Decision Boundary
sigma_inv = np.linalg.inv(sigma)
theta = sigma_inv @ (mu_1 - mu_0)
<A NAME="11"></A><FONT color = #00FF00><A HREF="match9-1.html#11" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta_0 = -0.5 * (mu_1.T @ sigma_inv @ mu_1) + 0.5 * (mu_0.T @ sigma_inv @ mu_0)

# Define range for plotting
x1_range = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_linear = -(theta[0] / theta[1]) * x1_range - (theta_0 / theta[1])
</FONT>
# Compute Quadratic Decision Boundary
sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)
A = 0.5 * (sigma_0_inv - sigma_1_inv)
B = (sigma_1_inv @ mu_1 - sigma_0_inv @ mu_0).T
C = (
    -0.5 * (mu_1.T @ sigma_1_inv @ mu_1)
    + 0.5 * (mu_0.T @ sigma_0_inv @ mu_0)
    - 0.5 * np.log(np.linalg.det(sigma_1) / np.linalg.det(sigma_0))
)

# Create meshgrid for quadratic boundary
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 200)
x2_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 200)
X1, X2 = np.meshgrid(x1_vals, x2_vals)
Z = np.zeros_like(X1)

for i in range(X1.shape[0]):
    for j in range(X1.shape[1]):
        x_vec = np.array([[X1[i, j]], [X2[i, j]]])
        Z[i, j] = x_vec.T @ A @ x_vec + B @ x_vec + C  # Quadratic boundary equation

# Plot data points
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Alaska (Class 0)')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Canada (Class 1)')

# Plot decision boundaries
plt.plot(x1_range, x2_linear, color="black", label="Linear Boundary")
plt.contour(X1, X2, Z, levels=[0], colors="green", linewidths=2)  # Quadratic Boundary

# Labels and Legend
plt.xlabel("Freshwater Growth Ring Diameter (X1)")
plt.ylabel("Marine Water Growth Ring Diameter (X2)")
plt.title("GDA Decision Boundaries (Linear and Quadratic)")
plt.legend()
plt.grid()
plt.savefig("../fig/gda_decision_boundary.png")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import os

X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)

alaska_indices = X[y == 'Alaska']
canada_indices = X[y == 'Canada']

plt.figure(figsize=(8, 6))
plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')

plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
plt.ylabel('x2 (Marine Growth Ring Diameter)')
plt.title('GDA Training Data: Alaska vs. Canada')
plt.legend()

# plt.savefig('plots/Q4/datapoints.png')
plt.show()




#!/usr/bin/env python
# coding: utf-8

# In[50]:


import pickle
import matplotlib.pyplot as plt
import importlib
import gda
importlib.reload(gda)
from gda import GaussianDiscriminantAnalysis

import time

import numpy as np


# In[51]:


X = np.loadtxt("../data/Q4/q4x.dat")  
y = np.loadtxt("../data/Q4/q4y.dat",dtype=str) 


# In[43]:


model=GaussianDiscriminantAnalysis()
mu_0,mu_1,sigma=model.fit(X,y,True)
print(y)
y_pred=model.predict(X)
print(y_pred)
print(f"mu_0: {mu_0}")
print(f"mu_1: {mu_1}")
print(f"sigma: {sigma}")




# In[10]:


def plot_data(X,y):
    y = np.where(y == "Canada", 1, 0)
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska")  
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada") 

    plt.xlabel("Feature 1 (Normalized Growth in Freshwater)")
    plt.ylabel("Feature 2 (Normalized Growth in Marine Water)")
    plt.title("Salmon Classification: Alaska vs Canada")
    plt.legend()
    plt.grid(True)
    plt.savefig("training_data.png")

plot_data(X,y)


# In[11]:


def normalise(X):

    X_normalized = (X - np.mean(X,axis=0)) / np.std(X,axis=0)
    return X_normalized
def plot_decision_boundary(mu_0, mu_1, sigma,X,y):
    y = np.where(y == "Canada", 1, 0) 
    X=normalise(X)
    plt.figure(figsize=(12, 8))

    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', color='blue', label="Alaska") 
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', color='red', label="Canada")   
   
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

   
    X_grid = np.c_[xx.ravel(), yy.ravel()]

  
    inv_sigma = np.linalg.inv(sigma)
    
    diff_mu = mu_1 - mu_0
    
  
    const_term = 0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))


    Z = np.dot(X_grid, np.dot(inv_sigma, diff_mu)) - const_term
    Z = Z.reshape(xx.shape)


    contour = plt.contour(xx, yy, Z, levels=[0], cmap="RdBu", linewidths=2)


 

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('GDA Decision Boundary')
    plt.legend()
  
    plt.savefig("decision_boundary.png")
    

plot_decision_boundary(mu_0,mu_1,sigma,X,y)


# In[52]:


model=GaussianDiscriminantAnalysis()
mu_0,mu_1,sigma_0,sigma_1=model.fit(X,y,False)
print(y)
y_pred=model.predict(X)
print(y_pred)
print(f"mu_0: {mu_0}")
print(f"mu_1: {mu_1}")
print(f"sigma_0: {sigma_0}")
print(f"sigma_1: {sigma_1}")


# In[53]:


def plot_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y):
    y = np.where(y == "Canada", 1, 0)  
    X=normalise(X)
    plt.figure(figsize=(10, 7))
    

  
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

   
    X_grid = np.c_[xx.ravel(), yy.ravel()]

    inv_sigma = np.linalg.inv(sigma)
    
 
    diff_mu = mu_1 - mu_0
    
    const_term = 0.5 * (np.dot(mu_1.T, np.dot(inv_sigma, mu_1)) - np.dot(mu_0.T, np.dot(inv_sigma, mu_0)))

  
    Z = np.dot(X_grid, np.dot(inv_sigma, diff_mu)) - const_term
    Z = Z.reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0], cmap="RdBu", linewidths=2)


    term_0 = np.linalg.inv(sigma_0)
    term_1 = np.linalg.inv(sigma_1)
    
    diff_0 = X_grid - mu_0
    diff_1 = X_grid - mu_1
    
    quadratic_0 = np.einsum('ij,jk,ik-&gt;i', diff_0, term_0, diff_0)
    quadratic_1 = np.einsum('ij,jk,ik-&gt;i', diff_1, term_1, diff_1)
    log_det_ratio = np.log(np.linalg.det(sigma_0)) - np.log(np.linalg.det(sigma_1))

   
    Z = quadratic_1 - quadratic_0 + log_det_ratio
    Z = Z.reshape(xx.shape) 
   
   
    plt.contour(xx, yy, Z, levels=[-0.01, 0, 0.01], colors='brown', linewidths=2)
    plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', label="Alaska")
    plt.scatter(X[y==1, 0], X[y==1, 1], color='red', marker='x', label="Canada")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("GDA Decision Boundary")
    plt.legend()


    plt.savefig("quadratic_decision_boundary.png")

plot_quadratic_boundary(mu_0, mu_1, sigma_0, sigma_1, X, y)


# In[ ]:








import numpy as np
import os
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

if __name__ == '__main__':
    X = np.genfromtxt(os.path.abspath('data/Q4/q4x.dat'))
    y = np.genfromtxt(os.path.abspath('data/Q4/q4y.dat'), dtype=str)
    
    gda = GaussianDiscriminantAnalysis()
    mu0, mu1, sigma = gda.fit(X, y, assume_same_covariance=True)
    normalized_graph = True
    
    print("Normalization parameters:")
    print("Estimated class means in normalized space:")
    print("mu0 (Alaska):", mu0)
    print("mu1 (Canada):", mu1)
    print("Pooled covariance matrix (in normalized space):")
    print(sigma)
    print()

    A, B, C = gda.w[0], gda.w[1], gda.b

    if normalized_graph:
        X_plot = (X - gda.mean) / gda.std
    else:
        A = A / gda.std[0]
        B = B / gda.std[1]
        C = C - ((gda.w[0] / gda.std[0]) * gda.mean[0] + (gda.w[1] / gda.std[1]) * gda.mean[1])
        X_plot = X

    alaska_indices = X_plot[y == 'Alaska']
    canada_indices = X_plot[y == 'Canada']
    plt.figure(figsize=(8, 6))
    plt.scatter(alaska_indices[:, 0], alaska_indices[:, 1], marker='o', color='blue', label='Alaska')
    plt.scatter(canada_indices[:, 0], canada_indices[:, 1], marker='x', color='red', label='Canada')    
    
    if np.abs(B) &gt; 1e-10:
        x_vals = np.linspace(np.min(X_plot[:, 0]) - 0.5, np.max(X_plot[:, 0]) + 0.5, 200)
        y_vals = -(A / B) * x_vals - (C / B)
        plt.plot(x_vals, y_vals, 'k--', label='Linear Boundary')
    elif np.abs(A) &gt; 1e-10:
        x_val = -C / A
        plt.axvline(x=x_val, color='k', linestyle='--', label='Linear Boundary')
    else:
        print("Invalid decision boundary: both A and B are zero.")

    # --- Quadratic Boundary ---
    gda_quad = GaussianDiscriminantAnalysis()
    mu0_quad, mu1_quad, sigma0, sigma1 = gda_quad.fit(X, y, assume_same_covariance=False)
    
    if not normalized_graph:
        mu0_quad_orig = mu0_quad * gda_quad.std + gda_quad.mean
        mu1_quad_orig = mu1_quad * gda_quad.std + gda_quad.mean
        sigma0_orig = np.diag(gda_quad.std) @ sigma0 @ np.diag(gda_quad.std)
        sigma1_orig = np.diag(gda_quad.std) @ sigma1 @ np.diag(gda_quad.std)
    else:
        mu0_quad_orig = mu0_quad
        mu1_quad_orig = mu1_quad
        sigma0_orig = sigma0
        sigma1_orig = sigma1

    inv_sigma0 = np.linalg.inv(sigma0_orig)
    inv_sigma1 = np.linalg.inv(sigma1_orig)
    det_sigma0 = np.linalg.det(sigma0_orig)
    det_sigma1 = np.linalg.det(sigma1_orig)

    constant_term = np.log(det_sigma0 / det_sigma1) - 2 * np.log(gda_quad.phi0 / gda_quad.phi1)

<A NAME="14"></A><FONT color = #FF00FF><A HREF="match9-1.html#14" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1_min, x1_max = np.min(X_plot[:, 0]) - 0.5, np.max(X_plot[:, 0]) + 0.5
</FONT>    x2_min, x2_max = np.min(X_plot[:, 1]) - 0.5, np.max(X_plot[:, 1]) + 0.5
    x1_grid, x2_grid = np.meshgrid(np.linspace(x1_min, x1_max, 300), np.linspace(x2_min, x2_max, 300))

    F = np.zeros_like(x1_grid)
    for i in range(x1_grid.shape[0]):
        for j in range(x1_grid.shape[1]):
            x_point = np.array([x1_grid[i, j], x2_grid[i, j]])
            diff0 = x_point - mu0_quad_orig
            diff1 = x_point - mu1_quad_orig
            F[i, j] = (diff0.T @ inv_sigma0 @ diff0) - (diff1.T @ inv_sigma1 @ diff1) + constant_term

    quad_contour = plt.contour(x1_grid, x2_grid, F, levels=[0], colors='green', linestyles='-', linewidths=2)
    quad_contour.collections[0].set_label('Quadratic Boundary')

    plt.xlabel('x1 (Freshwater Growth Ring Diameter)')
    plt.ylabel('x2 (Marine Growth Ring Diameter)')
    plt.title('GDA Training Data: Alaska vs. Canada')
    plt.legend()
    # plt.show()
    plt.savefig('plots/Q4/quadratic.png')




#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('linearX.csv', delimiter=',')
Y = np.loadtxt('linearY.csv', delimiter=',')
m = len(X)
X = np.c_[np.ones(m), X]
theta = np.zeros(2)
alpha = 0.01
epsilon = 1e-20
max_iterations = 100000
cost_history = []
thet_history=[]

def compute_cost(X, Y, theta):
    """Compute the cost function J(θ)."""
    m = len(Y)
    error = X.dot(theta) - Y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

for i in range(max_iterations):

    h = X.dot(theta)
    gradient = (1/m) * X.T.dot(h - Y)
    theta = theta - alpha * gradient
    cost = compute_cost(X, Y, theta)
    cost_history.append(cost)
    thet_history.append(theta)


    if i &gt; 0 and abs(cost_history[i] - cost_history[i-1]) &lt; epsilon:
        print(f"Convergence reached at iteration {i}.")
        break


print(f"Final theta: {theta}")
print(f"Final cost: {cost:.6f}")


plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label='Cost (J(θ))')
plt.xlabel('Iterations')
plt.ylabel('Cost (J(θ))')
plt.title('Convergence of Gradient Descent')
plt.legend()
plt.show()


# In[6]:


plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], Y, color='b', label='Data points')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis function (hθ(x))')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.title('Linear Regression: Data and Hypothesis Function')
plt.legend()
plt.show()


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
theta0_vals = np.linspace(-100, 100, 200)
theta1_vals = np.linspace(-100, 100, 200)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, Y, t)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(T0, T1, J_vals.T, cmap='viridis', alpha=0.8)
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Cost Function Value")
ax.set_xlabel(r'$\theta_0$')
ax.set_ylabel(r'$\theta_1$')
ax.set_zlabel(r'$J(\theta)$')
ax.set_title('3D Mesh Plot of Cost Function with Color Gradient')

plt.show()


# In[1]:


i=0
for theta in thet_history:
  ax.scatter(theta[0],theta[1],cost[i],color='red',s=50)
  plt.pause(2)


# In[13]:


import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)
theta_true = np.array([3, 1, 2])

num_samples = 1_000_000
x1 = np.random.normal(3, np.sqrt(4), num_samples)
x2 = np.random.normal(-1, np.sqrt(4), num_samples)
epsilon = np.random.normal(0, np.sqrt(2), num_samples)
y = theta_true[0] + theta_true[1] * x1 + theta_true[2] * x2 + epsilon


X = np.column_stack((np.ones(num_samples), x1, x2))


m_train = int(0.8 * num_samples)
X_train, y_train = X[:m_train], y[:m_train]
X_test, y_test = X[m_train:], y[m_train:]


# In[22]:


import numpy as np
from tqdm import tqdm

import numpy as np
from tqdm import tqdm

def stochastic_gradient_descent(X, y, batch_size, lr=0.001, max_epochs=5000, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
    indices = np.random.permutation(m)
    X, y = X[indices], y[indices]
    epoch_progress = tqdm(range(max_epochs), desc=f"Batch Size {batch_size}")
    it = 0
    prev_loss = float('inf')
    for _ in epoch_progress:
        it += 1
        prev_theta = theta.copy()
        gradients = np.zeros(n)

        for i in range(0, m, batch_size):
            X_batch = X[i:i+batch_size]
            y_batch = y[i:i+batch_size]
            gradients = -X_batch.T @ (y_batch - X_batch @ theta) / batch_size
            theta -= lr * gradients
        theta_history.append(theta.copy())
        loss = np.mean((y - X @ theta) ** 2)
        if batch_size == 1:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size == 80:
            if np.linalg.norm(theta - prev_theta) &lt; tol:
                break
        elif batch_size in [8000, 800000]:
            if abs(loss - prev_loss) &lt; tol:
                break
        prev_loss = loss
    return theta, np.array(theta_history), it
batch_sizes = [1,80,8000, 800000]
theta_results = {}
theta_histories = {}

for batch in batch_sizes:
    theta, history,it = stochastic_gradient_descent(X_train, y_train, batch_size=batch)
    theta_results[batch] = theta
    theta_histories[batch] = history
    print(f"Batch Size {batch}: Learned Theta = {theta}, Iterations = {it}")


# In[23]:


def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)
print(f"Closed Form Solution Theta: {theta_closed_form}")


# In[24]:


def mean_squared_error(X, y, theta):
    predictions = X @ theta
    return np.mean((y - predictions) ** 2)
mse_closed_form_train = mean_squared_error(X_train, y_train, theta_closed_form)
mse_closed_form_test = mean_squared_error(X_test, y_test, theta_closed_form)
print("\nMean Squared Errors:")
print(f"Closed Form: Train MSE = {mse_closed_form_train}, Test MSE = {mse_closed_form_test}")


# In[ ]:


for batch in batch_sizes:
    history = theta_histories[batch]
    print(f"Batch {batch}: Shape {history.shape}")
    print(history[:5])


# In[25]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
for i, batch in enumerate([1,80,8000,800000]):
    history = theta_histories[batch]
    color = plt.cm.viridis(i / len(batch_sizes))
    ax.plot3D(history[:, 0], history[:, 1], history[:, 2], marker='o', color=color, label=f'Batch {batch}')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Theta 2')
ax.set_title('Parameter updates in 3D space')
ax.legend()
plt.show()
plt.savefig()


# In[15]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack([np.ones((X.shape[0], 1)), X])
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
def gradient(X, y, theta):
    h = sigmoid(X @ theta)
    return X.T @ (h - y)
def hessian(X, theta):
    h = sigmoid(X @ theta)
    S = np.diag(h * (1 - h))
    return X.T @ S @ X
def newton_method(X, y, tol=1e-6, max_iter=500):
    theta = np.zeros(X.shape[1])
    for i in range(max_iter):
        grad = gradient(X, y, theta)
        H = hessian(X, theta)
        theta -= np.linalg.inv(H) @ grad
        if np.linalg.norm(grad) &lt; tol:
            break
    return theta
theta_final = newton_method(X, y)
print("Optimized Theta:", theta_final)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='o', color='b', label="Class 1")
plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='x', color='r', label="Class 0")
x1_vals = np.linspace(-2, 2, 100)
x2_vals = -(theta_final[0] + theta_final[1] * x1_vals) / theta_final[2]
plt.plot(x1_vals, x2_vals, 'k--', label="Decision Boundary")
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Logistic Regression Decision Boundary")
plt.show()


# In[14]:


import numpy as np
import matplotlib.pyplot as plt
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
phi = np.mean(y)
mu_0 = np.mean(X[y == 0], axis=0)
mu_1 = np.mean(X[y == 1], axis=0)
print(mu_0, mu_1)
m = len(y)
Sigma = np.zeros((X.shape[1], X.shape[1]))
for i in range(m):
    x_i = X[i].reshape(-1, 1)
    mu_yi = mu_1 if y[i] == 1 else mu_0
    mu_yi = mu_yi.reshape(-1, 1)
    Sigma += (x_i - mu_yi) @ (x_i - mu_yi).T
Sigma /= m
print(Sigma)
Sigma_inv = np.linalg.inv(Sigma)
w = Sigma_inv @ (mu_1 - mu_0)
w0 = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0) + np.log(phi / (1 - phi))
Sigma_0 = np.cov(X[y == 0].T, bias=True)
Sigma_1 = np.cov(X[y == 1].T, bias=True)
print(Sigma_0)
print(Sigma_1)

def quadratic_boundary(x1, x2):
    x = np.array([x1, x2]).reshape(-1, 1)
    term1 = -0.5 * (x - mu_1.reshape(-1, 1)).T @ np.linalg.inv(Sigma_1) @ (x - mu_1.reshape(-1, 1))
    term2 = 0.5 * (x - mu_0.reshape(-1, 1)).T @ np.linalg.inv(Sigma_0) @ (x - mu_0.reshape(-1, 1))
    return term1 + term2 + np.log(np.linalg.det(Sigma_1) / np.linalg.det(Sigma_0))
quad_decision_function = np.vectorize(lambda x1, x2: quadratic_boundary(x1, x2)[0, 0])
X1, X2 = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))
Z = quad_decision_function(X1, X2)
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='b', label="Alaska")
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='r', label="Canada")
x_vals = np.linspace(-2, 2, 100)
y_vals = -(w[0] * x_vals + w0) / w[1]
plt.plot(x_vals, y_vals, 'k--', label="Linear Decision Boundary")
contour = plt.contour(X1, X2, Z, levels=[0], colors='g', linewidths=1.5, linestyles='solid')
plt.clabel(contour, inline=True, fontsize=8)
plt.legend(['Quadratic Decision Boundary'], loc='best')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Gaussian Discriminant Analysis (Linear & Quadratic Boundaries)")
plt.show()





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
<A NAME="15"></A><FONT color = #FF0000><A HREF="match9-1.html#15" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.maxInterations = 100000 
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        
        for iter in range(self.maxInterations):
            y_pred = XWithIntercept.dot(self.theta)
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            oldTheta = self.theta.copy()
            self.theta -= learning_rate * gradient
            if(np.linalg.norm(oldTheta - self.theta)&lt;1e-6):
                # print("Converged at iteration: ", iter)
                break
        else:
            # print("Did not converge")
            pass
        return self.theta
    
    def initialize_theta(self, n_features):
        self.theta = np.zeros(n_features + 1)

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        return XWithIntercept.dot(self.theta)[0]
    
    def fit_with_theta_details(self, X, y,learning_rate=0.01):
        
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        prev_time = time.time()
        time.sleep(0.2)
        theta0 = []
        theta1 = []
        m = []
        for iter in range(self.maxInterations):
            y_pred = XWithIntercept.dot(self.theta)
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            oldTheta = self.theta.copy()
            self.theta -= learning_rate * gradient
            m.append(self.calculate_cost(X, y))
            theta0.append(self.theta[0])
            theta1.append(self.theta[1])
            
            if(np.linalg.norm(oldTheta - self.theta)&lt;1e-6):
                # print("Converged at iteration: ", iter)
                break
        else:
            # print("Did not converge")
            pass
        
        return (theta0, theta1, m)
    
    def calculate_cost(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        error = y_pred - y[:,0]
        return (1/(2*len(y))) * np.sum(error**2)
    
    def plot_cost_function(self, X, y, theta_range=((-50, 50),(-50, 50)), resolution=100):
        # Preparing data
        theta_vals = np.linspace(theta_range[0][0], theta_range[0][1], resolution), np.linspace(theta_range[1][0], theta_range[1][1], resolution)

        theta = np.meshgrid(theta_vals[0], theta_vals[1])

        J_vals = np.zeros(theta[0].shape)

        for i in range(len(theta_vals[0])):
            for j in range(len(theta_vals[1])):
                theta0 = theta[0][i, j]
                theta1 = theta[1][i, j]
                predictions = theta0 + theta1 * X
                errors = y - predictions
                J_vals[i, j] = (1 / (2 * len(y))) * np.sum(errors ** 2)
        


        # Plot the cost function surface
        fig = plt.figure(figsize=(10, 7))

        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(theta[0], theta[1], J_vals, cmap='viridis', edgecolor='none',alpha=0.6)

        theta_details = self.fit_with_theta_details(X, y)
        ax.plot(theta_details[0], theta_details[1], theta_details[2], color='b', marker='o', linestyle='dashed', linewidth=2, markersize=5)

        ax.set_xlabel(r'$\theta 0$')
        ax.set_ylabel(r'$\theta 1$')
        ax.set_zlabel(r'$J(\theta)$')
        ax.set_title('3D Cost Function Surface')

        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)

        plt.show()

    def plot_contours(self, X, y,learning_rate=0.01, theta_range=((-50, 50),(-50, 50)), resolution=100):
        # Preparing data
        theta_vals = np.linspace(theta_range[0][0], theta_range[0][1], resolution), np.linspace(theta_range[1][0], theta_range[1][1], resolution)

        theta = np.meshgrid(theta_vals[0], theta_vals[1])

        J_vals = np.zeros(theta[0].shape)

        for i in range(len(theta_vals[0])):
            for j in range(len(theta_vals[1])):
                theta0 = theta[0][i, j]
                theta1 = theta[1][i, j]
                predictions = theta0 + theta1 * X
                errors = y - predictions
                J_vals[i, j] = (1 / (2 * len(y))) * np.sum(errors ** 2)
        
        # Plot the cost function contours
        plt.figure(figsize=(10, 7))
        plt.contour(theta[0], theta[1], J_vals, levels=np.logspace(-2, 3, 20))
        plt.xlabel(r'$\theta 0$')
        plt.ylabel(r'$\theta 1$')
        plt.title('Cost Function Contours')
        
        theta_details = self.fit_with_theta_details(X, y,learning_rate)
        plt.scatter(theta_details[0], theta_details[1], color='green', s=10)

        plt.show()

    def drawHypothesis(self, X, y):

        plt.scatter(X[:,0], y[:,0], color='#003366')
      
        self.fit(X, y)
        
        line_x = np.linspace(np.min(X[:,0])-0.25, np.max(X[:,0])+0.25)
        line_y = self.theta[0] + self.theta[1] * line_x 
        plt.plot(line_x, line_y, color='#3D7317')
        plt.xlabel("X axis")
        plt.ylabel("Y axis")
       
        plt.show()

# X = pd.read_csv('data/Q1/linearX.csv').values
# y = pd.read_csv('data/Q1/linearY.csv').values

# lr = LinearRegressor()
# print(lr.fit(X, y, 0.01))

# lr.drawHypothesis(X, y)
# lr.plot_cost_function(X, y)
# lr.plot_contours(X, y,0.01)

# for x in (0.001,0.025, 0.1):
#     lr.plot_contours(X, y, x)




import os
import csv
from linear_regression import LinearRegressor
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

def read_data(file_path):
        data = []
        with open(file_path, 'r') as file:
            reader = csv.reader(file)
            for row in reader:
                data.append(float(row[0]))
        print(len(data))
        return np.array(data)

def plot_data(X, Y, theta):
    X = np.c_[np.ones((X.shape[0], 1)), X]
    plt.scatter(X[:, 1], Y, color='blue', label='Data Points')
    plt.plot(X[:, 1], X.dot(theta), color='red', label='Regression Line')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Linear Regression')
    plt.legend()
    plt.savefig('linear_regression_plot.png')

def plot_error_surface(X, Y, theta_results):
    """Plot a 3D error function mesh and show gradient descent progression."""
    X = np.c_[np.ones((X.shape[0], 1)), X]
    theta0_vals = np.linspace(-10, 10, 100)
    theta1_vals = np.linspace(-1, 4, 100)
    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
    
    for i, t0 in enumerate(theta0_vals):
        for j, t1 in enumerate(theta1_vals):
            t = np.array([t0, t1])
            J_vals[i, j] = (1 / (2 * len(Y))) * np.sum((X.dot(t) - Y) ** 2)
    
    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
    
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='coolwarm', alpha=0.7)
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Cost J')
    ax.set_title('Error Surface')
    print(theta_results.shape)
    m = len(Y)
    for theta in theta_results:
        predictions = X.dot(theta)
        cost_value = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        ax.scatter(theta[0], theta[1], cost_value, color='red', marker='o')
        plt.pause(0.2)
    plt.savefig('error_surface.png')
    plt.show()



def plot_contour(X, Y, thetaVals):
    X = np.c_[np.ones((X.shape[0], 1)), X]
    theta0_vals = np.linspace(-10, 10, 100)
    theta1_vals = np.linspace(-1, 4, 100)
    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            t = np.array([theta0_vals[i], theta1_vals[j]])
            J_vals[i, j] = (1 / (2 * len(Y))) * np.sum((X.dot(t) - Y) ** 2)

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

    plt.contour(theta0_vals, theta1_vals, J_vals, levels=np.logspace(-2, 3, 20), cmap=cm.viridis)
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.title('Contour Plot of Cost Function')

    for theta in thetaVals:
        plt.scatter(theta[0], theta[1], color='r')
        plt.draw()
        plt.pause(0.2)

    plt.savefig('contour_plot.png')
    plt.show()

current_directory = os.getcwd()
x_file_path = os.path.join(current_directory, "..", "data/Q1/linearX.csv")
y_file_path = os.path.join(current_directory, "..", "data/Q1/linearY.csv")
X = read_data(x_file_path)
y = read_data(y_file_path)

lr = LinearRegressor()
theta_vals = lr.fit(X, y)
predicted = lr.predict(X[:10])
print(predicted)
plot_data(X, y, lr.theta)

"""
Uncomment the respective line to see the plot of
the error surface or contour plot respectively.
"""
#plot_error_surface(X, y, theta_vals)
#plot_contour(X, y, theta_vals)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise

    return np.column_stack((x1, x2)), y

def split_train_test(N, X, y, train_ratio=0.8):
    data = pd.DataFrame({'x1': X[:,0], 'x2': X[:,1], 'y': y})

    data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index

    train_size = int(train_ratio * N)

    train_data = data.iloc[:train_size]
    test_data = data.iloc[train_size:]

    return (train_data[['x1', 'x2']].to_numpy(), train_data['y'].to_numpy()), (test_data[['x1', 'x2']].to_numpy(), test_data['y'].to_numpy())

class StochasticLinearRegressor:
    def __init__(self):
        self.r = 80
        self.theta = None
        self.maxInterations = 1000
    
    def updateIteration(self,val):
        if(val &lt; 1000):
            self.maxInterations = 1000
        else:
            self.maxInterations = 100000

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        self.updateIteration(self.r)
        prev_cost = 0
        indices = range(m)
        X_shuffled = XWithIntercept[indices]
        y_shuffled = y[indices]
        for iter in range(self.maxInterations):
            cost = 0
            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                
                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)
                cost += np.mean((error)**2)
                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                # print(self.theta)
            
            cost /= (m // self.r)
            # if np.linalg.norm(oldTheta - self.theta) &lt; 1e-5:
            #     print("Converged at iteration:", iter)
            #     return self.theta
            if iter &gt; 0 and np.linalg.norm(prev_cost - cost) &lt; 1e-6:
                # print("Converged at iteration:", iter)
                return self.theta
            prev_cost = cost
        
        # print("Did not converge")

        return self.theta
    
    def initialize_theta(self, n_features):
        self.theta = np.zeros(n_features + 1)

    def fit_with_batch_size(self, X, y, learning_rate=0.001, batch_size=1):
        self.r = batch_size
        return self.fit(X, y, learning_rate)
    
    def fit_get_all_thetas(self, X, y, learning_rate=0.001):
        self.initialize_theta(X.shape[1])
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        m = XWithIntercept.shape[0]
        all_thetas = []
        self.updateIteration(self.r)
        indices = np.random.permutation(m)
        X_shuffled = XWithIntercept[indices]
        y_shuffled = y[indices]
        prev_cost = 0
        for iter in range(self.maxInterations):
            cost = 0
            for i in range(0, m, self.r):
                X_batch = X_shuffled[i:i+self.r]
                y_batch = y_shuffled[i:i+self.r]

                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                
                gradient = (1 / len(y_batch)) * X_batch.T.dot(error)
                cost += np.mean((error)**2)
                oldTheta = self.theta.copy()
                self.theta -= learning_rate * gradient
                all_thetas.append(self.theta.copy())
            cost /= (m // self.r)
            if iter &gt; 0 and np.linalg.norm(prev_cost - cost) &lt; 1e-6:
                # print("Converged at iteration:", iter)
                return np.array(all_thetas)
            prev_cost = cost
        return np.array(all_thetas)

    def draw_plot_3d_theta_movement(self, X, y, number_of_points=100, learning_rate=0.001):
        batch_sizes = [1, 80, 8000, 800000]
        color = {1: 'r', 80: 'g', 8000: 'b', 800000: 'y'}

        for i in batch_sizes:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111, projection='3d')
            self.r = i
            all_thetas = self.fit_get_all_thetas(X, y, learning_rate)

            theta_0 = [theta[0] for theta in all_thetas]
            theta_1 = [theta[1] for theta in all_thetas]
            theta_2 = [theta[2] for theta in all_thetas]

            ax.plot(theta_0, theta_1, theta_2, c=color[i], marker='o', label='Theta Movement ' + str(i))
            ax.scatter(self.theta[0], self.theta[1], self.theta[2], c=color[i], marker='x')

            ax.set_xlabel(r'$\theta_0$')
            ax.set_ylabel(r'$\theta_1$')
            ax.set_zlabel(r'$\theta_2$')
            ax.set_title(f'3D Plot of Theta Updates for Batch Size {i}')

            # Ensure label is only added once
            if len(all_thetas) &gt; 1:
                ax.legend()

        plt.show()

    def fit_with_closed_form(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        self.theta = np.linalg.inv(XWithIntercept.T.dot(XWithIntercept)).dot(XWithIntercept.T).dot(y)
        return self.theta

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return y_pred
    
    def test(self, X, y):
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = XWithIntercept.dot(self.theta)
        return np.mean((y_pred - y)**2)
    

# X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), np.sqrt(2))

# train, test = split_train_test(1000000, X, y)

# slr = StochasticLinearRegressor()

# theta_with_closed_form = slr.fit_with_closed_form(X, y)
# print(theta_with_closed_form)
# for i in [1,80,8000,800000]:
#     print(i)

#     theta_with_slr = slr.fit_with_batch_size(train[0], train[1], learning_rate=0.001, batch_size=i)
   
#     mse_test_data = slr.test(test[0], test[1])
#     mse_train_data = slr.test(train[0], train[1])

#     print(theta_with_slr)
#     print(mse_test_data)
#     print(mse_train_data)

# slr.draw_plot_3d_theta_movement(train[0], train[1])



import numpy as np
import matplotlib.pyplot as plt
from sampling_sgd import generate
from sampling_sgd import StochasticLinearRegressor


def plot_theta_updates(theta_updates):
    """Plot the movement of theta values for different batch sizes."""
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    batch_sizes = [1, 80, 8000, 800000]
    colors = ['r', 'g', 'b', 'm']
    
    for i, theta_history in enumerate(theta_updates):
        theta_history = np.array(theta_history)
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], color=colors[i], label=f'Batch size {batch_sizes[i]}')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    ax.set_title('SGD Convergence Path for Different Batch Sizes')
    ax.legend()
    plt.savefig('theta_updates_plot.png')

def linear_regression_closed_form(X, y):
    """Linear regression using the closed form solution.
    The closed form solution is given by:
    theta = (X.T * X)^-1 * X.T * y
    """
    part1 = np.linalg.inv(X.T.dot(X))
    part2 = X.T.dot(y)
    theta = part1.dot(part2)
    return theta

def mean_squared_error(y_test, x_test, theta):
    """Mean squared error between true and predicted values.
    It is calculated by: 1/m * sum((y_true - y_pred)^2)"""
    m = y_test.shape[0]
    y_calculated = x_test.dot(theta)
    return 1/m * np.sum((y_test - y_calculated)**2)


# Generate data
N = 100000
theta = np.array([3.0, 1.0, 2.0])
input_mean = np.array([3.0, -1.0])
input_sigma = np.array([4.0, 4.0])
noise_sigma = 0.1
X, y= generate(N, theta, input_mean, input_sigma, noise_sigma)
# Split the data into training and testing sets without using sklearn
split_index = int(0.8 * N)
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

#Test for StochasticLinearRegressor
slr= StochasticLinearRegressor()
thetaVals = slr.fit(X_train, y_train)
result = slr.predict(X_test[:10])
print(f"Predicted values: {result}")
print(f"True values: {y_test[:10]}")
plot_theta_updates(thetaVals)
print(f"Theta using stochastic linear regressor: {slr.theta}")

# Calculate mean squared error for the test data
mse = mean_squared_error(y_test, X_test, slr.theta_batch_1)
print(f"Mean squared error for batch 1: {mse}")
mse = mean_squared_error(y_test, X_test, slr.theta_batch_80)
print(f"Mean squared error for batch 80: {mse}")
mse = mean_squared_error(y_test, X_test, slr.theta_batch_8000)
print(f"Mean squared error for batch 8000: {mse}")
mse = mean_squared_error(y_test, X_test, slr.theta_batch_800000)
print(f"Mean squared error for batch 800000: {mse}")
# Relearn theta for Linear regression using closed form
theta = linear_regression_closed_form(X_train, y_train)
print(f"Theta using closed form: {theta}")



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        norm_X = self.normalize(X)
        self.theta = np.zeros(norm_X.shape[1] + 1)
        XWithIntercept = np.c_[np.ones((norm_X.shape[0], 1)), norm_X]
        m = XWithIntercept.shape[0]
        for iter in range(1000):
            y_pred = self.sigmoid(XWithIntercept.dot(self.theta))
            error = y_pred - y[:,0]
            gradient = (1/len(y)) * XWithIntercept.T.dot(error)
            R = np.diag((y_pred * (1 - y_pred)).flatten()) 
            H = XWithIntercept.T.dot(R).dot(XWithIntercept)
            oldTheta = self.theta.copy()
            self.theta -= np.linalg.inv(H).dot(gradient)

        return self.theta

    def sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    def normalize(self, X):
        return (X - X.mean(axis=0))/X.std(axis=0)
        
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # norm_X = self.normalize(X)
        XWithIntercept = np.c_[np.ones((X.shape[0], 1)), X]
        probabilities = self.sigmoid(np.array(XWithIntercept.dot(self.theta)))
        return (probabilities &gt;= 0.5).astype(int)


    def draw_plot(self,X,y):
        theta = self.fit(X,y)
        for i in range(len(y)):
            if y[i] == 0:
                plt.scatter(X[i,0], X[i,1], color='red', marker='o')
            else:
                plt.scatter(X[i,0], X[i,1], color='blue', marker='x')
        # draw using theta on the same plot
        x1 = np.linspace(0, 10, 100)
        x2 = (-theta[0] - theta[1]*x1)/theta[2]
        plt.plot(x1, x2, color='black')

        plt.show()
        

# X = pd.read_csv('data/Q3/logisticX.csv').values
# y = pd.read_csv('data/Q3/logisticY.csv').values

# lr = LogisticRegressor()
# theta = lr.fit(X, y)
# print(theta)
# lr.draw_plot(X,y)
# print(lr.predict(np.array([[1,2]]))==0)
# print(lr.predict(np.array([[5,20]]))==0)
# print(lr.predict(np.array([[10,2]]))==0)




from logistic_regression import LogisticRegressor
import os
import csv
import matplotlib.pyplot as plt
import numpy as np

def read_data( file_path):
        data = []
        with open(file_path, 'r') as file:
            reader = csv.reader(file)
            for row in reader:
                data.append(row)
        print(f"Loaded data of size: {len(data)}")
        return np.array(data, dtype=float)

def plot_decision_boundary(X, y, theta):
    X = np.c_[np.ones((X.shape[0], 1)), X]
    plt.scatter(X[y[:, 0] == 0][:, 1], X[y[:, 0] == 0][:, 2], marker='o', label='Class 0')
    plt.scatter(X[y[:, 0] == 1][:, 1], X[y[:, 0] == 1][:, 2], marker='x', label='Class 1')
    
    x_values = np.linspace(min(X[:, 1]) - 1, max(X[:, 1]) + 1, 100)
    y_values = -(theta[0] + theta[1] * x_values) / theta[2]
    plt.plot(x_values, y_values, 'g', label='Decision Boundary')
    
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.title('Logistic Regression Decision Boundary')
    plt.savefig('logistic_regression_plot.png')


# Load the data
current_directory = os.getcwd()
x_file_path = os.path.join(current_directory, "..", "data/Q3/logisticX.csv")
y_file_path = os.path.join(current_directory, "..", "data/Q3/logisticY.csv")
X = read_data(x_file_path)
Y = read_data(y_file_path)

# Train the model
lr = LogisticRegressor()
lr.fit(X, Y)
plot_decision_boundary(X, Y, lr.theta)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.initialize_parameters()
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.initialize_parameters()
        standardized_features = (X - X.mean(axis=0)) / (X.std(axis=0) )

        self.X_mean = X.mean(axis=0)
        self.X_std = X.std(axis=0)

        # Split data by class labels
        features_class_0, features_class_1 = standardized_features[y == 0], standardized_features[y == 1]
        self.mean_vector_class_0 = features_class_0.mean(axis=0)
        self.mean_vector_class_1 = features_class_1.mean(axis=0)

        # Compute covariance matrices for both classes
        num_samples_class_0, num_samples_class_1 = len(features_class_0), len(features_class_1)
        self.covariance_matrix_class_0 = np.cov(features_class_0.T, bias=True)
        self.covariance_matrix_class_1 = np.cov(features_class_1.T, bias=True)

        # Compute shared covariance matrix if assumption holds
        if assume_same_covariance:
            self.shared_covariance_matrix = np.array((
                num_samples_class_0 * self.covariance_matrix_class_0 + 
                num_samples_class_1 * self.covariance_matrix_class_1
            ) / (num_samples_class_0 + num_samples_class_1))

        # Compute prior probability for class 1
        self.prior_probability_class_1 = y.mean()

        if assume_same_covariance:
<A NAME="16"></A><FONT color = #00FF00><A HREF="match9-1.html#16" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            return self.mean_vector_class_0, self.mean_vector_class_1, self.shared_covariance_matrix
        else:
            return self.mean_vector_class_0, self.mean_vector_class_1, self.covariance_matrix_class_0, self.covariance_matrix_class_1
        
    def initialize_parameters(self):
</FONT>        self.mean_vector_class_0 = None
        self.mean_vector_class_1 = None
        self.shared_covariance_matrix = None
        self.covariance_matrix_class_0 = None
        self.covariance_matrix_class_1 = None
        self.prior_probability_class_1 = None
        self.X_mean = None
        self.X_std = None
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Standardize using training mean and std
        X = (X - self.X_mean) / self.X_std  
        
        if(self.shared_covariance_matrix is not None):
            # Compute the inverse of the shared covariance matrix
            inv_cov = np.linalg.inv(self.shared_covariance_matrix)

            # Compute quadratic discriminant scores (delta functions)
            delta_0 = -0.5 * np.sum((X - self.mean_vector_class_0) @ inv_cov * (X - self.mean_vector_class_0), axis=1) + np.log(1 - self.prior_probability_class_1)
            delta_1 = -0.5 * np.sum((X - self.mean_vector_class_1) @ inv_cov * (X - self.mean_vector_class_1), axis=1) + np.log(self.prior_probability_class_1)

            # Predict class 1 if delta_1 &gt; delta_0, otherwise class 0
            return (delta_1 &gt; delta_0).astype(int)

def plot_data(X, y, title="Salmon Growth Ring Diameters by Region"):
    """
    Plot the training data with different symbols for each class.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input features (growth ring diameters)
    y : numpy array of shape (n_samples,)
        The target labels (0 for Alaska, 1 for Canada)
    title : str, optional
        The title for the plot
    """
    plt.figure(figsize=(10, 6))
    
    # Plot Alaska samples (class 0) with blue circles
    plt.scatter(X[y == 0, 0], X[y == 0, 1], 
               c='green', marker='o', label='Alaska', 
               alpha=0.6)
    
    # Plot Canada samples (class 1) with red triangles
    plt.scatter(X[y == 1, 0], X[y == 1, 1], 
               c='red', marker='^', label='Canada', 
               alpha=0.6)
    
    # Add labels and title
    plt.xlabel('Growth Ring Diameter (Fresh Water)')
    plt.ylabel('Growth Ring Diameter (Marine Water)')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()

def decision_boundary(X, y, mu_0, mu_1, sigma, prior_prob_1, X_mean, X_std):
    """
    Plot the data points and decision boundary for GDA.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, 2)
        The input features (original, non-normalized data)
    y : numpy array of shape (n_samples,)
        The target labels (0 for Alaska, 1 for Canada)
    mu_0 : numpy array of shape (2,)
        Mean vector for class 0 (Alaska) in normalized space
    mu_1 : numpy array of shape (2,)
        Mean vector for class 1 (Canada) in normalized space
    sigma : numpy array of shape (2, 2)
        Shared covariance matrix in normalized space
    prior_prob_1 : float
        Prior probability of class 1 (Canada)
    X_mean : numpy array of shape (2,)
        Mean used for normalization
    X_std : numpy array of shape (2,)
        Standard deviation used for normalization
    """
    plt.figure(figsize=(10, 6))
    
    # Plot original data points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], 
               c='green', marker='o', label='Alaska', 
               alpha=0.6)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], 
               c='red', marker='^', label='Canada', 
               alpha=0.6)
    
    # Calculate decision boundary
    sigma_inv = np.linalg.inv(sigma)
    
    # Get the range for plotting in original scale
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    
    # Generate points for the decision boundary
    x1_grid = np.linspace(x_min, x_max, 100)
    
    # Transform grid points to normalized space
    x1_grid_norm = (x1_grid - X_mean[0]) / X_std[0]
    
    # Calculate the decision boundary in normalized space
    diff_mu = mu_1 - mu_0
    w = sigma_inv @ diff_mu
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)
    b += np.log(prior_prob_1 / (1 - prior_prob_1))
    
    # Calculate x2 in normalized space
    x2_grid_norm = (-w[0] * x1_grid_norm - b) / w[1]
    
    # Transform back to original space
    x2_grid = x2_grid_norm * X_std[1] + X_mean[1]
    
    # Plot decision boundary
    plt.plot(x1_grid, x2_grid, 'blue', label='Decision Boundary')
    
    # Add labels and title
    plt.xlabel('Growth Ring Diameter (Fresh Water)')
    plt.ylabel('Growth Ring Diameter (Marine Water)')
    plt.title('GDA Decision Boundary for Salmon Classification')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()

def quadratic_decision_boundary(X, y, gda_linear, gda_quadratic):
    """
    Plot both linear and quadratic decision boundaries along with normalized data.
    Uses explicit quadratic equation: X^T A X + B X + C = 0
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, 2)
        The input features
    y : numpy array of shape (n_samples,)
        The target labels
    gda_linear : GaussianDiscriminantAnalysis
        Fitted GDA model with shared covariance
    gda_quadratic : GaussianDiscriminantAnalysis
        Fitted GDA model with separate covariances
    """
    plt.figure(figsize=(10, 8))
    
    # Normalize the data for plotting
    X_normalized = (X - gda_quadratic.X_mean) / gda_quadratic.X_std
    
    # Plot normalized data points
    plt.scatter(X_normalized[y == 0, 0], X_normalized[y == 0, 1], 
               c='red', marker='o', label='Alaska')
    plt.scatter(X_normalized[y == 1, 0], X_normalized[y == 1, 1], 
               c='blue', marker='x', label='Canada')
    
    # Create a grid of points
    x_min, x_max = X_normalized[:, 0].min() - 0.5, X_normalized[:, 0].max() + 0.5
    y_min, y_max = X_normalized[:, 1].min() - 0.5, X_normalized[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), 
                        np.linspace(y_min, y_max, 100))
    
    # Calculate quadratic boundary parameters
    Sigma0_inv = np.linalg.inv(gda_quadratic.covariance_matrix_class_0)
    Sigma1_inv = np.linalg.inv(gda_quadratic.covariance_matrix_class_1)
    mu0 = gda_quadratic.mean_vector_class_0
    mu1 = gda_quadratic.mean_vector_class_1
    phi = gda_quadratic.prior_probability_class_1
    
    # Calculate A, B, C matrices for quadratic equation
    A = Sigma0_inv - Sigma1_inv
    B = -2 * (mu0.dot(Sigma0_inv) - mu1.dot(Sigma1_inv))
    C = (mu0.dot(Sigma0_inv).dot(mu0) - 
         mu1.dot(Sigma1_inv).dot(mu1) - 
         2 * np.log(((1/phi) - 1) * 
         np.sqrt(np.linalg.det(gda_quadratic.covariance_matrix_class_1) / 
                np.linalg.det(gda_quadratic.covariance_matrix_class_0))))
    
    # Calculate quadratic boundary values
    Z_quad = np.zeros_like(xx)
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            X_point = np.array([xx[i,j], yy[i,j]])
            Z_quad[i,j] = X_point.dot(A).dot(X_point) + B.dot(X_point) + C
    
    # Calculate linear decision boundary using shared covariance
    Sigma_shared_inv = np.linalg.inv(gda_linear.shared_covariance_matrix)
    mu0_linear = gda_linear.mean_vector_class_0
    mu1_linear = gda_linear.mean_vector_class_1
    phi_linear = gda_linear.prior_probability_class_1
    
    # Calculate linear boundary values
    Z_linear = np.zeros_like(xx)
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            X_point = np.array([xx[i,j], yy[i,j]])
            Z_linear[i,j] = (X_point - 0.5*(mu0_linear + mu1_linear)).dot(Sigma_shared_inv).dot(
                mu1_linear - mu0_linear) + np.log(phi_linear/(1-phi_linear))
    
    # Plot decision boundaries
    plt.contour(xx, yy, Z_linear, colors='green', levels=[0], 
                linestyles='-', label='Linear Boundary')
    plt.contour(xx, yy, Z_quad, colors='red', levels=[0], 
                linestyles='-', label='Quadratic Boundary')
    
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Linear vs Quadratic Decision Boundaries')
    plt.legend()
    plt.grid(True)
    plt.show()   


# X = pd.read_csv('data/Q4/q4x.dat').values
# y = pd.read_csv('data/Q4/q4y.dat').values


# X = np.array([list(map(int, item[0].split())) for item in X])

# y = np.where(y == 'Alaska', 0, 1)[:,0]

# gda_linear = GaussianDiscriminantAnalysis()
# gda_quadratic = GaussianDiscriminantAnalysis()

# mu_0, mu_1, sigma = gda_linear.fit(X, y, assume_same_covariance=True)

# # Print parameter estimates
# print("\nParameter Estimates:")
# print("μ₀ (Alaska):", mu_0)
# print("μ₁ (Canada):", mu_1)
# print("\nΣ (Alaska and Canada):\n", sigma)

# plot_data(X, y)

# # Fit GDA model


    
# # Plot data and decision boundary
# decision_boundary(X, y, mu_0, mu_1, sigma, gda_linear.prior_probability_class_1,gda_linear.X_mean, gda_linear.X_std)

# mu_0, mu_1, sigma_0, sigma_1 = gda_quadratic.fit(X, y, assume_same_covariance=False)

# # Print parameter estimates
# print("\nParameter Estimates:")
# print("μ₀ (Alaska):", mu_0)
# print("μ₁ (Canada):", mu_1)
# print("\nΣ₀ (Alaska):\n", sigma_0)
# print("\nΣ₁ (Canada):\n", sigma_1)

# # Plot the data and decision boundary
# quadratic_decision_boundary(X, y, gda_linear,gda_quadratic)



from gda import GaussianDiscriminantAnalysis
import os
import numpy as np
import matplotlib.pyplot as plt


def read_X_data(file_path):
        data = []
        with open(file_path, 'r') as file:
            for line in file:
                data.append(line.strip().split())
        print(f"Loaded data of size: {len(data)}")
        return np.array(data, dtype=float)
    
def read_Y_data(file_path):
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            val = 0 if line.strip() == 'Alaska' else 1
            data.append(val)
    print(f"Loaded data of size: {len(data)}")
    return np.array(data, dtype=int)

def create_linear_decision_boundary(mu_0, mu_1, sigma):
    """
    The function calculates the parameters of the linear decision boundary.
    It adds the parameters to the decision_boundry_parameters dictionary.

    Linear decision boundary is given by w^T * x = b
    Calculation of w: w = sigma^-1 * (mu_1 - mu_0)
    Calculation of b: b = 0.5 * (mu_0^T * sigma^-1 * mu_0 - mu_1^T * sigma^-1 * mu_1)
    
    Parameters
    ---------- 
    mu_0 : numpy array of shape (n_features,)
        The mean of the input data for class 0.
        
    mu_1 : numpy array of shape (n_features,)
        The mean of the input data for class 1.
        
    sigma : numpy array of shape (n_features, n_features)
        The covariance matrix of the input data for both classes.
    
    Returns
    -------
    w : numpy array of shape (n_features,)
        The parameters of the linear decision boundary.
    """
    inv_sigma = np.linalg.inv(sigma)
    w = inv_sigma.dot(mu_1 - mu_0)
    decision_boundry_parameters.update({'w': w})
    b = 0.5 * (mu_0.T.dot(inv_sigma).dot(mu_0) - mu_1.T.dot(inv_sigma).dot(mu_1))
    decision_boundry_parameters.update({'b': b})

    
def create_quadratic_decision_boundary(mu_0, mu_1, sigma_0, sigma_1):
    """
    The function calculates the parameters of the quadratic decision boundary.
    It adds the parameters to the decision_boundry_parameters dictionary.

    Quadratic decision boundary is given by w_0 + w_1^T * x + x^T * w_2 * x = 0
    Calculation of w_0: w_0 = -0.5 * mu_1^T * sigma_1^-1 * mu_1 + 0.5 * mu_0^T * sigma_0^-1 * mu_0 + log(det(sigma_0)/det(sigma_1))
    Calculation of w_1: w_1 = mu_1^T * sigma_1^-1 - mu_0^T * sigma_0^-1
    Calculation of w_2: w_2 = -0.5 * sigma_1^-1 + 0.5 * sigma_0^-1
    
    Parameters
    ----------       
    mu_0 : numpy array of shape (n_features,)
        The mean of the input data for class 0.
        
    mu_1 : numpy array of shape (n_features,)
        The mean of the input data for class 1.
        
    sigma_0 : numpy array of shape (n_features, n_features)
        The covariance matrix of the input data for class 0.
        
    sigma_1 : numpy array of shape (n_features, n_features)
        The covariance matrix of the input data for class 1.
    
    """
    inv_sigma_0 = np.linalg.inv(sigma_0)
    inv_sigma_1 = np.linalg.inv(sigma_1)
    det_sigma_0 = np.linalg.det(sigma_0)
    det_sigma_1 = np.linalg.det(sigma_1)
    w_0 = -0.5 * mu_1.T.dot(inv_sigma_1).dot(mu_1) + 0.5 * mu_0.T.dot(inv_sigma_0).dot(mu_0) + np.log(det_sigma_0/det_sigma_1)
    w_1 = mu_1.T.dot(inv_sigma_1) - mu_0.T.dot(inv_sigma_0)
    w_2 = -0.5 * inv_sigma_1 + 0.5 * inv_sigma_0
    decision_boundry_parameters.update({'w_0': w_0, 'w_1': w_1, 'w_2': w_2})

def plot_linear_boundary(X, y, w, b):
    """
    Plot the decision boundary for the Gaussian Discriminant Analysis model.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target labels - 0 or 1.
        
    w : numpy array of shape (n_features,)
        Parameters of the linear decision boundary.
        
    b : float
        The bias term of the linear decision boundary.
    """
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = np.c_[xx.ravel(), yy.ravel()]
    
    decision_values = np.array([
        w.dot(z) + b for z in Z
    ])
    decision_values = decision_values.reshape(xx.shape)
    
    plt.contourf(xx, yy, decision_values, levels=[decision_values.min(), 0, decision_values.max()], alpha=0.3, colors=['blue', 'red'])
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Gaussian Discriminant Analysis Linear Decision Boundary')
    
    for i, label in enumerate(['Alaska', 'Canada']):
        plt.scatter([], [], c=['blue', 'red'][i], label=label)
    plt.legend()
    
    plt.savefig('gda_linear_plot.png')

def plot_quadratic_boundary(X, y, w_0, w_1, w_2):
    """
    Plot the decision boundary for the Gaussian Discriminant Analysis model.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target labels - 0 or 1.
        
    w_0, w_1, w_2 : numpy arrays
        Parameters of the quadratic decision boundary.
    """
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = np.c_[xx.ravel(), yy.ravel()]
    
    decision_values = np.array([
        w_0 + w_1.dot(z) + z.T.dot(w_2).dot(z) for z in Z
    ])
    decision_values = decision_values.reshape(xx.shape)
    
    plt.contourf(xx, yy, decision_values, levels=[decision_values.min(), 0, decision_values.max()], alpha=0.3, colors=['blue', 'red'])
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Gaussian Discriminant Analysis Quadratic Decision Boundary')
    
    for i, label in enumerate(['Alaska', 'Canada']):
        plt.scatter([], [], c=['blue', 'red'][i], label=label)
    plt.legend()
    
    plt.savefig('gda_quadratic_plot.png')
    plt.show()


#load data
current_directory = os.getcwd()
x_file_path = os.path.join(current_directory, "..", "data/Q4/q4x.dat")
y_file_path = os.path.join(current_directory, "..", "data/Q4/q4y.dat")
X = read_X_data(x_file_path)
Y = read_Y_data(y_file_path)

decision_boundry_parameters = dict()

#Get the parameters of the Gaussian Discriminant Analysis model
#Assume same covariance
gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma =  gda.fit(X, Y, True)
print(gda.predict(X[:10]))
print(Y[:10])
create_linear_decision_boundary(mu_0, mu_1, sigma)
plot_linear_boundary(X, Y, decision_boundry_parameters['w'], decision_boundry_parameters['b'])

#Assume different covariances
mu_0, mu_1, sigma_0, sigma_1 =  gda.fit(X, Y, False)
print(gda.predict(X[:10]))
print(Y[:10])
print(create_quadratic_decision_boundary(mu_0, mu_1, sigma_0, sigma_1))
plot_quadratic_boundary(X, Y, decision_boundry_parameters['w_0'], decision_boundry_parameters['w_1'], decision_boundry_parameters['w_2'])



</PRE>
</PRE>
</BODY>
</HTML>
