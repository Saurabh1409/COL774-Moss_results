<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_AQPT5.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.error = []
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m,n = X.shape
        self.theta = np.zeros(n+1)
        
        X = np.hstack((np.ones((m, 1)), X))
        currError = 1
        prevError = 0
        threshold = 0.0000001
        history = []
        count = 0
        while abs(currError - prevError) &gt; threshold:
            for j in range(n+1):
                sumOverm = 0
                for i in range(m):
                    hypo = X[i].dot(self.theta)
                    error = y[i] - hypo
                    term = error * X[i][j]
                    sumOverm = sumOverm + term
                # print(sumOverm)
                sumOverm /= m
                self.theta[j] = self.theta[j] + learning_rate * sumOverm
            prevError = currError
            currError = self.calculateError(X,y)
            self.error.append(currError.copy())
            # print(currError)
            history.append(self.theta.copy())
            count += 1
            # if count &gt; 10000:
            #     print(currError)
            #     break
            # print("count : ",count,currError)
        # print("count :- ", count)
        return np.array(history)

    def calculateError(self,X,y):
        m,n = X.shape
        error = 0
        for i in range (m):
            temp = y[i] - X[i].dot(self.theta)
            error = error + temp ** 2

        calcError = error / (2*m)
        return calcError


    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="1"></A><FONT color = #00FF00><A HREF="match215-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m,n = X.shape
        # self.theta = np.zeros(n+1)
        
        X = np.hstack((np.ones((m, 1)), X))

        pred = X.dot(self.theta)
</FONT>
        return pred

    # def plot3D(self, thetas, store_j_theta, X, y):
    #     stored_thetas = np.array(thetas)
    #     stored_corresponding_j = np.array(store_j_theta)
    #     theta_0_values = stored_thetas[:, 0]
    #     theta_1_values = stored_thetas[:, 1]

    #     m = X.shape[0]
    #     ones_intercept_column = np.ones((m, 1))
    #     if X.ndim == 1:
    #         X = X.reshape(-1, 1)

    #     if y.ndim == 1:
    #         y = y.reshape(-1, 1)

    #     X_features = np.hstack((ones_intercept_column, X))

    #     theta_0_vals = np.linspace(min(theta_0_values) - 5, max(theta_0_values) + 10, 100)
    #     theta_1_vals = np.linspace(min(theta_1_values) - 5, max(theta_1_values) + 30, 100)
    #     T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
    #     Z = np.zeros_like(T0)
    #     for i in range(len(theta_0_vals)):
    #         for j in range(len(theta_1_vals)):
    #             temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
    #             Z[i, j] = self.compute_J_theta(X_features, y, temp_theta)


    #     fig = plt.figure(figsize=(10, 7))
    #     ax = fig.add_subplot(111, projection='3d')
    #     ax.plot_surface(T0, T1, Z, cmap='viridis', alpha=0.5)
    #     ax.set_xlabel("Theta 0")
    #     ax.set_ylabel("Theta 1")
    #     ax.set_zlabel("Cost J(Theta)")
    #     ax.set_title("3D Visualization")
    #     for i in range(1, len(stored_thetas)):
    #         ax.scatter(stored_thetas[i, 0], stored_thetas[i, 1], stored_corresponding_j[i],
    #                    color='r', marker='o')
    #         plt.draw()
    #         # plt.pause(0.2)
    #     plt.savefig("Q1_part3.png")
    #     plt.show()

    # def compute_J_theta(self,X,y,theta):
    #     m = X.shape[0]
    #     error =  np.dot(X, theta) - y
    #     squarred_error = np.square(error)
    #     total_error = np.sum(squarred_error)
    #     J_theta = total_error / (2 * m)
    #     return J_theta

    # def cost_corresponding_theta(self, X, y, theta_array):
    #     m = X.shape[0]
    #     ones_intercept_column = np.ones((m, 1))
    #     if X.ndim == 1:
    #         X = X.reshape(-1, 1)

    #     if y.ndim == 1:
    #         y = y.reshape(-1, 1)

    #     X_features = np.hstack((ones_intercept_column, X))
    #     store_J_theta = []
    #     for i in theta_array:
    #         i = i.reshape(-1, 1)
    #         new_cost = self.compute_J_theta(X_features, y, i)
    #         store_J_theta.append(new_cost)
    #     return store_J_theta

    # def plot2D_contour(self, thetas, store_j_theta, X, y):
    #     stored_thetas = np.array(thetas)
    #     stored_corresponding_j = np.array(store_j_theta)
    #     theta_0_values = stored_thetas[:, 0]
    #     theta_1_values = stored_thetas[:, 1]

    #     m = X.shape[0]
    #     ones_intercept_column = np.ones((m, 1))
    #     if X.ndim == 1:
    #         X = X.reshape(-1, 1)

    #     if y.ndim == 1:
    #         y = y.reshape(-1, 1)

    #     X_features = np.hstack((ones_intercept_column, X))

    #     theta_0_vals = np.linspace(min(theta_0_values) - 5, max(theta_0_values) + 10, 100)
    #     theta_1_vals = np.linspace(min(theta_1_values) - 5, max(theta_1_values) + 30, 100)
    #     T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
    #     Z = np.zeros_like(T0)
        
    #     for i in range(len(theta_0_vals)):
    #         for j in range(len(theta_1_vals)):
    #             temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
    #             Z[i, j] = self.compute_J_theta(X_features, y, temp_theta)

    #     fig, ax = plt.subplots(figsize=(8, 6))
    #     ax.contour(T0, T1, Z, levels=np.logspace(0, 5, 20), cmap='viridis')
    #     ax.set_xlabel("Theta 0")
    #     ax.set_ylabel("Theta 1")
    #     ax.set_title("Contour Plot of the Cost Function")

    #     for i in range(1, len(stored_thetas)):
    #         ax.scatter(stored_thetas[i, 0], stored_thetas[i, 1], color='r', marker='o')
    #         # plt.draw()
    #         # plt.pause(0.2)
    #     plt.savefig("Q1_part4.png")
    #     plt.show()




# import pandas as pd
# import matplotlib.pyplot as plt


# X = pd.read_csv("../data/Q1/linearX.csv").values
# y = pd.read_csv("../data/Q1/linearY.csv").values

# print(X.shape)
# print(y.shape)
# print(y[0])

# linear = LinearRegressor()
# start = time.time()
# history = linear.fit(X,y)
# print(linear.theta)
# end = time.time()
# print(end - start)
# # linear.plot2D_contour(history,linear.error, X,y)
# # linear.plot3D(history,linear.error, X,y)
# pred = linear.predict(X)
# mse = np.mean((y - pred) ** 2)
# print(mse)



# Imports - you can add any other permitted libraries
import numpy as np
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match215-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise

    return np.column_stack((x1, x2)), y
</FONT><A NAME="2"></A><FONT color = #0000FF><A HREF="match215-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>


class StochasticLinearRegressor:
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m,n = X.shape
        self.theta = np.zeros(n+1)
        
        X = np.hstack((np.ones((m, 1)), X))
        currError = 1
        prevError = 0
        threshold = 0.0000001
        history = []
        history.append(self.theta.copy())
        r = 1
        cnt = m//r
        count = 0
        start_time = time.time()
        while abs(currError - prevError) &gt; threshold:
            for i in range(cnt):
                for j in range(n+1):
                    sumOverm = 0
                    temp = i * r
                    # for k in range(r):
                    #     hypo = X[temp+k].dot(self.theta)
                    #     error = y[temp+k] - hypo
                    #     term = error * X[temp+k][j]
                    #     sumOverm = sumOverm + term
                    # sumOverm /= r
                    batchX = X[temp:temp+r]
                    batchy = y[temp:temp+r]
                    hypo = batchX.dot(self.theta)
                    error = batchy - hypo
                    term = error * batchX[:,j]
                    sumOverm = term.sum() / r
                    self.theta[j] = self.theta[j] + learning_rate * sumOverm
            prevError = currError
            currError = self.calculateError(X,y)
            count += 1
            
            # print(f"count {count}", currError,self.theta)
            history.append(self.theta.copy())
        end_time = time.time()
        totalTime = end_time - start_time
        # print(f"time taken for batchsize {r} :- ", totalTime)
        return np.array(history)

     
    def calculateError(self,X,y):
        m,n = X.shape
        error = 0
        for i in range (m):
            temp = y[i] - X[i].dot(self.theta)
            error = error + temp ** 2

        calcError = error / (2*m)
        return calcError


    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m,n = X.shape
        # self.theta = np.zeros(n+1)
        
        X = np.hstack((np.ones((m, 1)), X))

        pred = X.dot(self.theta)

        return pred
    



# import pandas as pd
# np.random.seed(42)

# # Number of data points
# n = 1000000

# # Parameters θ = [θ0, θ1, θ2]
# theta_0 = 3
# theta_1 = 1
# theta_2 = 2

# # Generate features
# x1 = np.random.normal(loc=3, scale=2, size=n)  # x1 ~ N(3, 4) -&gt; mean=3, std=2
# x2 = np.random.normal(loc=-1, scale=2, size=n)  # x2 ~ N(-1, 4) -&gt; mean=-1, std=2

# # Generate noise
# noise = np.random.normal(loc=0, scale=np.sqrt(2), size=n)  # Noise with variance 2

# # Generate target variable y
# y = theta_0 + theta_1 * x1 + theta_2 * x2 + noise

# # Split the data into 80% training and 20% testing
# train_size = int(0.8 * n)
# X_train = np.column_stack(( x1[:train_size], x2[:train_size]))  # Add intercept term
# y_train = y[:train_size]

# X_test = np.column_stack((x1[train_size:], x2[train_size:]))  # Add intercept term
# y_test = y[train_size:]
# # X = np.column_stack((np.ones((n,1)),x1,x2))

# X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), np.sqrt(2))
# linear = StochasticLinearRegressor()
# history = linear.fit(X_train,y_train)
# pred = linear.predict(X_test)
# mse = np.mean((y_test - pred) ** 2)
# print("test erro :",mse)
# xpred = linear.predict(X_train)
# m = np.mean((y_train - xpred) ** 2)
# print("train error: ",m)
# import matplotlib.pyplot as plt


# print(X.shape)
# m,n = X.shape
# X = np.hstack((np.ones((m, 1)), X))

# XtX = np.dot(X.T, X) 
# X_inv = np.linalg.inv(XtX)
# closedFormTheta = np.dot(X_inv, np.dot(X.T, y))
# print(closedFormTheta)

# fig = plt.figure(figsize=(10, 8))
# ax = fig.add_subplot(111, projection='3d')

# # Plot the movement of theta for different batch sizes
# # for batch_size in batch_sizes:
# # trajectory = gradient_descent(batch_size, learning_rate=0.1, epochs=epochs, initial_theta=initial_theta)
# trajectory = history

# ax.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], label=f"Batch Size: {800000}")

# # Labels and title
# ax.set_xlabel('θ₁')
# ax.set_ylabel('θ₂')
# ax.set_zlabel('θ₃')
# ax.set_title('Movement of θ in Parameter Space with Varying Batch Sizes')
# ax.legend()
# # plt.savefig("batchSize_800000_Q2.png")
# # Show plot
# plt.show()




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        m,n = X.shape
        # print(X.shape)
        for i in range(n):
            meanx1 = X[:,i].mean()
            stdx1 = X[:,i].std()
            X[:, i] -= meanx1  
            X[:, i] /= stdx1
        X = np.hstack((np.ones((m, 1)), X))
        
        currError = 1
        prevError = 0
        threshold = 0.0000001
        history = []
        self.theta = np.zeros((n+1,1))
        # print("tehre",self.theta.shape)
        # print(X.shape)

        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        
        def hessian(X):
            m,n = X.shape
            hypo = X @ self.theta
            # print(hypo.shape)
            for i in range(m):
                hypo[i] = sigmoid(hypo[i])
            
            temp = 1- hypo
              
           
            t1 = hypo * temp
            
            
            dig = np.zeros((m,m))
            for i in range (m):
                dig[i][i] = t1[i]
            hessian_matrix = X.T @ dig @ X
            # hessian_matrix = hessian_matrix 
            return hessian_matrix
        
        while currError &gt; threshold:
            hypo = X @ self.theta
            sig = sigmoid(hypo)
            gradient = X.T.dot(sig - y)
            hessian_matrix = hessian(X)
            temp = np.linalg.inv(hessian_matrix) @ gradient
            currTheta = self.theta - temp
            # print(hessian_matrix)
            currError = np.linalg.norm(currTheta - self.theta, ord = 1)
            self.theta = currTheta
            # print(currError)
            history.append(self.theta)
        return history

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m,n = X.shape
        # print(X.shape)
        for i in range(n):
            meanx1 = X[:,i].mean()
            stdx1 = X[:,i].std()
            X[:, i] -= meanx1  
            X[:, i] /= stdx1
        X = np.hstack((np.ones((m, 1)), X))
        hypo = X @ self.theta
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        for i in range(m):
            hypo[i] = 1 if sigmoid(hypo[i]) &gt; 0.5 else 0
        hypo = [int(1 if sigmoid(val) &gt; 0.5 else 0) for val in hypo]
        return hypo



# import pandas as pd


# X = pd.read_csv("../data/Q3/logisticX.csv",header=None).to_numpy()
# y = pd.read_csv("../data/Q3/logisticY.csv",header=None).to_numpy()


# logistic = LogisticRegressor()
# history = logistic.fit(X,y)
# print("theta ",logistic.theta)
# x_test = np.array(X)
# ypred = logistic.predict(x_test)
# print(ypred)

# mse = np.mean((y - ypred) ** 2)
# print(mse)


# import matplotlib.pyplot as plt

# def plot_decision_boundary(X, y, theta, ax=None):
#     """
#     Plot the decision boundary and training data for logistic regression.
    
#     Parameters
#     ----------
#     X : numpy array of shape (n_samples, n_features)
#         The input data.
        
#     y : numpy array of shape (n_samples,)
#         The target labels (0 or 1).
        
#     theta : numpy array of shape (n_features+1,)
#         The parameters of the logistic regression model.
    
#     ax : matplotlib axis, optional
#         The axis to plot on. If None, a new figure is created.
#     """
#     if ax is None:
#         ax = plt.gca()
    
#     # Create a mesh grid for plotting the decision boundary
#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    
#     # Calculate the decision boundary (sigmoid(theta^T * x) = 0.5)
#     Z = np.c_[np.ones(xx.ravel().shape), xx.ravel(), yy.ravel()] @ theta
#     Z = Z.reshape(xx.shape)
    
#     # Plot the contour of the decision boundary
#     ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)
    
#     # Plot the training data
#     ax.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='x', label='Class 0')
#     ax.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', label='Class 1')

#     ax.set_xlabel('x1')
#     ax.set_ylabel('x2')
#     ax.set_title('Training Data and Decision Boundary')
#     ax.legend()


# # Assuming the logistic regression model `linear` has been trained using `fit`
# # Now we can plot the decision boundary

# # Add intercept term to X
# X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])

# # Plotting the decision boundary along with training data
# fig, ax = plt.subplots(figsize=(8, 6))
# plot_decision_boundary(X, y.flatten(), linear.theta.flatten(), ax)
# plt.savefig("logisticPlotQ3.png")
# # Show the plot
# plt.show()



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """

        X = X.astype(np.float64)

        m,n = X.shape
        for i in range(n):
            meanx1 = X[:,i].mean()
            stdx1 = X[:,i].std()
            X[:, i] -= meanx1  
            X[:, i] /= stdx1
        
        # print(X[:,0].mean())
        # print(X[:,1].mean())
        def indicator(input, value):
            if input == value:
                return 1
            return 0
        
        mu_0 = 0
        mu_1 = 0
        classx0 = 0
        classx1 = 0
        
        for i in range(m):
            mu_0 += X[i] * indicator(y[i], "Alaska")
            mu_1 += X[i] * indicator(y[i], "Canada")
            classx0 += indicator(y[i], "Alaska")
            classx1 += indicator(y[i], "Canada")

        mu_0 /= classx0
        mu_1 /= classx1
        
        sigma_0 = np.zeros((n,n))
        sigma_1 = np.zeros((n,n))
        
        for i in range(m):
            
            if indicator(y[i],"Alaska"):
                temp = X[i].T - mu_0
                matrix = np.outer(temp, temp)
                sigma_0 += matrix
            else:
                temp = X[i].T - mu_1
                matrix = np.outer(temp, temp)
                sigma_1 += matrix



        sigma = sigma_0 + sigma_1
        sigma /= m
        sigma_1 /= classx1
        sigma_0 /= classx0

        # print(sigma)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match215-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        if assume_same_covariance == True:
            self.mu_0 = mu_0
            self.mu_1 = mu_1
            self.sigma = sigma
            self.sigma_0 = sigma
            self.sigma_1 = sigma
</FONT>            return mu_0, mu_1, sigma
        else:
            self.mu_0 = mu_0
            self.mu_1 = mu_1
            self.sigma = sigma
            self.sigma_0 = sigma_0
            self.sigma_1 = sigma_1
            return mu_0, mu_1, sigma_0, sigma_1

        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match215-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m,n = X.shape

        ypred = np.empty(m, dtype=str)
</FONT>
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match215-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        sigma_0_inv = np.linalg.inv(self.sigma_0)
        sigma_1_inv = np.linalg.inv(self.sigma_1)

        log_det_sigma_0 = np.log(np.linalg.det(self.sigma_0))
</FONT>        log_det_sigma_1 = np.log(np.linalg.det(self.sigma_1))

        for i in range(m):
            temp = X[i].T
            t1= temp - self.mu_0

            log_y0 = -0.5 * (log_det_sigma_0 + t1.T @ sigma_0_inv @ t1)
            log_y1 = -0.5 * (log_det_sigma_1 + t1.T @ sigma_1_inv @ t1)

            if log_y0 &gt; log_y1 :
                ypred[i] = "Alaska"
            else:
                ypred[i] = "Canada"
        
        return ypred
        


# import pandas as pd

# import numpy as np


# X = np.loadtxt('../data/Q4/q4x.dat')  # Assuming this file contains the feature data (x1, x2)
# y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)

# from gda import GaussianDiscriminantAnalysis 
# import matplotlib.pyplot as plt
# gda = GaussianDiscriminantAnalysis()

# mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X,y)
# pred = gda.predict(X)
# print(y)

# X = X.astype(np.float64)
# a,b,Sigma = gda.fit(X,y,assume_same_covariance=True)
# print(mu_0, mu_1, Sigma, sigma_0, sigma_1)

# m,n = X.shape
# for i in range(n):
#     meanx1 = X[:,i].mean()
#     stdx1 = X[:,i].std()
#     X[:, i] -= meanx1  
#     X[:, i] /= stdx1
# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# # Define the quadratic decision boundary using the GDA equation for each class
# def quadratic_boundary(x, y, mu_0, mu_1, sigma_0, sigma_1):
#     diff_0 = np.array([x, y]) - mu_0
#     diff_1 = np.array([x, y]) - mu_1
    
#     term_0 = np.dot(diff_0.T, np.linalg.inv(sigma_0)).dot(diff_0)
#     term_1 = np.dot(diff_1.T, np.linalg.inv(sigma_1)).dot(diff_1)
    
#     return term_0 - term_1

# # Compute the decision boundary over the mesh grid
# Z_quadratic = np.array([[quadratic_boundary(x, y, mu_0, mu_1, sigma_0, sigma_1) for x, y in zip(xx_row, yy_row)] 
#                         for xx_row, yy_row in zip(xx, yy)])

# Sigma_inv = np.linalg.inv(Sigma)
# w = Sigma_inv.dot(mu_1 - mu_0)

# # b = 0.5 * (mu_0^T * Sigma_inv * mu_0 - mu_1^T * Sigma_inv * mu_1) + log(Prior_0) - log(Prior_1)
# # Assuming equal priors for simplicity
# b = 0.5 * (mu_0.T.dot(Sigma_inv).dot(mu_0) - mu_1.T.dot(Sigma_inv).dot(mu_1))

# # Create a mesh grid for plotting the decision boundary
# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# # Decision boundary equation: w^T x + b = 0
# Z = w[0] * xx + w[1] * yy + b
# # Plotting the training data and quadratic decision boundary
# plt.figure(figsize=(8, 6))
# plt.contour(xx, yy, Z, levels=[0], colors='green', linewidths=2)

# # Plot data for 'Alaska' (e.g., with blue circles)
# plt.scatter(X[y == 'Alaska', 0], X[y == 'Alaska', 1], color='blue', marker='o', label='Alaska')

# # Plot data for 'Canada' (e.g., with red crosses)
# plt.scatter(X[y == 'Canada', 0], X[y == 'Canada', 1], color='red', marker='x', label='Canada')

# # Plot the quadratic decision boundary using contour
# plt.contour(xx, yy, Z_quadratic, levels=[0], colors='black', linewidths=2)

# # Adding labels and title
# plt.xlabel('Freshwater Growth Ring Diameter (x1)')
# plt.ylabel('Marine Growth Ring Diameter (x2)')
# plt.title('Training Data and Quadratic Decision Boundary (GDA with Different Covariance Matrices)')
# plt.legend()
# plt.savefig("Q4.png")
# # Show the plot
# plt.show()


</PRE>
</PRE>
</BODY>
</HTML>
