<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_41V6T.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_41V6T.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[2]:


# Imports - you can add any other permitted libraries
import numpy as np
<A NAME="2"></A><FONT color = #0000FF><A HREF="match138-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []
        self.j_history = []
</FONT>        self.tol = 1e-6

    def compute_cost(self, X, y):
        """Compute the least squares cost function J(theta)."""
        m = len(y)
        predictions = X.dot(self.theta)
        cost = (1 / (2 * m)) * np.sum((y - predictions) ** 2)
        return cost
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X = X.reshape(-1, 1) if X.ndim == 1 else X
        m, n = X.shape
        X = np.c_[np.ones(m), X]  # Add intercept term
        self.theta = np.zeros(n + 1)  # Initialize parameters
        theta_history = []  # Store theta values for visualization

        epoch = 0
        self.cost_history.append(self.compute_cost(X, y))

        while epoch==0 or abs(self.cost_history[-1] - self.cost_history[-2]) &gt; self.tol:

            gradient = (1 / m) * X.T.dot(X.dot(self.theta) - y)
            self.theta -= learning_rate * gradient  # Gradient descent update
            cost = self.compute_cost(X, y)
            self.cost_history.append(cost)
            theta_history.append(self.theta.copy())
            epoch += 1

        self.cost_history.pop(0)  # Remove initial cost value
        return np.array(theta_history)  # Return theta updates for visualization
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        X = np.c_[np.ones(m), X]  # Add intercept term
        return X.dot(self.theta)


# In[3]:


# Load the data
X = np.loadtxt("../data/Q1/linearX.csv", delimiter=",")
y = np.loadtxt("../data/Q1/linearY.csv", delimiter=",")

# Train the model
model = LinearRegressor()
print('start')
theta_history = model.fit(X, y, learning_rate=0.04)

# Print results
print(f"Learned parameters (theta): {model.theta}")
print(f"Final cost: {model.cost_history[-1]}")
print(f"Number of iterations: {len(model.cost_history)}")


# In[7]:


# Plot the data and hypothesis function
plt.scatter(X, y, color='blue', label="Training Data")
plt.plot(X, model.predict(X), color='red', label="Hypothesis Function")
plt.xlabel("Acidity (Feature X)")
plt.ylabel("Density (Label Y)")
plt.legend()
plt.title("Linear Regression Fit (Learning Rate=0.04)")
plt.show()


# In[ ]:


# Create a mesh grid for plotting the cost function
theta0_vals = theta_history[:, 0]
theta1_vals = theta_history[:, 1]
J_vals = model.cost_history

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
J_vals=np.array(J_vals)
J_vals,_=np.meshgrid(J_vals,J_vals)


# In[ ]:


# Create an animation of the cost function surface and the gradient descent path

fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap="viridis", alpha=0.6)
ax.set_xlabel("Theta 0")
ax.set_ylabel("Theta 1")
ax.set_zlabel("Cost J(θ)")
plt.title("3D Surface of Cost Function")
plt.show()

# Initialize the scatter plot
scatter = ax.scatter([], [], [], color='r')

print(theta_history.shape)

def update(frame):
    scatter._offsets3d = (theta_history[:frame, 0], theta_history[:frame, 1], model.cost_history[:frame])
    print(f"Frame: {frame}")
    return scatter,

ani = FuncAnimation(fig, update, frames=len(theta_history), interval=200, blit=False)

ani.save("linear_regression.gif")


# In[ ]:


# Create an animation of the contour plot and the gradient descent path
fig, ax = plt.subplots(figsize=(10, 6))
ax.contour(theta0_vals, theta1_vals, J_vals, levels=50, cmap="viridis")
ax.set_xlabel("Theta 0")
ax.set_ylabel("Theta 1")
plt.title("Contour Plot of Cost Function (Learning Rate = 0.04)")

# Initialize the scatter plot
scatter = ax.scatter([], [], color='r')

def update(frame):
    scatter.set_offsets(theta_history[:frame, :2])
    print(frame)
    return scatter,

ani = FuncAnimation(fig, update, frames=len(theta_history), interval=200, blit=False)

ani.save("linear_regression_contour.gif")


# In[ ]:


<A NAME="3"></A><FONT color = #00FFFF><A HREF="match138-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

learning_rates = [0.001]

for lr in learning_rates:
    model = LinearRegressor()
    theta_history = model.fit(X, y, learning_rate=lr)
    print(f"Learned parameters (theta) for learning rate {lr}: {model.theta}")
    print(f"Final cost for learning rate {lr}: {model.cost_history[-1]}")
    print(f"Number of iterations for learning rate {lr}: {len(model.cost_history)}")
</FONT>
    theta0_vals = theta_history[:, 0]
    theta1_vals = theta_history[:, 1]
    J_vals = model.cost_history

    theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
    J_vals=np.array(J_vals)
    J_vals,_=np.meshgrid(J_vals,J_vals)

    # Create an animation of the contour plot and the gradient descent path
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.contour(theta0_vals, theta1_vals, J_vals, levels=50, cmap="viridis")
    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    plt.title(f"Contour Plot of Cost Function (Learning Rate = {lr})")
    ax.scatter(theta_history[:, 0], theta_history[:, 1], color='r', label='Theta Points', marker='x')
    ax.legend()
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match138-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.show()

    # # Initialize the scatter plot
    # scatter = ax.scatter([], [], color='r')

    # def update(frame):
    #     scatter.set_offsets(theta_history[:frame, :2])
    #     print(frame)
    #     return scatter,

    # ani = FuncAnimation(fig, update, frames=len(theta_history), interval=200, blit=False)
    # ani.save(f"linear_regression_contour_lr_{lr}.gif")





#!/usr/bin/env python
# coding: utf-8

# In[9]:


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
</FONT>    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    X_with_bias = np.hstack((np.ones((N, 1)), X))
    noise = np.random.normal(0, noise_sigma, N)
    y = X_with_bias @ theta + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.epochs = 0
        self.batch_size = 1
        self.epsilon = 1e-6
    
    def fit(self, X, y, learning_rate=0.01, batch_size=1, epsilon=1e-6):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features+1)
        self.batch_size = batch_size
        self.epsilon = epsilon
        X = np.hstack((np.ones((n_samples, 1)), X))

        theta_history = [self.theta.copy()]
        n_batches = n_samples // self.batch_size

        prev_cost = None
        curr_cost = 1/(2*n_samples) * np.sum((X @ self.theta - y)**2)
        epoch = 0

        while epoch == 0 or np.abs(prev_cost - curr_cost) &gt; self.epsilon:

            for i in range(n_batches):
            
                X_batch = X[i*self.batch_size:(i+1)*self.batch_size]
                y_batch = y[i*self.batch_size:(i+1)*self.batch_size]

                y_pred = X_batch @ self.theta
                cost = 1/(2*self.batch_size) * np.sum((y_pred - y_batch)**2)
                gradient = (1/self.batch_size) * X_batch.T @ (y_pred - y_batch)
                self.theta -= learning_rate * gradient

                theta_history.append(self.theta.copy())

            prev_cost = curr_cost
            curr_cost = 1/(2*n_samples) * np.sum((X @ self.theta - y)**2)
            epoch += 1
            
        self.epochs = epoch
          
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        X = np.hstack((np.ones((n_samples, 1)), X))
        y_pred = X @ self.theta
        return y_pred


# In[11]:


# set the seed to ensure reproducibility
N = 1000000
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)
X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

# split the data into training and testing sets
train_size = int(0.8 * N)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]


# In[12]:


import time

learning_rate = 0.001
batch_sizes = [(1, 1e-6) , (80, 1e-6), (8000, 1e-6), (800000, 1e-6)]
theta_histories = []
for batch_size, epsilon in batch_sizes:
    start_time = time.time()
    regressor = StochasticLinearRegressor()
    theta_history = regressor.fit(X_train, y_train, learning_rate, batch_size, epsilon)
    theta_histories.append(theta_history)
    print(f'Batch Size: {batch_size}, Epsilon: {epsilon}, Iterations: {len(theta_history)}, epochs: {regressor.epochs}, Time: {time.time() - start_time}')
    print(f"The final parameters are: {theta_history[-1]}")


# In[13]:


X_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
Closed_form = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y_train

regressor = StochasticLinearRegressor()
theta_history = regressor.fit(X_train, y_train, 0.01, 1, 1e-6)

print(f'Closed Form: {Closed_form}')
print(f'Gradient Descent: {theta_history[-1]}')


# In[17]:


regressor = StochasticLinearRegressor()
theta_history = regressor.fit(X_train, y_train, 0.01, 1, 1e-6)
y_pred = regressor.predict(X_test)
mse = np.mean((y_pred - y_test)**2)
training_error = np.mean((regressor.predict(X_train) - y_train)**2)


print(f'Training Error: {training_error}')
print(f'Mean Squared Error: {mse}')


# In[13]:


fig = plt.figure(figsize=(15, 10))

# Create a 3D subplot for each batch size
for i, (batch_size, epsilon) in enumerate(batch_sizes):
    ax = fig.add_subplot(2, 2, i+1, projection='3d')
    theta_history = theta_histories[i]
    
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o')
    ax.set_title(f'Batch Size: {batch_size}, Epsilon: {epsilon}')
    ax.set_xlabel('θ0')
    ax.set_ylabel('θ1')
    ax.set_zlabel('θ2')
    
    # Save the figure for each subplot
    plt.savefig(f'plot_batch_size_{batch_size}_epsilon_{epsilon}.png')





#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.tol = 1e-4

    def sigmoid(self, z):
        return 1/(1+np.exp(-z))                             # Sigmoid function = 1 / (1 + exp(-z))
    
    def log_likelihood(self, X, y):
        h = self.sigmoid(np.dot(X, self.theta))
        return np.sum(y*np.log(h) + (1-y)*np.log(1-h))      # Log likelihood = sum(y*log(h) + (1-y)*log(1-h))
    
    def gradient(self, X, y):
        h = self.sigmoid(np.dot(X, self.theta))
        return np.dot(X.T, (h-y))                           # Gradient = X^T * (h - y)
    
    def hessian(self, X, y):
        h = self.sigmoid(np.dot(X, self.theta))             
        return np.dot(X.T * h * (1 - h), X)                 # Hessian matrix = X^T * W * X
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        self.theta = np.zeros(X.shape[1]+1)                 # Initializing the parameters
        X = np.hstack((np.ones((X.shape[0], 1)), X))        # Adding a intercept to the input data
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)    # Normalizing the input data
        theta_history = [self.theta.copy()]
        
        while np.linalg.norm(self.gradient(X, y)) &gt; self.tol:       # Stopping criteria - L2 norm of the gradient
            hessian = self.hessian(X, y)
            gradient = self.gradient(X, y)
            self.theta -= learning_rate * np.dot(np.linalg.inv(hessian), gradient)
            theta_history.append(self.theta.copy())

        return theta_history

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return np.round(self.sigmoid(np.dot(X, self.theta)))


# In[ ]:


X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")
y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")

model = LogisticRegressor()
theta_history = model.fit(X, y)
print('Final Parameters:', model.theta)
# y_pred = model.predict(X)


# In[13]:


import matplotlib.pyplot as plt

# Plot the training data
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', marker='o', label='Class 0')
<A NAME="0"></A><FONT color = #FF0000><A HREF="match138-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', marker='x', label='Class 1')

# Plot the decision boundary
x_values = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
y_values = -(model.theta[0] + model.theta[1] * x_values) / model.theta[2]
plt.plot(x_values, y_values, label='Decision Boundary')

plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
</FONT>plt.show()





#!/usr/bin/env python
# coding: utf-8

# In[2]:


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mean_0 = None
        self.mean_1 = None
        self.same_covariance = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.pi = None

    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def discriminant_function(self, X, mean, inv_sigma, log_det_sigma, pi):
        return -0.5 * np.dot(np.dot((X - mean).T, inv_sigma), (X - mean)) - 0.5 * log_det_sigma + np.log(pi)

    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mean_0, mean_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mean_0, mean_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        n_samples, n_features = X.shape
        X = self.normalize(X)
        self.pi = np.mean(y)

        self.mean_0 = np.mean(X[y == 0], axis=0)
        self.mean_1 = np.mean(X[y == 1], axis=0)

        if assume_same_covariance:
            self.same_covariance = True
            diff_0 = X[y == 0] - self.mean_0
            diff_1 = X[y == 1] - self.mean_1
            self.sigma = (np.dot(diff_0.T, diff_0) + np.dot(diff_1.T, diff_1)) / n_samples
            self.mean_0 = np.reshape(self.mean_0, (-1, 1))
            self.mean_1 = np.reshape(self.mean_1, (-1, 1))
            return self.mean_0, self.mean_1, self.sigma
        else:
            self.same_covariance = False
            diff_0 = X[y == 0] - self.mean_0
            diff_1 = X[y == 1] - self.mean_1
            self.sigma_0 = np.dot(diff_0.T, diff_0) / len(diff_0)
            self.sigma_1 = np.dot(diff_1.T, diff_1) / len(diff_1)
            self.mean_0 = np.reshape(self.mean_0, (-1, 1))
            self.mean_1 = np.reshape(self.mean_1, (-1, 1))
            return self.mean_0, self.mean_1, self.sigma_0, self.sigma_1

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="1"></A><FONT color = #00FF00><A HREF="match138-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        if self.same_covariance:
            # Same covariance
            sigma_inv = np.linalg.inv(self.sigma)
            w = np.dot(sigma_inv, (self.mean_1 - self.mean_0))
</FONT>            b = np.log((self.pi) / (1 - self.pi)) + 0.5 * (np.dot(np.dot(self.mean_0.T, sigma_inv), self.mean_0) - np.dot(np.dot(self.mean_1.T, sigma_inv), self.mean_1))
            y_pred = np.dot(X, w) + b
            y_pred = np.reshape(y_pred, (y_pred.shape[0],))
            return np.where(y_pred &gt; 0, "Canada", "Alaska")
        else:
            # Different covariance
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)
            z = []
            for i in range(X.shape[0]):
                g_0 = self.discriminant_function(X[i].reshape(-1,1), self.mean_0, sigma_0_inv, np.log(np.linalg.det(self.sigma_0)), 1 - self.pi)
                g_1 = self.discriminant_function(X[i].reshape(-1,1), self.mean_1, sigma_1_inv, np.log(np.linalg.det(self.sigma_1)), self.pi)
                z.append(g_1 - g_0)
            y_pred = np.array(z).reshape(X.shape[0])
            return np.where(y_pred &gt; 0, "Canada", "Alaska")


# In[3]:


X = np.loadtxt('../data/Q4/q4x.dat', dtype=float)
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

model = GaussianDiscriminantAnalysis()
mean_0, mean_1, sigma = model.fit(X, y, assume_same_covariance=True)
print("mean_0 (Alaska):", mean_0)
print("mean_1 (Canada):", mean_1)
print("sigma", sigma)
print("pi", model.pi)



# 1. Plot the data
X = model.normalize(X)
plt.figure(figsize=(10, 8))
plt.scatter(X[y == 0, 0], X[y == 0, 1], label='Alaska', marker='o')
plt.scatter(X[y == 1, 0], X[y == 1, 1], label='Canada', marker='x')
plt.xlabel('Growth ring diameter in fresh water')
plt.ylabel('Growth ring diameter in marine water')
plt.legend()
plt.title('Salmon Data: Canada vs Alaska')

# 3. Plot linear decision boundary
x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
sigma_inv = np.linalg.inv(sigma)
mean_0 = mean_0.reshape(-1, 1)
mean_1 = mean_1.reshape(-1, 1)

Sigma_inv = np.linalg.inv(sigma)
w = np.dot(sigma_inv, (mean_1 - mean_0))
b = np.log((model.pi) / (1 - model.pi)) + 0.5 * (np.dot(np.dot(mean_0.T, sigma_inv), mean_0) - np.dot(np.dot(mean_1.T, sigma_inv), mean_1))
x2 = - (w[0] * x1 + b) / w[1]
x2 = np.reshape(x2, (x2.shape[1],))
plt.plot(x1, x2, label='Linear Decision Boundary')


model = GaussianDiscriminantAnalysis()
mean_0, mean_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)
print("mean_0 (Alaska):", mean_0)
print("mean_1 (Canada):", mean_1)
print('sigma_0: ', sigma_0)
print('sigma_1: ', sigma_1)
print("pi", model.pi)
print('predict:', model.predict(X))

# # 4. plot Quadratic Boundary
X = model.normalize(X)
x1 = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
x2 = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
xx, yy = np.meshgrid(x1, x2)
points = np.c_[xx.ravel(), yy.ravel()]

sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)
z = []
for i in range(points.shape[0]):
    g_0 = model.discriminant_function(points[i].reshape(-1,1), mean_0, sigma_0_inv, np.log(np.linalg.det(sigma_0)), 1 - model.pi)
    g_1 = model.discriminant_function(points[i].reshape(-1,1), mean_1, sigma_1_inv, np.log(np.linalg.det(sigma_1)), model.pi)
    z.append(g_1 - g_0)

z = np.array(z).reshape(xx.shape)
plt.contour(xx, yy, z, levels=[0], colors='r', label='Quadratic Decision Boundary')
plt.legend()
plt.show()








</PRE>
</PRE>
</BODY>
</HTML>
