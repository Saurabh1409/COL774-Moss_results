<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_EQX8B.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.tolerance = 1e-9
        self.n_iter = 1000
        self.theta = None

    def compute_error(self,X_aug,param, y):
        error = X_aug.dot(param) - y
        return error
    def compute_cost(self,m, err):
        return (1 / (2 * m)) * np.sum(err ** 2)
    def calc_grad(self, X,err):
        m = X.shape[0]
        return (1 / m) * (X.T.dot(err))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m = X.shape[0]
        n= X.shape[1]
        y=y.flatten()
        p_cost = float('inf')
        X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
        self.theta = np.zeros(n+1)
        parameters_list = []
        i=0
        while True:
            i+=1
            error= self.compute_error(X_aug,self.theta,y)
            cost = self.compute_cost(m,error)
            grad = self.calc_grad(X_aug , error)
           
            if  i &gt; self.n_iter or abs(cost-p_cost)&lt;self.tolerance:
                break
            p_cost=cost
            self.theta = self.theta - learning_rate*grad
            curr_theta= self.theta.copy()
            parameters_list.append(curr_theta)
        return parameters_list

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="5"></A><FONT color = #FF0000><A HREF="match111-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
</FONT>        output = X_aug.dot(self.theta)
        return output



#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from mpl_toolkits.mplot3d import Axes3D 
from IPython.display import HTML
import time
get_ipython().run_line_magic('matplotlib', 'inline')
# %matplotlib notebook



from linear_regression import LinearRegressor

X = pd.read_csv('../data/Q1/linearX.csv', header=None).values  # shape (m, 1) expected
y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()  

print("X shape:", X.shape)
print("y shape:", y.shape)
# Part 1: Train model and get parameters
model = LinearRegressor(tolerance=1e-9, n_iter=10000)
learning_rate = 0.01
params_history = model.fit(X, y, learning_rate=learning_rate)
theta_final = model.theta
print(len(params_history))
print(params_history[len(params_history)-1])
# Part 2: Plot data and hypothesis
plt.figure(figsize=(10,6))
plt.scatter(X, y, label='Training Data', alpha=0.9)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match111-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x_line = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
y_line = model.predict(x_line)
output = y_line
plt.plot(x_line, y_line, 'r-', lw=4, label='Learned Hypothesis')
</FONT>plt.xlabel('Acidity ')
plt.ylabel('Density')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()


# In[ ]:


# %% [code]
# Create a grid for theta0 and theta1
theta0_vals = np.linspace(-10, 15, 100)
theta1_vals = np.linspace(-5, 50, 100)
Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

m = X.shape[0]
J_vals = np.zeros(Theta0.shape)
# Compute cost function values over the grid
for i in range(Theta0.shape[0]):
    for j in range(Theta0.shape[1]):
        theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
        predictions = theta_temp[0] + theta_temp[1] * X.flatten()
        J_vals[i, j] = (1/(2*m)) * np.sum((predictions - y)**2)

# # Plot the 3D surface
# fig = plt.figure(figsize=(10,8))
# ax = fig.add_subplot(111, projection='3d')
# surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.7)
# ax.set_xlabel('Theta0')
# ax.set_ylabel('Theta1')
# ax.set_zlabel('Cost J(theta)')
# ax.set_title('3D Mesh of the Cost Function')

# # Animate the gradient descent path
# for theta in params_history:
#     cost = (1/(2*m)) * np.sum((theta[0] + theta[1]*X.flatten() - y)**2)
#     print(cost)
#     # ax.scatter(theta[0], theta[1], cost, color='r', s=50)
#     # plt.draw()
#     # plt.pause(0.2)

# plt.show()


# In[51]:


import numpy as np
import matplotlib.pyplot as plt
import time

# Make sure interactive mode is on for animations in a notebook.

# Suppose you already have:
# 1. Theta0, Theta1 meshgrid for plotting cost function
# 2. J_vals: cost values over that mesh
# 3. theta_history: list of (theta0, theta1) pairs from each iteration

fig, ax = plt.subplots(figsize=(8,6))

# Draw contour lines for J(theta0, theta1). 
# log-spaced levels help visualize wide ranges of cost values.
contour = ax.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
ax.set_xlabel('Theta0')
ax.set_ylabel('Theta1')
ax.set_title('Contour Plot of Cost Function')

# We'll store the trajectory of theta0, theta1 to draw a line at the end.
theta0_trajectory = []
theta1_trajectory = []

# Animate the gradient descent trajectory on the contour plot.
for t in params_history:
    theta0_trajectory.append(t[0])
    theta1_trajectory.append(t[1])
    
    # Plot the current point as a red 'x'
    ax.plot(t[0], t[1], 'rx')
    
    # Force the plot to update
    plt.draw()
    plt.pause(0.2)  # Wait 0.2 seconds so we can see the movement
    
# After the loop, connect all the red dots with a line for the full path
ax.plot(theta0_trajectory, theta1_trajectory, 'r-', label='GD Trajectory')
ax.legend()

plt.show()


# In[3]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from mpl_toolkits.mplot3d import Axes3D 
from IPython.display import HTML
from matplotlib import cm
import time
# %matplotlib inline

# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self,tol,it):
        self.tolerance = tol
        self.n_iter = it
        self.theta = None

    def compute_error(self,X_aug,param, y):
        error = X_aug.dot(param) - y
        return error
    def compute_cost(self,m, err):
        return (1 / (2 * m)) * np.sum(err ** 2)
    def calc_grad(self, X,err):
        m = X.shape[0]
        return (1 / m) * (X.T.dot(err))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m = X.shape[0]
        n= X.shape[1]
        y=y.flatten()
        p_cost = float('inf')
        X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
        self.theta = np.zeros(n+1)
        parameters_list = []
        i=0
        while True:
            i+=1
            error= self.compute_error(X_aug,self.theta,y)
            cost = self.compute_cost(m,error)
            grad = self.calc_grad(X_aug , error)
           
            if  i &gt; self.n_iter or abs(cost-p_cost)&lt;self.tolerance:
                break
            p_cost=cost
            self.theta = self.theta - learning_rate*grad
            curr_theta= self.theta.copy()
            parameters_list.append(curr_theta)
        return parameters_list

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
<A NAME="6"></A><FONT color = #00FF00><A HREF="match111-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
</FONT>        output = X_aug.dot(self.theta)
        return output

X = pd.read_csv(r'C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q1\linearX.csv', header=None).values  # shape (m, 1) expected
y = pd.read_csv(r'C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q1\linearY.csv', header=None).values.flatten()  

print("X shape:", X.shape)
print("y shape:", y.shape)
# Part 1: Train model and get parameters
model = LinearRegressor(tol=1e-9, it=1000)
learning_rate = 0.05
params_history = model.fit(X, y, learning_rate=learning_rate)
theta_final = model.theta
print(theta_final)
print("Len of params: ", len(params_history))
print(params_history)
# Part 2: Plot data and hypothesis
plt.figure(figsize=(8,6))
plt.scatter(X, y, label='Training Data', alpha=0.9)
x_line = np.linspace(X.min(), X.max(), 1000).reshape(-1,1)
y_line = model.predict(x_line)
output = y_line
plt.plot(x_line, y_line, 'r-', lw=4, label='Learned Hypothesis')
plt.xlabel('Acidity ')
plt.ylabel('Density')
plt.title('Linear Regression Fit')
plt.legend()
# plt.show()

# Create a grid for theta0 and theta1
# theta0_vals = np.linspace(-10, 15, 100)  # Creates 100 evenly spaced values from -10 to 15
# theta1_vals = np.linspace(-5, 50, 100)   # Creates 100 evenly spaced values from -5 to 50

theta0_vals = np.linspace(-25, 30, 300)
theta1_vals = np.linspace(-10, 60, 300)
Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

m = X.shape[0]
J_vals = np.zeros_like(Theta0)
# Compute cost function values over the grid
for i in range(Theta0.shape[0]):
    for j in range(Theta0.shape[1]):
        theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
        predictions = theta_temp[0] + theta_temp[1] * X.flatten()
        J_vals[i, j] = (1/(2*m)) * np.sum((predictions - y)**2)

# Plot the 3D surface
# fig = plt.figure(figsize=(10,8))
# ax = fig.add_subplot(111, projection='3d')
# surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap= cm.gist_rainbow, alpha=0.5)
# surf._facecolors2d = surf._facecolor3d
# surf._edgecolors2d = surf._edgecolor3d
# ax.set_xlabel('Theta0')
# ax.set_ylabel('Theta1')
# ax.set_zlabel('Cost J(theta)')
# ax.set_title('3D Mesh of the Cost Function')

# # Animate the gradient descent path
# for theta in params_history:
#     cost = (1/(2*m)) * np.sum((theta[0] + theta[1]*X.flatten() - y)**2)
#     ax.scatter(theta[0], theta[1], cost, color='r', s=50)
#     plt.draw()
#     plt.pause(0.2)

# plt.show()

# %%

# fig, ax = plt.subplots(figsize=(8,6))

# # Draw contour lines for J(theta0, theta1). 
# # log-spaced levels help visualize wide ranges of cost values.
# contour = ax.contour(Theta0, Theta1, J_vals, levels=200, cmap= cm.gist_rainbow)
# ax.set_xlabel('Theta0')
# ax.set_ylabel('Theta1')
# ax.set_title('Contour Plot of Cost Function')

# # We'll store the trajectory of theta0, theta1 to draw a line at the end.
# theta0_trajectory = []
# theta1_trajectory = []

# # Animate the gradient descent trajectory on the contour plot.
# for t in params_history:
#     theta0_trajectory.append(t[0])
#     theta1_trajectory.append(t[1])
    
#     # Plot the current point as a red 'x'
#     ax.plot(t[0], t[1], 'rx')
    
#     # Force the plot to update
#     plt.draw()
#     plt.pause(0.005)  # Wait 0.2 seconds so we can see the movement
    
# # After the loop, connect all the red dots with a line for the full path
# ax.plot(theta0_trajectory, theta1_trajectory, 'r-', label='GD Trajectory')
# ax.legend()

# plt.show()
# # %%





#!/usr/bin/env python
# coding: utf-8

# Run the first code block 

# In[ ]:


# Imports - you can add any other permitted libraries
import numpy as np
import time
<A NAME="0"></A><FONT color = #FF0000><A HREF="match111-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    Note that we have 2 input features.
    
    Parameters
    ----------
    N : int
        The number of samples to generate.
    theta : numpy array of shape (3,)
</FONT>        The true parameters of the linear regression model.
    input_mean : numpy array of shape (2,)
        The mean of the input data.
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
    y : numpy array of shape (N,)
        The target values.
    """
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)
    
    X = np.column_stack((x1, x2))
    y = theta[0] + np.dot(X, theta[1:]) + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.iter = None
    
    def unisonShuffle(self, a, b):
        idx = np.random.permutation(a.shape[0])
        return a[idx], b[idx]
    
    def calc_grad(self, X, theta, y, r):
        # X is assumed to be (r, n) and theta is (n,1) and y is (r,1)
        outcome = X.T.dot(X.dot(theta) - y)
        outcome = outcome / r
        return outcome
    
    def fit(self, X, y, learning_rate=0.01, store_every=100):
        """
        Fit the linear regression model to the data using SGD.
        
        Parameters
        ----------
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match111-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X : numpy array of shape (n_samples, n_features)
            The input data (without intercept).
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
</FONT>            The learning rate.
        store_every : int
            Only store theta after every 'store_every' iterations to avoid memory overload.
            
        Returns
        -------
        epochs : int
            Number of epochs run.
        param_list : list of numpy arrays
            A list of stored theta values (each is shape (n_features+1,)) from the updates.
        time_elapsed : float
            Total time taken.
        """
        # We are going to run only for batch size 1
        batch_sizes = [1]
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # shape (m, n+1)
        param_list = []
        m, n = X_aug.shape

        n_epchs, threshold = 1000, 1e-7
        epoch = 0
        total_iter = 0
        # For batch size 1 only
        r = 800000
        theta = np.zeros((n, 1))  # Initialize as column vector
        epoch_theta = []  # To store theta updates (subsampled)
        t0 = time.time()
        prev_loss = float('inf')
        
        while epoch &lt; n_epchs:
            epoch += 1
            # Shuffle the data at the start of each epoch
            X_shuffled, y_shuffled = self.unisonShuffle(X_aug, y)
            y_shuffled = y_shuffled.reshape(-1, 1)
            for b in range(0, m, r):
                total_iter += 1
                X_batch = X_shuffled[b:b+r]
                y_batch = y_shuffled[b:b+r]
                grad = self.calc_grad(X_batch, theta, y_batch, r)
                theta = theta - learning_rate * grad
                # Subsample storage to avoid huge memory usage
                if total_iter % store_every == 0:
                    epoch_theta.append(theta.flatten().copy())
            # Compute epoch loss (MSE)
            curr_loss = np.mean((X_aug.dot(theta) - y.reshape(-1,1))**2)
            if abs(curr_loss - prev_loss) &lt;= threshold:
                self.theta = theta.copy()
                break
            prev_loss = curr_loss
        time_elapsed = time.time() - t0
        self.theta = theta.copy()
        self.iter = total_iter
        param_list.append(np.array(epoch_theta))
        return epoch, param_list, time_elapsed

    def predict(self, X):
        """
        Predict target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
        
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
        """
        m = X.shape[0]
        X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
        return X_aug.dot(self.theta).flatten()

# def closed_form_solution(X, y):
#     """
#     Compute the closed form solution for linear regression.
#     """
#     m = X.shape[0]
#     X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
#     theta = np.linalg.pinv(X_aug.T.dot(X_aug)).dot(X_aug.T).dot(y.reshape(-1, 1))
#     return theta

# def mean_squared_error(X, y, theta):
#     """
#     Compute mean squared error (MSE) given data and parameter theta.
#     """
#     m = X.shape[0]
#     X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
#     error = y.reshape(-1, 1) - X_aug.dot(theta)
#     mse = np.mean(error**2)
#     return mse

def plot_theta_movement_separate(theta_history_list, batch_sizes, subsample_rate=1):
    """
    Plot the trajectory of theta (θ0, θ1, θ2) in 3D for each batch size on a separate plot.
    theta_history_list: list of numpy arrays, each of shape (n_updates, 3)
    batch_sizes: list of batch sizes corresponding to each history.
    subsample_rate: int, used if further subsampling is desired.
    """
    from mpl_toolkits.mplot3d import Axes3D
    colors = ['r', 'g', 'b', 'm']
    
    for i, theta_hist in enumerate(theta_history_list):
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        subsampled = theta_hist
        ax.plot(subsampled[:, 0], subsampled[:, 1], subsampled[:, 2],
                marker='o', color=colors[i % len(colors)], label=f'Batch Size {batch_sizes[i]}')
        ax.scatter(subsampled[-1, 0], subsampled[-1, 1], subsampled[-1, 2],
                   color=colors[i % len(colors)], marker='X', s=100)
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_zlabel('θ2')
        ax.set_title(f'Trajectory of θ (Batch Size {batch_sizes[i]})')
        ax.legend()
        plt.show()

# -----------------------------------
# Main Execution
# -----------------------------------
if __name__ == "__main__":
    # 1. Generate 1 million samples.
    N = 1000000
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])       # x1 ~ N(3,4) and x2 ~ N(-1,4); variance=4 =&gt; std=2.
    input_sigma = np.array([2, 2])
    noise_sigma = np.sqrt(2)             # noise variance=2, so std = sqrt(2)
    X_all, y_all = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
    print("Data shapes:", X_all.shape, y_all.shape)
    
    # Split into 80% training and 20% testing.
    split_index = int(0.8 * N)
    X_train, y_train = X_all[:split_index], y_all[:split_index]
    X_test, y_test = X_all[split_index:], y_all[split_index:]
    
    # 2. Run SGD with batch size = 1.
    sgd_model = StochasticLinearRegressor()
    epochs, sgd_histories, elapsed_time = sgd_model.fit(X_train, y_train, learning_rate=0.001, store_every=100)
    # Since we're only using batch size 1 here, we expect one entry in batch_sizes.
    batch_sizes = [800000]
    print("Total iterations:", sgd_model.iter)
    print("Epochs run:", epochs)
    print("Time elapsed:", elapsed_time, "seconds")
    print("Theta history shape:", sgd_histories[0].shape)
    print("SGD history shape: ", len(sgd_histories))
    # print("SGD history: ", sgd_histories)
    # 3. Compute closed form solution.
    # theta_closed = closed_form_solution(X_train, y_train)
    # print("True θ:", theta_true)
    # print("Closed Form θ:", theta_closed.flatten())
    # final_theta_sgd = sgd_histories[0][-1]
    # print("SGD θ for batch size 1:", final_theta_sgd)
    
    # # 4. Compute training and test errors.
    # train_error_closed = mean_squared_error(X_train, y_train, theta_closed)
    # test_error_closed = mean_squared_error(X_test, y_test, theta_closed)
    # print("Closed Form Training MSE:", train_error_closed)
    # print("Closed Form Test MSE:", test_error_closed)
    
    # theta_sgd_1 = sgd_histories[0][-1].reshape(-1, 1)
    # train_error_sgd = mean_squared_error(X_train, y_train, theta_sgd_1)
    # test_error_sgd = mean_squared_error(X_test, y_test, theta_sgd_1)
    # print("SGD (batch size 1) Training MSE:", train_error_sgd)
    # print("SGD (batch size 1) Test MSE:", test_error_sgd)
    # print("SGD (batch size 1) Difference MSE:", abs(test_error_sgd - train_error_sgd))
    
    # 5. Plot the movement of θ in 3D (for batch size 1) using the subsampled history.
    plot_theta_movement_separate(sgd_histories, batch_sizes, subsample_rate=1)
    
    # For answering the questions:
    # Compare true parameters, closed form, and SGD parameters.
    # Print them out:
    


# In[ ]:





# (1000000, 2) (1000000,)
# Iterations: 7658
# EPoch:  7658
#  time:  521.0405750274658
# 1
# (7658, 3)
# True θ: [3 1 2]
# Closed Form θ: [3.00049606 0.99887837 2.00112176]
# SGD θ for batch size 800000: [2.98858638 1.0000511  2.00228327]
# Closed Form Training MSE: 2.007352151379074
# Closed Form Test MSE: 2.0011322411718813
# SGD (batch size 1) Training MSE: 2.0074541906374535
# SGD (batch size 1) Test MSE: 2.001297633490761
# SGD (batch size 1) Difference MSE: 0.0061565571466926095

# In[ ]:





# In[ ]:


arr=[] # r, theta, iterations, epochs, time taken
epoch,sgd_histories,t = sgd_model.fit(X_train, y_train,batch_sizes, learning_rate=0.001)

    # batch =[batch_sizes[i]]
    # r=batch_sizes[i]
    # epoch,sgd_histories,t = sgd_model.fit(X_train, y_train,batch, learning_rate=0.001)
    # # sgd_histories is a list of 4 numpy arrays, each corresponding to batch sizes [1, 80, 8000, 800000].
    # final_theta_sgd = sgd_histories[0][-1]
    # iterations =sgd_model.iter
    # print(f"SGD θ for batch size {batch_sizes[i]}:", final_theta_sgd)
    # print("Epoch", epoch)
    # print("Iterations:", sgd_model.iter)
    # print("Length of sgd_histories",len(sgd_histories))
    # print(sgd_histories[0].shape)
    # print("Time: ", t)
    # print("-----------------------")
    # temp =[r,final_theta_sgd,iterations,epoch,t]
    # arr.append(temp)
    
print("Table: ", arr)


# In[ ]:


theta_closed = closed_form_solution(X_train, y_train)
print("True θ:", theta_true)
print("Closed Form θ:", theta_closed.flatten())
# For each SGD run, print final theta:
# for i, r in enumerate(batch_sizes):
#     final_theta_sgd = sgd_histories[i][-1]
#     print(f"SGD θ for batch size {r}:", final_theta_sgd)

# 4. Compute training and test error (MSE) for closed form and one of the SGD models.
train_error_closed = mean_squared_error(X_train, y_train, theta_closed)
test_error_closed = mean_squared_error(X_test, y_test, theta_closed)
print("Closed Form Training MSE:", train_error_closed)
print("Closed Form Test MSE:", test_error_closed)

error_arr=[]
for i in range(4):
    theta_sgd_1 = arr[i][1].reshape(-1,1)
    train_error_sgd = mean_squared_error(X_train, y_train, theta_sgd_1)
    test_error_sgd = mean_squared_error(X_test, y_test, theta_sgd_1)
    differ =abs(test_error_sgd-train_error_sgd)
    print(f"SGD Train MSE for batch size {batch_sizes[i]}:", train_error_sgd)
    print(f"SGD Test MSE for batch size {batch_sizes[i]}:", test_error_sgd)
    print(f"SGD Differ MSE for batch size {batch_sizes[i]}:", abs(test_error_sgd-train_error_sgd))
    temp =[batch_sizes[i],train_error_sgd,test_error_sgd,differ]
    error_arr.append(temp)
        
print("ERROR ARR: ",error_arr)


# In[ ]:


def plot_theta_movement_separate(theta_history_list, batch_sizes,subsample_rate=50):
    """
    Plot the trajectory of theta (θ0, θ1, θ2) in 3D for each batch size in a separate plot.
    
    Parameters:
        theta_history_list: list of numpy arrays, each of shape (n_epochs, 3),
                            representing the parameter trajectory for a batch size.
        batch_sizes: list of batch sizes corresponding to each history in theta_history_list.
    """
    from mpl_toolkits.mplot3d import Axes3D
    colors = ['r', 'g', 'b', 'm']
    
    for i, theta_hist in enumerate(theta_history_list):

        
        fig = plt.figure(figsize=(6, 8))
        ax = fig.add_subplot(111, projection='3d')

        subsampled = theta_hist[::subsample_rate, :]
        ax.plot(theta_hist[:, 0], theta_hist[:, 1], theta_hist[:, 2],
                marker='o', color=colors[i % len(colors)], label=f'Batch Size {batch_sizes[i]}')
        # Mark the final parameter with a larger marker.
        ax.scatter(theta_hist[-1, 0], theta_hist[-1, 1], theta_hist[-1, 2],
                   color=colors[i % len(colors)], marker='X', s=100)
        ax.set_xlabel('θ0')
        ax.set_ylabel('θ1')
        ax.set_zlabel('θ2')
        ax.set_title(f'Trajectory of θ (Batch Size {batch_sizes[i]})')
        ax.legend()
        plt.show()

plot_theta_movement_separate(sgd_histories, batch_sizes)
    





# Imports - you can add any other permitted libraries
import numpy as np
import time
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    # We know X is normally distributed so we use this
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    # Generate here noise
    noise = np.random.normal(0,noise_sigma,N)
    
    X = np.array([x1, x2]).reshape(N, 2)
    y = theta[0] + np.dot(X, theta[1:])
    y+= noise

    return X,y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta=None
    
    def unisonShuffle(self, a, b):
        x= np.random.permutation(a.shape[0])
        return a[x],b[x]
    
    def calc_grad(self,X,theta,y,r):
        outcome = X.T.dot(X.dot( theta) - y)
        outcome = outcome/r
        return outcome
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        batch_sizes = [1, 80, 8000, 800000]
        
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1) 
        param_list =[]
        m, n = X_aug.shape

        n_epchs , threshold = 1000, 1e-7
        for r in batch_sizes:
            theta, prev_loss,curr_loss = np.zeros(n), 0,1
            epoch,epoch_theta=0,[]  # List to store params at end of eachh epoch
            
            t= time.time()
            while epoch&lt;n_epchs:
                epoch+=1
                a, c = self.unisonShuffle(X_aug,y)
                # c = c.reshape(-1,1)
                for b in range(0,m,r):
                    X_batch, y_batch = a[b:b+r], c[b:b+r]
                    grad = self.calc_grad(X_batch,theta,y_batch,r)
                    theta =theta - learning_rate*grad
                theta_c= theta.copy()
                epoch_theta.append(theta_c.flatten())

                curr_loss = np.mean(X_aug.dot(theta)-y)**2
                if abs(curr_loss-prev_loss) &lt;= threshold:
                    prev_loss =curr_loss
                    self.theta=theta
                    break
                prev_loss=curr_loss
            param_list.append(np.array(epoch_theta))
            if epoch==n_epchs: self.theta=theta
            time_elapsed= time.time()-t

            # if r == X.shape[0]: self.theta = theta
        return param_list



    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m,n= X.shape[0], X.shape[1]
<A NAME="7"></A><FONT color = #0000FF><A HREF="match111-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return  np.concatenate([np.ones((m, 1)), X], axis=1).dot(self.theta).flatten()
    
    



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.tol =1e-15
</FONT>        self.n_iter= 10000
        self.theta = None

    def normalize(self, X):
        mean = np.mean(X,axis=0)
        std= np.std(X, axis=0)
       
        output = (X- mean)/std
        return output
    
    def sigm(self, z,bool=True):
        out = 1+ np.exp(-z)
        if (bool!=True):
          d= 1/out
          d= np.clip(d, 1e-15, 1 - 1e-15)
          return d
        else:
            return 1/out
    
    def calc_H(self, X, D):
        return -X.T.dot(D).dot(X)
    def compute_loss(self,X,y,param):
        hypo= np.dot(X,param)
        h= self.sigm(hypo,False)
        cost =  -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
        return cost
    
    def compute_predictions(self, X):
        X_aug= np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
        probabilities = self.sigm(np.dot(X_aug, self.theta))
        return probabilities


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        m = X.shape[0]
        n = X.shape[1]
        # first normalize X
        X_norm = self.normalize(X)
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X_norm], axis=1)

        self.theta = np.zeros(n+1)
        param_list =[]
        y = y.reshape(-1, 1)
        i=0
        curr_loss=-1
        prev_loss=float('inf')
        while True:
            z = X_aug.dot(self.theta).reshape(-1,1)
            h = self.sigm(z)

            # let us compute hessian here 
            D = np.diagflat(h * (1 - h))
            H = self.calc_H(X_aug,D)
            # finding grad here which will be used to compute delta

            grad = np.dot(X_aug.T, (y - h))
            # finding  delta here
            # I have used pseudo inverse which is more robust for practical implemenations
            delta = np.dot(np.linalg.pinv(H), grad).flatten()
            self.theta = self.theta - learning_rate*delta
            prev_loss = curr_loss
            curr_loss = self.compute_loss(X_aug,y,self.theta)
            # Here I am creating a copy of current parameters and then storing them in the list
            theta_c= self.theta.copy()
            param_list.append(theta_c)
            i+=1
            if i&gt;self.n_iter or abs(prev_loss-curr_loss)&lt;self.tol:
                break

        return param_list


               
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        
        predictions = np.where(self.compute_predictions(X) &gt;= 0.5, 1, 0)
        return predictions





#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self, tol=1e-20, n_iter=10000):
        self.tol =tol
        self.n_iter= n_iter
        self.theta = None

    def normalize(self, X):
        mean = np.mean(X,axis=0)
        std= np.std(X, axis=0)
       
        output = (X- mean)/std
        return output
    
    def sigm(self, z,bool=True):
        out = 1+ np.exp(-z)
        if (bool!=True):
          d= 1/out
          d= np.clip(d, 1e-15, 1 - 1e-15)
          return d
        else:

            return 1/out
    
    def calc_H(self, X, D):
        return -X.T.dot(D).dot(X)
    def compute_loss(self,X,y,param):
        hypo= np.dot(X,param)
        h= self.sigm(hypo,False)
        cost =  -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
        return cost
    
    def compute_predictions(self, X):
        X_aug= np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)
        probabilities = self.sigm(np.dot(X_aug, self.theta))
        return probabilities


    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        
        m = X.shape[0]
        n = X.shape[1]
        # first normalize X
        X_norm = self.normalize(X)
        X_aug = np.concatenate([np.ones((X.shape[0], 1)), X_norm], axis=1)

        self.theta = np.zeros(n+1)
        param_list =[]
        y = y.reshape(-1, 1)
        i=0
        curr_loss=-1
        prev_loss=float('inf')
        while True:
            z = X_aug.dot(self.theta).reshape(-1,1)
            h = self.sigm(z)

            # let us compute hessian here 
            D = np.diagflat(h * (1 - h))
            H = self.calc_H(X_aug,D)
            # find grad here which will be used to compute delta

            grad = np.dot(X_aug.T, (y - h))
            # find delta here
            delta = np.dot(np.linalg.pinv(H), grad).flatten()
            self.theta = self.theta - learning_rate*delta
            prev_loss = curr_loss
            curr_loss = self.compute_loss(X_aug,y,self.theta)

            theta_c= self.theta.copy()
            param_list.append(theta_c)
            i+=1
            if i&gt;self.n_iter or abs(prev_loss-curr_loss)&lt;self.tol:
                break

        return param_list


               
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = self.normalize(X)
        predictions= self.compute_predictions(X)
        return ((predictions) &gt;=0.5).astype(int)



# make changes in the below functions as per your need before uncommenting to draw graph

# def plot_decision_boundary(X, y, model):
#     # Normalization parameters from model
#     x1_min, x1_max = X[:,0].min(), X[:,0].max()
#     mean = np.mean(X,axis=0)
#     std= np.std(X, axis=0)

    
#     # Decision boundary: θ₀ + θ₁x₁ + θ₂x₂ = 0
#     x1_vals = np.linspace(x1_min, x1_max, 100)
#     # x2_vals = -(model.theta[0] + model.theta[1]*x1_vals)/model.theta[2]
#     x2_vals = mean[1] - (std[1]/model.theta[2]) * (model.theta[0] + model.theta[1] * ((x1_vals - mean[0]) / std[0]))

#     plt.figure(figsize=(10,6))
#     plt.scatter(X[y==0][:,0], X[y==0][:,1], color = 'blue' , marker='x', label='Class 0')
#     plt.scatter(X[y==1][:,0], X[y==1][:,1], color = 'red' , marker='o', label='Class 1')
#     plt.plot(x1_vals, x2_vals, 'k-', label='Decision Boundary')
#     plt.xlabel('x₁')
#     plt.ylabel('x₂')
#     plt.legend()
#     plt.show()

X = pd.read_csv(r'C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q3\logisticX.csv', header=None).values  # Expected shape: (m, 2)
y = pd.read_csv(r'C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q3\logisticY.csv', header=None).values.flatten()  # Expected shape: (m,)

# Create an instance of LogisticRegressor.
log_reg = LogisticRegressor()

# Fit the model using Newton's method.
theta_history = log_reg.fit(X, y, learning_rate=0.1)
final_theta = log_reg.theta
print("Final coefficients (including intercept):", final_theta.flatten())

print(log_reg.predict(X).shape)

# plot_decision_boundary(X,y,log_reg)





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.phi=None
        self.sigma=None
        self.sigma0=None
        self.sigma1=None
        self.mean=None
        self.std=None
    
    def normalize(self,X):
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        X_norm = (X - mean) / std
        self.mean = mean
        self.std=std
        return X_norm
    def getPhi(self,Y):
        return np.sum(Y[Y==1])/Y.shape[0]
    
    def comp_cov_param(self,X,cl0, cl1, bool):
        m,n = X.shape
        sigma0 = (X[cl0,:]- self.mu0).T.dot(X[cl0]- self.mu0)/X[cl0].shape[0]
        sigma1 = (X[cl1,:]- self.mu1).T.dot(X[cl1]- self.mu1)/X[cl1].shape[0]
        if(bool):
            sig = X[cl0].shape[0] * sigma0 + X[cl1].shape[0] * sigma1
            return sig/m
        else:
            return sigma0, sigma1
        

    def compute_LDA(self, xi):  
        inv_sig = np.linalg.pinv(self.sigma)
        d1 = xi - self.mu0  
        d2 = xi - self.mu1
        t1 = -0.5 * (d1).T.dot(inv_sig).dot(d1) + np.log(1 - self.phi)
        t2 = -0.5 * (d2).T.dot(inv_sig).dot(d2) + np.log(self.phi)
                
        # t1 = -0.5 * np.sum(d1 @ inv_sig * d1, axis=1) + np.log(1 - self.phi)
        # t2 = -0.5 * np.sum(d2 @ inv_sig * d2, axis=1) + np.log(self.phi)
        return t2 &gt; t1  
# Using np.linalg.slogdet for better numerical stability:
    def compute_QDA(self,X):
        sig0_inv =np.linalg.pinv(self.sigma0)
        sig1_inv =np.linalg.pinv(self.sigma1)
        d0 = X - self.mu0
        d1 = X - self.mu1

        s1, logdt0 = np.linalg.slogdet(self.sigma0)
        s2, logdt1 = np.linalg.slogdet(self.sigma1)
        t0 = -0.5 * (np.sum(d0 * (d0 @ sig0_inv), axis=1) + logdt0) + np.log(1 - self.phi)
        t1 = -0.5 * (np.sum(d1 * (d1 @ sig1_inv), axis=1) + logdt1) + np.log(self.phi)
        return t0,t1

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = self.normalize(X)
        class0 = (y==0)  # Alaska
        class1 = (y==1) # Canada
        self.phi = self.getPhi(y)
        self.mu0 = np.mean(X[class0,:], axis=0)
        self.mu1 = np.mean(X[class1,:], axis=0)

        if(assume_same_covariance):
            sigma = self.comp_cov_param(X,class0,class1,assume_same_covariance)
            self.sigma=sigma
            return (self.mu0, self.mu1,sigma)
        else:
            sigma1,sigma2 = self.comp_cov_param(X,class0,class1,assume_same_covariance)
            self.sigma0=sigma1
            self.sigma1=sigma2
            return (self.mu0, self.mu1,sigma1,sigma2)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # X= self.normalize(X)
        X= (X - self.mean) / self.std
        m,n = X.shape
        y_pred= np.zeros(m)

        if self.sigma is not None:
            for i in range(m):
                boolean = self.compute_LDA(X[i])
                if(boolean):
                    y_pred[i]=1
            return y_pred

        else:
            t1, t2 = self.compute_QDA(X)
            out = np.where(t2&gt;t1,1,0)
            return out




#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.phi=None
        self.sigma=None
        self.sigma0=None
        self.sigma1=None
        self.mean=None
        self.std=None
    
    def normalize(self,X):
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        X_norm = (X - mean) / std
        self.mean = mean
        self.std=std
        return X_norm
    def getPhi(self,Y):
        return np.sum(Y[Y==1])/Y.shape[0]
    
    def comp_cov_param(self,X,cl0, cl1, bool):
        m,n = X.shape
        sigma0 = (X[cl0,:]- self.mu0).T.dot(X[cl0]- self.mu0)/X[cl0].shape[0]
        sigma1 = (X[cl1,:]- self.mu1).T.dot(X[cl1]- self.mu1)/X[cl1].shape[0]
        if(bool):
            sig = X[cl0].shape[0] * sigma0 + X[cl1].shape[0] * sigma1
            return sig/m
        else:
            return sigma0, sigma1
        

    def compute_LDA(self, xi):  
        inv_sig = np.linalg.pinv(self.sigma)
        d1 = xi - self.mu0  
        d2 = xi - self.mu1
        t1 = -0.5 * (d1).T.dot(inv_sig).dot(d1) + np.log(1 - self.phi)
        t2 = -0.5 * (d2).T.dot(inv_sig).dot(d2) + np.log(self.phi)
                
        # t1 = -0.5 * np.sum(d1 @ inv_sig * d1, axis=1) + np.log(1 - self.phi)
        # t2 = -0.5 * np.sum(d2 @ inv_sig * d2, axis=1) + np.log(self.phi)
        return t2 &gt; t1  
# Using np.linalg.slogdet for better numerical stability:
    def compute_QDA(self,X):
        sig0_inv =np.linalg.pinv(self.sigma0)
        sig1_inv =np.linalg.pinv(self.sigma1)
        d0 = X - self.mu0
        d1 = X - self.mu1

        s1, logdt0 = np.linalg.slogdet(self.sigma0)
        s2, logdt1 = np.linalg.slogdet(self.sigma1)
        t0 = -0.5 * (np.sum(d0 * (d0 @ sig0_inv), axis=1) + logdt0) + np.log(1 - self.phi)
        t1 = -0.5 * (np.sum(d1 * (d1 @ sig1_inv), axis=1) + logdt1) + np.log(self.phi)
        return t0,t1

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = self.normalize(X)
        class0 = (y==0)  # Alaska
        class1 = (y==1) # Canada
        self.phi = self.getPhi(y)
        self.mu0 = np.mean(X[class0,:], axis=0)
        self.mu1 = np.mean(X[class1,:], axis=0)

        if(assume_same_covariance):
            sigma = self.comp_cov_param(X,class0,class1,assume_same_covariance)
            self.sigma=sigma
            return (self.mu0, self.mu1,sigma)
        else:
            sigma1,sigma2 = self.comp_cov_param(X,class0,class1,assume_same_covariance)
            self.sigma0=sigma1
            self.sigma1=sigma2
            return (self.mu0, self.mu1,sigma1,sigma2)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # X= self.normalize(X)
        X= (X - self.mean) / self.std
        m,n = X.shape
        y_pred= np.zeros(m)

        if self.sigma is not None:
            for i in range(m):
                boolean = self.compute_LDA(X[i])
                if(boolean):
                    y_pred[i]=1
            return y_pred

        else:
            t1, t2 = self.compute_QDA(X)
            out = np.where(t2&gt;t1,1,0)
            return out

def normalize(X):
    """Normalize X column-wise and return the normalized X along with mean and std."""
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    std[std == 0] = 1  # avoid division by zero
    X_norm = (X - mean) / std
    return X_norm, mean, std

def read_data(Xpath, Ypath):
    """Read the data files and normalize X."""
    X = np.array(pd.read_csv(Xpath, header=None, delim_whitespace=True).values)
    Y = np.array(pd.read_csv(Ypath, header=None, delim_whitespace=True).values)
    # Here, Y is assumed to contain strings "Alaska" or "Canada". 
    # We map them to 0 and 1.
    print("X shape:", X.shape)
    print("---",Y.shape)
    Y = np.where(Y=="Canada", 1, 0)
    print("---",Y.shape)
    X_norm, mean, std = normalize(X)
    return X_norm, Y.flatten(), mean, std


def plot_data(X_orig, y, mean, std, title="GDA Data"):
    """
    Plot the original data points (in original feature space).
    X_orig: original (un-normalized) data.
    y: labels (0 or 1)
    """
    # Separate points by class.
    class0 = X_orig[y == 0]
    class1 = X_orig[y == 1]
    
    plt.figure(figsize=(8,6))
<A NAME="1"></A><FONT color = #00FF00><A HREF="match111-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.scatter(class0[:,0], class0[:,1], color='blue', marker='o', label='Class 0 (Alaska)')
    plt.scatter(class1[:,0], class1[:,1], color='red', marker='x', label='Class 1 (Canada)')
    plt.xlabel("Fresh Water")
    plt.ylabel("Marine Water")
    plt.title(title)
</FONT>    plt.legend()
    
# uncomment this during demo 
# def plot_linear_boundary(mu_0, mu_1, sigma, psi, mean, std):
    """
    Plot the linear decision boundary (for the equal-covariance case).
    mu0, mu1, sigma, psi are in normalized space.
    The boundary in normalized space is given by:
         A^T x_norm = c,
    where A = sigma^{-1}(mu1 - mu0) and 
         c = 0.5*(mu1^T sigma^{-1} mu1 - mu0^T sigma^{-1} mu0) + log(psi/(1-psi)).
    Convert x_norm back to original space: x_norm = (x - mean)/std.
    """
#     const_term = np.dot(np.dot(mu_0.transpose(),np.linalg.pinv(sigma)),mu_0) - np.dot(np.dot(mu_1.transpose(),np.linalg.pinv(sigma)),mu_1)
#     x_coeff = 2*np.dot((mu_1.transpose()-mu_0.transpose()),np.linalg.pinv(sigma))
#     x1 = np.linspace(-2,2,2)
#     print(const_term,x_coeff)
#     x2 = -1*(const_term+x_coeff[0]*x1)/x_coeff[1]
#     plt.plot(x1,x2,label='Linear Hypothesis')

def plot_linear_boundary1(mu0, mu1, sigma, psi, mean, std):
    """
    Plot the linear decision boundary (for the equal-covariance case).
    mu0, mu1, sigma, psi are in normalized space.
    The boundary in normalized space is given by:
         A^T x_norm = c,
    where A = sigma^{-1}(mu1 - mu0) and 
         c = 0.5*(mu1^T sigma^{-1} mu1 - mu0^T sigma^{-1} mu0) + log(psi/(1-psi)).
    Convert x_norm back to original space: x_norm = (x - mean)/std.
    """
    sigma_inv = np.linalg.pinv(sigma)
    A = sigma_inv.dot(mu1 - mu0)
    c = 0.5 * (mu1.T.dot(sigma_inv).dot(mu1) - mu0.T.dot(sigma_inv).dot(mu0)) + np.log(psi/(1-psi))
    # We will plot the boundary over a range of x1 values.
    x1_vals = np.linspace(40, 200, 200)
    # In normalized space, for a point x = [x1, x2]:
    # (x1 - mean1)/std1 * A[0] + (x2 - mean2)/std2 * A[1] = c.
    # Solve for x2:
    #   (x2 - mean2)/std2 = (c - A[0]*(x1 - mean1)/std1) / A[1]
    #   x2 = mean2 + std2*(c - A[0]*(x1 - mean1)/std1)/A[1]
    x2_vals = mean[1] + std[1] * (c - A[0]*(x1_vals - mean[0])/std[0]) / A[1]
    plt.plot(x1_vals, x2_vals, 'k-', label='Linear Boundary')

# def plot_quadratic(mu_0, mu_1, sigma_0,sigma_1,psi):
#     C1=np.log(((1-psi)*np.linalg.det(sigma_1)**0.5)/(psi*np.linalg.det(sigma_0)**0.5))
#     """
#     Plot the quadratic decision boundary (for the unequal covariance case).
#     In normalized space, define the discriminant difference function f(x_norm) = g1(x_norm) - g0(x_norm),
#     where g_k(x_norm) = -0.5*log(det(sigma_k)) -0.5*(x_norm-mu_k)^T sigma_k^{-1}(x_norm-mu_k) + log(pi_k).
#     Plot the contour where f(x_norm)=0, converting the coordinates back to the original space.
#     """
#     C2=0.5*(np.dot(np.dot(mu_1.transpose(),np.linalg.pinv(sigma_1)),mu_1)-np.dot(np.dot(mu_0.transpose(),np.linalg.pinv(sigma_0)),mu_0))
#     A=0.5*(np.linalg.pinv(sigma_1)-np.linalg.pinv(sigma_0))
#     B=-1*(np.dot(mu_1.transpose(),np.linalg.pinv(sigma_1))-np.dot(mu_0.transpose(),np.linalg.pinv(sigma_0)))
#     a=A[0][0]+A[0][1]
#     b=A[1][0]+A[1][1]
#     c=A[0][0]+A[0][1]+A[1][0]+A[1][1]
#     d=B[0]
#     e=B[1]
#     f=C1+C2
#     print("a=",a,"b=",b,"c=",c,"d=",d,"e=",e,"f=",f)
#     x1=np.linspace(-3,3,200)
#     x2=np.linspace(-3,3,200)
#     x1,x2=np.meshgrid(x1,x2)
#     return plt.contour(x1,x2,(a*x1**2+b*x2**2+c*x1*x2+d*x1+e*x2+f),[0],colors='m')


def plot_quadratic_boundary(mu0, mu1, sigma0, sigma1, psi, mean, std):
    """
    Plot the quadratic decision boundary (for the unequal covariance case).
    In normalized space, define the discriminant difference function f(x_norm) = g1(x_norm) - g0(x_norm),
    where g_k(x_norm) = -0.5*log(det(sigma_k)) -0.5*(x_norm-mu_k)^T sigma_k^{-1}(x_norm-mu_k) + log(pi_k).
    Plot the contour where f(x_norm)=0, converting the coordinates back to the original space.
    """
    # Create a meshgrid in the original space.
    x1 = np.linspace(-3, 3, 200)
    x2 = np.linspace(-3, 3, 200)
    X1, X2 = np.meshgrid(x1, x2)
    # Convert each grid point to normalized space.
    X_norm = np.zeros((X1.shape[0], X1.shape[1], 2))
    X_norm[:,:,0] = (X1 - mean[0]) / std[0]
    X_norm[:,:,1] = (X2 - mean[1]) / std[1]
    
    # Compute discriminant functions in normalized space.
    # For class 0:
    sigma0_inv = np.linalg.pinv(sigma0)
    sigma1_inv = np.linalg.pinv(sigma1)
    # Precompute determinants (with a small epsilon to avoid log(0)).
    det0 = np.linalg.det(sigma0) + 1e-15
    det1 = np.linalg.det(sigma1) + 1e-15
    f = np.zeros_like(X1)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match111-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for i in range(X_norm.shape[0]):
        for j in range(X_norm.shape[1]):
            x_pt = X_norm[i,j,:]
</FONT>            g0 = -0.5*np.log(det0) - 0.5*(x_pt - mu0).T.dot(sigma0_inv).dot(x_pt - mu0) + np.log(1-psi)
            g1 = -0.5*np.log(det1) - 0.5*(x_pt - mu1).T.dot(sigma1_inv).dot(x_pt - mu1) + np.log(psi)
            f[i,j] = g1 - g0
    # Plot the zero contour of f.
    contour = plt.contour(X1, X2, f, levels=[0], colors='m')
    plt.clabel(contour, fmt="Quadratic Boundary", colors='m')
    return contour
    
def scale_to_range(X, new_min=-3, new_max=3):
    """
    Scales each feature of X (column-wise) to the specified range [new_min, new_max].
    
    Parameters:
        X : numpy array of shape (m, n)
            The input data.
        new_min : float
            The desired minimum value of the scaled data.
        new_max : float
            The desired maximum value of the scaled data.
    
    Returns:
        X_scaled : numpy array of shape (m, n)
            The data scaled so that each column is in [new_min, new_max].
    """
    X_scaled = np.empty_like(X, dtype=float)
    for i in range(X.shape[1]):
        col = X[:, i]
        col_min = col.min()
        col_max = col.max()
        # Apply the affine transformation
        X_scaled[:, i] = new_min + (col - col_min) * (new_max - new_min) / (col_max - col_min)
    return X_scaled



# In[ ]:


if __name__ == "__main__":
    # Read the data from files (update the paths if needed)
    X_norm, Y, mean, std = read_data(r"C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q4\q4x.dat", r"C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q4\q4y.dat")
    # Note: X_norm is the normalized data.
    # We also want the original data for plotting in original units.
    X_orig = pd.read_csv(r"C:\Users\Abhinav Singh\Desktop\COL774_A1\Assignment1\data\Q4\q4x.dat", header=None, delim_whitespace=True).values
    
    # Create an instance of GaussianDiscriminantAnalysis.
    gda_model = GaussianDiscriminantAnalysis()
    # Store normalization parameters in the model.
    print("Y shape, X shape: ", Y.shape, X_orig.shape)
    # --- Part 1 and 2: Learn parameters assuming same covariance (linear boundary)
    params_linear = gda_model.fit(X_norm, Y, assume_same_covariance=True)
    mu0, mu1, sigma = params_linear
    print("Equal Covariance Case:")
    print("mu0:", mu0)
    print("mu1:", mu1)
    print("Sigma (common):", sigma)
    print(gda_model.predict(X_norm).shape)
    X_orig = scale_to_range(X_orig, new_min=-3, new_max=3)

    # --- Part 3: Plot data and linear decision boundary ---
    plot_data(X_orig, Y, mean, std, title="GDA: Data Points ")
    # plot_linear_boundary(mu0, mu1, sigma, gda_model.phi, mean, std)
    
    # --- Part 4: Learn parameters for separate covariances (quadratic boundary)
    params_quad = gda_model.fit(X_norm, Y, assume_same_covariance=False)
    mu0_q, mu1_q, sigma0, sigma1 = params_quad
    print("\nUnequal Covariance Case:")
    print("mu0:", mu0_q)
    print("mu1:", mu1_q)
    print("Sigma0:", sigma0)
    print("Sigma1:", sigma1)
    print(gda_model.predict(X_norm).shape)
    
    # # --- Part 5: Plot quadratic decision boundary on the same data ---
    # # (Keep the previous plot, then overlay quadratic boundary.)
    # plot_quadratic(mu0_q, mu1_q, sigma0, sigma1, gda_model.phi)
    
    plt.title("GDA: Decision Boundary (Linear and Quadratic Separator)")
    
    plt.show()
    



</PRE>
</PRE>
</BODY>
</HTML>
