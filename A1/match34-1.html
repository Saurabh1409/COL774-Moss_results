<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_1L6WT.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match34-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

from mpl_toolkits.mplot3d import Axes3D

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.cost_history = []  # Store cost history
        self.theta_history = []  # Store theta updates

    def compute_cost(self, X, y, theta):
</FONT>        m = len(y)
        predictions = X.dot(theta)
        errors = predictions - y
        return (1 / (2 * m)) * np.sum(errors ** 2)

<A NAME="5"></A><FONT color = #FF0000><A HREF="match34-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.01):
        n_iter = 10000
        tol = 1e-6
        m, n = X.shape
        X = np.c_[np.ones((m, 1)), X]  # Adding intercept term
</FONT>        self.theta = np.zeros(n + 1)
        self.cost_history = []  # Reset cost history
        self.theta_history = []  # Store theta updates

        for iteration in range(n_iter):
            predictions = X.dot(self.theta)
            errors = predictions - y
            gradient = (1 / m) * X.T.dot(errors)
            new_theta = self.theta - learning_rate * gradient

            self.theta_history.append(self.theta.copy())  # Store theta updates
            self.cost_history.append(self.compute_cost(X, y, self.theta))  # Store cost

            if np.linalg.norm(new_theta - self.theta, ord=1) &lt; tol:
                break

            self.theta = new_theta
        
        print('Iterations =&gt;',iteration)
        print('Parameter Theta 0 =&gt;',self.theta[0])
        print('Parameter Theta 1 =&gt;',self.theta[1])
        # print(self.cost_history)
        return self.theta_history

    def predict(self, X):
        X = np.c_[np.ones((X.shape[0], 1)), X]  # Adding intercept term
        return X.dot(self.theta)

    def plot_error_surface(self):
        theta_0_vals = np.linspace(self.theta[0] - 18, self.theta[0] + 18, 100)
        theta_1_vals = np.linspace(self.theta[1] - 25, self.theta[1] + 25, 100)
        theta_0_grid, theta_1_grid = np.meshgrid(theta_0_vals, theta_1_vals)

        J_vals = np.zeros_like(theta_0_grid)
        for i in range(theta_0_grid.shape[0]):
            for j in range(theta_0_grid.shape[1]):
<A NAME="12"></A><FONT color = #0000FF><A HREF="match34-0.html#12" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                theta_test = np.array([theta_0_grid[i, j], theta_1_grid[i, j]])
                J_vals[i, j] = self.compute_cost(np.c_[np.ones((len(X), 1)), X], y, theta_test)
</FONT>
        fig = plt.figure(figsize=(10, 7))
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match34-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta_0_grid, theta_1_grid, J_vals, cmap='viridis', alpha=0.7)
        ax.set_xlabel("Theta 0")
        ax.set_ylabel("Theta 1")
</FONT>        ax.set_zlabel("Cost Function J(Theta)")
        ax.set_title("3D Error Surface with Gradient Descent Path")

        theta_0_hist = [t[0] for t in self.theta_history]
        theta_1_hist = [t[1] for t in self.theta_history]
        cost_hist = self.cost_history

        for i in range(len(theta_0_hist)):
            ax.scatter(theta_0_hist[i], theta_1_hist[i], cost_hist[i], color='red', marker='o', s=5)
            plt.pause(0.2)  # Pause for visualization

        plt.show()

    def plot_data(self, X, y):
        plt.scatter(X, y, color='red', marker='x', label='Data Points')
        X_range = np.linspace(min(X), max(X), 100)
        y_pred = self.predict(X_range.reshape(-1, 1))
        plt.plot(X_range, y_pred, color='blue', label='Regression Line')
        plt.xlabel("Feature")
        plt.ylabel("Target")
        plt.legend()
        plt.show()

    def plot_contour(self, X, y):
        theta_0_vals = np.linspace(self.theta[0] - 18, self.theta[0] + 18, 100)
        theta_1_vals = np.linspace(self.theta[1] - 25, self.theta[1] + 25, 100)
        theta_0_grid, theta_1_grid = np.meshgrid(theta_0_vals, theta_1_vals)

        J_vals = np.zeros_like(theta_0_grid)
        m = len(y)
        X_bias = np.c_[np.ones((m, 1)), X]

        for i in range(theta_0_grid.shape[0]):
            for j in range(theta_0_grid.shape[1]):
                theta_test = np.array([theta_0_grid[i, j], theta_1_grid[i, j]])
                J_vals[i, j] = self.compute_cost(X_bias, y, theta_test)

        plt.figure(figsize=(8, 6))
        plt.contour(theta_0_grid, theta_1_grid, J_vals, levels=np.logspace(-2, 3, 20), cmap="viridis")
        plt.xlabel("Theta 0")
        plt.ylabel("Theta 1")
        plt.title("Gradient Descent on Contour Plot")

<A NAME="0"></A><FONT color = #FF0000><A HREF="match34-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_0_hist = [t[0] for t in self.theta_history]
        theta_1_hist = [t[1] for t in self.theta_history]

        for i in range(len(theta_0_hist)):
            plt.plot(theta_0_hist[:i+1], theta_1_hist[:i+1], 'ro-', markersize=2)
</FONT>            # plt.pause(0.2)  # Pause for visualization

        plt.show()

# Load feature and label data
X = np.loadtxt("linearX.csv", delimiter=",").reshape(-1, 1)
y = np.loadtxt("linearY.csv", delimiter=",")

# Create and fit the model
regressor = LinearRegressor()
regressor.fit(X, y, learning_rate=0.01)

# Predict and plot results
# regressor.plot_data(X, y)

# Plot the 3D error function with gradient descent path
# regressor.plot_error_surface()

# Plot the contour of the error function with the gradient descent path
regressor.plot_contour(X, y)



import numpy as np
import matplotlib.pyplot as plt

# Data Generation
def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(input_mean, input_sigma, size=(N, 2))
    X = np.hstack((np.ones((N, 1)), X))  # Add intercept term x0 = 1
    noise = np.random.normal(0, noise_sigma, N)
    y = X @ theta + noise
    return X, y

# Stochastic Gradient Descent
class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
    
    def fit(self, X, y, learning_rate=0.001, batch_size=1, tol=1e-6, max_epochs=20):
        m, n = X.shape
        self.theta = np.zeros(n)
        self.theta_history = []
        prev_theta = np.copy(self.theta)
        indices = np.arange(m)
        
        for epoch in range(max_epochs):
            print('Epoch =&gt;', epoch)  # Print statement re-added
            np.random.shuffle(indices)  # Shuffle data
            for i in range(0, m, batch_size):
                batch_indices = indices[i:i+batch_size]
                X_batch, y_batch = X[batch_indices], y[batch_indices]
                gradient = -X_batch.T @ (y_batch - X_batch @ self.theta) / batch_size
                self.theta -= learning_rate * gradient
                self.theta_history.append(np.copy(self.theta))  # Store theta at each step
            
            if np.linalg.norm(self.theta - prev_theta) &lt; tol:
                break
            prev_theta = np.copy(self.theta)
    
    def predict(self, X):
        return X @ self.theta

# Closed Form Solution
def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

# Main Execution
if __name__ == "__main__":
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2])
    noise_sigma = np.sqrt(2)
    
    # Generate Data
    X, y = generate(1_000_000, theta_true, input_mean, input_sigma, noise_sigma)
    print('Points Generated')  # Print statement re-added
    X_train, y_train = X[:800_000], y[:800_000]
    X_test, y_test = X[800_000:], y[800_000:]
    
    # Stochastic Gradient Descent
    batch_sizes = [1, 80, 8000, 800000]
    results = {}
    theta_histories = {}  # Store theta history for each batch size

    for batch_size in batch_sizes:
        model = StochasticLinearRegressor()
        model.fit(X_train, y_train, learning_rate=0.001, batch_size=batch_size)
        print('Fit Done')  # Print statement re-added
        results[batch_size] = model.theta
        theta_histories[batch_size] = np.array(model.theta_history)  # Store theta history
        print(f"Batch size {batch_size}: {model.theta}")  # Print statement re-added

    # Closed Form Solution
    theta_closed = closed_form_solution(X_train, y_train)
    print(f"Closed Form Solution: {theta_closed}")  # Print statement re-added

    # Compare Results
    print(f"True Parameters: {theta_true}")  # Print statement re-added
    for batch_size, theta_sgd in results.items():
        print(f"Batch size {batch_size}: {theta_sgd}")  # Print statement re-added
    
    # Error Calculation
    def mse(y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)
    
    for batch_size, theta_sgd in results.items():
        train_pred = X_train @ theta_sgd
        test_pred = X_test @ theta_sgd
        print(f"Batch size {batch_size}: Train Error = {mse(y_train, train_pred)}, Test Error = {mse(y_test, test_pred)}")  # Print statement re-added
    
    # Plot Trajectories
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    colors = ['r', 'g', 'b', 'c']
    
    for idx, batch_size in enumerate(batch_sizes):
        theta_history = theta_histories[batch_size]  # Use stored history for each batch size
<A NAME="1"></A><FONT color = #00FF00><A HREF="match34-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], color=colors[idx], label=f'Batch {batch_size}')
    
    ax.set_xlabel('$\theta_0$')
    ax.set_ylabel('$\theta_1$')
    ax.set_zlabel('$\theta_2$')
    ax.legend()
</FONT>    plt.show()




import numpy as np
import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match34-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.theta = None
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
</FONT>    
    def fit(self, X, y, tol=1e-6, max_iter=100):
        X = self.normalize(X)  # Normalize features
        X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term
        n_samples, n_features = X.shape
        
        self.theta = np.zeros(n_features)
        
        for _ in range(max_iter):
            h = self.sigmoid(X @ self.theta)
            gradient = X.T @ (h - y)  # Gradient of log-likelihood
            
            # Compute Hessian matrix
            R = np.diag(h * (1 - h))
            Hessian = X.T @ R @ X
            
            # Newton's update step
            delta = np.linalg.inv(Hessian) @ gradient
            self.theta -= delta
            
            # Convergence check
            if np.linalg.norm(delta) &lt; tol:
                break
        
        return self.theta
    
    def predict(self, X):
        X = self.normalize(X)
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)

# Load data
X = np.loadtxt("logisticX.csv", delimiter=",")
y = np.loadtxt("logisticY.csv", delimiter=",")

# Train model
model = LogisticRegressor()
theta = model.fit(X, y)
print("Learned coefficients:", theta)

# Plot decision boundary
def plot_decision_boundary(X, y, model):
    X_norm = model.normalize(X)
    plt.scatter(X[y == 0, 0], X[y == 0, 1], label="Class 0", marker="o")
    plt.scatter(X[y == 1, 0], X[y == 1, 1], label="Class 1", marker="x")
    
    # Extending decision boundary way beyond data points
    x1_vals = np.linspace(X_norm[:, 0].min() + 1, X_norm[:, 0].max() + 5, 100)
<A NAME="7"></A><FONT color = #0000FF><A HREF="match34-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2_vals = -(model.theta[0] + model.theta[1] * x1_vals) / model.theta[2]
    plt.plot(x1_vals, x2_vals, "r--", label="Decision Boundary", linewidth=2)
</FONT>    
    # Compute Hessian diagonal and classify points
    X_ext = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])
    h = model.sigmoid(X_ext @ model.theta)
    hessian_diag = h * (1 - h)  # Diagonal elements of Hessian matrix
    
    # Mark points where Hessian &gt; 0.5 (only for Class 1)
    plt.scatter(X[y == 1][hessian_diag[y == 1] &gt; 0.5, 0], 
                X[y == 1][hessian_diag[y == 1] &gt; 0.5, 1], 
                facecolors='none', edgecolors='green', linewidth=1.5, s=100, label="Hessian &gt; 0.5 (Class 1)")

    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.show()

plot_decision_boundary(X, y, model)




import numpy as np
import matplotlib.pyplot as plt

# Load Data
def load_data():
    X = np.loadtxt("q4x.dat")  # Load feature data (x1, x2)
    y = np.loadtxt("q4y.dat", dtype=str)  # Load labels ("Alaska" or "Canada")
    
    # Convert labels to binary (0 for Alaska, 1 for Canada)
    y = np.where(y == "Alaska", 0, 1)
    
    # Normalize features
    X_mean = X.mean(axis=0)
    X_std = X.std(axis=0)
    X = (X - X_mean) / X_std
    
    return X, y

# Compute GDA parameters for linear case
def compute_gda_linear(X, y):
    m = len(y)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match34-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    phi = np.mean(y)  # P(y=1)
    
    mu_0 = np.mean(X[y == 0], axis=0)
    mu_1 = np.mean(X[y == 1], axis=0)
</FONT>    
    # Compute shared covariance matrix
    Sigma = np.zeros((2, 2))
    for i in range(m):
        xi = X[i] - (mu_1 if y[i] == 1 else mu_0)
        Sigma += np.outer(xi, xi)
    Sigma /= m
    
    return phi, mu_0, mu_1, Sigma

# Plot training data
def plot_data(X, y):
<A NAME="11"></A><FONT color = #00FF00><A HREF="match34-0.html#11" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label="Alaska", alpha=0.7)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', label="Canada", alpha=0.7)
</FONT>    plt.xlabel("x1 (Freshwater growth ring diameter)")
    plt.ylabel("x2 (Marine water growth ring diameter)")
    plt.legend()

# Compute and plot linear decision boundary
def plot_linear_boundary(mu_0, mu_1, Sigma):
    Sigma_inv = np.linalg.inv(Sigma)
    w = Sigma_inv @ (mu_1 - mu_0)
    b = -0.5 * (mu_1.T @ Sigma_inv @ mu_1 - mu_0.T @ Sigma_inv @ mu_0)
    
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match34-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x_vals = np.linspace(-2, 2, 100)
    y_vals = -(w[0] * x_vals + b) / w[1]
    plt.plot(x_vals, y_vals, label="Linear Boundary", color='red')
</FONT>
# Compute GDA parameters for quadratic case
def compute_gda_quadratic(X, y):
    mu_0 = np.mean(X[y == 0], axis=0)
    mu_1 = np.mean(X[y == 1], axis=0)
    
<A NAME="6"></A><FONT color = #00FF00><A HREF="match34-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    Sigma_0 = np.cov(X[y == 0].T, bias=True)
    Sigma_1 = np.cov(X[y == 1].T, bias=True)
</FONT>    
    # print('Mean 0 =&gt;', mu_0)
    # print('Mean 1 =&gt;', mu_1)
    # print('Sigma 0 =&gt;', Sigma_0)
    # print('Sigma 1 =&gt;', Sigma_1)
    return mu_0, mu_1, Sigma_0, Sigma_1

# Compute and plot quadratic decision boundary
def plot_quadratic_boundary(mu_0, mu_1, Sigma_0, Sigma_1):
    Sigma_0_inv = np.linalg.inv(Sigma_0)
    Sigma_1_inv = np.linalg.inv(Sigma_1)
    
    A = Sigma_1_inv - Sigma_0_inv
    B = 2 * (mu_0.T @ Sigma_0_inv - mu_1.T @ Sigma_1_inv)
<A NAME="10"></A><FONT color = #FF0000><A HREF="match34-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    C = (mu_1.T @ Sigma_1_inv @ mu_1 - mu_0.T @ Sigma_0_inv @ mu_0)
    
    x_vals = np.linspace(-2, 2, 100)
    y_vals = np.linspace(-2, 2, 100)
    X_grid, Y_grid = np.meshgrid(x_vals, y_vals)
</FONT>    
    Z = (A[0, 0] * X_grid ** 2 + (A[0, 1] + A[1, 0]) * X_grid * Y_grid + A[1, 1] * Y_grid ** 2 
         + B[0] * X_grid + B[1] * Y_grid + C)
    
    plt.contour(X_grid, Y_grid, Z, levels=[0], colors='blue', label="Quadratic Boundary")

# Main function
def main():
    X, y = load_data()
    phi, mu_0, mu_1, Sigma = compute_gda_linear(X, y)
    
    print("Linear GDA Parameters:")
    print(f"Phi: {phi}")
    print(f"Mu_0: {mu_0}")
    print(f"Mu_1: {mu_1}")
    print(f"Shared Covariance Matrix: \n{Sigma}")
    
    plt.figure(figsize=(8, 6))
    plot_data(X, y)
    plot_linear_boundary(mu_0, mu_1, Sigma)
    plt.title("GDA Linear Decision Boundary")
    plt.show()
    
    mu_0, mu_1, Sigma_0, Sigma_1 = compute_gda_quadratic(X, y)
    
    print("Quadratic GDA Parameters:")
    print(f"Mu_0: {mu_0}")
    print(f"Mu_1: {mu_1}")
    print(f"Sigma_0: \n{Sigma_0}")
    print(f"Sigma_1: \n{Sigma_1}")
    
    plt.figure(figsize=(8, 6))
    plot_data(X, y)
    plot_linear_boundary(mu_0, mu_1, Sigma)
    plot_quadratic_boundary(mu_0, mu_1, Sigma_0, Sigma_1)
    plt.title("GDA Linear vs. Quadratic Decision Boundaries")
    plt.show()

if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
