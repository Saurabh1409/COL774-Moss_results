<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_0VNUY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_0VNUY.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# ***LINEAR REGRESSION***

# IMPORTS

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.animation as animation


# LINEAR REGRESSOR CLASS

# In[2]:


class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = None
        self.cost_history = None
        
    def compute_cost(self, X, y, theta):
        m = len(y)
        predictions = X.dot(theta)
        return 1/(2*m) * np.sum(np.square(predictions - y.flatten()))
    
    def compute_gradient(self, X, y, theta):
        m = len(y)
        predictions = X.dot(theta)
        return 1/m * X.T.dot(predictions - y.flatten())
    
    def fit(self, X, y, learning_rate=0.1, max_iterations=100000):
        """
        Fit the linear regression model using batch gradient descent.
        
        Parameters
        ----------
        X : numpy array
            Input features
        y : numpy array 
            Target values
        learning_rate : float
            Learning rate for gradient descent
        max_iterations : int
            Maximum number of iterations
        """
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
            
        X = np.c_[np.ones(X.shape[0]), X]
        
        threshold = 1e-8  # Stopping criteria
        self.theta = np.zeros(X.shape[1])
        self.theta_history = [self.theta.copy()]
        self.cost_history = [self.compute_cost(X, y, self.theta)]
        
        iteration = 0
        prev_cost = float('inf')
        
        while iteration &lt; max_iterations:
            self.theta = self.theta - learning_rate * self.compute_gradient(X, y, self.theta)
            self.theta_history.append(self.theta.copy())
            
            current_cost = self.compute_cost(X, y, self.theta)
            self.cost_history.append(current_cost)
            
            # Check convergence
            if abs(prev_cost - current_cost) &lt; threshold:
                break
                
            prev_cost = current_cost
            iteration += 1
        print(f"learning rate: {learning_rate}")
        print(f"Final parameters: theta_0 = {self.theta[0]:.4f}, theta_1 = {self.theta[1]:.4f}")
        print(f"Stopping criteria met after {iteration} iterations")
        print(f"Final cost: {current_cost:.6f}")
        
        return np.array(self.theta_history)

    def predict(self, X):
        if self.theta is None:
            raise Exception("Model not trained yet.")
        
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
            
        X = np.c_[np.ones(X.shape[0]), X]
        return X.dot(self.theta)


# PLOT DATA AND HYPOTHESIS

# In[3]:


def plot_data_hypothesis(X, y, y_pred, lr):
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color='blue', label='Data points', s=15)
    plt.plot(X, y_pred, color='red', label='Hypothesis')
    plt.xlabel('Wine Acidity')
    plt.ylabel('Wine Density')
    plt.title(f'Linear Regression - Learning Rate: {lr}')
    plt.legend()
    plt.savefig(f'data_hypothesis_{lr}.png')
    plt.show()


# PLOT LOSS VS ITERATIONS

# In[4]:


def plot_loss_vs_iterations(X, y, lr):
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(model.cost_history)), model.cost_history, 'b-', linewidth=2)
    plt.xlabel("Iterations")
    plt.ylabel("Cost Function Value")
    plt.title(f"Loss vs Iterations - Learning Rate: {lr}")
    plt.savefig(f'loss_vs_iterations_{lr}.png')
    plt.show()
    


# PLOT 3D SURFACE

# In[5]:


def plot_3d_surface(X, y, lr):
    theta_history = np.array(model.theta_history)
    cost_history = np.array(model.cost_history)

    theta_0_vals = np.linspace(-10, 20, 100)
    theta_1_vals = np.linspace(0, 60, 100)
    T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

    Z = np.zeros(T0.shape)
    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match100-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            theta_test = np.array([T0[i, j], T1[i, j]])
            Z[i, j] = model.compute_cost(np.c_[np.ones(X.shape[0]), X], y, theta_test)
</FONT>
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(T0, T1, Z, cmap='viridis', edgecolor='none', alpha=0.6)

    # Plot the final trajectory (red line)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match100-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax.plot(theta_history[:, 0], theta_history[:, 1], cost_history, 'r.-', markersize=5, label="Gradient Descent Path")
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Cost Function J(θ)')
    ax.set_title(f' Surface Plot - Learning Rate: {lr}')
    ax.legend()
    
    plt.savefig(f'3d_surface_{lr}.png')
</FONT>    plt.show()
    


# PLOT ANIMATION OF THETA MOVEMENT ON 3D SURFACE

# In[6]:


def plot_3d_surface_anim(X, y, lr):
    theta_history = np.array(model.theta_history)
    cost_history = np.array(model.cost_history)

    theta_0_vals = np.linspace(-10, 20, 100) 
    theta_1_vals = np.linspace(0, 60, 100)
    T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
    
    Z = np.zeros(T0.shape)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta_test = np.array([T0[i, j], T1[i, j]]) 
            Z[i, j] = model.compute_cost(np.c_[np.ones(X.shape[0]), X], y, theta_test)

    fig1 = plt.figure(figsize=(12, 8))
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match100-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax1 = fig1.add_subplot(111, projection='3d')
    ax1.plot_surface(T0, T1, Z, cmap='viridis', edgecolor='none', alpha=0.6)

    ax1.set_xlabel('Theta 0')
    ax1.set_ylabel('Theta 1')
</FONT>    ax1.set_zlabel('Cost Function J(θ)')
    ax1.set_title(f'Surface Plot of Cost Function with Gradient Descent Path - Learning rate : {lr}')

    line1, = ax1.plot([], [], [], 'r.-', markersize=5)

    def animate_3d(i):
        line1.set_data(theta_history[:i+1, 0], theta_history[:i+1, 1])
        line1.set_3d_properties(cost_history[:i+1])
        return line1,

    ani3d = animation.FuncAnimation(fig1, animate_3d, frames=len(theta_history), interval=200, blit=True)
    
    plt.show()  
   


# PLOT CONTOUR

# In[7]:


def plot_contour(X, y, lr):
    theta_history = np.array(model.theta_history)

    theta_0_vals = np.linspace(-10, 20, 100)
    theta_1_vals = np.linspace(-5, 40, 100)
    T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
    Z = np.zeros(T0.shape)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta_test = np.array([T0[i, j], T1[i, j]])
            Z[i, j] = model.compute_cost(np.c_[np.ones(X.shape[0]), X], y, theta_test)

<A NAME="1"></A><FONT color = #00FF00><A HREF="match100-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.contour(T0, T1, Z, levels=np.logspace(-3, 4, 30), cmap='coolwarm')

    # Plot final trajectory (red path)
    ax.plot(theta_history[:, 0], theta_history[:, 1], 'r.-', markersize=5, label="Gradient Descent Path")
</FONT>
    ax.set_xlabel("Theta_0")
    ax.set_ylabel("Theta_1")
    ax.set_title(f"Contour Plot - Learning Rate: {lr}")
    ax.legend()
    
    plt.savefig(f'contour_{lr}.png')
    plt.show()


# PLOT ANIMATION OF THETA MOVEMENT ON CONTOUR

# In[8]:


def plot_contour_anim(X, y, lr):
    fig, ax = plt.subplots(figsize=(10, 6))
    
    theta_0_vals = np.linspace(-10, 20, 100) 
    theta_1_vals = np.linspace(-5, 40, 100)
    T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)  
    Z = np.zeros(T0.shape)

    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta_test = np.array([T0[i, j], T1[i, j]])  
            Z[i, j] = model.compute_cost(np.c_[np.ones(X.shape[0]), X], y, theta_test)

    ax.contour(T0, T1, Z, levels=np.logspace(-3, 4, 30), cmap='coolwarm')
    ax.set_xlabel("Theta_0")
    ax.set_ylabel("Theta_1")
    ax.set_title(f"Cost Function Contour Plot - learning rate : {lr}")

    def animate(i):
        ax.scatter(model.theta_history[i][0], model.theta_history[i][1], color='red')

    contour_ani = animation.FuncAnimation(fig, animate, frames=min(len(model.theta_history), 300), interval=200)
    
    plt.show()


# MAIN FUNCTION

# In[9]:


if __name__ == "__main__":
    # Load data
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values
    
    learning_rates = [0.0001, 0.001, 0.025, 0.05, 0.1, 0.5, 1, 2]
    
    for lr in learning_rates:
        
        model = LinearRegressor()
        model.fit(X, y, learning_rate=lr)
        y_pred = model.predict(X)
        plot_data_hypothesis(X, y,y_pred, lr)
        plot_loss_vs_iterations(X, y, lr)
        plot_3d_surface(X, y, lr)
        plot_contour(X, y, lr)
        plot_3d_surface_anim(X, y, lr)
        plot_contour_anim(X, y, lr)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import time
import matplotlib.pyplot as plt


# In[2]:


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    """
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*X[:, 0] + theta[2]*X[:, 1] + noise
    return X, y


# In[3]:


class StochasticLinearRegressor:
    def __init__(self):
        self.thetas = []
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Stochastic Gradient Descent.
        Returns a list of parameter histories for different batch sizes.
        """
        batch_sizes = [1, 80, 8000, 800000]  
        stop_threshold = 1e-6
        X = np.c_[np.ones(X.shape[0]), X]
        y = y.reshape(-1, 1)
        
        theta_histories = []
        
        for batch_size in batch_sizes:
            print(f"Training with batch size: {batch_size}")
            theta = np.zeros((X.shape[1], 1))
            data = np.hstack((y, X))
            np.random.shuffle(data)

            num_samples, dim = data.shape
            num_batches = num_samples // batch_size
            data_batched = data.reshape((num_batches, batch_size, dim))

            loss_history = []
            epochs = 0
            theta_history = [theta.flatten()]
            window_size = 10
            
            if batch_size == 800000:
                window_size = 1
            
            t = time.time()
            while True:
                total_loss = 0
                for batch in data_batched:
                    Y_batch = batch[:, 0].reshape(-1, 1)
                    X_batch = batch[:, 1:]

                    gradient = (X_batch.T @ (X_batch @ theta - Y_batch)) / (2 * X_batch.shape[0])
                    theta -= learning_rate * gradient

                    total_loss += np.linalg.norm(Y_batch - X_batch @ theta) ** 2 / (2 * X_batch.shape[0])
                    theta_history.append(theta.flatten())

                loss_history.append(total_loss / num_batches)
                epochs += 1
                
                if len(loss_history) &gt;= 2 * window_size:
                    avg_old = np.mean(loss_history[-2 * window_size : -window_size])
                    avg_new = np.mean(loss_history[-window_size :])
                    if abs(avg_old - avg_new) &lt; stop_threshold:
                        break

            print(f"Batch {batch_size} converged in {epochs} epochs, time taken {time.time() - t}")
            self.thetas.append(theta)
            print(f'theta {theta} ')
            diff = np.linalg.norm(theta - np.array([3, 1, 2]).reshape(-1, 1))
            print(f'diff {diff}')
            theta_histories.append(np.array(theta_history))

        return theta_histories
                    
    def predict(self, X):
        """
        Predict target values for the input data using all trained models.
        Returns a list of numpy arrays where each element corresponds to a different batch size.
        """
        X = np.c_[np.ones(X.shape[0]), X]
        return [(X @ theta).flatten() for theta in self.thetas]


# In[4]:


def plot_data_hypothesis(theta_histories, batch_sizes):
   
    for i, theta_history in enumerate(theta_histories):
        theta_history = np.array(theta_history)  
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], 
                marker='o', alpha=0.7, label=f'Batch Size: {batch_sizes[i]}', markersize=2, color='blue')
        
        ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2], 
                   color='red', label='Start (Initial Theta)', s=10)
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], 
                   color='red', label='End (Converged Theta)', s=100, marker='x')
        
        ax.set_xlabel('Theta 0 (Bias)')
        ax.set_ylabel('Theta 1 (Coefficient 1)')
        ax.set_zlabel('Theta 2 (Coefficient 2)')
        ax.set_title(f'Movement of Theta (Batch {batch_sizes[i]})')
        ax.legend()
        plt.savefig(f'theta_mov_{batch_sizes[i]}.png')
        plt.show()


# In[7]:


def closed_form_solution(X, y):
    n_samples = X.shape[0]
    X_with_intercept = np.column_stack([np.ones(n_samples), X])
    
    Xt = X_with_intercept.T
    theta = np.linalg.inv(Xt @ X_with_intercept) @ Xt @ y
    return theta


# In[11]:


if __name__ == '__main__':
   
    N = 1000000  
    theta_true = np.array([3, 1, 2]) 
    input_mean = np.array([3, -1])
    input_sigma = np.array([4, 4])
    noise_sigma = np.sqrt(2)  
    
<A NAME="2"></A><FONT color = #0000FF><A HREF="match100-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
    
    train_size = int(0.8 * N)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
</FONT>    
    learning_rate = 0.001
    
    model = StochasticLinearRegressor()
    theta_histories = model.fit(X_train, y_train, learning_rate)
    
    y_preds = model.predict(X_test)
    y_preds_train = model.predict(X_train)
    
    batch_sizes = [1, 80, 8000, 800000]
    
    for i, batch in enumerate(batch_sizes):
        mse_test = np.mean((y_test - y_preds[i]) ** 2)
        mse_train = np.mean((y_train - y_preds_train[i]) ** 2)
        print(f"Batch {batch}: Train MSE = {mse_train}, Test MSE = {mse_test}")
        diff = np.linalg.norm(model.thetas[i] - np.array([3, 1, 2]).reshape(-1, 1))
        print(f'diff between train and test mse {diff}')
    
    plot_data_hypothesis(theta_histories, batch_sizes)
    
    closed_theta = closed_form_solution(X_train, y_train).reshape(-1,1)
    diff = np.linalg.norm(closed_theta - np.array([3, 1, 2]).reshape(-1, 1))
    print(f'Closed form solution: {closed_theta}')
    print(f'diff {diff}')


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# **LOGISTIC REGRESSION - ANALYSIS FILE**

# IMPORTS

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# LOGISTIC REGRESSOR CLASS

# In[8]:


class LogisticRegressor:
    def __init__(self):
        self.theta = None
        
    def normalize(self, X):
        return (X-np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def add_intercept(self, X):
        return np.column_stack([np.ones(X.shape[0]), X])
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match100-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = self.normalize(X)
        
        X = self.add_intercept(X)
</FONT>        
        self.theta = np.zeros(X.shape[1])
        
        n_iter = 10
        theta_history = []
        tolerance = 1e-7
        
        for _ in range(n_iter):
            z = np.dot(X, self.theta)
            h = self.sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / y.size
            H = np.dot(X.T, X * (h * (1 - h)).reshape(-1, 1)) / y.size
            prev_loss = float('inf')
            try:
                theta_update = np.linalg.pinv(H).dot(gradient)
                self.theta -= theta_update
                theta_history.append(self.theta.copy())
                loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
                if abs(prev_loss - loss) &lt; tolerance:
                    break
                prev_loss = loss
            except np.linalg.LinAlgError:
                print("Hessian is not invertible. Stopping optimization.")
                break
            
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        
        X = self.add_intercept(X)
        
        prob = self.sigmoid(np.dot(X, self.theta))
        
        return (prob &gt;= 0.5).astype(int)


# PLOT DECISION BOUNDARY

# In[9]:


def plot_decision_boundary(X, y, model):
    # Normalize X for plotting
    X_norm = model.normalize(X)
    
    x_min, x_max = X_norm[:, 0].min(), X_norm[:, 0].max()
    x_points = np.array([x_min, x_max])
<A NAME="7"></A><FONT color = #0000FF><A HREF="match100-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y_points = -(model.theta[0] + model.theta[1] * x_points) / model.theta[2]
    
    plt.plot(x_points, y_points, 'purple')
    plt.scatter(X_norm[y == 0][:, 0], X_norm[y == 0][:, 1], c='blue', label='Class 0',s=15, marker='x')
</FONT>    plt.scatter(X_norm[y == 1][:, 0], X_norm[y == 1][:, 1], c='red', label='Class 1', s=15, marker='o')
    plt.xlabel('x₁')
    plt.ylabel('x₂')
    plt.legend()
    plt.savefig('data.png')
    


# MAIN

# In[10]:


if __name__== "__main__":
    
    X = pd.read_csv('../data/Q3/logisticX.csv', header=None).values
    y = pd.read_csv('../data/Q3/logisticY.csv', header=None).values.ravel()
    
    model = LogisticRegressor()
    theta_history = model.fit(X, y)
    
    final_coefficients = model.theta
    
    print("Final coefficients (including intercept):", final_coefficients)
    
    plot_decision_boundary(X, y, model)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# ***GAUSSIAN DISCRIMINANT ANALYSIS***

# IMPORTS

# In[10]:


import numpy as np
import matplotlib.pyplot as plt


# In[ ]:


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
            
    def normalize(self, X):
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = self.normalize(X)
        
        m = y.shape[0]
        
        indicator_0 = 1*(y == 0).reshape(-1, 1)
        indicator_1 = 1*(y == 1).reshape(-1, 1)
        
        self.phi = np.sum(indicator_1) / m
        
        self.mu_0 = np.sum(X * indicator_0, axis=0) / np.sum(indicator_0)
        self.mu_1 = np.sum(X * indicator_1, axis=0) / np.sum(indicator_1)
        
        if assume_same_covariance:
            mu = indicator_0*self.mu_0 + indicator_1*self.mu_1
            self.sigma = np.dot((X - mu).T, (X - mu)) / m
            
            print(f'mu_0: {self.mu_0}')
            print(f'mu_1: {self.mu_1}')
            print(f'sigma: {self.sigma}')
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.dot((X - self.mu_0).T, indicator_0* (X - self.mu_0)) / np.sum(indicator_0)
            self.sigma_1 = np.dot((X - self.mu_1).T, indicator_1* (X - self.mu_1)) / np.sum(indicator_1)
            print(f'mu_0: {self.mu_0}')
            print(f'mu_1: {self.mu_1}')
            print(f'sigma_0: {self.sigma_0}')
            print(f'sigma_1: {self.sigma_1}')
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        
        if self.sigma is not None:
            sigma_inv = np.linalg.pinv(self.sigma)
            coef = -np.dot((self.mu_1 - self.mu_0).T, sigma_inv)
            linear_term = np.dot(X, coef)
            
            c_1 = np.dot(np.dot(self.mu_1.T, sigma_inv), self.mu_1)/2
            c_2 = np.dot(np.dot(self.mu_0.T, sigma_inv), self.mu_0)/2
            c_3 = np.log((1 - self.phi)/self.phi)
            constant_term = (c_1 - c_2) + c_3
            
            decision_values = linear_term + constant_term
            
        else:
            sigma_0_inv = np.linalg.pinv(self.sigma_0)
            sigma_1_inv = np.linalg.pinv(self.sigma_1)
            
            quad_term = np.array([0.5 * np.dot(np.dot(x, sigma_1_inv - sigma_0_inv), x) for x in X])
            
            coef = -(np.dot(self.mu_1, sigma_1_inv) - np.dot(self.mu_0, sigma_0_inv))
            linear_term = np.dot(X, coef)
            
            c_1 = np.dot(np.dot(self.mu_1.T, sigma_1_inv), self.mu_1)/2
            c_2 = np.dot(np.dot(self.mu_0.T, sigma_0_inv), self.mu_0)/2
            c_3 = np.log((1 - self.phi)/self.phi)
            c_4 = 0.5 * np.log(np.linalg.det(self.sigma_1)/np.linalg.det(self.sigma_0))
            constant_term = (c_1 - c_2) + c_3 + c_4
            
            decision_values = quad_term + linear_term + constant_term
            
        return (decision_values &lt; 0).astype(int)


# In[12]:


def plot_data_points(X, y, title="Training Data"):
    plt.figure(figsize=(10, 6))
    
    mask_canada = (y == 1)
    plt.scatter(X[mask_canada, 0], X[mask_canada, 1], 
               marker='^', c='g', label='Canada', s=100)
    
    mask_alaska = (y == 0)
    plt.scatter(X[mask_alaska, 0], X[mask_alaska, 1], 
               marker='x', c='r', label='Alaska', s=100)
    
    plt.xlabel('Normalized growth ring diameters in fresh water')
    plt.ylabel('Normalized growth ring diameters in marine water')
    plt.title(title)
    plt.legend()
    plt.savefig(f'{title}.png')


# In[ ]:


def plot_decision_boundary(gda, X, y, boundary_type="both"):
    
<A NAME="0"></A><FONT color = #FF0000><A HREF="match100-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005),
</FONT>                        np.arange(y_min, y_max, 0.005))
    
    plot_data_points(X, y, title=f"Decision boundary {boundary_type}")
    
    if boundary_type in ["linear", "both"]:
        
        gda_linear = GaussianDiscriminantAnalysis()
        gda_linear.fit(X, y, assume_same_covariance=True)
        
        Z = gda_linear.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        if boundary_type=="linear":
            plt.contourf(xx, yy, Z, alpha=0.3, levels=[-0.5, 0.5, 1.5],
                        colors=['lightgreen', 'lightcoral'])
        
        plt.contour(xx, yy, Z, colors='blue', levels=[0.5])
    
    if boundary_type in ["quadratic", "both"]:
        
        gda_quad = GaussianDiscriminantAnalysis()
        gda_quad.fit(X, y, assume_same_covariance=False)
        
        Z = gda_quad.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        if boundary_type=="quadratic":
            plt.contourf(xx, yy, Z, alpha=0.3, levels=[-0.5, 0.5, 1.5],
                        colors=['lightgreen', 'lightcoral'])
        
        plt.contour(xx, yy, Z, colors='red', levels=[0.5])
    
    plt.legend()
    plt.savefig(f'decision_boundary_{boundary_type}.png')


# In[16]:


def main():
    X = np.loadtxt('../data/Q4/q4x.dat')
    y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
    y = np.where(y == "Alaska", 0, 1)
    
    gda = GaussianDiscriminantAnalysis()
    
    X = gda.normalize(X)
    
    plot_data_points(X, y, 'Training data')
    
    plot_decision_boundary(gda, X, y, boundary_type="linear")
    plot_decision_boundary(gda, X, y, boundary_type="quadratic")
    plot_decision_boundary(gda, X, y, boundary_type="both")

if __name__ == "__main__":
    main()


# In[ ]:






</PRE>
</PRE>
</BODY>
</HTML>
