<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_0Y80D.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_0Y80D.py<p><PRE>



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import argparse 
import sys



def calculate_loss(y, theta, x):
    
    h_x = x @ theta  # Shape (999,)
    y = y.reshape(-1,1)
    
    loss = np.sum(((y - h_x) ** 2)) / (2 * len(y))
    
    return loss

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match239-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def calculate_rev_gradient(y, theta, x):
    
    h_x = x @ theta  
    h_x = h_x.reshape((-1, 1)) 
    y = y.reshape(-1, 1) 
    
    gradient_matrix = (x*(y - h_x))
</FONT>    
    gradient_matrix = np.sum(gradient_matrix,axis = 0)
    
    gradient_matrix = gradient_matrix.reshape(-1,1)
    gradient_matrix = gradient_matrix/len(y)
    
    
    
    return gradient_matrix  

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
        pass

    
    def fit(self, X, y, learning_rate=0.1):
        
        
        n_iter = 1000
        m = len(y)
        x = np.c_[np.ones(m), X]
        num_features = x.shape[1]
        self.theta = np.zeros((num_features, 1))
        loss = 1e10
        diff_loss =  1e10
        iters = 0
        best_loss = 1e10
        best_theta = self.theta
        while diff_loss &gt; 1e-7 and diff_loss&gt;0:
            #print(self.theta)
            self.theta_history.append(self.theta)
            temp_theta = self.theta
            self.theta = self.theta + learning_rate * calculate_rev_gradient(y, self.theta, x)
            ini_loss = loss
            loss = calculate_loss(y, self.theta, x)
            if loss &lt; best_loss:
                best_loss = loss
                best_theta = self.theta
            diff_loss = ini_loss - loss
            #print("diff ",diff_loss)
            iters+=1

        self.theta = best_theta
        # print("total number of iterations are",iters)
        # print("final theta is",self.theta)
        # print("final loss is",loss)
        # print("diff_loss is",diff_loss) 
        fit_return = np.array(self.theta_history)
        reshape_val = fit_return.shape[1]
        fit_return = fit_return.reshape(-1,reshape_val)
        return fit_return
        
    
    def predict(self, X):
        
        m = len(X)
        x = np.c_[np.ones(m), X]
        y_pred = x @ self.theta
        y_pred = y_pred.reshape(-1,)
        
        return y_pred



#!/usr/bin/env python
# coding: utf-8

# In[23]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import argparse 
import sys


# In[24]:


def calculate_loss(y, theta, x):
    
    h_x = x @ theta  
    y = y.reshape(-1,1)
    
    loss = np.sum(((y - h_x) ** 2)) / (2 * len(y))
    
    return loss

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match239-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def calculate_rev_gradient(y, theta, x):
    
    h_x = x @ theta  
    h_x = h_x.reshape((-1, 1)) 
    y = y.reshape(-1, 1) 
    
    gradient_matrix = (x*(y - h_x))
</FONT>    
    gradient_matrix = np.sum(gradient_matrix,axis = 0)
    
    gradient_matrix = gradient_matrix.reshape(-1,1)
    gradient_matrix = gradient_matrix/len(y)
    
    
    
    return gradient_matrix  

    


# In[45]:


def load_data(path):
    try:
        data = pd.read_csv(path)
    except:
        print("File not found on the path or not a csv file")
        sys.exit(1)
    return data

X = load_data("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q1/linearX.csv")
y = load_data("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q1/linearY.csv")

X = X.to_numpy()
y = y.to_numpy()


# In[47]:


class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []
        pass

    
    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        n_iter = 1000
        m = len(y)
        x = np.c_[np.ones(m), X]
        num_features = x.shape[1]
        self.theta = np.zeros((num_features, 1))
        loss = 1e10
        diff_loss =  1e10
        iters = 0
        best_loss = 1e10
        best_theta = self.theta
        while diff_loss &gt; 1e-7 and diff_loss&gt;0:
            #print(self.theta)
            self.theta_history.append(self.theta)
            temp_theta = self.theta
            self.theta = self.theta + learning_rate * calculate_rev_gradient(y, self.theta, x)
            ini_loss = loss
            loss = calculate_loss(y, self.theta, x)
            if loss &lt; best_loss:
                best_loss = loss
                best_theta = self.theta
            diff_loss = ini_loss - loss
            #print("diff ",diff_loss)
            iters+=1

        self.theta = best_theta
        # print("total number of iterations are",iters)
        # print("final theta is",self.theta)
        # print("final loss is",loss)
        # print("diff_loss is",diff_loss) 
        fit_return = np.array(self.theta_history)
        reshape_val = fit_return.shape[1]
        fit_return = fit_return.reshape(-1,reshape_val)
        return fit_return
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = len(X)
        x = np.c_[np.ones(m), X]
        y_pred = x @ self.theta
        y_pred = y_pred.reshape(-1,)
        
        return y_pred


# In[48]:


lr = LinearRegressor()
th = lr.fit(X, y,0.1)
print(th.shape)



# In[49]:


pr = lr.predict(X)
print(pr.shape)


# In[18]:


plt.scatter(X, y, color = 'yellow')
plt.plot(X, lr.predict(X), color = 'red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression (hypothesis function and data)')
plt.plot([], [], color='red', label='Hypothesis Function')
plt.plot([], [], color='yellow', label='Data')
plt.legend()
plt.show()



# In[19]:


theta0 = np.linspace(-20, 30, 100)
theta1 = np.linspace(0, 60, 100)
theta0, theta1 = np.meshgrid(theta0, theta1)
J = np.zeros_like(theta0)
for i in range(100):
    for j in range(100):
        J[i, j] = calculate_loss(y, np.array([[theta0[i, j]], [theta1[i, j]]]), np.c_[np.ones(len(y)), X])
<A NAME="0"></A><FONT color = #FF0000><A HREF="match239-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(theta0, theta1, J, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
ax.set_zlabel('J(theta)')
plt.title('Error Function J(theta) vs theta0 and theta1')
</FONT>plt.show()



# In[20]:


theta0 = np.linspace(0, 10, 100)
theta1 = np.linspace(0, 40, 100)
theta0, theta1 = np.meshgrid(theta0, theta1)
X1 = np.c_[np.ones(len(y)), X]
J = np.zeros_like(theta0)
for i in range(100):
    for j in range(100):
        J[i, j] = calculate_loss(y, np.array([[theta0[i, j]], [theta1[i, j]]]), np.c_[np.ones(len(y)), X])
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')  
ax.view_init(elev=30, azim=180)
ax.plot_surface(theta0, theta1, J, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
ax.set_zlabel('J(theta)')
plt.title('Error Function J(theta) vs theta0 and theta1')

for i in range(len(lr.theta_history)):
    ax.scatter(lr.theta_history[i][0], lr.theta_history[i][1], calculate_loss(y, lr.theta_history[i],X1), color='r')
    
    plt.pause(0.2)
plt.show()


# In[21]:


theta0 = np.linspace(-10, 10, 100)
theta1 = np.linspace(0, 50, 100)
theta0, theta1 = np.meshgrid(theta0, theta1)
X1 = np.c_[np.ones(len(y)), X]
J = np.zeros_like(theta0)
for i in range(100):
    for j in range(100):
        J[i, j] = calculate_loss(y, np.array([[theta0[i, j]], [theta1[i, j]]]), np.c_[np.ones(len(y)), X])
fig = plt.figure()
ax = fig.add_subplot(111)
ax.contour(theta0, theta1, J,levels = 50, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
plt.title('Error Function J(theta) vs theta0 and theta1')

for i in range(len(lr.theta_history)):
    ax.scatter(lr.theta_history[i][0], lr.theta_history[i][1], color='r')
    plt.pause(0.2)

plt.show()


# In[ ]:


lr001 = LinearRegressor()
lr001.fit(X, y,0.001)

theta0 = np.linspace(-10, 10, 100)
theta1 = np.linspace(0, 50, 100)
theta0, theta1 = np.meshgrid(theta0, theta1)
X1 = np.c_[np.ones(len(y)), X]
J = np.zeros_like(theta0)
for i in range(100):
    for j in range(100):
        J[i, j] = calculate_loss(y, np.array([[theta0[i, j]], [theta1[i, j]]]), np.c_[np.ones(len(y)), X])
fig = plt.figure()
ax = fig.add_subplot(111)
ax.contour(theta0, theta1, J,levels = 50, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
plt.title('Error Function J(theta) vs theta0 and theta1 for learning rate 0.001')

for i in range(len(lr001.theta_history)):
    ax.scatter(lr001.theta_history[i][0], lr001.theta_history[i][1], color='r')
    plt.pause(0.2)

plt.show()


# In[ ]:


lr025 = LinearRegressor()
lr025.fit(X, y,0.025)

theta0 = np.linspace(-10, 10, 100)
theta1 = np.linspace(0, 50, 100)
theta0, theta1 = np.meshgrid(theta0, theta1)
X1 = np.c_[np.ones(len(y)), X]
J = np.zeros_like(theta0)
for i in range(100):
    for j in range(100):
        J[i, j] = calculate_loss(y, np.array([[theta0[i, j]], [theta1[i, j]]]), np.c_[np.ones(len(y)), X])

fig = plt.figure()
ax = fig.add_subplot(111)
ax.contour(theta0, theta1, J,levels = 50, cmap='viridis')
ax.set_xlabel('theta0')
ax.set_ylabel('theta1')
plt.title('Error Function J(theta) vs theta0 and theta1 for learning rate 0.025')

for i in range(len(lr025.theta_history)):
    ax.scatter(lr025.theta_history[i][0], lr025.theta_history[i][1], color='r')
    plt.pause(0.2)  
plt.show()






#!/usr/bin/env python
# coding: utf-8

# In[6]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[23]:


def calculate_loss(y, theta, x):
    
    h_x = x @ theta  # Shape (999,)
    y = y.reshape(-1,1)
    # print("h_x shape ",h_x.shape)
    # print("y shape ",y.shape)
    # print("theta shape ",theta.shape)   
    # print("x shape ",x.shape)
    loss = np.sum(((y - h_x) ** 2)) / (2 * len(y))
    #print(loss)
    return loss

def calculate_rev_gradient(y, theta, x):
    
    h_x = x @ theta  
    h_x = h_x.reshape((-1, 1)) 
    y = y.reshape(-1, 1) 
    # print("h_x shape ",h_x.shape)
    # print("y shape ",y.shape)
    # print("theta shape ",theta.shape)   
    # print("x shape ",x.shape)
    gradient_matrix = (x*(y - h_x))
    
    gradient_matrix = np.sum(gradient_matrix,axis = 0)
    
    gradient_matrix = gradient_matrix.reshape(-1,1)
    gradient_matrix = gradient_matrix/len(y)
    
    
    
    return gradient_matrix  

def custom_threshold(batch_size):
    if batch_size &lt;5:
        return 1e-3
    elif batch_size &lt;100:
        return 1e-6
    elif batch_size&lt;10000:
        return 1e-11
    else:
        return 1e-20




    


# In[37]:


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.

    """


    # Generate feature matrix X with shape (N, 2)
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))

    # Compute the linear transformation: X * theta[1:] + theta[0]
    linear_output = np.dot(X, theta[1:]) + theta[0]

    # Add Gaussian noise to the output
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute the final target variable y
    y = linear_output + noise

    return X, y
    




class StochasticLinearRegressor_custom_batch_size:
    def __init__(self,batch_size1):
        
        self.theta = np.array([[0], [0], [0]])
        self.theta_history = [self.theta]
        self.loss_history = []
        self.batch_size = batch_size1
        self.theta_diff_norm = 1e10
        self.loss_diff = 1e10
        self.gradient_diff_norm = 1e10
        self.gradient_diff_norm_diff = 1e10
        self.variance_last_10_thetas = 1e10
        self.loss_variance_last_10_thetas = 1e10
        self.best_theta = self.theta
        self.best_loss = 1e10
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.loss_history.append(calculate_loss(y,self.theta,X))
        np.random.seed(49)
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]

        m = len(y)
        num_batches = int(m/self.batch_size)
        x_batches = np.array_split(X,num_batches)
        y_batches = np.array_split(y,num_batches)
        num_iters = 0

        threshold = custom_threshold(self.batch_size)
        
        print("threshold ",threshold)
        num_osc = 0
        
        while (self.loss_variance_last_10_thetas &gt;threshold):

            for j in range(num_batches):
                gradient = calculate_rev_gradient(y_batches[j],self.theta,x_batches[j])
                self.theta = self.theta + learning_rate * gradient
                self.theta_history.append(self.theta)
            
            current_loss = calculate_loss(y,self.theta,X)
            self.loss_history.append(current_loss)
            self.theta_diff_norm = np.linalg.norm(self.theta - self.theta_history[-2])
            self.loss_diff = calculate_loss(y,self.theta_history[-2],X) - current_loss
            temp_gradient_diff_norm = self.gradient_diff_norm
            self.gradient_diff_norm = np.linalg.norm(gradient)
            self.gradient_diff_norm_diff = self.gradient_diff_norm - temp_gradient_diff_norm
            theta_array = np.stack(self.theta_history)

            if len(self.theta_history) &gt; 10:
                theta_variance = np.var(theta_array, axis=0)
                self.variance_last_10_thetas = np.linalg.norm(theta_variance)
                self.loss_variance_last_10_thetas = np.var(self.loss_history[-10:])                
            else:
                theta_variance = np.var(theta_array, axis=0)
                self.variance_last_10_thetas= np.linalg.norm(theta_variance)
                self.loss_variance_last_10_thetas = np.var(self.loss_history[-10:])

            if current_loss &lt; self.best_loss:
                self.best_loss = current_loss
                self.best_theta = self.theta
                
            print("theta_diff_norm ",self.theta_diff_norm)
            print("loss_diff ",self.loss_diff)
            print("gradient_diff_norm ",self.gradient_diff_norm)
            print("gradient_diff_norm_diff ",self.gradient_diff_norm_diff)
            print("variance_last_10_thetas ",self.variance_last_10_thetas)
            print("loss_variance_last_10_thetas ",self.loss_variance_last_10_thetas)
            print("theta ",self.theta)
            print("num_iters ",num_iters)

            num_iters += 1 
            if self.loss_diff &lt; 0:
                num_osc += 1
            # if num_osc &gt; 10:
            #     break
            # print(calculate_loss(y,self.theta,X))
            # print(self.theta)
            np.random.seed(49)
            indices = np.random.permutation(len(X))
            X = X[indices]
            y = y[indices]
            num_batches = int(m/self.batch_size)
            x_batches = np.array_split(X,num_batches)
            y_batches = np.array_split(y,num_batches)

            if(self.batch_size == 1 and num_iters &gt; 20):
                break
        self.theta = self.best_theta
        print("final theta for batch size ",self.batch_size," is ",self.best_theta)
        print("final num_iters for batch size ",self.batch_size," is ",num_iters)
        fit_return  = self.theta_history
        fit_return = np.array(fit_return)
        reshape_val = fit_return.shape[1]
        fit_return  = fit_return.reshape(-1,reshape_val)
        return fit_return

        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        y_pred = X @ self.best_theta
        y_pred = y_pred.reshape(-1)
        return y_pred



class StochasticLinearRegressor:
    def __init__(self):
        self.batch_sizes = [1,80,8000,800000]
        self.sgd1 = None
        self.sgd2 = None
        self.sgd3 = None
        self.sgd4 = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        self.sgd1 = StochasticLinearRegressor_custom_batch_size(1)
        self.sgd2 = StochasticLinearRegressor_custom_batch_size(80)
        self.sgd3 = StochasticLinearRegressor_custom_batch_size(8000)
        self.sgd4 = StochasticLinearRegressor_custom_batch_size(800000)
        fr1 = self.sgd1.fit(X,y,learning_rate)
        fr2 = self.sgd2.fit(X,y,learning_rate)
        fr3 = self.sgd3.fit(X,y,learning_rate)
        fr4 = self.sgd4.fit(X,y,learning_rate)

        return [fr1,fr2,fr3,fr4]
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        pr1 = self.sgd1.predict(X)
        pr2 = self.sgd2.predict(X)
        pr3 = self.sgd3.predict(X)
        pr4 = self.sgd4.predict(X)
        return [pr1,pr2,pr3,pr4]
        


# In[9]:


#1.a.

<A NAME="2"></A><FONT color = #0000FF><A HREF="match239-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X,y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), np.sqrt(2))
X = np.c_[np.ones(X.shape[0]), X]
</FONT>
X_train = X[:800000]
y_train = y[:800000]
X_test = X[800000:]
y_test = y[800000:]


# In[38]:


# sgd80 = StochasticLinearRegressor(80)
# sgd80.fit(X_train,y_train)

# sgd8000 = StochasticLinearRegressor(8000)
# sgd8000.fit(X_train,y_train)

# sgd800000 = StochasticLinearRegressor(800000)
# sgd800000.fit(X_train,y_train)

# sgd1 = StochasticLinearRegressor(1)
# sgd1.fit(X_train,y_train)

sgd = StochasticLinearRegressor()
th = sgd.fit(X_train,y_train)
pr = sgd.predict(X_test)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match239-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

print(pr[0][-1])
print(pr[1][-1])
print(pr[2][-1])
print(pr[3][-1])
</FONT>









# In[165]:


print("size of theta history of sgd1 ",len(sgd1.theta_history))
print("size of theta history of sgd80 ",len(sgd80.theta_history))
print("size of theta history of sgd8000 ",len(sgd8000.theta_history))
print("size of theta history of sgd800000 ",len(sgd800000.theta_history))




# In[173]:


import matplotlib as mpl 

mpl.rcParams['agg.path.chunksize'] = 10_000


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
theta_array = np.stack(sgd800000.theta_history)
ax.plot(theta_array[:,0], theta_array[:,1], theta_array[:,2], label='batch size 800000',color = 'violet')
plt.legend()
plt.show()





# In[161]:


loss_test_1 = calculate_loss(y_test,sgd1.best_theta,X_test)
loss_train_1 = calculate_loss(y_train,sgd1.best_theta,X_train)

loss_test_80 = calculate_loss(y_test,sgd80.best_theta,X_test)
loss_train_80 = calculate_loss(y_train,sgd80.best_theta,X_train)

loss_test_8000 = calculate_loss(y_test,sgd8000.best_theta,X_test)
loss_train_8000 = calculate_loss(y_train,sgd8000.best_theta,X_train)

loss_test_800000 = calculate_loss(y_test,sgd800000.best_theta,X_test)
loss_train_800000 = calculate_loss(y_train,sgd800000.best_theta,X_train)

print("Loss for batch size 1 on test data is ",loss_test_1)
print("Loss for batch size 1 on train data is ",loss_train_1)

print("Loss for batch size 80 on test data is ",loss_test_80)
print("Loss for batch size 80 on train data is ",loss_train_80)

print("Loss for batch size 8000 on test data is ",loss_test_8000)
print("Loss for batch size 8000 on train data is ",loss_train_8000)

print("Loss for batch size 800000 on test data is ",loss_test_800000)
print("Loss for batch size 800000 on train data is ",loss_train_800000)


# final theta for batch size  1  is  [[3.01713472]
#  [0.99280098]
#  [1.99716827]]
# final num_iters for batch size  1  is  35
# 
# final theta for batch size  80  is  [[3.00008838]
#  [1.00130674]
#  [2.00140794]]
# final num_iters for batch size  80  is  39
# 
# 
# final theta for batch size  8000  is  [[2.99726093]
#  [1.00034395]
#  [1.99961791]]
# final num_iters for batch size  8000  is  248
# 
# 
# final theta for batch size  800000  is  [[2.99949283]
#  [0.99987215]
#  [1.9996965]]
# final num_iters for batch size  800000  is  33687
# 
# 
# 
# 

# In[ ]:


def closed_form_learning(X,y):
    X_transpose = np.transpose(X)
    X_transpose_X = X_transpose @ X
    X_transpose_X_inv = np.linalg.inv(X_transpose_X)
    X_pseudo_inverse = X_transpose_X_inv @ X_transpose
    theta = X_pseudo_inverse @ y
    return theta


theta_closed_form = closed_form_learning(X_train,y_train)
print("theta_closed_form ",theta_closed_form)





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt




def calculate_loss(y, theta, x):
    
    h_x = x @ theta  # Shape (999,)
    y = y.reshape(-1,1)
    
    loss = np.sum(((y - h_x) ** 2)) / (2 * len(y))
    #print(loss)
    return loss

def calculate_rev_gradient(y, theta, x):
    
    h_x = x @ theta  
    h_x = h_x.reshape((-1, 1)) 
    y = y.reshape(-1, 1) 
    
    gradient_matrix = (x*(y - h_x))
    
    gradient_matrix = np.sum(gradient_matrix,axis = 0)
    
    gradient_matrix = gradient_matrix.reshape(-1,1)
    gradient_matrix = gradient_matrix/len(y)
    
    
    
    return gradient_matrix  

def custom_threshold(batch_size):
    if batch_size &lt;5:
        return 1e-3
    elif batch_size &lt;100:
        return 1e-6
    elif batch_size&lt;10000:
        return 1e-11
    else:
        return 1e-20




def generate(N, theta, input_mean, input_sigma, noise_sigma):
    
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    linear_output = np.dot(X, theta[1:]) + theta[0]
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    y = linear_output + noise
    return X, y
    




class StochasticLinearRegressor_custom_batch_size:
    def __init__(self,batch_size1):
        
        self.theta = None
        self.theta_history = [self.theta]
        self.loss_history = []
        self.batch_size = batch_size1
        self.theta_diff_norm = 1e10
        self.loss_diff = 1e10
        self.gradient_diff_norm = 1e10
        self.gradient_diff_norm_diff = 1e10
        self.variance_last_10_thetas = 1e10
        self.loss_variance_last_10_thetas = 1e10
        self.best_theta = self.theta
        self.best_loss = 1e10
    s
    def fit(self, X, y, learning_rate=0.001):
        self.theta = np.zeros((X.shape[1],1))
        self.loss_history.append(calculate_loss(y,self.theta,X))
        np.random.seed(49)
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]

        m = len(y)
        num_batches = int(m/self.batch_size)
        x_batches = np.array_split(X,num_batches)
        y_batches = np.array_split(y,num_batches)
        num_iters = 0

        threshold = custom_threshold(self.batch_size)
        
        # print("threshold ",threshold)
        num_osc = 0
        
        while (self.loss_variance_last_10_thetas &gt;threshold):

            for j in range(num_batches):
                gradient = calculate_rev_gradient(y_batches[j],self.theta,x_batches[j])
                self.theta = self.theta + learning_rate * gradient
                self.theta_history.append(self.theta)
            
            current_loss = calculate_loss(y,self.theta,X)
            self.loss_history.append(current_loss)
            self.theta_diff_norm = np.linalg.norm(self.theta - self.theta_history[-2])
            self.loss_diff = calculate_loss(y,self.theta_history[-2],X) - current_loss
            temp_gradient_diff_norm = self.gradient_diff_norm
            self.gradient_diff_norm = np.linalg.norm(gradient)
            self.gradient_diff_norm_diff = self.gradient_diff_norm - temp_gradient_diff_norm
            theta_array = np.stack(self.theta_history)

            if len(self.theta_history) &gt; 10:
                theta_variance = np.var(theta_array, axis=0)
                self.variance_last_10_thetas = np.linalg.norm(theta_variance)
                self.loss_variance_last_10_thetas = np.var(self.loss_history[-10:])                
            else:
                theta_variance = np.var(theta_array, axis=0)
                self.variance_last_10_thetas= np.linalg.norm(theta_variance)
                self.loss_variance_last_10_thetas = np.var(self.loss_history[-10:])

            if current_loss &lt; self.best_loss:
                self.best_loss = current_loss
                self.best_theta = self.theta
                
            # print("theta_diff_norm ",self.theta_diff_norm)
            # print("loss_diff ",self.loss_diff)
            # print("gradient_diff_norm ",self.gradient_diff_norm)
            # print("gradient_diff_norm_diff ",self.gradient_diff_norm_diff)
            # print("variance_last_10_thetas ",self.variance_last_10_thetas)
            # print("loss_variance_last_10_thetas ",self.loss_variance_last_10_thetas)
            # print("theta ",self.theta)
            # print("num_iters ",num_iters)

            num_iters += 1 
            if self.loss_diff &lt; 0:
                num_osc += 1
            # if num_osc &gt; 10:
            #     break
            # print(calculate_loss(y,self.theta,X))
            # print(self.theta)
            np.random.seed(49)
            indices = np.random.permutation(len(X))
            X = X[indices]
            y = y[indices]
            num_batches = int(m/self.batch_size)
            x_batches = np.array_split(X,num_batches)
            y_batches = np.array_split(y,num_batches)

            if(self.batch_size == 1 and num_iters &gt; 20):
                break
        self.theta = self.best_theta
        # print("final theta for batch size ",self.batch_size," is ",self.best_theta)
        # print("final num_iters for batch size ",self.batch_size," is ",num_iters)
        fit_return  = self.theta_history
        fit_return = np.array(fit_return)
        reshape_val = fit_return.shape[1]
        fit_return  = fit_return.reshape(-1,reshape_val)
        return fit_return

        
    
    def predict(self, X):
       
        y_pred = X @ self.best_theta
        y_pred = y_pred.reshape(-1)
        return y_pred



class StochasticLinearRegressor:
    def __init__(self):
        self.batch_sizes = [1,80,8000,800000]
        self.sgd1 = None
        self.sgd2 = None
        self.sgd3 = None
        self.sgd4 = None
    
    def fit(self, X, y, learning_rate=0.01):
        
        self.sgd1 = StochasticLinearRegressor_custom_batch_size(1)
        self.sgd2 = StochasticLinearRegressor_custom_batch_size(80)
        self.sgd3 = StochasticLinearRegressor_custom_batch_size(8000)
        self.sgd4 = StochasticLinearRegressor_custom_batch_size(800000)
        fr1 = self.sgd1.fit(X,y,learning_rate)
        fr2 = self.sgd2.fit(X,y,learning_rate)
        fr3 = self.sgd3.fit(X,y,learning_rate)
        fr4 = self.sgd4.fit(X,y,learning_rate)

        return [fr1,fr2,fr3,fr4]
    
    def predict(self, X):
        
        pr1 = self.sgd1.predict(X)
        pr2 = self.sgd2.predict(X)
        pr3 = self.sgd3.predict(X)
        pr4 = self.sgd4.predict(X)
        return [pr1,pr2,pr3,pr4]
        




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



def calculate_hessian_inverse(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    p_array.reshape(-1)
    
    diag_array = p_array*(1-p_array)
    diag_array = diag_array.flatten()
    D = np.diag(diag_array)
    X_transpose = np.transpose(X)
    # print(diag_array.shape)
    # print(D)
    # print(D.shape)

    hessian = X_transpose@D@X
    return np.linalg.inv(hessian)

def calculate_gradient_loss(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    gradient = np.dot(X.T, (p_array - y))
    return gradient

def calculate_loss(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    loss = -1*np.sum(y*np.log(p_array) + (1-y)*np.log(1-p_array))
    return loss



class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.theta_history = []
        self.best_theta = self.theta
        self.best_loss = 1e10
    
    def fit(self, X, y, learning_rate=0.01):
        mean = X.mean()
        std = X.std()
        X = (X - mean)/std
        X = np.c_[np.ones(X.shape[0]), X]
        self.theta = np.zeros((X.shape[1],1))
        loss_diff = 1e10
        current_loss = calculate_loss(X,y,self.theta)
        num_iters = 0
        while loss_diff &gt; 1e-6:
            self.theta = self.theta - (calculate_hessian_inverse(X,y,self.theta)@calculate_gradient_loss(X,y,self.theta))
            self.theta_history.append(self.theta)
            prev_loss = current_loss
            current_loss = calculate_loss(X,y,self.theta)
            if current_loss &lt; self.best_loss:
                self.best_loss = current_loss
                self.best_theta = self.theta
            loss_diff = prev_loss - current_loss
            num_iters += 1
            #print("Current Loss: ",current_loss)

        print("Best Loss: ",self.best_loss)
        print("Best Theta: ",self.best_theta)
        print("Number of Iterations: ",num_iters)
        fit_return  = self.theta_history
        fit_return = np.array(fit_return)
        
        reshape_val = fit_return.shape[1]
        fit_return = fit_return.reshape(-1,reshape_val)
        return fit_return
        
    
    def predict(self, X):
        
        y_pred = np.exp(np.dot(X,self.best_theta))/(1+np.exp(np.dot(X,self.best_theta)))
        y_pred = np.where(y_pred &gt;= 0.5, 1, 0)
        y_pred = y_pred.reshape(-1)
        return y_pred



#!/usr/bin/env python
# coding: utf-8

# In[22]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys


# In[23]:


def load_data(path):
    try:
        data = pd.read_csv(path)
    except:
        print("File not found on the path or not a csv file")
        sys.exit(1)
    return data

X = load_data("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q3/logisticX.csv")
y = load_data("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q3/logisticY.csv")

X = X.to_numpy()
y = y.to_numpy()





# In[24]:


def calculate_hessian_inverse(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    p_array.reshape(-1)
    
    diag_array = p_array*(1-p_array)
    diag_array = diag_array.flatten()
    D = np.diag(diag_array)
    X_transpose = np.transpose(X)
    # print(diag_array.shape)
    # print(D)
    # print(D.shape)

    hessian = X_transpose@D@X
    return np.linalg.inv(hessian)

def calculate_gradient_loss(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    gradient = np.dot(X.T, (p_array - y))
    return gradient

def calculate_loss(X,y,theta):
    p_array = np.exp(np.dot(X,theta))/(1+np.exp(np.dot(X,theta)))
    loss = -1*np.sum(y*np.log(p_array) + (1-y)*np.log(1-p_array))
    return loss


# In[25]:


class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.theta_history = []
        self.best_theta = self.theta
        self.best_loss = 1e10
    
    def fit(self, X, y, learning_rate=0.01):
        mean = X.mean()
        std = X.std()
        X = (X - mean)/std
        X = np.c_[np.ones(X.shape[0]), X]
        self.theta = np.zeros((X.shape[1],1))
        loss_diff = 1e10
        current_loss = calculate_loss(X,y,self.theta)
        num_iters = 0
        while loss_diff &gt; 1e-6:
            self.theta = self.theta - (calculate_hessian_inverse(X,y,self.theta)@calculate_gradient_loss(X,y,self.theta))
            self.theta_history.append(self.theta)
            prev_loss = current_loss
            current_loss = calculate_loss(X,y,self.theta)
            if current_loss &lt; self.best_loss:
                self.best_loss = current_loss
                self.best_theta = self.theta
            loss_diff = prev_loss - current_loss
            num_iters += 1
            #print("Current Loss: ",current_loss)

        print("Best Loss: ",self.best_loss)
        print("Best Theta: ",self.best_theta)
        print("Number of Iterations: ",num_iters)
        fit_return  = self.theta_history
        fit_return = np.array(fit_return)
        
        reshape_val = fit_return.shape[1]
        fit_return = fit_return.reshape(-1,reshape_val)
        return fit_return
        
    
    def predict(self, X):
        
        y_pred = np.exp(np.dot(X,self.best_theta))/(1+np.exp(np.dot(X,self.best_theta)))
        y_pred = np.where(y_pred &gt;= 0.5, 1, 0)
        y_pred = y_pred.reshape(-1)
        return y_pred


# In[28]:


lgr = LogisticRegressor()
lgr.fit(X,y)

mean = X.mean()
std = X.std()
X = (X - mean)/std
X = np.c_[np.ones(X.shape[0]), X]


# In[29]:


plt.scatter(X[:,1],X[:,2],c=y.flatten())
plt.xlabel("X1")
plt.ylabel("X2")
plt.title("Logistic Regression data and decision boundary")
x1 = np.linspace(-2.5,2.5,100)

best_theta = lgr.best_theta
x2 = (-best_theta[0] - best_theta[1]*x1)/best_theta[2]
plt.plot(x1,x2)


plt.show()







import numpy as np
import matplotlib.pyplot as plt
import pandas as pd




def quadratic_score(x, mu, phi,sigma_inv):
        diff = (x - mu).reshape(-1, 1)
        return -0.5 * diff.T @ sigma_inv @ diff + np.log(phi)




class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.phi = None
        self.sigma0 = None
        self.sigma1 = None
        self.same_covariance = None
        
    
    def fit(self, X, y, assume_same_covariance=False):
        
        # y = np.array([1 if y_i == 'Alaska' else 0 for y_i in y1])

        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        print(X.shape)
        print(y.shape)
        n = len(X)
        self.same_covariance = assume_same_covariance
        self.phi = np.mean(y)  
        
        self.mu0 = np.mean(X[y == 0], axis=0)  
        self.mu1 = np.mean(X[y == 1], axis=0)  
        
        num_1 = np.sum(y)
        num_0 = n - num_1
    
        
        self.sigma = np.zeros((X.shape[1], X.shape[1]))  
    
        for i in range(n):
            mu_yi = self.mu1 if y[i] == 1 else self.mu0  
            diff = (X[i] - mu_yi).reshape(-1, 1)  
            self.sigma += diff @ diff.T  
    
        self.sigma /= n  

        if not assume_same_covariance:
            self.sigma0 = np.zeros((X.shape[1], X.shape[1]))
            self.sigma1 = np.zeros((X.shape[1], X.shape[1]))
            for i in range(n):
                mu_yi = self.mu1 if y[i] == 1 else self.mu0
                diff = (X[i] - mu_yi).reshape(-1, 1)
                sigma_yi = self.sigma1 if y[i] == 1 else self.sigma0
                sigma_yi += diff @ diff.T
            self.sigma0 /= num_0
            self.sigma1 /= num_1
            return self.mu0, self.mu1, self.sigma0, self.sigma1

        if assume_same_covariance:
            return self.mu0, self.mu1, self.sigma
    

        

    def predict(self, X):
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        if self.same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)

            predictions = []
            for x in X:
                score0 = quadratic_score(x, self.mu0, 1 - self.phi,sigma_inv)  
                score1 = quadratic_score(x, self.mu1, self.phi,sigma_inv)  
                predictions.append(1 if score1 &gt; score0 else 0)
            return np.array(predictions)
        else:
            sigma0_inv = np.linalg.inv(self.sigma0)
            sigma1_inv = np.linalg.inv(self.sigma1)

            predictions = []
            for x in X:
                score0 = quadratic_score(x, self.mu0, 1 - self.phi, sigma0_inv)
                score1 = quadratic_score(x, self.mu1, self.phi, sigma1_inv)
                predictions.append(1 if score1 &gt; score0 else 0)
            return np.array(predictions)

        



#!/usr/bin/env python
# coding: utf-8

# In[1]:


<A NAME="5"></A><FONT color = #FF0000><A HREF="match239-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[3]:


X = np.loadtxt("/Users/akshadmhaske/Desktop/mlass/2022cs11611_akshad_mhaske/data/Q4/q4x.dat", dtype=int) 
y_str = np.loadtxt("/Users/akshadmhaske/Desktop/mlass/2022cs11611_akshad_mhaske/data/Q4/q4y.dat", dtype=str) 
</FONT>




# In[4]:


def quadratic_score(x, mu, phi,sigma_inv):
        diff = (x - mu).reshape(-1, 1)
        return -0.5 * diff.T @ sigma_inv @ diff + np.log(phi)


# In[5]:


class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.phi = None
        self.sigma0 = None
        self.sigma1 = None
        self.same_covariance = None
        
    
    def fit(self, X, y1, assume_same_covariance=False):
       
        y = np.array([1 if y_i == 'Alaska' else 0 for y_i in y1])

        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        print(X.shape)
        print(y.shape)
        n = len(X)
        self.same_covariance = assume_same_covariance
        self.phi = np.mean(y)  
        self.mu0 = np.mean(X[y == 0], axis=0)  
        self.mu1 = np.mean(X[y == 1], axis=0)  
        
        num_1 = np.sum(y)
        num_0 = n - num_1
    
        
        self.sigma = np.zeros((X.shape[1], X.shape[1]))  
    
        for i in range(n):
            mu_yi = self.mu1 if y[i] == 1 else self.mu0  
            diff = (X[i] - mu_yi).reshape(-1, 1)  
            self.sigma += diff @ diff.T  
    
        self.sigma /= n 

        if not assume_same_covariance:
            self.sigma0 = np.zeros((X.shape[1], X.shape[1]))
            self.sigma1 = np.zeros((X.shape[1], X.shape[1]))
            for i in range(n):
                mu_yi = self.mu1 if y[i] == 1 else self.mu0
                diff = (X[i] - mu_yi).reshape(-1, 1)
                sigma_yi = self.sigma1 if y[i] == 1 else self.sigma0
                sigma_yi += diff @ diff.T
            self.sigma0 /= num_0
            self.sigma1 /= num_1
            return self.mu0, self.mu1, self.sigma0, self.sigma1

        if assume_same_covariance:
            return self.mu0, self.mu1, self.sigma
    

        

    def predict(self, X):
        
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        if self.same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)



            predictions = []
            for x in X:
                score0 = quadratic_score(x, self.mu0, 1 - self.phi,sigma_inv)  # Class 0 score
                score1 = quadratic_score(x, self.mu1, self.phi,sigma_inv)  # Class 1 score
                predictions.append(1 if score1 &gt; score0 else 0)
            return np.array(predictions)
        else:
            sigma0_inv = np.linalg.inv(self.sigma0)
            sigma1_inv = np.linalg.inv(self.sigma1)

            predictions = []
            for x in X:
                score0 = quadratic_score(x, self.mu0, 1 - self.phi, sigma0_inv)
                score1 = quadratic_score(x, self.mu1, self.phi, sigma1_inv)
                predictions.append(1 if score1 &gt; score0 else 0)
            return np.array(predictions)

        


# In[7]:


gda = GaussianDiscriminantAnalysis()
gda.fit(X, y_str,True)




print("mu0 ",gda.mu0)
print("mu1 ",gda.mu1)
print("sigma ",gda.sigma)


y = np.where(y_str == "Alaska", 0, 1)

X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)




# In[80]:


plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Training Data')

plt.legend()

plt.show()


# In[81]:


x1 = np.linspace(-3, 3, 1000)
x2 = np.linspace(-3, 3, 1000)
X1, X2 = np.meshgrid(x1, x2)
X_grid = np.array([X1.flatten(), X2.flatten()]).T
predictions = gda.predict(X_grid)
print(predictions)
predictions = predictions.reshape(X1.shape)

plt.contourf(X1, X2, predictions, alpha=0.3)
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Boundary')

plt.legend()
plt.show()


# In[82]:


sigma_inv = np.linalg.inv(gda.sigma)
theta = sigma_inv @ (gda.mu1 - gda.mu0)
theta_0 = -0.5 * (gda.mu1.T @ sigma_inv @ gda.mu1 - gda.mu0.T @ sigma_inv @ gda.mu0)
print(theta)
print(theta_0)

plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Training Data')
plt.legend()

x1 = np.linspace(-3, 3, 1000)
x2 = (-theta[0] * x1 - theta_0) / theta[1]
plt.plot(x1, x2, color='red')
plt.show()


# In[83]:


X = np.loadtxt("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q4/q4x.dat", dtype=int) 
y_str = np.loadtxt("/Users/akshadmhaske/Desktop/mlass/Assignment1/data/Q4/q4y.dat", dtype=str) 

gda2 = GaussianDiscriminantAnalysis()
gda2.fit(X, y_str, False)

y = np.where(y_str == "Alaska", 0, 1)

X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)



print("mu0: ", gda2.mu0)
print("mu1: ", gda2.mu1)
print("sigma0: ", gda2.sigma0)
print("sigma1: ", gda2.sigma1)


# In[84]:


sigma0 = gda2.sigma0
sigma1 = gda2.sigma1
mu0 = gda2.mu0
mu1 = gda2.mu1
sigma0_inv = np.linalg.inv(sigma0)
sigma1_inv = np.linalg.inv(sigma1)

det_sigma0 = np.linalg.det(sigma0)
det_sigma1 = np.linalg.det(sigma1)

phi = 0.5  

Q = sigma1_inv - sigma0_inv

L = -2 * (mu1.T @ sigma1_inv - mu0.T @ sigma0_inv)

C = mu1.T @ sigma1_inv @ mu1 - mu0.T @ sigma0_inv @ mu0 + np.log(det_sigma1 / det_sigma0) + 2 * (np.log(phi) - np.log(1 - phi))

x1_vals = np.linspace(-3, 3, 500)
x2_vals = np.linspace(-3, 3, 500)
X1, X2 = np.meshgrid(x1_vals, x2_vals)

Z = (Q[0, 0] * X1**2 + Q[1, 1] * X2**2 +
     (Q[0, 1] + Q[1, 0]) * X1 * X2 +
     L[0] * X1 + L[1] * X2 + C)

x1 = np.linspace(-3, 3, 1000)
x2 = (-theta[0] * x1 - theta_0) / theta[1]
plt.plot(x1, x2, color='red',label = "same sigma")
contour = plt.contour(X1, X2, Z, levels=0, colors='green')
plt.plot([], [], color='green', label="Different Sigma")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Linear and Quadratic Decsion boundaries')
plt.legend()
plt.show()





# In[ ]:






</PRE>
</PRE>
</BODY>
</HTML>
