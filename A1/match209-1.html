<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_DALFY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_DHHK1.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.is_fitted = False

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        self.theta = np.zeros(n_features + 1)
        self.is_fitted = True

        theta_his = []
        cost_diff = float('inf')
        prev_cost = 0
        theta_his.append(self.theta.copy())
        while abs(cost_diff) &gt; 1e-8:
            j_theta = y - np.dot(X, self.theta)
            j_theta = j_theta ** 2

            # Find the current cost
            cur_cost = np.sum(j_theta) / (2 * n_samples)
            cost_diff = cur_cost - prev_cost
            prev_cost = cur_cost

            # Need to find the gradient now
            # simply do X^T * (Y - x *THETA)
            cur_grad = np.dot(X.T, y - np.dot(X, self.theta))
            cur_grad = cur_grad / n_samples
            self.theta = self.theta + learning_rate * cur_grad
            theta_his.append(self.theta.copy())

        return np.array(theta_his)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.is_fitted is False:
            raise ValueError("Model has not been fitted yet. Call fit() first.")
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        return np.dot(X, self.theta)

    # Plot for Q2
    def plot_data_and_hypothesis(self, x_train, y_train, y_pred):
        plt.plot(x_train, y_pred, c='#00008B', label='Our Prediction')
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match209-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(x_train, y_train, marker='o', c='r', label='Actual Values')
        plt.title("Wine Density vs Acidity")
        plt.xlabel("Feature X - Acidity")
        plt.ylabel("Target Y - Density")
        plt.legend()
        plt.show()

    # Plot for Q3
    def plot_3d(self, x_train, y_train, theta_his):
</FONT>        # x_train is of shape (n_samples, n_features)
        # we append a 1 to make it consistent
        n_samples, n_features = x_train.shape
        ones = np.ones((n_samples, 1))
        x_train = np.hstack((ones, x_train))
        theta_0 = theta_his[:, 0]
        theta_1 = theta_his[:, 1]
<A NAME="0"></A><FONT color = #FF0000><A HREF="match209-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_0_vals = np.linspace(min(theta_0) - 5, max(theta_0) + 10, 100)
        theta_1_vals = np.linspace(min(theta_1) - 5, max(theta_1) + 30, 100)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
        J = np.zeros(T0.shape)
</FONT>
        for i in range(T0.shape[0]):
            for j in range(T0.shape[1]):
                cur_theta = np.array([T0[i, j], T1[i, j]])
                j_theta = y_train - np.dot(x_train, cur_theta)
                j_theta = j_theta ** 2
                J[i, j] = np.sum(j_theta) / (2 * n_samples)

        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')

        surf = ax.plot_surface(T0, T1, J, cmap='viridis', alpha=0.5)
        ax.set_xlabel("Theta 0 - Intercept")
        ax.set_ylabel("Theta 1 - Acidity Slope")
        ax.set_zlabel("Cost J(Theta)")
        ax.set_title("3D Visualization")

        ax.scatter([], [], [], color='r', marker='o')
        for i in range(1, len(theta_his)):
            cur_theta = theta_his[i]
            j_theta = y_train - np.dot(x_train, cur_theta)
            j_theta = j_theta ** 2
            cost = np.sum(j_theta) / (2 * n_samples)
            if i%2 == 0:
                ax.scatter(theta_his[i, 0], theta_his[i, 1], cost, color='r', marker='o')
            else:
                ax.scatter(theta_his[i, 0], theta_his[i, 1], cost, color='b', marker='o')
            plt.draw()
            plt.pause(0.2)
            if not plt.fignum_exists(fig.number):
                break

        plt.show()
        plt.close(fig)

    #Plot for Q4, contour
    def plot_contour_2D(self, x_train, y_train, theta_his):
        n_samples, n_features = x_train.shape
        ones = np.ones((n_samples, 1))
        x_train = np.hstack((ones, x_train))
        theta_0 = theta_his[:, 0]
        theta_1 = theta_his[:, 1]
<A NAME="1"></A><FONT color = #00FF00><A HREF="match209-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_0_vals = np.linspace(min(theta_0) - 5, max(theta_0) + 10, 100)
        theta_1_vals = np.linspace(min(theta_1) - 5, max(theta_1) + 30, 100)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
        J = np.zeros(T0.shape)
</FONT>
        for i in range(T0.shape[0]):
            for j in range(T0.shape[1]):
                cur_theta = np.array([T0[i, j], T1[i, j]])
                j_theta = y_train - np.dot(x_train, cur_theta)
                j_theta = j_theta ** 2
                J[i, j] = np.sum(j_theta) / (2 * n_samples)
        # plot contour
<A NAME="2"></A><FONT color = #0000FF><A HREF="match209-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        fig, ax = plt.subplots(figsize=(8, 6))
        contour = ax.contourf(T0, T1, J, levels=50, cmap="viridis", alpha=0.7)
        plt.colorbar(contour)
</FONT>
        #ax.plot(theta_0, theta_1, 'r-o', markersize=4, label="Gradient Descent Path")
        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope - Acidity)')
        ax.set_title("2D Contour Plot of Cost Function J(Î¸)")
        ax.legend()

        for i in range(len(theta_his)):
            #ax.plot(theta_his[i - 1:i + 1, 0], theta_his[i - 1:i + 1, 1], 'r-o', markersize=4)
            ax.scatter(theta_his[i, 0], theta_his[i, 1], color='r', marker='o', s=20)
            plt.draw()
            plt.pause(0.2)
            if not plt.fignum_exists(fig.number):
                break
        plt.show()


def solve_for_cases(x_train, y_train, learning_rate = 0.01):
    model = LinearRegressor()
    theta_his = model.fit(x_train, y_train, learning_rate = learning_rate)
    final_theta = model.theta
    print(final_theta)
    y_pred = model.predict(x_train)
    model.plot_data_and_hypothesis(x_train, y_train, y_pred)
    model.plot_3d(x_train, y_train, theta_his)
    model.plot_contour_2D(x_train, y_train, theta_his)

x_train = pd.read_csv("../data/Q1/linearX.csv", header = None)
y_train = pd.read_csv("../data/Q1/linearY.csv", header = None)
x_train = x_train.to_numpy()
y_train = y_train.to_numpy()
y_train = y_train.reshape(-1)

solve_for_cases(x_train, y_train)
solve_for_cases(x_train, y_train, 0.001)
solve_for_cases(x_train, y_train, 0.025)
solve_for_cases(x_train, y_train, 0.1)






# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.

    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.

    input_mean : numpy array of shape (2,)
        The mean of the input data.

    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.

    noise_sigma : float
        The standard deviation of the Gaussian noise.

    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.

    y : numpy array of shape (N,)
        The target values.
    """
    np.random.seed(42)
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    X = np.column_stack((x1, x2))
    X_concat = np.column_stack((np.ones(N), X))
    epsilon = np.random.normal(0, noise_sigma, N)
    y = np.dot(X_concat, theta) + epsilon
    return X, y


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.is_fitted = False
        self.batch_size = 1

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        self.theta = np.zeros(n_features + 1)
        self.is_fitted = True
        theta_his = []
        cost = float('inf')
        batch_size = self.batch_size
        previous_cost = float('inf')
        max_epochs = 4e7 * (self.batch_size / n_samples)
        while abs(cost) &gt; 2e-5 and max_epochs &gt; 0:
            new_order = np.random.permutation(n_samples)
            cur_x_train = X[new_order]
            cur_y_train = y[new_order]
            j_cost = 0
            epoch_cost = 0
            for cur_batch in range(0, n_samples, batch_size):
                cur_x = cur_x_train[cur_batch: min(cur_batch + batch_size, n_samples)]
                cur_y = cur_y_train[cur_batch: min(cur_batch + batch_size, n_samples)]
                num_samples, _ = cur_x.shape
                j_theta = cur_y - np.dot(cur_x, self.theta)
                j_theta = j_theta ** 2
                epoch_cost += np.sum(j_theta)
                j_cost = j_cost + np.sum(j_theta) / (2 * num_samples)
                cur_grad = np.dot(cur_x.T, cur_y - np.dot(cur_x, self.theta))
                cur_grad = cur_grad / num_samples
                self.theta = self.theta + learning_rate * cur_grad
                theta_his.append(self.theta.copy())

            epoch_cost = epoch_cost / (2 * n_samples)
            cost = previous_cost - epoch_cost
            previous_cost = epoch_cost
            max_epochs -= 1
            print(cost)
        return theta_his

    def closed_formula(self, X, y):
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        return np.dot(np.linalg.pinv(X), y)

    def plot_3D_path(self, theta_his):
        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')
        theta_his = np.array(theta_his)
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_zlabel('Theta 2')

        theta_0 = theta_his[:, 0]
        theta_1 = theta_his[:, 1]
        theta_2 = theta_his[:, 2]
        ax.plot(theta_0, theta_1, theta_2, 'b-', label="Theta Path", alpha=0.7)
        ax.legend()
        plt.title('Parameter Trajectories in 3D Space')
        plt.show()

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.is_fitted is False:
            raise ValueError("Model has not been fitted yet. Call fit() first.")
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        return np.dot(X, self.theta)

def train_sgd(x_train, y_train, x_test, y_test, learning_rate=0.001, batch_size = 1):
    model = StochasticLinearRegressor()
    model.batch_size = batch_size
    theta_his = model.fit(x_train, y_train, learning_rate=learning_rate)
    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)
    closed_formula_theta = model.closed_formula(x_train, y_train)

    #now find the error with the last theta predicted
    mean_training_error = np.mean((y_train - y_train_pred) ** 2)
    mean_testing_error = np.mean((y_test - y_test_pred) ** 2)
    print(batch_size)
    print(model.theta)
    print(mean_training_error, mean_testing_error, end = " ")
    print()
    print(closed_formula_theta)

    model.plot_3D_path(theta_his)


N = 1000000
X, y = generate(N, [3, 1, 2], [3, -1], [2, 2], np.sqrt(2))
train_size = int(0.8 * N)
x_train, y_train = X[:train_size], y[:train_size]
x_test, y_test = X[train_size:], y[train_size:]

train_sgd(x_train, y_train, x_test, y_test, learning_rate=0.001, batch_size=1)
train_sgd(x_train, y_train, x_test, y_test, learning_rate=0.001, batch_size=80)
train_sgd(x_train, y_train, x_test, y_test, learning_rate=0.001, batch_size=8000)
train_sgd(x_train, y_train, x_test, y_test, learning_rate=0.001, batch_size=800000)

print("Training on Complete Set")
test_model = StochasticLinearRegressor()
total_closed_formula_theta = test_model.closed_formula(X, y)
print(total_closed_formula_theta)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.is_fitted = False
        pass

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        def sigmoid(z):
            return 1 / (1 + np.exp(-z))

        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        self.theta = np.zeros(n_features + 1)
        self.is_fitted = True
        theta_his = []
        cost_diff = float('inf')
        prev_theta = np.zeros(n_features + 1)
        theta_his.append(self.theta.copy())
        while abs(cost_diff) &gt; 1e-15:
            sigma_mu = sigmoid(np.dot(X, self.theta))  # (m, n) * (n, 1) -&gt; (m, 1)
            sigma_mu_copy = sigma_mu.copy()
            hessian_mu = np.diag(sigma_mu_copy.flatten() * (1 - sigma_mu_copy.flatten()))  # (m, m)
            gradient = np.dot(X.T, y - sigma_mu)  # (n, m) * (m, 1) -&gt; (n, 1)
            hessian = np.dot(X.T, np.dot(hessian_mu, X))  # (n, m) * (m, m) * (m, n) -&gt; (n * n)
            inverse_hessian = -np.linalg.pinv(hessian)  # (n, n)
            self.theta = self.theta - np.dot(inverse_hessian, gradient)  # (n, n) * (n, 1) -&gt; (n, 1)
            cost_diff = np.linalg.norm(self.theta - prev_theta)
            prev_theta = self.theta.copy()
            theta_his.append(self.theta.copy())
        pass

<A NAME="5"></A><FONT color = #FF0000><A HREF="match209-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def plot_2d(self, X, y):
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X_norm = (X - X_mean) / X_std
</FONT>
        plt.figure(figsize=(8, 6))
        plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', color='red', label="Class 0")
        plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', color='blue', label="Class 1")
        x1_vals = np.linspace(min(X_norm[:, 0]), max(X_norm[:, 0]), 100)
        theta_0, theta_1, theta_2 = self.theta
        x2_vals = - (theta_0 + theta_1 * x1_vals) / theta_2
        plt.plot(x1_vals, x2_vals, color='green', label="Decision Boundary")
        plt.xlabel("Normalized x1")
        plt.ylabel("Normalized x2")
        plt.legend()
        plt.title("Logistic Regression Decision Boundary")
        plt.show()

        pass

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        def sigmoid(z):
            return 1 / (1 + np.exp(-z))

        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std
        n_samples, n_features = X.shape
        ones = np.ones((n_samples, 1))
        X = np.hstack((ones, X))
        pred_y = sigmoid(np.dot(X, self.theta))
        pred_y = np.where(pred_y &gt;= 0.5, 1, 0)
        return pred_y

x_train = pd.read_csv("../data/Q3/logisticX.csv", header = None)
y_train = pd.read_csv("../data/Q3/logisticY.csv", header = None)
x_train = x_train.to_numpy()
y_train = y_train.to_numpy()
y_train = y_train.reshape(-1)

model = LogisticRegressor()
N, _ = x_train.shape
new_order = np.random.permutation(N)
x_train_new = x_train[new_order]
y_train_new = y_train[new_order]
train_size = int(0.9 * N)
X, y = x_train_new[:train_size], y_train_new[:train_size]
x_test, y_test = x_train_new[train_size:], y_train_new[train_size:]

model.fit(X, y)
print(model.theta)
model.plot_2d(X, y)

predicted_y = model.predict(x_test)
print(predicted_y)
print(y_test)



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.phi_0 = None
        self.phi_1 = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.is_fitted = False

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.is_fitted = True
        n_samples, n_features = X.shape
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std
        phi_1 = np.mean(y)
        phi_0 = 1 - phi_1
        self.phi_0 = phi_0
        self.phi_1 = phi_1
        mu_0 = np.dot(X.T, (1 - y)) / np.sum(1 - y)  # (n_features, )
        mu_1 = np.dot(X.T, y) / np.sum(y)  # (n_features, )
        self.mu_0 = mu_0
        self.mu_1 = mu_1
        mu_0_shape = mu_0.shape[0]
        mu_1_shape = mu_1.shape[0]
        reshaped_mu0 = mu_0.reshape(mu_0_shape, 1)
        reshaped_mu1 = mu_1.reshape(mu_1_shape, 1)
        y_shape = y.shape[0]
        reshaped_y = y.reshape(y_shape, 1)
        inverse_y = 1 - y
        inverse_y = inverse_y.reshape(y_shape, 1)

        if assume_same_covariance == False:
            diff_x = X - np.dot(reshaped_y, reshaped_mu1.T) - np.dot(inverse_y, reshaped_mu0.T)
            sigma = np.dot(diff_x.T, diff_x) / n_samples
            self.sigma_0 = sigma
            self.sigma_1 = sigma
            return mu_0, mu_1, sigma
        # X is m * n, y is m * 1, whereever y = 0, I need to keep those rows in X only and make others 0
        diff_x_0 = X * (y.reshape(-1, 1) == 0) - np.dot(inverse_y, reshaped_mu0.T)
        diff_x_1 = X * (y.reshape(-1, 1) == 1) - np.dot(reshaped_y, reshaped_mu1.T)
        sigma_0 = np.dot(diff_x_0.T, diff_x_0) / np.sum(1 - y)
        sigma_1 = np.dot(diff_x_1.T, diff_x_1) / np.sum(y)
        self.sigma_0 = sigma_0
        self.sigma_1 = sigma_1
        return mu_0, mu_1, sigma_0, sigma_1

    def plot_decision_boundary(self, X, y, assume_same_covariance=False):
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std
        if assume_same_covariance == False:
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match209-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            sigma_inv = np.linalg.pinv(self.sigma_0)
            w = sigma_inv @ (self.mu_0 - self.mu_1)
            b = np.log(self.phi_0 / self.phi_1)
</FONT>            b = b - 0.5 * np.dot(self.mu_0.T, np.dot(sigma_inv, self.mu_0)) + 0.5 * np.dot(self.mu_1.T,
                                                                                           np.dot(sigma_inv, self.mu_1))

            print("Equation is w^T * x + b = 0")
            print(f"w = {w}")
            print(f"b = {b}")

            plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o")
            plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x")
            x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
            x2_vals = (- (w[0] * x1_vals + b) / w[1])

            # Plot decision boundary
            plt.plot(x1_vals, x2_vals, label="Decision Boundary", color="red")

            plt.xlabel("Diameter in Fresh Water (Normalized)")
            plt.ylabel("Diameter in Marine Water (Normalized)")
            plt.legend()
            plt.title("GDA Decision Boundary")
            plt.show()
            return

        sigma_0_inv = np.linalg.pinv(self.sigma_0)
        sigma_0_det = np.linalg.det(self.sigma_0)
        sigma_1_inv = np.linalg.pinv(self.sigma_1)
        sigma_1_det = np.linalg.det(self.sigma_1)

        A = (sigma_1_inv - sigma_0_inv) / 2
        w = np.dot(sigma_0_inv, self.mu_0) - np.dot(sigma_1_inv, self.mu_1)
        b = np.log(self.phi_0 * sigma_1_det / (self.phi_1 * sigma_0_det))
        b = b - 0.5 * np.dot(self.mu_0.T, np.dot(sigma_0_inv, self.mu_0)) + 0.5 * np.dot(self.mu_1.T,
                                                                                         np.dot(sigma_1_inv, self.mu_1))
        # x^T * A * x + w^T * x + b = 0
        print("Equation is x^T * A * x + w^T * x + b = 0")
        print(f"A = {A}")
        print(f"w = {w}")
        print(f"b = {b}")
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o")
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x")
        x1_vals = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
        x2_vals = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
        X1, X2 = np.meshgrid(x1_vals, x2_vals)
        Z = (A[0, 0] * X1 ** 2 + A[1, 1] * X2 ** 2 + A[0, 1] * X1 * X2 + A[1, 0] * X1 * X2 + w[0] * X1 + w[1] * X2 + b)
        plt.contour(X1, X2, Z, levels=[0], colors='green')
        proxy_line = Line2D([0], [0], color='green', lw=1, label='Decision Boundary')
        handles, labels = plt.gca().get_legend_handles_labels()
        handles.append(proxy_line)
        labels.append('Decision Boundary')
        plt.xlabel("Diameter in Fresh Water (Normalized)")
        plt.ylabel("Diameter in Marine Water (Normalized)")
        plt.legend(handles=handles, labels=labels)
        plt.title("GDA Quadratic Decision Boundary")
        plt.show()
        pass

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        if self.is_fitted is False:
            raise ValueError("Model has not been fitted yet. Call fit() first.")
        # p(y = 1|x) = p(x | y = 1) * p(y = 1)/(p(x|y = 1) * p(y = 1) + p(x|y = 0) * p(y = 0))
        n_samples, n_features = X.shape
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std

        def compute_gaussian_pdf(X, mu, sigma):
            d = X.shape[1]
            sigma_inv = np.linalg.pinv(sigma)
            sigma_det = np.linalg.det(sigma)
            constant = 1 / (((2 * np.pi) ** (d / 2)) * np.sqrt(sigma_det))
            x_minus_mu = X - mu
            prod = np.dot(x_minus_mu, np.dot(sigma_inv, x_minus_mu.T)) / (-2)
            diagonal_entry = np.diag(prod)
            return constant * np.exp(diagonal_entry)

        # p(x|y = 0)
        x_given_y0 = compute_gaussian_pdf(X, self.mu_0, self.sigma_0)
        # p(x|y = 1)
        x_given_y1 = compute_gaussian_pdf(X, self.mu_1, self.sigma_1)

        y1_given_x = (x_given_y1 * self.phi_1) / (x_given_y1 * self.phi_1 + x_given_y0 * self.phi_0)

        y_pred = np.where(y1_given_x &gt;= 0.5, "Canada", "Alaska")
        return y_pred

x_train = pd.read_csv("../data/Q4/q4x.dat", header = None, sep = '\\s+')
y_train = pd.read_csv("../data/Q4/q4y.dat", header = None)
x_train = x_train.to_numpy()
y_train = y_train.to_numpy()
y_train = y_train.reshape(-1)
orig_y_train = y_train
y_train = np.where(y_train == "Alaska", 0, 1)

model = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = model.fit(x_train, y_train)
phi_0 = model.phi_0
phi_1 = model.phi_1
print("Linear Boundary Values")
print(f"mu_0 = {mu_0}")
print(f"mu_1 = {mu_1}")
print(f"sigma = {sigma}")
model.plot_decision_boundary(x_train, y_train)
y_pred = model.predict(x_train)
accuracy = np.mean(y_pred == orig_y_train)
print(f"Training Accuracy with Linear: {accuracy * 100:.2f}%")
print()

print("Quadratic Boundary Values")
new_mu_0, new_mu_1, new_sigma_0, new_sigma_1 = model.fit(x_train, y_train, True)
new_phi_0 = model.phi_0
new_phi_1 = model.phi_1
print(f"New mu_0 = {new_mu_0}")
print(f"New mu_1 = {new_mu_1}")
print(f"New sigma_0 = {new_sigma_0}")
print(f"New sigma_1 = {new_sigma_1}")
model.plot_decision_boundary(x_train, y_train, True)

y_pred = model.predict(x_train)
accuracy = np.mean(y_pred == orig_y_train)
print(f"Training Accuracy with Quadratic: {accuracy * 100:.2f}%")

</PRE>
</PRE>
</BODY>
</HTML>
