<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_41V6T.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_BLTHQ.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.iterations = 0
        self.loss_history = []
        self.theta_history = []
        # pass
    
    # Define hypothesis function
    def h(self, theta, X):
        return X.dot(theta)

    # Define cost(loss) function
    def J(self, theta, X, y):
        m = len(y)
        # calculate the difference between the actual and predicted values
        diff = self.h(theta, X) - y
        # mean of squared sum
        J = (1 / (2 * m)) * np.sum(diff ** 2)
        return J
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        theta = np.zeros((X.shape[1], 1))
        m = len(y)
        #learning_rate is the Learning rate
        iterations = 0
        prev_cost = self.J(theta, X, y)
        eps = 1e-9
        converged = False
        loss_history = []
        theta_history = []
        while not converged:
            iterations += 1
            H = self.h(theta, X)
            gradient = np.dot(X.T,(H-y))/(2*m)
            # print(X.T.shape, H.shape, gradient.shape)
            theta -= learning_rate * gradient
            current_cost = self.J(theta, X, y)
            if abs(current_cost - prev_cost) &lt; eps or iterations &gt; 1000:
                converged = True
            prev_cost = current_cost
            loss_history.append(current_cost)
            theta_history.append(theta.flatten())

        # Storing the values
        self.theta = theta
        self.iterations = iterations
        self.loss_history = loss_history
        self.theta_history = theta_history
        # Hypothesis plot
        plt.title('Input Dataset')
        plt.scatter(X[:,1:],y,label='Given Data', color='blue')
        plt.plot(X[:,1:],H,label='Hypothesis', color='red')
        plt.xlabel(' Acidity(X) ')
        plt.ylabel(' Density(Y) ')
        plt.legend()
        plt.show()

        # returning the requirement
        return np.array(theta_history)
        # pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="1"></A><FONT color = #00FF00><A HREF="match188-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
</FONT>
        if self.theta is None:
            raise Exception('Fit the model before prediction')
        # X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        y_pred = self.h(self.theta, X)
        return y_pred.flatten()
        # pass

    # Gradient Descent plot
    def plot_gd(self):
        fig, ax1 = plt.subplots()
        # plot thetas over time
        theta0_history = [theta[0] for theta in self.theta_history]
        theta1_history = [theta[1] for theta in self.theta_history]
        ax1.plot(theta0_history, label='$\\theta_{0}$', linestyle='--', color='blue')
        ax1.plot(theta1_history, label='$\\theta_{1}$', linestyle='-', color='blue')
        ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\theta$')

        # plot loss function over time
        ax2 = ax1.twinx()
        ax2.plot(self.loss_history, label='Loss function', color='red')
        ax2.set_title('Values of $\\theta$ and $J(\\theta)$ over iterations')
        ax2.set_ylabel('$J(\\theta)$')
        
        # Combine legends from both axes
        lines, labels = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax2.legend(lines + lines2, labels + labels2, loc='center')

        fig.tight_layout()
        plt.show()

    # Draw a 3-dimensional mesh showing the error function (J(θ)) on z-axis and the parameters in the x − y plane.
    def plot_mesh(self, X, y):
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        theta0_history = [theta[0] for theta in self.theta_history]
        theta1_history = [theta[1] for theta in self.theta_history]
        # A meshgrid for theta0 and theta1
        theta0_vals = np.linspace(0, 12, 100)
        theta1_vals = np.linspace(0, 60, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

        # filling the cost values for each combination of theta0 and theta1
        Cost = np.zeros_like(Theta0)
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                theta = np.array([[theta0_vals[i]], [theta1_vals[j]]])
                # X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
                Cost[i, j] = self.J(theta, X, y)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        # Plot the surface
        surface = ax.plot_surface(Theta0, Theta1, Cost, cmap='viridis')

        # labels and title
        ax.set_xlabel('$\\theta_{0}$')
        ax.set_ylabel('$\\theta_{1}$')
        ax.set_zlabel('Cost J($\\theta$)')
        ax.set_title('3-dimensional mesh for Error Function')

        # Initialising scatter plot for displaying current parameter values during GD
        scatter = ax.scatter([], [], [], color='red', s=50)

        # Function to update scatter plot data and pause for visualization
        def update_scatter(i):
            scatter._offsets3d = (theta0_history[:i+1], theta1_history[:i+1], self.loss_history[:i+1])
            plt.pause(0.2)  # Pause for 0.2 seconds for visualization

        # Animate the scatter plot over iterations
        for i in range(self.iterations):
            update_scatter(i)
        plt.show()

    # the contours of the error function 
    def plot_contours(self, X, y):
        # A meshgrid for theta0 and theta1
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        theta0_history = [theta[0] for theta in self.theta_history]
        theta1_history = [theta[1] for theta in self.theta_history]
        theta0_vals = np.linspace(0, 12, 20)
        theta1_vals = np.linspace(0, 60, 20)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)

        # cost values for each combination of theta0 and theta1
        Cost = np.zeros_like(Theta0)
        for i in range(len(theta0_vals)):
            for j in range(len(theta1_vals)):
                theta = np.array([[theta0_vals[i]], [theta1_vals[j]]])
                Cost[i, j] = self.J(theta, X, y)

        fig, ax = plt.subplots()

<A NAME="5"></A><FONT color = #FF0000><A HREF="match188-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        contour = ax.contour(Theta0, Theta1, Cost, cmap='viridis')

        # Set labels and title
        ax.set_xlabel('$\\theta_{0}$')
        ax.set_ylabel('$\\theta_{1}$')
        ax.set_title('Contour Plot of Error Function $J(\\theta)$')

        # Initialize scatter plot for displaying current parameter values during GD
        scatter = ax.scatter([], [], color='red', s=1)
</FONT>
        # Create a function to update scatter plot data and pause for visualization
        def update_scatter(i):
            scatter.set_offsets(np.column_stack((theta0_history[:i+1], theta1_history[:i+1])))
            plt.pause(0.2)  # Pause for 0.2 seconds for visualization

        # Animate the scatter plot over iterations
        for i in range(self.iterations):
            update_scatter(i)

        plt.show()

    


def main():
    # Read the CSV file
    X = pd.read_csv('../data/Q1/linearX.csv', header=None)
    y = pd.read_csv('../data/Q1/linearY.csv', header=None)
    X = X.values
    # print(X.shape)
    y = y.values
    model = LinearRegressor()
    theta_history = model.fit(X, y, 0.1)
    y_pred = model.predict(X)
    # Print the parameters
    # model.plot_gd()
    # model.plot_mesh(X, y)
    # model.plot_contours(X, y)
    print(model.theta)
    print(model.iterations)
    print(model.loss_history[-1])
    # np.savetxt('../data/Q1/linearY_pred.csv', y_pred, fmt='%.4f', delimiter=',')
    

<A NAME="6"></A><FONT color = #00FF00><A HREF="match188-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
</FONT>    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))

    noise = np.random.normal(0, noise_sigma, size=N)
    
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise
    
    return X, y
    # pass

def shuffle_data(X, Y):
    indices = np.random.permutation(len(X))
    return X[indices], Y[indices]

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.iterations = 0
        self.theta0_history = []
        self.theta1_history = []
        self.theta2_history = []
        self.loss = 0
        # pass
    
    def h(self, theta, X):
        return X.dot(theta)
    
    def J(self, theta, X, y):
        m = len(y)
        diff = self.h(theta, X) - y
        J = (1 / (2 * m)) * np.sum(diff ** 2)
        return J
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        X, y = shuffle_data(X, y)
        m = len(y)
        theta = np.zeros((X.shape[1], 1))
        r = 80
        k = 1000
        eps = 1e-9
        batches = m//r
        iterations = 0
        converged = False
        theta0_history = []
        theta1_history = []
        theta2_history = []
        last_k_loss = []
        last_k_loss_sum = 0
        next_k_loss = []
        next_k_loss_sum = 0
        while not converged:
            X, y = shuffle_data(X, y)

            for i in range(batches):
                iterations += 1
                X_b = X[i*r:(i+1)*r]
                y_b = y[i*r:(i+1)*r].reshape(-1, 1)
                H = self.h(theta, X_b)
                gradient = np.dot(X_b.T,(H-y_b))/(2*r)
                theta -= learning_rate * gradient
                curr_loss = self.J(theta, X_b, y_b)
                theta0_history.append(theta[0][0])
                theta1_history.append(theta[1][0])
                theta2_history.append(theta[2][0])
                if iterations &lt;= k:
                    last_k_loss.append(curr_loss)
                    last_k_loss_sum += curr_loss
                    continue
                elif (iterations &gt; k) and (iterations &lt;= 2*k):
                    next_k_loss.append(curr_loss)
                    next_k_loss_sum+=curr_loss
                    continue
                if abs(last_k_loss_sum/k - next_k_loss_sum/k) &lt; eps * abs(last_k_loss_sum/k) or iterations &gt; 1000000:
                    converged = True
                    break
                last_k_loss_sum = last_k_loss_sum - last_k_loss.pop(0) + next_k_loss[0]
                last_k_loss.append(next_k_loss[0])
                next_k_loss_sum = next_k_loss_sum - next_k_loss.pop(0) + curr_loss
                next_k_loss.append(curr_loss)

        self.theta = theta
        self.iterations = iterations
        self.theta0_history = theta0_history
        self.theta1_history = theta1_history
        self.theta2_history = theta2_history
        self.loss = curr_loss
        return np.array([theta0_history, theta1_history, theta2_history]).T

        # pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.theta is None:
            raise Exception('Fit the model before prediction')
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        y_pred = self.h(self.theta, X)
        return y_pred.flatten()
        # pass

    def plot_theta(self):
        # Create a figure and 3D axis
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        # Plot the values in 3D space
        ax.plot(self.theta0_history, self.theta1_history, self.theta2_history, color='red')

        # Set labels for each axis
        ax.set_xlabel('$\\theta_{0}$')
        ax.set_ylabel('$\\theta_{1}$')
        ax.set_zlabel('$\\theta_{2}$')
        ax.set_title('Movement of Parameters')
        plt.show()


def closed_form_solution(X, y):

    X_aug = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
    
    # Compute the closed form solution
    theta = np.linalg.inv(X_aug.T.dot(X_aug)).dot(X_aug.T).dot(y)
    return theta


def main():
    # Parameters for generating data
    N = 1000000
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2])
    noise_sigma = np.sqrt(2)

    # Generate the data
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)
    
    # Split into 80% training and 20% testing
    split_index = int(0.8 * N)
    X_train, X_test = X[:split_index], X[split_index:]
    y_train, y_test = y[:split_index], y[split_index:]
    
    theta_closed = closed_form_solution(X_train, y_train)
    print("Closed form theta:", theta_closed)
    
    model = StochasticLinearRegressor()
    theta_history = model.fit(X_train, y_train, learning_rate=0.01)
    
    # Predict on the test set
    y_pred = model.predict(X_test)
    
    # Compute the test loss using the cost function
    # Note: We need to augment X_test with a column of ones to account for the intercept.
    X_test_aug = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)
    
    # Print out the results
    print(y_test[0], y_pred[0])
    model.plot_theta()

    print("Final theta from training:", model.theta.flatten())
    print(model.iterations)
    print("Final loss from training:", model.loss)
    # print the loss on the test set
    print("Loss on test set:", model.J(model.theta.flatten(), X_test_aug, y_test))
    # print the original loss with respect to known original theta
    print("Original loss:", model.J(theta_true, X_test_aug, y_test))


if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

# Normalizing the data
def normalize(data):
    mean = np.mean(data, axis = 0)
    std = np.std(data, axis = 0)
    # std[std == 0] = 1e-8
    return (data - mean) / std

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.iterations = 0
        pass
    
    #define sigmoid function
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    # Define hypothesis function(sigmoid here)
    def h(self, theta, X):
        return 1 / (1 + np.exp(-np.dot(X, theta)))

    # Define log-likelihood function
    def log_likelihood(self, theta, X, Y):
        m = len(Y)
        H = self.sigmoid(np.dot(X, theta))
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match188-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (1 / m) * np.sum(Y * np.log(H) + (1 - Y) * np.log(1 - H))

    # Define hessian function
    def hessian(self, theta, X):
        m = len(X)
</FONT>        H = self.sigmoid(np.dot(X, theta))
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match188-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return (1 / m) * np.dot(X.T, np.dot(np.diag(H * (1 - H)), X))

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
</FONT>        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        X = normalize(X)
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)  # Add term for interceptum
        theta = np.zeros(X.shape[1])
        eps = 1e-6
        m = len(y)
        iterations = 0
        converged = False
        prev_cost = self.log_likelihood(theta, X, y)
        theta_history = []
        while not converged:
            iterations += 1
            # prev_theta = theta
            theta_history.append(theta)
            H = self.sigmoid(np.dot(X, theta))
            # performing newton update
            gradient = np.dot(X.T, (H - y)) / m
            diag_mat = np.diag((H * (1 - H)).flatten())
            # print(temp)
            hessian = np.dot(X.T, np.dot(diag_mat, X)) / m
            theta -= np.dot(np.linalg.inv(hessian), gradient)
            curr_cost = self.log_likelihood(theta, X, y)
            if abs(curr_cost-prev_cost) &lt; eps or iterations &gt; 10000:
                converged = True
            prev_cost = curr_cost
        self.theta = theta
        self.iterations = iterations
        return np.array(theta_history)
        # pass
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = normalize(X)
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
        y_pred = self.h(self.theta, X)
        y_pred = np.where(y_pred &gt;= 0.5, 1, 0)
        return y_pred
        # pass

    
    def plot(self, X, Y):
        # Separate the data based on labels
        X = normalize(X)
        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)  # Add term for interceptum
        X_label_0 = np.array([X[i] for i in range(len(Y)) if Y[i] == 0])
        X_label_1 = np.array([X[i] for i in range(len(Y)) if Y[i] == 1])
        
        # Define x1 range for plotting using the second column (x₁)
        x1_range = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
        # Compute x2 range using the decision boundary (θ₀ + θ₁*x₁ + θ₂*x₂ = 0)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match188-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x2_range = -(self.theta[0] + self.theta[1] * x1_range) / self.theta[2]
        
        # Plot decision boundary and the data
        
        plt.plot(x1_range, x2_range, color='green', label="Decision Boundary")
</FONT>        plt.scatter(X_label_0[:, 1], X_label_0[:, 2], color='red', label='Label 0')
        plt.scatter(X_label_1[:, 1], X_label_1[:, 2], color='blue', label='Label 1')
        plt.xlabel(r'$X_{1}$')
        plt.ylabel(r'$X_{2}$')
        plt.legend()
        plt.title('Logistic Regression')
        plt.show()


def main():
    # Read the CSV files from the previous directory; assuming no header
    X = pd.read_csv("../data/Q3/logisticX.csv", header=None)
    y = pd.read_csv("../data/Q3/logisticY.csv", header=None)
    
    # Convert to numpy arrays
    X = X.values
    y = y.values.flatten()
    
    model = LogisticRegressor()
    theta_history = model.fit(X, y)
    
    # Make predictions
    y_pred = model.predict(X)
    
    # Compute training accuracy
    accuracy = (y_pred == y).mean()
    
    # Display final model parameters and performance
    print("Final theta:", model.theta)
    print("Iterations:", model.iterations)
    print("Training Accuracy: {:.2%}".format(accuracy))
    model.plot(X, y)

if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def normalize(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    std[std == 0] = 1e-8
    return (data - mean) / std

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None         
        self.sigma_0 = None       
        self.sigma_1 = None 
        self.phi = None           
        self.assume_same_covariance = None
        # pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        X = normalize(X)
        m = X.shape[0]
        self.assume_same_covariance = assume_same_covariance
        y = np.array([0 if label.strip()=='Alaska' else 1 for label in y])
        phi = np.mean(y == 1)
        self.phi = phi

        # Separate the data into the two classes
        X_class_0 = X[y == 0]
        X_class_1 = X[y == 1]

        # Compute class means
        mu_0 = np.mean(X_class_0, axis=0)
        mu_1 = np.mean(X_class_1, axis=0)
        self.mu_0 = mu_0
        self.mu_1 = mu_1

        if assume_same_covariance:
            # Compute common covariance by subtracting the corresponding class mean
            X_modified = []
            for i in range(m):
                if y[i] == 0:
                    X_modified.append(X[i] - mu_0)
                else:
                    X_modified.append(X[i] - mu_1)
            X_modified = np.array(X_modified)
            sigma = np.dot(X_modified.T, X_modified) / m
            self.sigma = sigma
            return (mu_0, mu_1, sigma)
        else:
            # Compute separate covariance matrices for each class
            sigma_0 = np.dot((X_class_0 - mu_0).T, (X_class_0 - mu_0)) / X_class_0.shape[0]
            sigma_1 = np.dot((X_class_1 - mu_1).T, (X_class_1 - mu_1)) / X_class_1.shape[0]
            self.sigma_0 = sigma_0
            self.sigma_1 = sigma_1
            return (mu_0, mu_1, sigma_0, sigma_1)

        # pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize the data as done during training.
        X = normalize(X)
        m = X.shape[0]
        y_pred = np.zeros(m)
        if self.assume_same_covariance:
            sigma_inv = np.linalg.pinv(self.sigma)
            for i in range(m):
                x = X[i]
                # Discriminant scores (log-posterior up to constants) for each class:
                score_alaska = -0.5 * np.dot(np.dot((x - self.mu_0).T, sigma_inv), (x - self.mu_0)) + np.log(1 - self.phi)
                score_canada = -0.5 * np.dot(np.dot((x - self.mu_1).T, sigma_inv), (x - self.mu_1)) + np.log(self.phi)
                y_pred[i] = 1 if score_canada &gt; score_alaska else 0
        else:
            sigma_0_inv = np.linalg.pinv(self.sigma_0)
            sigma_1_inv = np.linalg.pinv(self.sigma_1)
            det0 = np.abs(np.linalg.det(self.sigma_0))
            det1 = np.abs(np.linalg.det(self.sigma_1))
            for i in range(m):
                x = X[i]
                score_alaska = (-0.5 * np.dot(np.dot((x - self.mu_0).T, sigma_0_inv), (x - self.mu_0))
                                - 0.5 * np.log(det0 + 1e-8)
                                + np.log(1 - self.phi))
                score_canada = (-0.5 * np.dot(np.dot((x - self.mu_1).T, sigma_1_inv), (x - self.mu_1))
                                - 0.5 * np.log(det1 + 1e-8)
                                + np.log(self.phi))
                y_pred[i] = 1 if score_canada &gt; score_alaska else 0
        # Map numeric predictions back to label names
        y_pred_labels = np.array(['Canada' if pred==1 else 'Alaska' for pred in y_pred])
        return y_pred_labels
        # pass
    
    def plot_linear(self):
        sigma_inv = np.linalg.pinv(self.sigma)
        coef = np.dot((self.mu_1 - self.mu_0).T, sigma_inv)
        # Note: constant term is moved to the right side.
<A NAME="0"></A><FONT color = #FF0000><A HREF="match188-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        const = 0.5 * (np.dot(np.dot(self.mu_1.T, sigma_inv), self.mu_1) - np.dot(np.dot(self.mu_0.T, sigma_inv), self.mu_0)) - np.log(self.phi/(1-self.phi))
</FONT><A NAME="0"></A><FONT color = #FF0000><A HREF="match188-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        # For 2-D features, let x = [x1, x2]. We plot x2 in terms of x1.
</FONT>        x1_vals = np.linspace(-3, 3, 100)
        x2_vals = (const - coef[0] * x1_vals) / coef[1]
        return x1_vals, x2_vals
    
    def plot_quadratic(self):
        sigma_0_inv = np.linalg.pinv(self.sigma_0)
        sigma_1_inv = np.linalg.pinv(self.sigma_1)
        d_0 = np.sqrt(np.abs(np.linalg.det(self.sigma_0)))
        d_1 = np.sqrt(np.abs(np.linalg.det(self.sigma_1)))
        coeff_term = 0.5 * (sigma_1_inv - sigma_0_inv)
        linear_term = np.dot(self.mu_1.T, sigma_1_inv) - np.dot(self.mu_0.T, sigma_0_inv)
        const_term = (0.5 * (np.dot(np.dot(self.mu_1.T, sigma_1_inv), self.mu_1) - np.dot(np.dot(self.mu_0.T, sigma_0_inv), self.mu_0))
                      + np.log((1-self.phi)*d_0/(self.phi*d_1)))
        # Create a meshgrid for plotting
        x1_vals = np.linspace(-3, 3, 200)
        x2_vals = np.linspace(-3, 3, 200)
        x1, x2 = np.meshgrid(x1_vals, x2_vals)
        term1 = coeff_term[0, 0]*x1**2 + (coeff_term[0, 1] + coeff_term[1, 0])*x1*x2 + coeff_term[1, 1]*x2**2
        term2 = linear_term[0]*x1 + linear_term[1]*x2
        h = term1 - term2 + const_term
        contour_plot = plt.contour(x1, x2, h, levels=[0], colors='orange')
        return contour_plot
    


def main():
    # Read the data using pandas (assumed whitespace separated and no header)
    X = pd.read_csv('../data/Q4/q4x.dat', sep='\s+', header=None).values
    Y = pd.read_csv('../data/Q4/q4y.dat', sep='\s+', header=None).values.flatten()
    
    # Create an instance of the GDA model. You can change the flag to True for linear boundary.
    model = GaussianDiscriminantAnalysis()
    
    # Fit with common covariance (linear decision boundary)
    params = model.fit(X, Y, assume_same_covariance=True)
    print("Parameters with shared covariance (Linear):")
    print("mu_0:", model.mu_0)
    print("mu_1:", model.mu_1)
    print("sigma:", model.sigma)
    print("phi:", model.phi)
    
    # Predict on training data and compute accuracy
    y_pred = model.predict(X)
    accuracy = np.mean(y_pred == Y)
    print("Training Accuracy (Linear): {:.2%}".format(accuracy))
    
    # Plot the normalized data and the linear decision boundary
    X_norm = normalize(X)
    X_class_0 = X_norm[[i for i, label in enumerate(Y) if label.strip()=='Alaska']]
    X_class_1 = X_norm[[i for i, label in enumerate(Y) if label.strip()=='Canada']]
    plt.figure(figsize=(8,6))
    plt.scatter(X_class_0[:,0], X_class_0[:,1], color='red', label='Alaska')
    plt.scatter(X_class_1[:,0], X_class_1[:,1], color='blue', label='Canada')
    x1_vals, x2_vals = model.plot_linear()
    plt.plot(x1_vals, x2_vals, color='green')
    
    # Now fit with separate covariance matrices (quadratic decision boundary)
    model.fit(X, Y, assume_same_covariance=False)
    print("\nParameters with separate covariances (Quadratic):")
    print("mu_0:", model.mu_0)
    print("mu_1:", model.mu_1)
    print("sigma_0:", model.sigma_0)
    print("sigma_1:", model.sigma_1)
    y_pred_quad = model.predict(X)
    accuracy_quad = np.mean(y_pred_quad == Y)
    print("Training Accuracy (Quadratic): {:.2%}".format(accuracy_quad))
    
    # Plot quadratic decision boundary on the same figure
    model.plot_quadratic()
    plt.plot
    plt.xlabel(r'$X_1$')
    plt.ylabel(r'$X_2$')
    plt.legend()
    plt.title('Gaussian Discriminant Analysis')
    plt.show()

if __name__ == "__main__":
    main()

</PRE>
</PRE>
</BODY>
</HTML>
