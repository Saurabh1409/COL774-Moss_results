<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_D7WS3.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from tqdm.notebook import tqdm
from IPython.display import clear_output, display


# In[2]:


plt.ion()  # Enable interactive mode


# In[3]:


data_path = ""
X = np.loadtxt('linearX.csv')
y = np.loadtxt('linearY.csv')

# Add intercept term
X = np.column_stack((np.ones(X.shape[0]), X))


# In[4]:


def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
    return cost


# In[5]:


def gradient_descent(X, y, theta, alpha, num_iters, epsilon):
    m = len(y)
    J_history = []
    theta_history = []

    for i in range(num_iters):
        h = X.dot(theta)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match4-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta = theta - (alpha / m) * X.T.dot(h - y)
        J = compute_cost(X, y, theta)
        J_history.append(J)
        theta_history.append(theta)
        
        if i &gt; 0 and abs(J_history[i] - J_history[i-1]) &lt; epsilon:
</FONT>            break

    return theta, J_history, theta_history


# In[6]:


alpha = 0.025  # Learning rate
epsilon = 1e-6  # Stopping criteria
num_iters = 1000
theta = np.zeros(X.shape[1])


# In[7]:


theta, J_history, theta_history = gradient_descent(X, y, theta, alpha, num_iters, epsilon)


# In[8]:


print(f"Learning rate: {alpha}")
print(f"Stopping criteria: Change in J(θ) &lt; {epsilon}")
print(f"Final parameters: {theta}")


# In[9]:


<A NAME="1"></A><FONT color = #00FF00><A HREF="match4-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.figure(figsize=(10, 6))
plt.scatter(X[:, 1], y, color='b', label='Data')
plt.plot(X[:, 1], X.dot(theta), color='r', label='Hypothesis')
plt.xlabel('Acidity')
plt.ylabel('Density')
plt.legend()
</FONT>plt.title('Wine Density vs Acidity')
plt.show()


# In[10]:


theta0_vals = np.linspace(theta[0] - 1, theta[0] + 1, 100)
theta1_vals = np.linspace(theta[1] - 1, theta[1] + 1, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))


# In[11]:


for i in range(len(theta0_vals)):
<A NAME="0"></A><FONT color = #FF0000><A HREF="match4-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for j in range(len(theta1_vals)):
        t = np.array([theta0_vals[i], theta1_vals[j]])
        J_vals[i, j] = compute_cost(X, y, t)


# In[12]:


theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)


# In[13]:


fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
</FONT>surf = ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')
ax.set_xlabel('θ0')
ax.set_ylabel('θ1')
ax.set_zlabel('J(θ)')
plt.colorbar(surf)


# In[14]:


for i, theta in enumerate(tqdm(theta_history)):
    J = compute_cost(X, y, theta)
    ax.scatter(theta[0], theta[1], J, color='r', s=50)
    display(plt.gcf())
    clear_output(wait=True)
    plt.pause(0.2)


# In[ ]:


plt.figure(figsize=(10, 8))
contour = plt.contour(theta0_vals, theta1_vals, J_vals, levels=np.logspace(-2, 3, 20))
plt.colorbar(contour)
plt.xlabel('θ0')
plt.ylabel('θ1')
plt.title('Contour Plot of J(θ)')


# In[ ]:


for i, theta in enumerate(tqdm(theta_history)):
    plt.plot(theta[0], theta[1], 'ro')
    plt.pause(0.2)

plt.show()


# In[ ]:


learning_rates = [0.001, 0.025, 0.1]


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
<A NAME="9"></A><FONT color = #FF00FF><A HREF="match4-0.html#9" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import pandas as pd
import matplotlib.pyplot as plt


# In[2]:


np.random.seed(42)


# In[3]:


theta_true = np.array([3, 1, 2])


# In[4]:


num_samples = 1000000
x0 = np.ones((num_samples, 1))
</FONT>x1 = np.random.normal(3, 2, size=(num_samples, 1)) 
x2 = np.random.normal(-1, 2, size=(num_samples, 1)) 

X = np.hstack((x0, x1, x2))

noise = np.random.normal(0, np.sqrt(2), size=(num_samples, 1))
y = X @ theta_true.reshape(-1, 1) + noise


# In[5]:


train_size = int(0.8 * num_samples)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]


# In[6]:


def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size, tolerance=1e-6):
    m, n = X.shape
    theta = np.zeros((n, 1))
    cost_history = []
    theta_history = [theta.copy()]

    for epoch in range(num_epochs):
        # Shuffle data at the start of each epoch
        indices = np.arange(m)
        np.random.shuffle(indices)
        X, y = X[indices], y[indices]

        # Process each batch
        for i in range(0, m, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Compute predictions and gradient
            predictions = X_batch @ theta
            errors = predictions - y_batch
            gradient = (1 / batch_size) * (X_batch.T @ errors)
            theta -= learning_rate * gradient

        theta_history.append(theta.copy())

        predictions_full = X @ theta
        cost = (1 / (2 * m)) * np.sum((predictions_full - y) ** 2)
        cost_history.append(cost)

        if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; tolerance:
            break

    return theta, cost_history, theta_history


# In[7]:


batch_sizes = [1, 80, 8000, 800000]
learning_rate = 0.001
num_epochs = 100


# In[ ]:


theta_results = {}
theta_histories = {}
for batch_size in batch_sizes:
    theta, cost_history, theta_history = stochastic_gradient_descent(X_train, y_train, learning_rate, num_epochs, batch_size)
    theta_results[batch_size] = theta.flatten()
    theta_histories[batch_size] = theta_history
    print(f"Batch size: {batch_size}")
    print(f"Learned parameters: {theta.flatten()}")
    print(f"Final cost: {cost_history[-1]}\n")


for batch_size in batch_sizes:
    _, cost_history = stochastic_gradient_descent(X_train, y_train, learning_rate, num_epochs, batch_size)
    plt.plot(cost_history, label=f"Batch size: {batch_size}")


plt.xlabel("Epoch")
plt.ylabel("Cost")
plt.title("Cost History for Different Batch Sizes")
plt.legend()
plt.show()


# In[ ]:


theta_closed_form = np.linalg.inv(X_train.T @ X_train) @ (X_train.T @ y_train)
print(f"Closed-form solution parameters: {theta_closed_form.flatten()}")


# In[ ]:


def mean_squared_error_manual(y_actual, y_predicted):
    errors = y_actual - y_predicted
    mse = np.mean(errors ** 2)
    return mse


# In[ ]:


train_predictions = X_train @ theta_closed_form
train_mse = mean_squared_error_manual(y_train, train_predictions)

test_predictions = X_test @ theta_closed_form
test_mse = mean_squared_error_manual(y_test, test_predictions)


# In[ ]:


print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")


# In[ ]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# In[ ]:


def plot_theta_movement(theta_history, batch_size):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    theta_history = np.array(theta_history)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match4-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta_0 = theta_history[:, 0]
    theta_1 = theta_history[:, 1]
    theta_2 = theta_history[:, 2]

    # Plot the trajectory of θ over time
    ax.plot(theta_0, theta_1, theta_2, marker='o', label=f'Batch size {batch_size}', alpha=0.7)
</FONT>
    # Mark the initial and final points
    ax.scatter(theta_0[0], theta_1[0], theta_2[0], color='red', label='Initial θ', s=50)
<A NAME="15"></A><FONT color = #FF0000><A HREF="match4-0.html#15" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax.scatter(theta_0[-1], theta_1[-1], theta_2[-1], color='green', label='Final θ', s=50)

    ax.set_xlabel('θ0')
    ax.set_ylabel('θ1')
</FONT>    ax.set_zlabel('θ2')
    ax.set_title(f'Movement of θ during Gradient Descent (Batch size {batch_size})')
    plt.legend()
    plt.show()


# In[ ]:





# In[ ]:





# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt


# In[2]:


data_path = "Assignment1\data\Q3"
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match4-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')


# In[3]:


X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
X = np.hstack((np.ones((X.shape[0], 1)), X))
</FONT>

# In[4]:


def sigmoid(z):
<A NAME="18"></A><FONT color = #00FFFF><A HREF="match4-0.html#18" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    return 1 / (1 + np.exp(-z))


# In[5]:


def log_likelihood(X, y, theta):
    h = sigmoid(X @ theta)
    return np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
</FONT>

# In[8]:


<A NAME="12"></A><FONT color = #0000FF><A HREF="match4-0.html#12" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def newton_method(X, y, max_iter=10, tol=1e-6):
    m, n = X.shape
    theta = np.zeros(n)
    theta_history = [theta.copy()]
</FONT>
    for _ in range(max_iter):
        h = sigmoid(X @ theta)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match4-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        gradient = X.T @ (y - h)
        D = np.diag(h * (1 - h))
        hessian = -(X.T @ D @ X)

        theta_update = np.linalg.inv(hessian) @ gradient
</FONT>        theta -= theta_update
        theta_history.append(theta.copy())

        if np.linalg.norm(theta_update, ord=1) &lt; tol:
            break

    return theta, theta_history


# In[15]:


theta, theta_history = newton_method(X, y)
print(f"Coefficients (theta): {theta}")
print(f"Coefficients (theta_history): {theta_history}")
theta_history = np.array(theta_history)
print(type(theta_history))
print(f"Coefficients (theta_history): {theta_history}")


# In[16]:


def plot_decision_boundary(X, y, theta):
    plt.scatter(X[y == 0, 1], X[y == 0, 2], label='Class 0', marker='o', color='red')
    plt.scatter(X[y == 1, 1], X[y == 1, 2], label='Class 1', marker='x', color='blue')

    # Plot decision boundary (h(x) = 0.5 =&gt; θ0 + θ1 * x1 + θ2 * x2 = 0)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match4-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1_vals = np.linspace(-2, 2, 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]

    plt.plot(x1_vals, x2_vals, label='Decision Boundary', color='green')
</FONT>    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Logistic Regression: Decision Boundary')
    plt.legend()
    plt.show()


# In[17]:


plot_decision_boundary(X, y, theta)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt


# In[2]:


data_path = "Assignment1\data\Q4"
X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)


# In[3]:


y_binary = np.where(y == "Canada", 1, 0)
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
x_normalized = (X - X_mean) / X_std


# In[4]:


def compute_gda_parameters(x, y):
    n0 = np.sum(y == 0)
    n1 = np.sum(y == 1)

    phi = n1 / (n0 + n1)
<A NAME="19"></A><FONT color = #FF00FF><A HREF="match4-0.html#19" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    mu0 = np.mean(x[y == 0], axis=0)
    mu1 = np.mean(x[y == 1], axis=0)
</FONT>    cov_matrix = (np.cov(x[y == 0].T, bias=True) * n0 + np.cov(x[y == 1].T, bias=True) * n1) / (n0 + n1)

    return phi, mu0, mu1, cov_matrix


# In[5]:


def compute_gda_separate_covariances(x, y):
    n0 = np.sum(y == 0)
    n1 = np.sum(y == 1)

    mu0 = np.mean(x[y == 0], axis=0)
    mu1 = np.mean(x[y == 1], axis=0)
<A NAME="10"></A><FONT color = #FF0000><A HREF="match4-0.html#10" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    cov0 = np.cov(x[y == 0].T, bias=True)
    cov1 = np.cov(x[y == 1].T, bias=True)
</FONT>
    return mu0, mu1, cov0, cov1


# In[6]:


phi, mu0, mu1, shared_cov_matrix = compute_gda_parameters(x_normalized, y_binary)


# In[7]:


mu0_sep, mu1_sep, cov0, cov1 = compute_gda_separate_covariances(x_normalized, y_binary)


# In[8]:


print(f"Phi: {phi}")
print(f"mu0: {mu0}")
print(f"mu1: {mu1}")
print(f"cov: {shared_cov_matrix}")


# In[9]:


print(f"mu0: {mu0}")
print(f"mu1: {mu1}")
print(f"cov0: {cov1}")
print(f"cov1: {cov0}")


# In[10]:


def plot_training_data(x, y, mu0, mu1, cov_matrix, phi, mu0_sep, mu1_sep, cov0, cov1):
    alaska = x[y == 0]
    canada = x[y == 1]
    plt.scatter(alaska[:, 0], alaska[:, 1], label="Alaska", marker="o", color="blue")
    plt.scatter(canada[:, 0], canada[:, 1], label="Canada", marker="x", color="red")
    plt.xlabel("x1 (normalized)")
    plt.ylabel("x2 (normalized)")
    plt.legend()
    plt.title("Training Data")

    cov_inv = np.linalg.inv(cov_matrix)
    w = cov_inv @ (mu1 - mu0)
    c = 0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0) - np.log((1 - phi) / phi)
    x1_vals = np.linspace(-2, 2, 100)
    x2_vals = (c - w[0] * x1_vals) / w[1]
    plt.plot(x1_vals, x2_vals, label="Linear Boundary", color="green")

    cov0_inv = np.linalg.inv(cov0)
    cov1_inv = np.linalg.inv(cov1)

    x1_vals = np.linspace(-2, 2, 100)
    x2_vals = np.linspace(-2, 2, 100)
    x1, x2 = np.meshgrid(x1_vals, x2_vals)

    grid_points = np.c_[x1.ravel(), x2.ravel()]

    def discriminant(x, mu, cov_inv):
        diff = x - mu
        return np.einsum("ij,ji-&gt;i", diff @ cov_inv, diff.T)

    z = discriminant(grid_points, mu0_sep, cov0_inv) - discriminant(grid_points, mu1_sep, cov1_inv)
    z = z.reshape(x1.shape)

    plt.contour(x1, x2, z, levels=[0], colors="purple", linestyles="dashed", label="Quadratic Boundary")

    plt.show()


# In[11]:


plot_training_data(x_normalized, y_binary,mu0, mu1, shared_cov_matrix, phi, mu0_sep, mu1_sep, cov0, cov1)


# In[ ]:








# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pickle

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        pass


    def compute_cost(self,X, y, theta):
        m = len(y)
        predictions = X.dot(theta)
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost

    def gradient_descent(self, X, y, theta, alpha, num_iters, epsilon):
        m = len(y)
        J_history = []
        theta_history = []

        for i in range(num_iters):
            h = X.dot(theta)
            theta = theta - (alpha / m) * X.T.dot(h - y)
<A NAME="7"></A><FONT color = #0000FF><A HREF="match4-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            J = self.compute_cost(X, y, theta)
            J_history.append(J)
            theta_history.append(theta)
            
            if i &gt; 0 and abs(J_history[i] - J_history[i-1]) &lt; epsilon:
</FONT>                break

        return theta, J_history, theta_history
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m = len(y)
        X = np.column_stack((np.ones(X.shape[0]), X))

        # learning_rate = 0.025
        num_iterations = 1000
        epsilon = 1e-6
        theta = np.zeros(X.shape[1])

        theta_final, cost_history, theta_history = self.gradient_descent(X, y, theta, learning_rate, num_iterations, epsilon)

        print("Observations:")
        print("- Learning rate 0.001: Slow convergence, small steps towards minimum")
        print("- Learning rate 0.025: Faster convergence, good balance between speed and stability")
        print("- Learning rate 0.1: Rapid convergence, but risk of overshooting or oscillation")
        print("The optimal learning rate depends on the specific problem and data.")
        print("Experimenting with different values helps find the best trade-off between speed and stability.")

        parameter_linear = {
            "theta": theta_final
        }

        with open("linear_parameter.pkl", "wb") as file:
            pickle.dump(parameter_linear, file)



        print(f'Final parameters: {theta_final}')
        print(f'Final cost: {cost_history[-1]}')

        # plt.scatter(X[:, 1], y, label="Training Data", color="blue")
        # plt.plot(X[:, 1], X.dot(theta_final), label="Linear Regression", color="red")
        # plt.xlabel('Acidity')
        # plt.ylabel('Density')
        # plt.legend()
        # plt.title('Linear Regression Fit')
        # plt.show()
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        with open("linear_parameter.pkl", "rb") as file:
            parameters = pickle.load(file)

        theta = parameters["theta"]
        theta = np.array(theta)


        y_pred = X.dot(theta)
        return y_pred








# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle

np.random.seed(42)

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def mean_squared_error_manual(y_actual, y_predicted):
    errors = y_actual - y_predicted
    mse = np.mean(errors ** 2)
    return mse

def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size, tolerance=1e-6):
    m, n = X.shape
    theta = np.zeros((n, 1))  # Initialize all parameters to 0
    cost_history = []

    for epoch in range(num_epochs):
        # Shuffle data at the start of each epoch
        indices = np.arange(m)
        np.random.shuffle(indices)
        X, y = X[indices], y[indices]

        # Process each batch
        for i in range(0, m, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Compute predictions and gradient
            predictions = X_batch @ theta
            errors = predictions - y_batch
            gradient = (1 / batch_size) * (X_batch.T @ errors)
            theta -= learning_rate * gradient

        predictions_full = X @ theta
        cost = (1 / (2 * m)) * np.sum((predictions_full - y) ** 2)
        cost_history.append(cost)

        if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; tolerance:
            break

    return theta, cost_history

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
<A NAME="17"></A><FONT color = #0000FF><A HREF="match4-0.html#17" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x0 = np.ones((N, 1))  # Intercept term
</FONT>    x1 = np.random.normal(input_mean[0], input_sigma[0], size=(N, 1))  # x1 ~ N(3, 4) (std = sqrt(4))
    x2 = np.random.normal(input_mean[1], input_sigma[1], size=(N, 1))  # x2 ~ N(-1, 4) (std = sqrt(4))
    X = np.hstack((x0, x1, x2))

    noise = np.random.normal(0, noise_sigma, size=(N, 1))
    y = X @ theta.reshape(-1, 1) + noise

    obj = StochasticLinearRegressor()
    obj.fit(X,y, learning_rate=0.01)

    

<A NAME="21"></A><FONT color = #00FF00><A HREF="match4-0.html#21" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

class StochasticLinearRegressor:
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        N = 1000000
        train_size = int(0.8 * N)
        X_train, y_train = X[:train_size], y[:train_size]
        X_test, y_test = X[train_size:], y[train_size:]


        batch_sizes = [1, 80, 8000, 800000]
        learning_rate = 0.01
        num_epochs = 100

        theta_results = {}
        for batch_size in batch_sizes:
            theta, cost_history = stochastic_gradient_descent(X_train, y_train, learning_rate, num_epochs, batch_size)
            theta_results[batch_size] = theta.flatten()
            print(f"Batch size: {batch_size}")
            print(f"Learned parameters: {theta.flatten()}")
            print(f"Final cost: {cost_history[-1]}\n")
        
            # plt.plot(cost_history, label=f"Batch size: {batch_size}")

        theta_closed_form = np.linalg.inv(X_train.T @ X_train) @ (X_train.T @ y_train)
        print(f"Closed-form solution parameters: {theta_closed_form.flatten()}")

        train_predictions = X_train @ theta_closed_form
        train_mse = mean_squared_error_manual(y_train, train_predictions)

        test_predictions = X_test @ theta_closed_form
        test_mse = mean_squared_error_manual(y_test, test_predictions)

        print(f"Training MSE: {train_mse}")
        print(f"Test MSE: {test_mse}")

        theta = np.array(theta)
        parameters = {
            "theta": theta
        }
        with open("sampling_sgd_parameters.pkl", "wb") as file:
            pickle.dump(parameters, file)
    

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        with open("sampling_sgd_parameters.pkl", "rb") as file:
            parameters = pickle.load(file)
        theta = parameters["theta"]

        theta = np.array(theta)

        predictions = X @ theta  # This gives the predicted values
    
        return predictions








# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import pickle

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        pass

    def sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    def log_likelihood(self,X, y, theta):
        h = self.sigmoid(X @ theta)
        return np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    
<A NAME="13"></A><FONT color = #00FFFF><A HREF="match4-0.html#13" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def newton_method(self, X, y, max_iter=10, tol=1e-6):
        m, n = X.shape
        theta = np.zeros(n)
        theta_history = [theta.copy()]  # Store initial theta
</FONT>
        for _ in range(max_iter):
            h = self.sigmoid(X @ theta)
            gradient = X.T @ (y - h)
            D = np.diag(h * (1 - h))
            hessian = -(X.T @ D @ X)

            theta_update = np.linalg.inv(hessian) @ gradient
            theta -= theta_update

            theta_history.append(theta.copy())

            if np.linalg.norm(theta_update, ord=1) &lt; tol:
                break

        return theta, theta_history
    
    def plot_decision_boundary(X, y, theta):
        plt.scatter(X[y == 0, 1], X[y == 0, 2], label='Class 0', marker='o', color='red')
        plt.scatter(X[y == 1, 1], X[y == 1, 2], label='Class 1', marker='x', color='blue')

        # Plot decision boundary (h(x) = 0.5 =&gt; θ0 + θ1 * x1 + θ2 * x2 = 0)
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match4-0.html#8" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        x1_vals = np.linspace(-2, 2, 100)
        x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]

        plt.plot(x1_vals, x2_vals, label='Decision Boundary', color='green')
</FONT>        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.title('Logistic Regression: Decision Boundary')
        plt.legend()
        plt.show()
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        theta, theta_history = self.newton_method(X, y)
        theta = np.array(theta)
        theta_history = np.array(theta_history)
        print(f"Coefficients (theta): {theta}")

        parameters = {
            "theta": theta
        }
        with open("logistic_parameters.pkl", "wb") as file:
            pickle.dump(parameters, file)

        # self.plot_decision_boundary(X, y, theta)

        return theta_history


    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        with open("logistic_parameters.pkl", "rb") as file:
            parameters = pickle.load(file)
        theta = parameters["theta"]

        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        theta = np.array(theta)
        probabilities = self.sigmoid(X @ theta)
    
        predictions = (probabilities &gt; 0.5).astype(int)
        return predictions



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import pickle

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        pass

    def compute_gda_parameters(self,x, y):
        n0 = np.sum(y == 0)
        n1 = np.sum(y == 1)

        phi = n1 / (n0 + n1)
<A NAME="20"></A><FONT color = #FF0000><A HREF="match4-0.html#20" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        mu0 = np.mean(x[y == 0], axis=0)
        mu1 = np.mean(x[y == 1], axis=0)
</FONT>        cov_matrix = (np.cov(x[y == 0].T, bias=True) * n0 + np.cov(x[y == 1].T, bias=True) * n1) / (n0 + n1)

        return phi, mu0, mu1, cov_matrix
    

    def compute_gda_separate_covariances(self,x, y):
        n0 = np.sum(y == 0)
        n1 = np.sum(y == 1)

        mu0 = np.mean(x[y == 0], axis=0)
        mu1 = np.mean(x[y == 1], axis=0)
<A NAME="11"></A><FONT color = #00FF00><A HREF="match4-0.html#11" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        cov0 = np.cov(x[y == 0].T, bias=True)
        cov1 = np.cov(x[y == 1].T, bias=True)
</FONT>
        return mu0, mu1, cov0, cov1
    

    def plot_training_data(x, y, mu0, mu1, cov_matrix, phi, mu0_sep, mu1_sep, cov0, cov1):
        alaska = x[y == 0]
        canada = x[y == 1]
        plt.scatter(alaska[:, 0], alaska[:, 1], label="Alaska", marker="o", color="blue")
        plt.scatter(canada[:, 0], canada[:, 1], label="Canada", marker="x", color="red")
        plt.xlabel("x1 (normalized)")
        plt.ylabel("x2 (normalized)")
        plt.legend()
        plt.title("Training Data")

        cov_inv = np.linalg.inv(cov_matrix)
        w = cov_inv @ (mu1 - mu0)
        c = 0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0) - np.log((1 - phi) / phi)
        x1_vals = np.linspace(-2, 2, 100)
        x2_vals = (c - w[0] * x1_vals) / w[1]
        plt.plot(x1_vals, x2_vals, label="Linear Boundary", color="green")

        cov0_inv = np.linalg.inv(cov0)
        cov1_inv = np.linalg.inv(cov1)

        x1_vals = np.linspace(-2, 2, 100)
        x2_vals = np.linspace(-2, 2, 100)
        x1, x2 = np.meshgrid(x1_vals, x2_vals)

        grid_points = np.c_[x1.ravel(), x2.ravel()]

        def discriminant(x, mu, cov_inv):
            diff = x - mu
            return np.einsum("ij,ji-&gt;i", diff @ cov_inv, diff.T)

        z = discriminant(grid_points, mu0_sep, cov0_inv) - discriminant(grid_points, mu1_sep, cov1_inv)
        z = z.reshape(x1.shape)

        plt.contour(x1, x2, z, levels=[0], colors="purple", linestyles="dashed", label="Quadratic Boundary")

        plt.show()

    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        y_binary = np.where(y == "Canada", 1, 0)
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        x_normalized = (X - X_mean) / X_std

        if assume_same_covariance:
            phi, mu0, mu1, shared_cov_matrix = self.compute_gda_parameters(x_normalized, y_binary)
            print(f"Phi: {phi}")
            print(f"mu0: {mu0}")
            print(f"mu1: {mu1}")
            print(f"cov: {shared_cov_matrix}")

            parameters_linear = {
                "mu0": mu0,                 
                "mu1": mu1,                 
                "cov": shared_cov_matrix,   
                "phi": phi                  
            }
            with open("gda_parameters_linear.pkl", "wb") as file:
                pickle.dump(parameters_linear, file)

            return (mu0, mu1, shared_cov_matrix)

        else:
            mu0_sep, mu1_sep, cov0, cov1 = self.compute_gda_separate_covariances(x_normalized, y_binary)
            print(f"mu0: {mu0_sep}")
            print(f"mu1: {mu1_sep}")
            print(f"cov0: {cov1}")
            print(f"cov1: {cov0}")

            parameters_quadratic = {
                "mu0_sep": mu0_sep,                 
                "mu1_sep": mu1_sep,                 
                "cov0": cov0,   
                "cov1": cov1
            }

            with open("gda_parameters_quadratic.pkl", "wb") as file:
                pickle.dump(parameters_quadratic, file)

            return (mu0_sep, mu1_sep, cov0, cov1)

        # self.plot_training_data(x_normalized, y_binary,mu0, mu1, shared_cov_matrix, phi, mu0_sep, mu1_sep, cov0, cov1)



    def predict_linear_gda(self, X, mu0, mu1, cov_matrix, phi):
        cov_inv = np.linalg.inv(cov_matrix)
        w = cov_inv @ (mu1 - mu0)
        c = 0.5 * (mu0 @ cov_inv @ mu0 - mu1 @ cov_inv @ mu1) - np.log((1 - phi) / phi)

        # Compute decision values for all samples in X
        decision_values = X @ w + c

        # Use numpy where to vectorize the if-else condition
        predictions = np.where(decision_values &gt; 0, 1, 0)

        return predictions
    
    def predict_quadratic_gda(self, x, mu0, mu1, cov0, cov1, phi):
        cov0_inv = np.linalg.inv(cov0)
        cov1_inv = np.linalg.inv(cov1)

        score0 = -0.5 * (x - mu0).T @ cov0_inv @ (x - mu0) - 0.5 * np.log(np.linalg.det(cov0)) + np.log(1 - phi)
        score1 = -0.5 * (x - mu1).T @ cov1_inv @ (x - mu1) - 0.5 * np.log(np.linalg.det(cov1)) + np.log(phi)

        return 1 if score1 &gt; score0 else 0


    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        with open("gda_parameters_linear.pkl", "rb") as file:
            loaded_parameters_linear = pickle.load(file)

<A NAME="14"></A><FONT color = #FF00FF><A HREF="match4-0.html#14" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        with open("gda_parameters_quadratic.pkl", "rb") as file:
            loaded_parameters_quadratic = pickle.load(file)

        mu0 = loaded_parameters_linear["mu0"]
        mu1 = loaded_parameters_linear["mu1"]
        shared_cov_matrix = loaded_parameters_linear["cov"]
        phi = loaded_parameters_linear["phi"]
</FONT>
<A NAME="16"></A><FONT color = #00FF00><A HREF="match4-0.html#16" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        mu0_sep = loaded_parameters_quadratic["mu0_sep"]
        mu1_sep = loaded_parameters_quadratic["mu1_sep"]
        cov0 = loaded_parameters_quadratic["cov0"]
        cov1 = loaded_parameters_quadratic["cov1"]

        X_mean = np.mean(X, axis=0)
</FONT>        X_std = np.std(X, axis=0)
        x_normalized = (X - X_mean) / X_std

        prediction_linear = self.predict_linear_gda(x_normalized, mu0, mu1, shared_cov_matrix, phi)
        # prediction_quadratic = self.predict_quadratic_gda(X, mu0_sep, mu1_sep, cov0, cov1, phi)
        
        return prediction_linear
    



</PRE>
</PRE>
</BODY>
</HTML>
