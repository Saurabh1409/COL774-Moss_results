<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_ARW4B.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_ARW4B.py<p><PRE>


# File: Q1/linear_regression.py
import numpy as np

class LinearRegressor:
    def __init__(self):
        self.theta = None  # will hold final parameters (including intercept)

    def compute_cost(self, X_bias, y, theta):
        m = y.shape[0]
        predictions = X_bias.dot(theta.reshape(-1,1))
        errors = predictions - y.reshape(-1,1)
        return (1/(2*m)) * np.sum(errors**2)

    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        theta_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m = X.shape[0]
        # Add intercept term
        X_bias = np.hstack((np.ones((m, 1)), X))
        n = X_bias.shape[1]
        theta = np.zeros((n, 1))
        theta_history = []
        y = y.reshape(-1, 1)
        
        def compute_cost(theta):
            predictions = X_bias.dot(theta)
            errors = predictions - y
            return (1/(2*m)) * np.sum(errors**2)
        
        cost_prev = compute_cost(theta)
        theta_history.append(theta.flatten())
        max_iter = 1000
        tol = 1e-6
        
        for i in range(max_iter):
            error = X_bias.dot(theta) - y
            grad = (1/m) * (X_bias.T.dot(error))
            theta = theta - learning_rate * grad
            cost_curr = compute_cost(theta)
            theta_history.append(theta.flatten())
            if abs(cost_prev - cost_curr) &lt; tol:
                break
            cost_prev = cost_curr
        
        self.theta = theta
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
<A NAME="2"></A><FONT color = #0000FF><A HREF="match211-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.theta is None:
            raise ValueError("Model has not been fitted yet.")
        m = X.shape[0]
</FONT>        X_bias = np.hstack((np.ones((m, 1)), X))
        y_pred = X_bias.dot(self.theta)
        return y_pred.flatten()




# File: Q1/plot_contours.py

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from linear_regression import LinearRegressor
import time  # Import the time module


def plot_contour(X, y, model, learning_rate=None):
    """Plot the contour of the error function with gradient descent path"""

    # Determine the range for theta0 and theta1 based on the gradient descent path
    theta_history = np.array(model.theta_history)
    theta0_min, theta0_max = theta_history[:, 0].min() - 1, theta_history[:, 0].max() + 1
    theta1_min, theta1_max = theta_history[:, 1].min() - 1, theta_history[:, 1].max() + 1

    # Create a grid of theta values
    theta0_vals = np.linspace(theta0_min, theta0_max, 100)
    theta1_vals = np.linspace(theta1_min, theta1_max, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

    # Compute the cost function for each combination of theta0 and theta1
    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
    J_vals = np.zeros(theta0_mesh.shape)

    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match211-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            J_vals[i, j] = model.compute_cost(X_with_intercept, y, theta)
</FONT>
    # Create the contour plot
    plt.figure(figsize=(10, 8))
    contours = plt.contour(theta0_mesh, theta1_mesh, J_vals, levels=20)
    plt.colorbar(contours)

    # Label the axes and add a title
    plt.xlabel('θ₀')
    plt.ylabel('θ₁')
    title = 'Contour Plot of Error Function'
    if learning_rate is not None:
        title += f' (η={learning_rate})'
    plt.title(title)
    
    # Plot the gradient descent path with animation
    for i in range(len(theta_history)):
        plt.plot(theta_history[:i+1, 0], theta_history[:i+1, 1], 'r.-', label='Gradient Descent Path' if i == 0 else '')
        plt.legend()
        plt.draw()
        plt.pause(0.2)  # Add a 0.2-second delay

    # Save the final plot
    if learning_rate is not None:
        plt.savefig(f'contour_eta_{learning_rate}.png')
    else:
        plt.savefig('contour.png')

    plt.show()
    plt.close()


def main():
    # Load the data
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()

    # Fit the model using a default learning rate (e.g. 0.01)
    model = LinearRegressor()
    theta_history = model.fit(X, y, learning_rate=0.025)

    # Store the theta history in the model (for use by the plotting function)
    model.theta_history = theta_history

    # Plot the contour
    plot_contour(X, y, model, learning_rate=0.025)


if __name__ == '__main__':
    main()




# File: Q1/plot_contours.py
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from linear_regression import LinearRegressor

def plot_contour(X, y, model, learning_rate=None):
    """Plot the contour of the error function with gradient descent path"""
    # Determine the range for theta0 and theta1 based on the gradient descent path
    theta_history = np.array(model.theta_history)
    theta0_min, theta0_max = theta_history[:, 0].min() - 1, theta_history[:, 0].max() + 1
    theta1_min, theta1_max = theta_history[:, 1].min() - 1, theta_history[:, 1].max() + 1
    
    # Create a grid of theta values
    theta0_vals = np.linspace(theta0_min, theta0_max, 100)
    theta1_vals = np.linspace(theta1_min, theta1_max, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    
    # Compute the cost function for each combination of theta0 and theta1
    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
    J_vals = np.zeros(theta0_mesh.shape)
    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match211-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            J_vals[i, j] = model.compute_cost(X_with_intercept, y, theta)
</FONT>    
    # Create the contour plot
    plt.figure(figsize=(10, 8))
    contours = plt.contour(theta0_mesh, theta1_mesh, J_vals, levels=20)
    plt.colorbar(contours)
    
    # Plot the gradient descent path
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'r.-', label='Gradient Descent Path')
    
    # Label the axes and add a title
    plt.xlabel('θ₀')
    plt.ylabel('θ₁')
    title = 'Contour Plot of Error Function'
    if learning_rate is not None:
        title += f' (η={learning_rate})'
    plt.title(title)
    plt.legend()
    
    # Save the plot
    plt.savefig(f'contour_eta_{learning_rate}.png')
    plt.show()
    plt.close()

def main():
    # Load the data
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()
    
    # Fit the model using a default learning rate (e.g. 0.01)
    model = LinearRegressor()
    theta_history = model.fit(X, y, learning_rate=0.001)
    # Store the theta history in the model (for use by the plotting function)
    model.theta_history = theta_history
    
    # Plot the contour
    plot_contour(X, y, model, learning_rate=0.001)
    
if __name__ == '__main__':
    main()




# File: Q1/plot_data.py
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from linear_regression import LinearRegressor

def load_data():
    """
    Loads the dataset for Q1.
    Returns
    -------
    X : numpy array of shape (n_samples, 1)
        The input features (Acidity).
    y : numpy array of shape (n_samples,)
        The target values (Density).
    """
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()
    return X, y

def main():
    # Load data
    X, y = load_data()
    model = LinearRegressor()

    # Set learning rate and stopping criteria
    learning_rate = 0.01
    tol = 1e-6  # Stopping criteria: change in cost &lt; tol
    
    # Fit the model
    theta_history = model.fit(X, y, learning_rate=learning_rate)
    y_pred = model.predict(X)
    
    # Get the final set of parameters
    final_theta = model.theta
    
    # Report learning rate, stopping criteria, and final parameters
    print(f"Learning rate (η): {learning_rate}")
    print(f"Stopping criteria: Change in cost &lt; {tol}")
    print("Final set of parameters (θ):", final_theta.flatten())
    
    # Plot data and regression line
    plt.figure(figsize=(8,6))
    plt.scatter(X, y, color='blue', label='Data')
    
    # Create a smooth regression line
    x_vals = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
    y_vals = model.predict(x_vals)
    plt.plot(x_vals, y_vals, color='red', label='Regression Line')
    
    plt.xlabel('Acidity')
    plt.ylabel('Density')
    plt.title('Linear Regression Fit')
    plt.legend()
    plt.savefig("plot_data.png")
    plt.show()

if __name__ == '__main__':
    main()




# File: Q1/plot_mesh.py
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from linear_regression import LinearRegressor
from mpl_toolkits.mplot3d import Axes3D

def load_data():
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values
    return X, y

def compute_cost(X_bias, y, theta):
    m = y.shape[0]
    predictions = X_bias.dot(theta)
    error = predictions - y
    return (1/(2*m)) * np.sum(error**2)

def main():
    X, y = load_data()
    m = X.shape[0]
    X_bias = np.hstack((np.ones((m,1)), X))
    y = y.reshape(-1,1)
    
    model = LinearRegressor()
    theta_history = model.fit(X, y.flatten(), learning_rate=0.01)
    
    # Create a grid for theta0 and theta1
    theta0_vals = np.linspace(-1, 5, 100)
    theta1_vals = np.linspace(0, 3, 100)
    T0, T1 = np.meshgrid(theta0_vals, theta1_vals)
    J_vals = np.zeros(T0.shape)
    for i in range(T0.shape[0]):
        for j in range(T0.shape[1]):
            theta_temp = np.array([[T0[i,j]], [T1[i,j]]])
            J_vals[i,j] = compute_cost(X_bias, y, theta_temp)
    
    fig = plt.figure(figsize=(10,8))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(T0, T1, J_vals, cmap='viridis', alpha=0.7)
    
    theta_hist_array = np.array(theta_history)
    for theta in theta_hist_array:
        theta = theta.reshape(-1,1)
        cost = compute_cost(X_bias, y, theta)
        ax.scatter(theta[0,0], theta[1,0], cost, color='red', s=50)
        plt.pause(0.2)
    
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_zlabel('Cost')
    plt.title('3D Cost Surface with Gradient Descent Path')
    plt.savefig("plot_mesh.png")
    plt.show()

if __name__ == '__main__':
    main()




# File: Q2/plot_theta_trajectory.py
import numpy as np
import matplotlib.pyplot as plt
from sampling_sgd import generate, StochasticLinearRegressor, closed_form, mse
import matplotlib as mpl  # Import matplotlib

mpl.rcParams['agg.path.chunksize'] = 10000 # Setting the chunksize

def plot_theta_trajectory(theta_history, batch_size):
    theta_history = np.array(theta_history)
    fig = plt.figure(figsize=(8,6))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(theta_history[:,0], theta_history[:,1], theta_history[:,2], marker='o')
    ax.set_xlabel('theta0')
    ax.set_ylabel('theta1')
    ax.set_zlabel('theta2')
    ax.set_title(f'Theta Trajectory (batch size = {batch_size})')
    plt.savefig(f"theta_trajectory_bs_{batch_size}.png")
    plt.show()

def main():
    N = 1000000
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2]) # since variance=4 (std=2)
    noise_sigma = np.sqrt(2)
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

    # Split data into training and test sets
    X_train, y_train = X[:800000], y[:800000]
    X_test, y_test = X[800000:], y[800000:]

    batch_sizes = [1, 80, 8000, 800000]

    # Calculate closed-form solution
    theta_cf = closed_form(X_train, y_train)
    print(f"Closed-form solution: {theta_cf}")

    # Calculate training and test errors for closed-form solution
    train_error_cf = mse(X_train, y_train, theta_cf)
    test_error_cf = mse(X_test, y_test, theta_cf)
    print(f"Closed-form training error: {train_error_cf}")
    print(f"Closed-form test error: {test_error_cf}")

    for bs in batch_sizes:
        print(f"\nProcessing batch size: {bs}")
        model = StochasticLinearRegressor(batch_size=bs)
        print("Fitting the model...")
        theta_history = model.fit(X_train, y_train, learning_rate=0.001)
        learned_theta = theta_history[-1] # Get the last theta values
        print(f"Learned theta for batch size {bs}: {learned_theta}")

        # Calculate training and test errors for SGD
        train_error_sgd = mse(X_train, y_train, learned_theta)
        test_error_sgd = mse(X_test, y_test, learned_theta)
        print(f"SGD training error: {train_error_sgd}")
        print(f"SGD test error: {test_error_sgd}")

        print("Plotting the theta trajectory...")
        plot_theta_trajectory(theta_history, bs)

    print("\nComparison of parameters:")
    print(f"  True parameters: {theta_true}")
    print(f"  Closed-form parameters: {theta_cf}")
    #Add True Parameters for comparison of Parameters for different batch sizes

if __name__ == '__main__':
    main()




# File: Q2/sampling_sgd.py

import numpy as np

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    Parameters
    ----------
    N : int
        The number of samples to generate.
    theta : numpy array of shape (3,)
        The true parameters [theta0, theta1, theta2].
    input_mean : numpy array of shape (2,)
        The mean of the input data for the two features.
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data for the two features.
    noise_sigma : float
        The standard deviation of the Gaussian noise.
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match211-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    noise = np.random.normal(0, noise_sigma, size=(N,))
</FONT>    y = theta[0] + theta[1]*X[:,0] + theta[2]*X[:,1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self, batch_size=1):
        self.theta = None
        self.batch_size = batch_size

    def fit(self, X, y, learning_rate=0.01, tol=1e-6, max_epochs=100):
        """
        Fit the linear regression model to the data using Stochastic Gradient Descent.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        y : numpy array of shape (n_samples,)
            The target values.
        learning_rate : float
            The learning rate to use in the update rule.
        tol : float
            Tolerance for convergence criteria
        max_epochs : int
            Maximum number of epochs
        Returns
        -------
        theta_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters obtained after each iteration.
        """
        m = X.shape[0]
        X_bias = np.hstack((np.ones((m, 1)), X))
        n = X_bias.shape[1]
        theta = np.zeros((n, 1))
        theta_history = [theta.flatten()]
        y = y.reshape(-1,1)
        prev_cost = np.inf

        for epoch in range(max_epochs):
            indices = np.random.permutation(m)
            X_bias_shuffled = X_bias[indices]
            y_shuffled = y[indices]

            for i in range(0, m, self.batch_size):
                X_batch = X_bias_shuffled[i:i+self.batch_size]
                y_batch = y_shuffled[i:i+self.batch_size]
                batch_m = X_batch.shape[0]
                grad = (1/batch_m) * (X_batch.T.dot(X_batch.dot(theta) - y_batch))
                theta = theta - learning_rate * grad
                theta_history.append(theta.flatten())

            cost = np.mean((X_bias.dot(theta) - y)**2) / 2

            if abs(prev_cost - cost) &lt; tol:
                print(f"Converged at epoch {epoch+1}")
                break
            prev_cost = cost

        self.theta = theta
        return np.array(theta_history)

    def predict(self, X):
        """
        Predict the target values for the input data.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        Returns
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match211-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.theta is None:
            raise ValueError("Model has not been fitted yet.")
        m = X.shape[0]
</FONT>        X_bias = np.hstack((np.ones((m, 1)), X))
        return X_bias.dot(self.theta).flatten()

def closed_form(X, y):
    """
    Compute the closed-form solution for linear regression.
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
    y : numpy array of shape (n_samples,)
        The target values.
    Returns
    -------
    theta : numpy array of shape (n_features,)
        The learned parameters.
    """
<A NAME="1"></A><FONT color = #00FF00><A HREF="match211-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))
    theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y
    return theta

def mse(X, y, theta):
</FONT>    """
    Compute the Mean Squared Error.
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
    y : numpy array of shape (n_samples,)
        The target values.
    theta : numpy array of shape (n_features,)
        The parameters.
    Returns
    -------
    error : float
        The mean squared error.
    """
    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))
    predictions = X_bias @ theta
    error = np.mean((predictions - y)**2)
    return error




# File: Q3/logistic_regression.py
import numpy as np

class LogisticRegressor:
    def __init__(self):
        self.theta = None
        self.mu = None
        self.sigma = None  # for normalization
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the logistic regression model to the data using Newton's Method.
        Normalizes the input data X before fitting.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        y : numpy array of shape (n_samples,)
            The target labels (0 or 1).
        learning_rate : float
            The learning rate (used in the Newton update).
            
        Returns
        -------
        theta_history : numpy array of shape (n_iter, n_features+1)
            The list of parameters from each Newton iteration.
        """
        # Normalize X
        self.mu = np.mean(X, axis=0)
        self.sigma = np.std(X, axis=0)
        X_norm = (X - self.mu) / self.sigma
        m = X_norm.shape[0]
        X_bias = np.hstack((np.ones((m, 1)), X_norm))
        n = X_bias.shape[1]
        theta = np.zeros((n, 1))
        y = y.reshape(-1, 1)
        max_iter = 50
        tol = 1e-6
        theta_history = [theta.flatten()]
        
        def sigmoid(z):
            return 1 / (1 + np.exp(-z))
        
        for i in range(max_iter):
            h = sigmoid(X_bias.dot(theta))
            grad = (1/m) * X_bias.T.dot(h - y)
            D = np.diag((h * (1-h)).flatten())
            H = (1/m) * X_bias.T.dot(D).dot(X_bias)
            delta = np.linalg.pinv(H).dot(grad)
            theta = theta - learning_rate * delta
            theta_history.append(theta.flatten())
            if np.linalg.norm(delta, 1) &lt; tol:
                break
        
        self.theta = theta
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target labels for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted binary labels.
        """
        if self.theta is None:
            raise ValueError("Model has not been fitted yet.")
        X_norm = (X - self.mu) / self.sigma
        m = X_norm.shape[0]
        X_bias = np.hstack((np.ones((m, 1)), X_norm))
        
        def sigmoid(z):
            return 1 / (1 + np.exp(-z))
        
        probs = sigmoid(X_bias.dot(self.theta))
        return (probs &gt;= 0.5).astype(int).flatten()




# File: Q3/plot_logistic.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

def load_data():
    X = pd.read_csv('../data/Q3/logisticX.csv', header=None).values
    y = pd.read_csv('../data/Q3/logisticY.csv', header=None).values.flatten()
    return X, y

def main():
    X, y = load_data()
    model = LogisticRegressor()
    theta_history = model.fit(X, y, learning_rate=1)  # Newton’s method typically converges fast
    theta = model.theta

    print("Coefficients (θ) from Logistic Regression Fit:")
    print(theta)

    # Plot normalized data
    X_norm = (X - model.mu) / model.sigma
    plt.figure(figsize=(8,6))
    pos = (y == 1)
    neg = (y == 0)
    plt.scatter(X_norm[pos, 0], X_norm[pos, 1], marker='o', color='blue', label='Class 1')
    plt.scatter(X_norm[neg, 0], X_norm[neg, 1], marker='x', color='red', label='Class 0')
    
    # Decision boundary: theta0 + theta1*x1 + theta2*x2 = 0
    x1_vals = np.linspace(X_norm[:,0].min()-1, X_norm[:,0].max()+1, 100)
    if theta[2,0] != 0:
        x2_vals = -(theta[0,0] + theta[1,0]*x1_vals) / theta[2,0]
        plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')
    else:
        plt.axvline(-theta[0,0]/theta[1,0], color='g', label='Decision Boundary')
    
    plt.xlabel('Feature 1 (normalized)')
    plt.ylabel('Feature 2 (normalized)')
    plt.title('Logistic Regression Decision Boundary')
    plt.legend()
    plt.savefig("logistic_decision_boundary.png")
    plt.show()

if __name__ == '__main__':
    main()




import numpy as np

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mean = None
        self.std = None
        self.mu0 = None
        self.mu1 = None
        self.sigma = None
        self.sigma0 = None
        self.sigma1 = None
        self.pi0 = None
        self.pi1 = None

    def fit(self, X, y, assume_same_covariance=False):
        # Normalize the data
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X_normalized = (X - self.mean) / self.std

        # Split data into classes
        mask0 = (y == 0)
        mask1 = (y == 1)
        X0 = X_normalized[mask0]
        X1 = X_normalized[mask1]

        # Compute class means
        self.mu0 = np.mean(X0, axis=0)
        self.mu1 = np.mean(X1, axis=0)

        # Compute class priors
        n0 = X0.shape[0]
        n1 = X1.shape[0]
        m = n0 + n1
        self.pi0 = n0 / m
        self.pi1 = n1 / m

        # Compute covariance matrices
        diff0 = X0 - self.mu0
        diff1 = X1 - self.mu1

        if assume_same_covariance:
            sigma = (diff0.T @ diff0 + diff1.T @ diff1) / m
            self.sigma = sigma
            return (self.mu0, self.mu1, sigma)
        else:
            self.sigma0 = (diff0.T @ diff0) / n0
            self.sigma1 = (diff1.T @ diff1) / n1
            return (self.mu0, self.mu1, self.sigma0, self.sigma1)

    def predict(self, X):
        if self.mu0 is None or self.mu1 is None:
            raise ValueError("Model must be fitted before prediction.")

        X_normalized = (X - self.mean) / self.std
        y_pred = []

        for x in X_normalized:
            if self.sigma is not None:
                inv_sigma = np.linalg.inv(self.sigma)
                score0 = -0.5 * (x - self.mu0) @ inv_sigma @ (x - self.mu0) + np.log(self.pi0)
                score1 = -0.5 * (x - self.mu1) @ inv_sigma @ (x - self.mu1) + np.log(self.pi1)
            else:
                inv_sigma0 = np.linalg.inv(self.sigma0)
                inv_sigma1 = np.linalg.inv(self.sigma1)
                score0 = -0.5 * (x - self.mu0) @ inv_sigma0 @ (x - self.mu0) - 0.5 * np.log(np.linalg.det(self.sigma0)) + np.log(self.pi0)
                score1 = -0.5 * (x - self.mu1) @ inv_sigma1 @ (x - self.mu1) - 0.5 * np.log(np.linalg.det(self.sigma1)) + np.log(self.pi1)
            y_pred.append(1 if score1 &gt; score0 else 0)
        return np.array(y_pred)



import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load data
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Alaska', 0, 1)

# Fit models
gda_linear = GaussianDiscriminantAnalysis()
_ = gda_linear.fit(X, y, assume_same_covariance=True)

gda_quad = GaussianDiscriminantAnalysis()
_ = gda_quad.fit(X, y, assume_same_covariance=False)

# Print the means and covariance matrix
print("Means and Covariance Matrix for Linear GDA:")
print("μ0 (Alaska):", gda_linear.mu0)
print("μ1 (Canada):", gda_linear.mu1)
print("Σ (Shared Covariance Matrix):", gda_linear.sigma)

print("\nMeans and Covariance Matrices for Quadratic GDA:")
print("μ0 (Alaska):", gda_quad.mu0)
print("μ1 (Canada):", gda_quad.mu1)
print("Σ0 (Covariance Matrix for Alaska):", gda_quad.sigma0)
print("Σ1 (Covariance Matrix for Canada):", gda_quad.sigma1)

# Generate meshgrid
x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 500),
                       np.linspace(x2_min, x2_max, 500))
X_grid = np.c_[xx1.ravel(), xx2.ravel()]

# Predictions
Z_linear = gda_linear.predict(X_grid).reshape(xx1.shape)
Z_quad = gda_quad.predict(X_grid).reshape(xx1.shape)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='o', label='Alaska')
plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='x', label='Canada')
plt.contour(xx1, xx2, Z_linear, levels=[0.5], colors='green', linestyles='dashed', linewidths=2)
plt.contour(xx1, xx2, Z_quad, levels=[0.5], colors='purple', linestyles='solid', linewidths=2)
plt.xlabel('Growth ring diameter in fresh water (x1)')
plt.ylabel('Growth ring diameter in marine water (x2)')
plt.title('GDA Decision Boundaries')
plt.legend()
plt.savefig('gda_decision_boundaries.png')
plt.show()


</PRE>
</PRE>
</BODY>
</HTML>
