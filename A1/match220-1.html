<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_MY1FP.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_QCOFJ.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

from linear_regression import LinearRegressor


def load_data():
    X = np.loadtxt("../data/Q1/linearX.csv", delimiter=',', dtype=float)
    if X.ndim == 1:
        X = X[:, np.newaxis]
    y = np.loadtxt("../data/Q1/linearY.csv", dtype=float)

    return X, y

X, y = load_data()


model = LinearRegressor()
params_list = model.fit(X, y, learning_rate=0.001)
theta_history = np.array(params_list)

theta = model.theta


theta0_vals = np.linspace(-2, theta[0] + 2, 50) 
theta1_vals = np.linspace(-2, theta[1] + 10, 50)
theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)


J_vals = np.zeros_like(theta0_mesh)
m = len(y)

for i in range(theta0_mesh.shape[0]):
    for j in range(theta0_mesh.shape[1]):
        theta_test = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
        predictions = np.c_[np.ones(m), X] @ theta_test
        J_vals[i, j] = np.sum((predictions - y) ** 2) / (2 * m)


plt.figure(figsize=(8, 6))
contour = plt.contourf(theta0_mesh, theta1_mesh, J_vals, levels=np.logspace(-2, 3, 20), cmap="YlGnBu")  
plt.colorbar(contour)  


theta0_path, theta1_path = theta_history[:, 0], theta_history[:, 1]


plt.xlabel("Theta 0")
plt.ylabel("Theta 1")
plt.title("Gradient Descent Path on Cost Function Contours")
plt.legend()


plt.axis('equal')

first_pause = True

for i in range(len(theta_history)):
    plt.scatter(theta_history[i, 0], theta_history[i, 1], c='red', marker='o', s=5)
    plt.draw() 
    if first_pause:
        first_pause = False
        plt.pause(5)
    plt.pause(0.2) 
print("done")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

from linear_regression import LinearRegressor


def load_data():
    X = np.loadtxt("../data/Q1/linearX.csv", delimiter=',', dtype=float)
    if X.ndim == 1:
        X = X[:, np.newaxis]
    y = np.loadtxt("../data/Q1/linearY.csv", dtype=float)

    return X, y

X, y = load_data()


model = LinearRegressor()
params_list = model.fit(X, y, learning_rate=0.001)
theta_history = np.array(params_list)  

theta = model.theta


theta0_vals = np.linspace(-2,  theta[0] + 2, 50)  
theta1_vals = np.linspace(-2,  theta[1] + 10, 50)
theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)


J_vals = np.zeros_like(theta0_mesh)
m = len(y)

for i in range(theta0_mesh.shape[0]):
    for j in range(theta0_mesh.shape[1]):
        theta_test = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
        predictions = np.c_[np.ones(m), X] @ theta_test
        J_vals[i, j] = np.sum((predictions - y) ** 2) / (2 * m)


fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')


ax.plot_surface(theta0_mesh, theta1_mesh, J_vals, cmap='viridis', alpha=0.7)


theta0_path, theta1_path = theta_history[:, 0], theta_history[:, 1]
J_path = [
    np.sum((np.c_[np.ones(m), X] @ theta - y) ** 2) / (2 * m)
    for theta in theta_history
]
#ax.scatter(theta0_path, theta1_path, J_path, c='red', marker='o', label="GD Path")

ax.plot(theta0_path, theta1_path, J_path, color='blue', label="GD Path Line")


ax.set_xlabel("Theta 0")
ax.set_ylabel("Theta 1")
ax.set_zlabel("Cost J_theta")
ax.set_title("Gradient Descent Path on Cost Function Surface")
ax.view_init(elev=20, azim=135)
ax.legend()

first_pause = True


for i in range(len(theta_history)):
    ax.scatter(theta_history[i, 0], theta_history[i, 1], J_path[i], c='red', marker='o',s=5)

    plt.draw()  
    if first_pause:
        first_pause = False
        plt.pause(5)
    plt.pause(0.2) 

plt.show()



#!/usr/bin/env python
# coding: utf-8

# In[16]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')

import numpy as np
import matplotlib.pyplot as plt
from linear_regression import LinearRegressor
from mpl_toolkits.mplot3d import Axes3D


# In[17]:


def load_data():
    X = np.loadtxt("../data/Q1/linearX.csv", delimiter=',',dtype=float)
    if X.ndim == 1:
        X = X[:, np.newaxis]
    y = np.loadtxt("../data/Q1/linearY.csv", dtype=float)

    return X, y


# In[18]:


def plot_data(X, y):
    plt.figure(figsize=(16, 6))

    plt.scatter(X, y, c='r', marker='x',s=1)
    plt.xlabel('')
    plt.ylabel('Y')
    plt.title('Data')
    plt.legend(['Data'])
    plt.show()
X,y = load_data()
plot_data(X, y)


# In[19]:


LR = LinearRegressor()

X, y = load_data()

print(X.shape, y.shape)
params_list = LR.fit(X, y,1.0)

print("Final Theta :" , LR.theta)

#accuracy   
y_pred = LR.predict(X)

def mse(y, y_pred):
    return np.mean((y - y_pred)**2)

print("Mean Squared Error: ", mse(y, y_pred))
print("Number of iterations: ", LR.iterations )




# In[20]:


theta = LR.theta
x_range = np.linspace(min(X), max(X), 100)
y_pred = theta[0] + theta[1] * x_range  


plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='blue', label="Training Data")  
plt.plot(x_range, y_pred, color='red', label="Learned Hypothesis")
plt.xlabel("Acidity (Feature)")
plt.ylabel("Density (Target)")
plt.title("Linear Regression: Learned Hypothesis")
plt.legend()
plt.show()


# In[21]:


def accuracy(y_true, y_pred):
    return np.mean((y - y_pred)**2)


Accuracy_per_learning_rate = []
Iterations_per_learning_rate = []

for learning_rate in np.linspace(0.01, 1, 100):
    LR = LinearRegressor()
    LR.fit(X, y, learning_rate)
    y_pred = LR.predict(X)
    Accuracy_per_learning_rate.append(accuracy(y, y_pred))
    Iterations_per_learning_rate.append(LR.iterations)

def plot3D_accuracy_iterations_learning_rate(learning_rate, accuracy, iterations):
    fig = plt.figure(figsize=(24, 12)) 
    
    # Main 3D plot
    ax = fig.add_subplot(131, projection='3d')
    ax.scatter(learning_rate, iterations, accuracy, s=20, color='red')
    ax.plot(learning_rate, iterations, accuracy, color='blue')
    ax.set_xlabel('Learning Rate')
    ax.set_ylabel('Iterations')
    ax.set_zlabel('Accuracy')
    ax.set_title('3D Plot')
    
    # Top view
    ax_top = fig.add_subplot(132, projection='3d')
    ax_top.scatter(learning_rate, iterations, accuracy, s=20, color='red')
    ax_top.plot(learning_rate, iterations, accuracy, color='blue')
    ax_top.set_xlabel('Learning Rate')
    ax_top.set_ylabel('Iterations')
    ax_top.set_zlabel('Accuracy')
    ax_top.set_title('Top View')
    ax_top.view_init(elev=90, azim=-90)  
    
    # Side view
    ax_side = fig.add_subplot(133, projection='3d')
    ax_side.scatter(learning_rate, iterations, accuracy, s=20, color='red')
    ax_side.plot(learning_rate, iterations, accuracy, color='blue')
    ax_side.set_xlabel('Learning Rate')
    ax_side.set_ylabel('Iterations')
    ax_side.set_zlabel('Accuracy')
    ax_side.set_title('Side View')
    ax_side.view_init(elev=0, azim=0) 
    
    plt.suptitle('Accuracy vs Iterations vs Learning Rate')
    plt.tight_layout()  
    plt.show()


plot3D_accuracy_iterations_learning_rate(np.linspace(0.01, 1, 100), Accuracy_per_learning_rate, Iterations_per_learning_rate)
    





#!/usr/bin/env python
# coding: utf-8

# In[38]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')

import numpy as np
import matplotlib.pyplot as plt
import sampling_sgd
from sampling_sgd import StochasticLinearRegressor
from mpl_toolkits.mplot3d import Axes3D


# In[39]:


N = 1000000
theta = np.array([3,1,2])
imput_mean = np.array([3,-1])
input_sigma = np.array([2,2])
noise_sigma = np.sqrt(2.0)


# In[ ]:





# In[40]:


X_train,y_train = sampling_sgd.generate(int(N*0.8),theta,imput_mean,input_sigma,noise_sigma)
X_test,y_test = sampling_sgd.generate(int(N*0.2),theta,imput_mean,input_sigma,noise_sigma)


def plot_train_data(X_train,y_train):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train[:,0], X_train[:,1], y_train ,s=1)
    ax.set_xlabel('X1')
    ax.set_ylabel('X2')
    ax.set_zlabel('y')
    plt.show()
#plot_train_data(X_train,y_train)



# In[47]:


SGD = StochasticLinearRegressor()

Thetas_epoch_history = SGD.fit(X_train,y_train,0.001)


# In[42]:


Thetas_history = SGD.theta_itr_history
Thetas = SGD.learned_thetas
print(Thetas)


# In[43]:


def mean_squared_error(y_true, y_pred):

    return np.mean((y_true - y_pred) ** 2)

# Compute predictions using closed-form solution


for i in range(0,4):

    theta = SGD.learned_thetas[i]
    batch_size = SGD.batch_sizes[i]

    print(theta)
    X_test__ = np.column_stack((np.ones(X_test.shape[0]), X_test))
    X_train__ = np.column_stack((np.ones(X_train.shape[0]), X_train))
    y_train_pred = X_train__ @ theta
    y_test_pred = X_test__ @ theta

    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    # Print results
    print("----------------------------")

    print(f"For batch size :{batch_size}")
    print(f"Training MSE: {train_mse:.6f}")
    print(f"Test MSE: {test_mse:.6f}")

    print("----------------------------")



# In[44]:


def closed_form_solution(X, y):
    
    # theta = (X^T * X)^(-1) * X^T * y
    X = np.c_[np.ones(X.shape[0]), X] 
    
    theta = np.linalg.inv(X.T @ X) @ X.T @ y
    return theta

Theta = closed_form_solution(X_train,y_train)

print("Theta from closed form solution: ", Theta)

X_test__ = np.column_stack((np.ones(X_test.shape[0]), X_test))
X_train__ = np.column_stack((np.ones(X_train.shape[0]), X_train))

print("MSE for closed form solution: ", mean_squared_error(y_train, X_train__ @ Theta))
print("MSE for closed form solution: ", mean_squared_error(y_test, X_test__ @ Theta))




# In[45]:


import matplotlib.pyplot as plt
import numpy as np

def plot_theta_updates(theta_histories, batch_sizes):
    """
    Plot the movement of θ (parameters) over iterations for different batch sizes.

    Parameters
    ----------
    theta_histories : list of numpy arrays
        List of numpy arrays, where each array contains θ values at each iteration.
        
    batch_sizes : list
        List of batch sizes corresponding to each θ history.
    """

    colors = ['r', 'g', 'b', 'm']
    
    for i, batch_size in enumerate(batch_sizes):
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match220-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_history = np.array(theta_histories[i])

        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')

        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], 
</FONT>                label=f'Batch Size {batch_size}', color=colors[i], alpha=0.8)

        ax.set_xlabel(r'$\theta_0$ (Intercept)')
        ax.set_ylabel(r'$\theta_1$ (Slope 1)')
        ax.set_zlabel(r'$\theta_2$ (Slope 2)')

        # Plot the final point in Theta 
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], 
                   color="black", s=100, marker='*')
    
        ax.set_title(f'Parameter Updates in 3D Space for Batch Size {batch_size}')
        ax.legend()
        plt.show()


plot_theta_updates(Thetas_history, SGD.batch_sizes)
print(SGD.batch_sizes)
print(Thetas_history[0].shape, Thetas_history[1].shape, Thetas_history[2].shape, Thetas_history[3].shape)


# In[46]:


print(SGD.theta_itr_history[0].shape, SGD.theta_itr_history[1].shape, SGD.theta_itr_history[2].shape, SGD.theta_itr_history[3].shape)


# In[ ]:








import numpy as np
from collections import deque

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match220-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

 
    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)
</FONT>    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)


    X = np.column_stack((x1, x2))

    # Generate Gaussian noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)

    # Compute target values: y = theta_0 + theta_1 * x1 + theta_2 * x2 + noise
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise

    return X, y

class StochasticLinearRegressor:
    def __init__(self):

        self.theta = None
        self.learned_thetas = None
        self.theta_itr_history = None
        self.batch_sizes = [1, 80, 8000, 800000]
        self.tol = {1: 1e-4, 80: 1e-4, 8000: 1e-6, 800000: 1e-7}
        self.max_epochs = {1: 10, 80: 100, 8000: 1000, 800000: 10000}

        # self.batch_sizes = [1, 8, 80 , 800]
        # self.tol = {1: 1e-3, 8: 1e-4, 80: 1e-4, 800: 1e-7}
        # self.max_epochs = {1: 10, 8: 100, 80: 1000, 800: 10000}

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m = X.shape[0]
        X = np.c_[np.ones(m), X]

        n_samples, n_features = X.shape

        learned_thetas = np.zeros((len(self.batch_sizes), n_features))

        theta_history = []
        theta_itr_history = []

        for batch_size in self.batch_sizes:

            # Initialize parameters to zeros
            theta = np.zeros(n_features)
            prev_theta = np.ones(n_features) 
            
            theta_history_batch = []
            theta_itr_history_batch = []


            epoch = 0
            rolling_window = deque(maxlen=10)  # Rolling window of size 10

            while epoch &lt; self.max_epochs[batch_size]:
                # if(epoch % (max(1,int(batch_size/100))) == 0):
                #     print(f"Epoch: {epoch} for {batch_size}")
                prev_theta = theta.copy()  # Store previous theta

                indices = np.random.permutation(n_samples)  # Shuffle data
                
                for i in range(0, n_samples, batch_size):
                    batch_indices = indices[i:i + batch_size]
                    X_batch, y_batch = X[batch_indices], y[batch_indices]

                    # Compute gradient: delta(J_theta) = (1/batch_size) * X_batch^T (X_batch θ - y_batch)
                    gradient = (1 / batch_size) * X_batch.T @ (X_batch @ theta - y_batch)
                    
                    # Update parameters
                    theta -= learning_rate * gradient
                    theta_itr_history_batch.append(theta.copy())
                
                theta_history_batch.append(theta.copy())
                
                rolling_window.append(np.linalg.norm(theta - prev_theta, ord=2))
                if len(rolling_window) == 10 and np.mean(rolling_window) &lt; self.tol[batch_size]:
                    break
                epoch += 1

            #print(f"\nBatch size: {batch_size}\n Epochs: {epoch} \n Theta: {theta}\n")
            
            theta_history.append(np.array(theta_history_batch))
            theta_itr_history.append(np.array(theta_itr_history_batch))

            learned_thetas[self.batch_sizes.index(batch_size)] = theta
        
        self.learned_thetas = learned_thetas
        self.theta = learned_thetas[-1]
        self.theta_itr_history = theta_itr_history

        return (theta_history[0], theta_history[1], theta_history[2], theta_history[3])

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """

        m = X.shape[0]
        X = np.c_[np.ones(m), X]

        y_pred_list = np.array([X @ theta for theta in self.learned_thetas])

        return y_pred_list



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):

        self.theta = None # Parameters of the model

        self.mean = None # Mean of each feature

        self.std = None # Standard deviation of each feature

        pass

    def _sigmoid(self, z):

        return 1 / (1 + np.exp(-z))

    def _normalize(self, X , mean=None, std=None):

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match220-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        if mean is None or std is None:
            mean = np.mean(X, axis=0)
            std = np.std(X, axis=0)
            self.mean = mean
            self.std = std
</FONT>
        return (X - mean) / std
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        

        # Normalize the input data

        X = self._normalize(X, self.mean, self.std)

        # Add intercept term
        X = np.hstack([np.ones((X.shape[0], 1)), X])


        M, N = X.shape
        self.theta = np.zeros(N)

        theta_history = []
        max_iter = 1000000
        tol = 1e-7


        for _ in range(max_iter):
  
            z = X @ self.theta

            # Probability of y=1 given X: h = 1 / (1 + exp(-z))
            h = self._sigmoid(z)

            # Gradient of the log-likelihood: X.T * (y - h)
            gradient = X.T @ (y - h)

            # Diagonal matrix R of the probabilities h
            R = np.diag(h * (1 - h))

            # Hessian matrix H = X.T * R * X
            # H[i][j] = d^2 ( h ) / d ( theta_i ) d ( theta_j )
            # H[i][j] = -X.T[i] * R * X[j] if i != j
            H = X.T @ R @ X 

            # Newton’s update step: θ_new = θ + H^(-1) * gradient
            try:
                delta_theta = learning_rate*np.linalg.inv(H) @ gradient

            except np.linalg.LinAlgError:
                
                H = X.T @ R @ X + 1e-9 * np.eye(N)
                delta_theta = learning_rate*np.linalg.inv(H) @ gradient

                break

            self.theta += delta_theta
            theta_history.append(self.theta.copy())

            # Convergence check
            if np.linalg.norm(delta_theta) &lt; tol:
                print(_)
                break

        return np.array(theta_history)


        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        X= self._normalize(X)

        X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term


        # P = 1 / (1 + exp(-theta(T).X))
        probabilities = self._sigmoid(X @ self.theta)

        y_pred = (probabilities &gt;= 0.5).astype(int)
    
        return y_pred
        pass



#!/usr/bin/env python
# coding: utf-8

# In[9]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')

import numpy as np
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor


# In[10]:


def load_data():
    
    X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=',' ,dtype=float)
    if X.ndim == 1:
        X = X[:, np.newaxis]
    y = np.loadtxt("../data/Q3/logisticY.csv", dtype=int)
    return X, y


# In[11]:


LR = LogisticRegressor()

X, Y = load_data()

LR.fit(X, Y)

print("Final Theta :", LR.theta)

plt.scatter(X[Y == 0, 0], X[Y == 0, 1], color='red', marker='x', label='0')
plt.scatter(X[Y == 1, 0], X[Y == 1, 1], color='blue', marker='o' , label='1')



x_min , x_max= -20 , 20
y_min , y_max= -20 , 20

x = np.linspace(x_min, x_max, 100)
y = - (LR.theta[0] + LR.theta[1] * x) / LR.theta[2]





plt.text(0, 0, "y = %.2f + %.2fX1 + %.2fX2" % (LR.theta[0], LR.theta[1], LR.theta[2]) , fontsize=10, position=(10, 0))

plt.fill_between(x, y, y_max, color='blue', alpha=0.2)
plt.fill_between(x, y, y_min, color='red', alpha=0.2)

plt.plot(x, y, color='green')


plt.xlim( np.min(X[:,0]) -1 , np.max(X[:,0]) + 1 )
plt.ylim( np.min(X[:,1]) -1 , np.max(X[:,1]) + 1 )

plt.xlabel("X1")
plt.ylabel("X2")
plt.title("Logistic Regression")

plt.legend()

plt.show()


# In[12]:


predictions = LR.predict(X)


print("Accuracy for shared covariance: ", np.mean(predictions == Y))








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):

        self.mu = None # Mean of each class

        self.sigma = None # Covariance matrix for classes

        self.classes_ = None # Class labels

        self.class_prior = None  # Prior probabilities of each class

        self.mean = None # Mean of each feature

        self.std = None # Standard deviation of each feature

        pass
    def normalize_features(self, X, means=None, stds=None):
        """
        Normalize features using mean and standard deviation.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        
        Returns
        -------
        X_norm : numpy array of shape (n_samples, n_features)
            The normalized input data.
        """
        if means is None or stds is None:
            means = np.mean(X, axis=0)
            stds = np.std(X, axis=0)
            self.mean = np.mean(X, axis=0)
            self.std = np.std(X, axis=0)

        return (X - means) / stds 

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """ 

        # Store class labels

        self.classes_ = np.unique(y)

        # Normalize features

<A NAME="5"></A><FONT color = #FF0000><A HREF="match220-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)

        X_norm = self.normalize_features(X)
        X = X_norm
</FONT>
        # Number of samples for each class(here only two classes - 0 , 1 )

        M_classes = [np.sum(y == c) for c in self.classes_] # Number of samples for each class

        mu = np.zeros((len(self.classes_), X.shape[1])) # Mean of each class
        
        M = X.shape[0] # Total number of samples

        # Prior probabilities of each class

        self.class_prior = [M_classes[i] / M for i in range(len(self.classes_))]


        """
        From Lecture notes :
            (For only 2 classes) ( 0 and 1 )

            mu_0 = 1/M0 * sum(x_i) where y_i = 0
            mu_1 = 1/M1 * sum(x_i) where y_i = 1
          
            sigma_0 = 1/M0 * sum((x_i - mu_0) * (x_i - mu_0)^T) where y_i = 0
            sigma_1 = 1/M1 * sum((x_i - mu_1) * (x_i - mu_1)^T) where y_i = 1
        
            If same covariance matrix is assumed, then 
            sigma = 1/N * sum((x_i - mu_y(i)) * (x_i - mu_y(i))^T) 
        """

        for idx, c in enumerate(self.classes_):
            mu[idx] = np.sum(X[y == c], axis=0) / M_classes[idx] # Mean of each class

        



        if assume_same_covariance:
            
            sigma = np.zeros((X.shape[1], X.shape[1]))

            for idx, c in enumerate(self.classes_):
                sigma += np.sum([np.outer(x - mu[idx], x - mu[idx]) for x in X[y == c]], axis=0)

            sigma /= M



        else :
            
            sigma = np.zeros((len(self.classes_), X.shape[1], X.shape[1]))

            for idx, c in enumerate(self.classes_):
                sigma[idx] = np.sum([np.outer(x - mu[idx], x - mu[idx]) for x in X[y == c]], axis=0) / M_classes[idx]
        

     



        if ( assume_same_covariance ):

            self.sigma = np.array([sigma for _ in range(len(self.classes_))])
            self.mu = mu

            mu_0 = mu[0]
            mu_1 = mu[1]

            return (mu_0, mu_1, sigma)
        
        else :

            self.sigma = sigma
            self.mu = mu
            

            mu_0 = mu[0]
            mu_1 = mu[1]
            sigma_0 = sigma[0]
            sigma_1 = sigma[1]


            return (mu_0, mu_1, sigma_0, sigma_1)
            
            
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize features
        X = self.normalize_features(X, self.mean, self.std)
        
        # Initialize array to store probabilities
        n_samples = X.shape[0]
        probs = np.zeros((n_samples, len(self.classes_)))
        
        # Calculate probability for each class
        for idx, c in enumerate(self.classes_):

            diff = X - self.mu[idx]
            inv_sigma = np.linalg.inv(self.sigma[idx])
            exponent = -0.5 * np.sum((diff @ inv_sigma) * diff,axis=1)
            norm_const = -0.5 * np.log(np.linalg.det(self.sigma[idx]))
            probs[:, idx] = exponent + norm_const + np.log(self.class_prior[idx])
        
        # Return class with highest probability
        return np.argmax(probs, axis=1)
    
    def pdf(self, x, class_idx):
        """
        Calculate the Gaussian probability density function for a given point and class.
        
        Parameters:
        x (np.array): The input point.
        class_idx (int): The class index.
        
        Returns:
        float: The probability density value.
        """

        mu = self.mu[class_idx]
        sigma = self.sigma[class_idx]
        size = len(x)
        
        det = np.linalg.det(sigma)
        norm_const = 1.0/(np.power((2*np.pi), float(size)/2) * np.power(det, 1.0/2))
        x_mu = np.matrix(x - mu)
        inv = np.linalg.inv(sigma)
        result = np.power(np.e, -0.5*( x_mu*inv*x_mu.T ))
        
        return norm_const * result



#!/usr/bin/env python
# coding: utf-8

# In[74]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')

import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis



# In[75]:


def load_data():
    
    X = np.loadtxt("../data/Q4/q4x.dat")
    y = np.loadtxt("../data/Q4/q4y.dat", dtype=str)
    y = np.where(y == 'Canada', 1, 0)
    return X, y



# In[76]:


def plot_data_Initial(X, y):
    
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o")
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x")
    plt.xlabel("x1 (fresh water)")
    plt.ylabel("x2 (marine water)")
    plt.legend()
    plt.title("Initial Data Distribution")
    plt.show()

def plot_data_Norm(X, y):

    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o")
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x")
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend()
    plt.title("Normalized Data Distribution")
    plt.show()

def plot_result(X, y, predictions):
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o", alpha=0.4)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x", alpha=0.4)
    plt.scatter(X[predictions == 0][:, 0], X[predictions == 0][:, 1], label="Predicted Alaska", marker="+", alpha=0.8)
    plt.scatter(X[predictions == 1][:, 0], X[predictions == 1][:, 1], label="Predicted Canada", marker=".", alpha=0.8)
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend()
    plt.title("Predictions vs True Labels")
    plt.show()
    


# In[84]:


plot_data_Initial(*load_data())
plot_data_Norm(*load_data())


# In[ ]:


def plot_theoretical_boundary(model, X, y, title="GDA Decision Boundary"):
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    if X.shape[1] != 2:
        raise ValueError("This function only works for 2D data")
    
    plt.figure(figsize=(8, 8))
    
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o", alpha=1)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x", alpha=1)
    
    # Plot means
    for i, mean in enumerate(model.mu):
        plt.scatter(mean[0], mean[1], c='black', marker='*', s=200, label=f'Mean class {i}')



    # For the line ax + by + c = 0
    inv_sigma = np.linalg.inv(model.sigma[0])
    diff_mu = model.mu[1] - model.mu[0]
    

    w = inv_sigma @ diff_mu
    

    b = -0.5 * (model.mu[1].T @ inv_sigma @ model.mu[1] - model.mu[0].T @ inv_sigma @ model.mu[0])
    
    if model.class_prior is not None:
        b += np.log(model.class_prior[1] / model.class_prior[0])
    

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    

    if abs(w[0]) &lt; 1e-10:
        x_boundary = np.array([b/w[1], b/w[1]])
        y_boundary = np.array([x_min, x_max])
    else:
        x_boundary = np.array([x_min, x_max])
        # y = -(w[0]x + b)/w[1]
        y_boundary = -(w[0] * x_boundary + b) / w[1]
    
    # Plot decision boundary
    plt.plot(x_boundary, y_boundary, 'k-', linewidth=2, 
            label='Theoretical Boundary')
    

    plt.xlabel('x0')
    plt.ylabel('x1')
    plt.title(title)
    plt.legend()
    
  
    plt.xlim(-2, 2)
    plt.ylim(-2, 2)
    

    for i in range(2):
        x = np.linspace(-3, 3, 100)
        y = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x, y)
        Z = np.zeros(X.shape)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match220-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(X.shape[0]):
            for k in range(X.shape[1]):
                Z[j, k] = model.pdf(np.array([X[j, k], Y[j, k]]), i)
</FONT>        plt.contour(X, Y, Z, levels=10, alpha=0.5)
    

    plt.show()


# In[ ]:


def plot_quadratic_boundary(model, X, y, title="GDA Quadratic Decision Boundary"):
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
   
    plt.figure(figsize=(8, 8))
    
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o", alpha=1)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x", alpha=1)
    

    for i, mean in enumerate(model.mu):
        plt.scatter(mean[0], mean[1], c='black', marker='*', s=200, label=f'Mean class {i}')

    

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),np.linspace(y_min, y_max, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    

    def quadratic_score(x, mu0, mu1, sigma0, sigma1, pi0, pi1):
        inv_sigma0 = np.linalg.inv(sigma0)
        inv_sigma1 = np.linalg.inv(sigma1)
        
        diff0 = x - mu0
        diff1 = x - mu1
        
        term0 = -0.5 * diff0 @ inv_sigma0 @ diff0 - 0.5 * np.log(np.linalg.det(sigma0)) + np.log(pi0)
        term1 = -0.5 * diff1 @ inv_sigma1 @ diff1 - 0.5 * np.log(np.linalg.det(sigma1)) + np.log(pi1)
        
        return term1 - term0
    
    quad_scores = np.array([
        quadratic_score(
            x, 
            model.mu[0], 
            model.mu[1], 
            model.sigma[0], 
            model.sigma[1],
            model.class_prior[0],
            model.class_prior[1]
        ) for x in grid_points
    ])
    
    quad_boundary = quad_scores.reshape(xx.shape)
    
    # Plot decision boundary
    plt.contour(xx, yy, quad_boundary, levels=[0], colors='red',linestyles='-', linewidths=2, label='Decision Boundary')


    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.legend()


    plt.xlim(-2, 2)
    plt.ylim(-2, 2)

    for i in range(2):
        x = np.linspace(-3, 3, 100)
        y = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x, y)
        Z = np.zeros(X.shape)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match220-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for j in range(X.shape[0]):
            for k in range(X.shape[1]):
                Z[j, k] = model.pdf(np.array([X[j, k], Y[j, k]]), i)
</FONT>        plt.contour(X, Y, Z, levels=10, alpha=0.5)
    
    plt.show()


# In[ ]:


X,y = load_data()

gda_shared = GaussianDiscriminantAnalysis()

Linear_Model = gda_shared.fit(X, y,True)
print("Same sigma :")
print("m_0=",Linear_Model[0])
print("m_1=",Linear_Model[1])
print("sigma=",Linear_Model[2])


predictions = gda_shared.predict(X)


plot_result(X, y, predictions)


print("Accuracy: ", np.mean(predictions == y))
plot_theoretical_boundary(gda_shared, X, y)



# In[ ]:


X,y = load_data()

gda_separate = GaussianDiscriminantAnalysis()

Normal_Model = gda_separate.fit(X, y,False)
print("Same sigma :")
print("m_0=",Normal_Model[0])
print("m_1=",Normal_Model[1])
print("sigma_0=",Normal_Model[2])
print("sigma_1=",Normal_Model[3])


predictions = gda_separate.predict(X)


plot_result(X, y, predictions)


print("Accuracy: ", np.mean(predictions == y))
plot_quadratic_boundary(gda_separate, X, y)


# In[ ]:


def plot_both_boundaries(model_shared, model_separate, X, y, title="GDA Decision Boundaries"):

    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    if X.shape[1] != 2:
        raise ValueError("This function only works for 2D data")
    
    
    plt.figure(figsize=(16, 16))
    
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Alaska", marker="o", alpha=1)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Canada", marker="x", alpha=1)
    


    
    # Create mesh grid for contour plotting
    # x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    # y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x_min , x_max = -10 , 10
    y_min , y_max = -10 , 10
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                        np.linspace(y_min, y_max, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    # Linear boundary (shared covariance)
    inv_sigma = np.linalg.inv(model_shared.sigma[0])
    diff_mu = model_shared.mu[1] - model_shared.mu[0]
    w = inv_sigma @ diff_mu
    b = -0.5 * (model_shared.mu[1].T @ inv_sigma @ model_shared.mu[1] - 
                model_shared.mu[0].T @ inv_sigma @ model_shared.mu[0])
    
    if model_shared.class_prior is not None:
        b += np.log(model_shared.class_prior[1] / model_shared.class_prior[0])
    
    linear_scores = grid_points @ w + b
    linear_boundary = linear_scores.reshape(xx.shape)
    
    # Quadratic boundary (separate covariances)
    def quadratic_score(x, mu0, mu1, sigma0, sigma1, pi0, pi1):
        inv_sigma0 = np.linalg.inv(sigma0)
        inv_sigma1 = np.linalg.inv(sigma1)
        
        diff0 = x - mu0
        diff1 = x - mu1
        
        term0 = -0.5 * diff0 @ inv_sigma0 @ diff0 - 0.5 * np.log(np.linalg.det(sigma0)) + np.log(pi0)
        term1 = -0.5 * diff1 @ inv_sigma1 @ diff1 - 0.5 * np.log(np.linalg.det(sigma1)) + np.log(pi1)
        
        return term1 - term0
    
    quad_scores = np.array([
        quadratic_score(
            x, 
            model_separate.mu[0], 
            model_separate.mu[1], 
            model_separate.sigma[0], 
            model_separate.sigma[1],
            model_separate.class_prior[0],
            model_separate.class_prior[1]
        ) for x in grid_points
    ])
    
    quad_boundary = quad_scores.reshape(xx.shape)
    

    plt.contour(xx, yy, linear_boundary, levels=[0], colors='black', 
                linestyles='-', linewidths=2, label='Linear Boundary')
    plt.contour(xx, yy, quad_boundary, levels=[0], colors='red', 
                linestyles='--', linewidths=2, label='Quadratic Boundary')
    

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.legend()
    
    plt.show()


# In[83]:


plot_both_boundaries(gda_shared, gda_separate, X, y)


# In[ ]:


predictions = gda_shared.predict(X)
print("Accuracy for shared covariance: ", np.mean(predictions == y))

predictions = gda_separate.predict(X)
print("Accuracy for separate covariance: ", np.mean(predictions == y) )




</PRE>
</PRE>
</BODY>
</HTML>
