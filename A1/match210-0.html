<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_B4G3R.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_B4G3R.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[41]:


# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[101]:


linearX = pd.read_csv('../data/Q1/linearX.csv', header=None).values 
linearY = pd.read_csv('../data/Q1/linearY.csv', header=None).values 


# In[102]:


X = linearX
Y = linearY
Y = Y.flatten()


# In[103]:


def compute_cost(X, Y, theta):
    m = len(Y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)
    return cost


# In[147]:


class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.iterations = 10000
        self.epsilon = 1e-15
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
<A NAME="1"></A><FONT color = #00FF00><A HREF="match210-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        """
        m, n = X.shape
        X = np.c_[np.ones(m), X] 
        self.theta = np.zeros(n + 1)
</FONT>        theta_history = []
        theta_history.append(self.theta.copy())
        J_prev = float('inf')
        
        for _ in range(self.iterations):
            predictions = X.dot(self.theta)
            errors = predictions - y
            gradient = (1 / m) * X.T.dot(errors)
            self.theta -= learning_rate * gradient
            J_curr = compute_cost(X, y, self.theta)
            theta_history.append(self.theta.copy())
            
            if abs(J_prev - J_curr) &lt; self.epsilon:
                break
            J_prev = J_curr
        
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
<A NAME="2"></A><FONT color = #0000FF><A HREF="match210-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        """
        m = X.shape[0]
        X = np.c_[np.ones(m), X]
        return X.dot(self.theta)
</FONT>

# In[148]:


regressor = LinearRegressor()
regressor.fit(X,Y)
theta_history = regressor.fit(X,Y)
print("Learned Parameters (Theta):", regressor.theta)


# In[135]:


print("X shape:", X.shape)  # Should be (n_samples, n_features)
print("Y shape:", Y.shape)  # Should be (n_samples,)
print("List of Parameters shape:", theta_history.shape)  # Should be (n_iter, n_features+1)


# In[136]:


theta_history


# ## 2

# In[144]:


import matplotlib.pyplot as plt

# Predict values for the hypothesis function
X_plot = np.c_[np.ones(X.shape[0]), X]  # Add bias term
Y_pred = X_plot.dot(regressor.theta)

# Plot the data points
plt.scatter(X, Y, color='blue', label='Data Points')

# Plot the learned hypothesis (regression line)
plt.plot(X, Y_pred, color='red', label='Hypothesis Function')

# Labels and title
plt.xlabel("Acidity")
plt.ylabel("Density")
plt.title("Linear Regression Fit")
plt.legend()
plt.show()


# In[138]:


m = len(Y)
X_bias = np.c_[np.ones(m), X]


# In[139]:


def J(theta):
    m = len(Y)
    predictions = X_bias.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)
    return cost


# ## 3a

# In[196]:


from matplotlib import cm
T0, T1 = np.mgrid[0:12:500j, 0:60:500j]
mesh = np.c_[T0.flatten(), T1.flatten()]

# Compute J_values for the grid
J_values = (
    np.array([J(point) for point in mesh])
    .reshape(500, 500)
)

plt.ion()

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(T0, T1, J_values, cmap=cm.RdBu_r)
ax.set_xlabel(r'$\theta_0$', labelpad=10)
ax.set_ylabel(r'$\theta_1$', labelpad=10)
ax.set_zlabel(r'$J(\theta)$', labelpad=10)

plt.show()


# In[153]:


from matplotlib import cm
T0, T1 = np.mgrid[2:10:500j, 25:33:500j]
mesh = np.c_[T0.flatten(), T1.flatten()]

# Compute J_values for the grid
J_values = (
    np.array([J(point) for point in mesh])
    .reshape(500, 500)
)

plt.ion()

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(T0, T1, J_values, cmap=cm.RdBu_r)
ax.set_xlabel(r'$\theta_0$', labelpad=10)
ax.set_ylabel(r'$\theta_1$', labelpad=10)
ax.set_zlabel(r'$J(\theta)$', labelpad=10)

plt.show()


# ## 3b

# In[195]:


from matplotlib import cm
T0, T1 = np.mgrid[0:12:500j, 0:60:500j]
mesh = np.c_[T0.flatten(), T1.flatten()]

# Compute J_values for the grid
J_values = np.array([J(point) for point in mesh]).reshape(500, 500)

plt.ion()

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(T0, T1, J_values, cmap=cm.RdBu_r, alpha = 0.5)
ax.set_xlabel(r'$\theta_0$', labelpad=10)
ax.set_ylabel(r'$\theta_1$', labelpad=10)
ax.set_zlabel(r'$J(\theta)$', labelpad=10)

# Extract values for the gradient descent path
theta_0_vals = theta_history[:, 0]
theta_1_vals = theta_history[:, 1]
J_vals = np.array([J(theta) for theta in theta_history])
print(J_vals)

# Plot gradient descent path
# ax.scatter(theta_0_vals, theta_1_vals, J_vals, color='black', s=0.2, label='GD Steps')
ax.plot(theta_0_vals, theta_1_vals, J_vals, linestyle='-',
                    color='r', marker='o', markersize=2.5, label='GD Path')

ax.legend()
plt.show()


# ## 4
# 

# In[250]:


def plot_contours(theta_history=None):
    T0, T1 = np.mgrid[0:12:50j, 0:60:50j]
    mesh = np.c_[T0.flatten(), T1.flatten()]

    J_values = (
        np.array([J(point) for point in mesh])
        .reshape(T0.shape)
    )

    plt.ion()
    plt.contour(T0, T1, J_values,30, colors="k")
    plt.xlabel(r'$\theta_0$', labelpad=10)
    plt.ylabel(r'$\theta_1$', labelpad=10)



    if theta_history is not None:
        for theta in theta_history:
            plt.plot([theta[0]], [theta[1]], linestyle='-',
                     color='r', marker='o', markersize=2.5)

    # plt.show()
    return plt


# In[246]:


plot_contours(theta_history)


# ## 5

# In[252]:


for eta in [0.001, 0.025, 0.1]:
    print("\n --- \n Eta: %.2f \n --- \n" % eta)

    theta_history = regressor.fit(X,Y,eta)

    plt = plot_contours(theta_history)
    plt.title(r"$Contours (\eta=$" + str(eta) + ")")

    plt.show()
    plt.close()





#!/usr/bin/env python
# coding: utf-8

# In[74]:


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    # Generate x1 and x2 independently
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    
    # Add intercept term (x0 = 1)
    X_intercept = np.hstack((np.ones((N, 1)), X))  # Shape (N, 3)
    
    # Generate noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    
    # Compute target variable y
    y = X_intercept @ theta + noise  # Matrix multiplication
    
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.epochs = 10000
        self.epsilon = 1e-5
    
    def fit(self, X, y, learning_rate=0.01,batch_size=32):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        N, d = X.shape
        X_intercept = np.hstack((np.ones((N, 1)), X))
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match210-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.theta = np.zeros(d + 1) 
        
        history = [] 
        prev_loss = float('inf')
        
        for epoch in range(self.epochs):
            indices = np.random.permutation(N) 
</FONT>            X_shuffled = X_intercept[indices]
            y_shuffled = y[indices]
            
            loss_sum = 0
            for i in range(0, N, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                # Compute gradient
                gradient = np.matmul(X_batch.T, np.matmul(X_batch, self.theta) - y_batch) / X_batch.shape[0]
                
                # Update parameters
                self.theta -= learning_rate * gradient
                history.append(self.theta.copy())
                
                # Compute batch loss
                loss_sum += np.mean((y_batch - X_batch @ self.theta) ** 2)
            
            avg_loss = loss_sum / (N // batch_size)
            # history.append(self.theta.copy())
            
            # Check convergence criteria
            if abs(avg_loss - prev_loss) &lt;= self.epsilon:
                print(f'Converged after {epoch} epochs')
                break
            prev_loss = avg_loss
        
        return np.array(history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        N = X.shape[0]
        X_intercept = np.hstack((np.ones((N, 1)), X)) 
        return X_intercept @ self.theta


# ## 1
# 

# In[53]:


# Sample 1 million data points
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([4, 4])
noise_sigma = np.sqrt(2)

X, Y = generate(1000000, theta, input_mean, input_sigma, noise_sigma)

# Split data into 80% training and 20% testing
train_size = int(0.8 * 1000000)
X_train, Y_train = X[:train_size], Y[:train_size]
X_test, Y_test = X[train_size:], Y[train_size:]


# In[36]:


pd.DataFrame(X_train).to_csv("generated_train_x.csv", index=False, header=False)
pd.DataFrame(Y_train).to_csv("generated_train_y.csv", index=False, header=False)
pd.DataFrame(X_test).to_csv("generated_test_x.csv", index=False, header=False)
pd.DataFrame(Y_test).to_csv("generated_test_y.csv", index=False, header=False)


# ## 2
# 

# In[75]:


# Train models with different batch sizes
batch_sizes = [1, 80, 8000, 800000]
learned_thetas = {}
histories = {}


# In[56]:


import time


# In[76]:


for batch_size in batch_sizes:
    model = StochasticLinearRegressor()
    start = time.time()
    history = model.fit(X_train, Y_train, learning_rate=0.001, batch_size=batch_size)
    end = time.time()
    elapsed_time = end - start
    learned_thetas[batch_size] = history[-1]  # Store final learned theta
    histories[batch_size] = history
    print(f"Learned θ for batch size {batch_size}: {history[-1]}")
    print(f"Time taken for batch size {batch_size}: {elapsed_time:.4f} seconds")


# ## 3b

# In[63]:


# Compute Closed Form Solution
def closed_form_solution(X, y):
    X_intercept = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term
    return np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ y

closed_form_theta = closed_form_solution(X_train, Y_train)
print(f"Closed-form solution for θ: {closed_form_theta}")


# ## 4

# In[64]:


# Compute Mean Squared Errors for all learned thetas
mse_results = {}

for batch_size, theta in learned_thetas.items():
    train_predictions = np.hstack((np.ones((X_train.shape[0], 1)), X_train)) @ theta
    test_predictions = np.hstack((np.ones((X_test.shape[0], 1)), X_test)) @ theta
    
    train_error = np.mean((Y_train - train_predictions) ** 2)
    test_error = np.mean((Y_test - test_predictions) ** 2)
    
    mse_results[batch_size] = (train_error, test_error)
    print(f"Batch Size {batch_size}: Training Error = {train_error}, Test Error = {test_error}")

# Compute Mean Squared Error for Closed-form Solution
train_predictions_closed = np.hstack((np.ones((X_train.shape[0], 1)), X_train)) @ closed_form_theta
test_predictions_closed = np.hstack((np.ones((X_test.shape[0], 1)), X_test)) @ closed_form_theta

train_error_closed = np.mean((Y_train - train_predictions_closed) ** 2)
test_error_closed = np.mean((Y_test - test_predictions_closed) ** 2)

print(f"Closed-form Solution: Training Error = {train_error_closed}, Test Error = {test_error_closed}")


# In[65]:


import matplotlib.pyplot as plt


# In[77]:


# Plot movement of θ for each batch size separately
for i, batch_size in enumerate(batch_sizes):
    fig = plt.figure(figsize=(6, 6))
    ax = fig.add_subplot(111, projection='3d')
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match210-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    history = np.array(histories[batch_size])
    ax.plot(history[:, 0], history[:, 1], history[:, 2], label=f"Batch size {batch_size}")
    ax.scatter(closed_form_theta[0], closed_form_theta[1], closed_form_theta[2], color='k', marker='*', s=100, label="Closed-form Solution")
</FONT>    ax.set_xlabel("Theta 0")
    ax.set_ylabel("Theta 1")
    ax.set_zlabel("Theta 2")
    ax.set_title(f"Movement of Theta for Batch Size {batch_size}")
    ax.legend()
    plt.show()





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    # Generate x1 and x2 independently
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    
    # Add intercept term (x0 = 1)
    X_intercept = np.hstack((np.ones((N, 1)), X))  # Shape (N, 3)
    
    # Generate noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    
    # Compute target variable y
    y = X_intercept @ theta + noise  # Matrix multiplication
    
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.epochs = 1000
        self.batch_size = 16
        self.epsilon = 1e-6
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        N, d = X.shape
        X_intercept = np.hstack((np.ones((N, 1)), X))
        self.theta = np.zeros(d + 1) 
        
        history = [] 
        prev_loss = float('inf')
        
        for epoch in range(self.epochs):
            indices = np.random.permutation(N) 
            X_shuffled = X_intercept[indices]
            y_shuffled = y[indices]
            
            loss_sum = 0
            for i in range(0, N, self.batch_size):
                X_batch = X_shuffled[i:i+self.batch_size]
                y_batch = y_shuffled[i:i+self.batch_size]
                
                # Compute gradient
                gradient = -2 / self.batch_size * X_batch.T @ (y_batch - X_batch @ self.theta)
                
                # Update parameters
                self.theta -= learning_rate * gradient
                
                # Compute batch loss
                loss_sum += np.mean((y_batch - X_batch @ self.theta) ** 2)
            
            avg_loss = loss_sum / (N // self.batch_size)
            history.append(self.theta.copy())
            
            # Check convergence criteria
            if abs(avg_loss - prev_loss) &lt;= self.epsilon:
                break
            prev_loss = avg_loss
        
        return np.array(history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        N = X.shape[0]
        X_intercept = np.hstack((np.ones((N, 1)), X)) 
        return X_intercept @ self.theta




# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
    
    def _normalize(self, X):
        """Normalize features using mean and standard deviation."""
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        return (X - mean) / std
    
    def _sigmoid(self, z):
        """Compute the sigmoid function."""
        return 1 / (1 + np.exp(-z))
    
    def _compute_gradient(self, X, y, h):
        """Compute the gradient of the log-likelihood function."""
        return X.T @ (h - y)
    
    def _compute_hessian(self, X, h):
        """Compute the Hessian matrix of the log-likelihood function."""
        S = np.diag(h * (1 - h))  # Diagonal matrix of weights
        return X.T @ S @ X
    
    def fit(self, X, y, learning_rate=0.01, max_iter=100, tol=1e-6):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = self._normalize(X)
        n_samples, n_features = X.shape
        X = np.c_[np.ones(n_samples), X]  # Add bias term
        self.theta = np.zeros(X.shape[1])
        theta_list = []
        
        for _ in range(max_iter):
            z = X @ self.theta
            h = self._sigmoid(z)
            gradient = self._compute_gradient(X, y, h)
            hessian = self._compute_hessian(X, h)
            
            # Newton's update step: theta = theta - H^(-1) * gradient
            theta_update = np.linalg.inv(hessian) @ gradient
            self.theta -= theta_update
            theta_list.append(self.theta.copy())
            
            # Convergence check
            if np.linalg.norm(theta_update) &lt; tol:
                break
        
        return np.array(theta_list)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self._normalize(X)
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        probabilities = self._sigmoid(X @ self.theta)
        return (probabilities &gt;= 0.5).astype(int)



#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
    
    def _normalize(self, X):
        """Normalize features using mean and standard deviation."""
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        return (X - mean) / std
    
    def _sigmoid(self, z):
        """Compute the sigmoid function."""
        return 1 / (1 + np.exp(-z))
    
    def _compute_gradient(self, X, y, h):
        """Compute the gradient of the log-likelihood function."""
        return X.T @ (h - y)
    
    def _compute_hessian(self, X, h):
        """Compute the Hessian matrix of the log-likelihood function."""
        S = np.diag(h * (1 - h))  # Diagonal matrix of weights
        return X.T @ S @ X
    
    def fit(self, X, y, learning_rate=0.01, max_iter=100, tol=1e-6):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X = self._normalize(X)
        n_samples, n_features = X.shape
        X = np.c_[np.ones(n_samples), X]  # Add bias term
        self.theta = np.zeros(X.shape[1])
        theta_list = []
        
        for _ in range(max_iter):
            z = X @ self.theta
            h = self._sigmoid(z)
            gradient = self._compute_gradient(X, y, h)
            hessian = self._compute_hessian(X, h)
            
            # Newton's update step: theta = theta - H^(-1) * gradient
            theta_update = np.linalg.inv(hessian) @ gradient
            self.theta -= theta_update
            theta_list.append(self.theta.copy())
            
            # Convergence check
            if np.linalg.norm(theta_update) &lt; tol:
                break
        
        return np.array(theta_list)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self._normalize(X)
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        probabilities = self._sigmoid(X @ self.theta)
        return (probabilities &gt;= 0.5).astype(int)


# In[2]:


# Load data and compute theta values
X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')

model = LogisticRegressor()
theta_values = model.fit(X, y)
print("Final theta values:", model.theta)


# In[3]:


# Split data into 80% training and 20% testing
train_size = int(0.8 * X.shape[0])
X_train, Y_train = X[:train_size], y[:train_size]
X_test, Y_test = X[train_size:], y[train_size:]


# In[4]:


import matplotlib.pyplot as plt


# In[5]:


# Plot training data
plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label='Class 0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label='Class 1')

# Plot decision boundary
x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
x2_vals = -(model.theta[0] + model.theta[1] * x1_vals) / model.theta[2]
plt.plot(x1_vals, x2_vals, 'r-', label='Decision Boundary')

plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
plt.show()





# Imports -do not import any other libraries other than these
import numpy as np
import pandas as pd
from scipy.stats import multivariate_normal

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None  # Class prior
        
    def normalize(self, X, use_stored_params=False):
        """
        Normalize the dataset X to have zero mean and unit variance.
        If use_stored_params is True, use the stored training mean and std.
        """
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        # Normalize input data
        X = self.normalize(X)
        
        # Compute class priors
        self.phi = np.mean(y)
        
        # Compute class-wise means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        # Compute class-wise covariance matrices
        if assume_same_covariance:
            n_0 = np.sum(y == 0)
            n_1 = np.sum(y == 1)
            sigma_0 = np.cov(X[y == 0].T, bias=True)
            sigma_1 = np.cov(X[y == 1].T, bias=True)
            self.sigma = (n_0 * sigma_0 + n_1 * sigma_1) / (n_0 + n_1)
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.cov(X[y == 0].T, bias=True)
            self.sigma_1 = np.cov(X[y == 1].T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
<A NAME="0"></A><FONT color = #FF0000><A HREF="match210-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X = self.normalize(X)  # Normalize input data
        
        if self.sigma is not None:
            # If shared covariance is used
            inv_sigma = np.linalg.inv(self.sigma)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma * (X - self.mu_0), axis=1)
</FONT><A NAME="5"></A><FONT color = #FF0000><A HREF="match210-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma * (X - self.mu_1), axis=1)
        else:
            # If separate covariance matrices are used
            term_0 = multivariate_normal.logpdf(X, mean=self.mu_0, cov=self.sigma_0)
</FONT>            term_1 = multivariate_normal.logpdf(X, mean=self.mu_1, cov=self.sigma_1)
        # Compute log posterior probability
        log_posterior_0 = term_0 + np.log(1 - self.phi)
        log_posterior_1 = term_1 + np.log(self.phi)
        
        # Predict class based on higher posterior probability
        return (log_posterior_1 &gt; log_posterior_0).astype(int)




#!/usr/bin/env python
# coding: utf-8

# In[26]:


# Imports -do not import any other libraries other than these
import numpy as np
import pandas as pd
from scipy.stats import multivariate_normal

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None  # Class prior
        
    def normalize(self, X, use_stored_params=False):
        """
        Normalize the dataset X to have zero mean and unit variance.
        If use_stored_params is True, use the stored training mean and std.
        """
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        # Normalize input data
        X = self.normalize(X)
        
        # Compute class priors
        self.phi = np.mean(y)
        
        # Compute class-wise means
        self.mu_0 = np.mean(X[y == 0], axis=0)
        self.mu_1 = np.mean(X[y == 1], axis=0)
        
        # Compute class-wise covariance matrices
        if assume_same_covariance:
            n_0 = np.sum(y == 0)
            n_1 = np.sum(y == 1)
            sigma_0 = np.cov(X[y == 0].T, bias=True)
            sigma_1 = np.cov(X[y == 1].T, bias=True)
            self.sigma = (n_0 * sigma_0 + n_1 * sigma_1) / (n_0 + n_1)
            return self.mu_0, self.mu_1, self.sigma
        else:
            self.sigma_0 = np.cov(X[y == 0].T, bias=True)
            self.sigma_1 = np.cov(X[y == 1].T, bias=True)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)  # Normalize input data
        
        if self.sigma is not None:
            # If shared covariance is used
            inv_sigma = np.linalg.inv(self.sigma)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma * (X - self.mu_0), axis=1)
            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma * (X - self.mu_1), axis=1)
        else:
            # If separate covariance matrices are used
            term_0 = multivariate_normal.logpdf(X, mean=self.mu_0, cov=self.sigma_0)
            term_1 = multivariate_normal.logpdf(X, mean=self.mu_1, cov=self.sigma_1)
        # Compute log posterior probability
        log_posterior_0 = term_0 + np.log(1 - self.phi)
        log_posterior_1 = term_1 + np.log(self.phi)
        
        # Predict class based on higher posterior probability
        return (log_posterior_1 &gt; log_posterior_0).astype(int)


# In[27]:


# Load data
<A NAME="6"></A><FONT color = #00FF00><A HREF="match210-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = np.loadtxt("../data/Q4/q4x.dat")
y = np.loadtxt("../data/Q4/q4y.dat", dtype=str)

# Convert target labels to numerical values (Alaska -&gt; 0, Canada -&gt; 1)
y = np.where(y == "Alaska", 0, 1)


# In[28]:


# Instantiate and train model
gda_linear = GaussianDiscriminantAnalysis()
params_linear = gda_linear.fit(X, y, assume_same_covariance=True)
</FONT>
# Print results
print("phi:", gda_linear.phi)
print("mu_0:", gda_linear.mu_0.T)
print("mu_1:", gda_linear.mu_1.T)
print("sigma:", gda_linear.sigma)


# In[29]:


gda_quad = GaussianDiscriminantAnalysis()
params_quad = gda_quad.fit(X, y, assume_same_covariance=False)

# Print results
print("phi:", gda_quad.phi)
print("mu_0:", gda_quad.mu_0.T)
print("mu_1:", gda_quad.mu_1.T)
print("sigma_0:", gda_quad.sigma_0)
print("sigma_1:", gda_quad.sigma_1)


# In[30]:


import matplotlib.pyplot as plt


# In[31]:


# Normalize data for plotting
X = gda_linear.normalize(X)


# In[32]:


# Plot training data

plt.figure(figsize=(8, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska', color='green')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada', color='blue')
plt.xlabel("x1 (Freshwater Growth Diameter)")
plt.ylabel("x2 (Marine Water Growth Diameter)")
plt.title("Training Data Plot")
plt.legend()
plt.show()


# In[33]:


# Plot training data along with decision boundaries

# Plot the training data
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Alaska')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Canada')

# Plot decision boundaries
x_min, x_max = X[:, 0].min(), X[:, 0].max()
y_min, y_max = X[:, 1].min(), X[:, 1].max()
x_vals = np.linspace(x_min, x_max, 1000)
y_vals = np.linspace(y_min, y_max, 1000)
X_grid, Y_grid = np.meshgrid(x_vals, y_vals)

# Linear decision boundary (shared covariance)
Z_linear = gda_linear.predict(np.c_[X_grid.ravel(), Y_grid.ravel()])
Z_linear = Z_linear.reshape(X_grid.shape)
plt.contour(X_grid, Y_grid, Z_linear, levels=[0.5], colors='black', linestyles='-', label='Linear Boundary')

# Quadratic decision boundary (separate covariances)
Z_quad = gda_quad.predict(np.c_[X_grid.ravel(), Y_grid.ravel()])
Z_quad = Z_quad.reshape(X_grid.shape)
plt.contour(X_grid, Y_grid, Z_quad, levels=[0.5], colors='red', linestyles='-', label='Quadratic Boundary')

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.title("GDA Decision Boundaries")
plt.show()




</PRE>
</PRE>
</BODY>
</HTML>
