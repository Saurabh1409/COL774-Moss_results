<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_83PSN.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_EQGVX.py<p><PRE>


#!/usr/bin/env python
# coding: utf-8

# In[4]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[5]:


X = np.loadtxt("q4x.dat")
Y_data = np.loadtxt("q4y.dat", dtype=str)
mean = np.mean(X,axis=0)
std = np.std(X,axis=0)
X_norm = (X-mean)/std
Y = np.where(Y_data == "Alaska",0,1)
phi = np.mean(Y)
mu_0 = np.mean(X_norm[Y==0],axis=0).reshape(-1,1)
mu_1 = np.mean(X_norm[Y==1],axis=0).reshape(-1,1)
sigma = np.zeros((2,2))
m = len(X)
for i in range(m):
	x_i = X_norm[i].reshape(-1,1)
	mu_i = mu_0 if(Y[i] == 0) else mu_1
	sigma += (x_i-mu_i)@((x_i-mu_i).T)
sigma /= m


# In[6]:


print(m,phi,mu_0,mu_1,sigma)


# In[7]:


Y_1 = Y[Y == 1]
Y_0 = Y[Y == 0]
pi_0 = len(Y_0)/len(Y)
pi_1 = len(Y_1)/len(Y)


# In[8]:


sigma_inv = np.linalg.inv(sigma)
w = sigma_inv @ (mu_1 - mu_0)
b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0) + np.log(pi_1/pi_0)

x_vals = np.linspace(min(X_norm[:, 0]), max(X_norm[:, 0]), 100).reshape(-1,1)
y_vals = -(w[0] * x_vals + b) / w[1]

# print(x_vals.shape,y_vals.shape)
plt.figure(figsize=(8, 6))
plt.scatter(X_norm[Y == 0, 0], X_norm[Y == 0, 1], marker='o', label='Alaska')
plt.scatter(X_norm[Y == 1, 0], X_norm[Y == 1, 1], marker='x', label='Canada')
plt.plot(x_vals, y_vals, 'r-', label='Decision Boundary')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Scatter Plot of Training Data with Decision Boundary")
plt.show()


# In[9]:


X = np.loadtxt("q4x.dat")
Y_data = np.loadtxt("q4y.dat", dtype=str)
mean = np.mean(X,axis=0)
std = np.std(X,axis=0)
X_norm = (X-mean)/std
Y = np.where(Y_data == "Alaska",0,1)
phi = np.mean(Y)
mu_0 = np.mean(X_norm[Y==0],axis=0).reshape(-1,1)
mu_1 = np.mean(X_norm[Y==1],axis=0).reshape(-1,1)
sigma_0 = np.zeros((2,2))
sigma_1 = np.zeros((2,2))
m = len(X)
for i in range(m):
	x_i = X_norm[i].reshape(-1,1)
	mu_i = mu_0 if(Y[i] == 0) else mu_1
	if(Y[i] == 0):
		sigma_0 += (x_i-mu_i)@((x_i-mu_i).T)
	else:
		sigma_1 += (x_i-mu_i)@((x_i-mu_i).T)
m_0 = np.sum(Y == 0)
m_1 = np.sum(Y == 1)
sigma_0 /= m_0
sigma_1 /= m_1

def decision_boundary(x1, x2, mu_0, mu_1, sigma_0, sigma_1, phi):
    inv_sigma_0 = np.linalg.inv(sigma_0)
    inv_sigma_1 = np.linalg.inv(sigma_1)
    term_0 = np.array([x1, x2]) - mu_0.flatten()
    term_1 = np.array([x1, x2]) - mu_1.flatten()

    quad_0 = term_0.T @ inv_sigma_0 @ term_0 + np.log(np.linalg.det(sigma_0))
    quad_1 = term_1.T @ inv_sigma_1 @ term_1 + np.log(np.linalg.det(sigma_1))
    
    return quad_0 - quad_1 - 2 * np.log((1 - phi) / phi)



x1_vals = np.linspace(-2, 2, 100)
x2_vals = np.linspace(-2, 2, 100)
X1, X2 = np.meshgrid(x1_vals, x2_vals)


Z = np.array([[decision_boundary(x1, x2, mu_0, mu_1, sigma_0, sigma_1, phi)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match236-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

               for x1 in x1_vals] for x2 in x2_vals])

sigma_inv = np.linalg.inv(sigma)
w = sigma_inv @ (mu_1 - mu_0)
b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0) + np.log(pi_1/pi_0)
</FONT>
x_vals = np.linspace(min(X_norm[:, 0]), max(X_norm[:, 0]), 100).reshape(-1,1)
y_vals = -(w[0] * x_vals + b) / w[1]

# print(x_vals.shape,y_vals.shape)
plt.figure(figsize=(8, 6))
plt.scatter(X_norm[Y == 0, 0], X_norm[Y == 0, 1], marker='o', label='Alaska')
plt.scatter(X_norm[Y == 1, 0], X_norm[Y == 1, 1], marker='x', label='Canada')
plt.plot(x_vals, y_vals, 'r-', label='Linear Decision Boundary')
plt.contour(X1, X2, Z, levels=[0], colors='b',label='Quadratic Boundary (QDA)')
plt.xlabel("Feature 1 (Normalized)")
plt.ylabel("Feature 2 (Normalized)")
plt.legend()
plt.title("Scatter Plot of Training Data with Decision Boundary")
plt.show()


# In[10]:


print(mu_0,mu_1,sigma_0,sigma_1)





#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import csv,pandas as pd,numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from IPython.display import display, clear_output


# In[43]:


<A NAME="2"></A><FONT color = #0000FF><A HREF="match236-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import csv,pandas as pd,numpy as np
import matplotlib.pyplot as plt

X = pd.read_csv('linearX.csv',header=None)
Y = pd.read_csv('linearY.csv',header=None)
X = np.array(X)  
</FONT>X_train  = np.hstack((np.ones((X.shape[0],1)),X))
Y_train = np.array(Y)
n,m = X_train.shape

def grad(X_batch,Y_batch,Y_pred):
	m_ = Y_batch.shape[0]
	return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(Y_batch,Y_pred):
	error = Y_batch-Y_pred
	return float(np.sum(error ** 2))
num_epoch = 100

lr, e = 0.01,0.0001 
params = []
W = np.zeros((X_train.shape[1],1))
epoch = 0
prev_cost,curr_cost=0,float('inf')
while(abs(prev_cost-curr_cost)&gt;e):
	train_loss = 0
	Y_pred = np.dot(X_train,W)
	dW = grad(X_train,Y_train,Y_pred)
	train_loss += compute_loss(Y_train,Y_pred)
	W -= lr*dW
	train_loss /= len(X_train)
	prev_cost = curr_cost
	curr_cost = train_loss
	epoch+=1	
	print(f"Epoch {epoch + 1}/{num_epoch}: Train Loss = {(train_loss):.4f}, {prev_cost-curr_cost}.{W}")
	params.append(W.copy())
	


# In[27]:


params


# In[ ]:





# In[4]:


Y_pred = X_train@W
plt.scatter(X_train[:,1], Y_train, color='blue', label='Training Data')
plt.scatter(X_train[:,1],Y_pred,color='red',label='Test Data')
# plt.scatter(X_val[:,1],Y_pred,color='green',label='Pred Data')


# In[11]:


def loss(X, Y, W):
    m = X.shape[0]
    error = X @ W - Y
    return (1 / (2 * m)) * np.sum(error ** 2)

theta0_vals = np.linspace(-40, 60, 100)
theta1_vals = np.linspace(-30, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = compute_loss(X_train, Y, W_temp)

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match236-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis')
</FONT>
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Loss J(θ)')
ax.set_title('3D Error Surface')

plt.show()


# In[28]:


params


# In[37]:


import time
from IPython.display import display, clear_output


def loss(X, Y, W): 
    m = X.shape[0]
    error = X @ W - Y
    return (1 / (2 * m)) * np.sum(error ** 2)

<A NAME="0"></A><FONT color = #FF0000><A HREF="match236-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

theta0_vals = np.linspace(0, 10, 100)
theta1_vals = np.linspace(0, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
</FONT>        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = loss(X_train, Y, W_temp)

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis', alpha=0.8)

ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Loss J(θ)')
ax.set_title('3D Error Surface')
plt.ion()
for i,W1 in enumerate(params):
    clear_output(wait=True)

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis', alpha=0.8)

    J = loss(X_train, Y, W1)

    ax.scatter(W1[0, 0], W1[1, 0], J, color='red', s=50)

    # Labels
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Loss J(θ)')
    ax.set_title(f'3D Error Surface (Iteration {i + 1})')

    display(fig)
    plt.pause(0.2)
    plt.close(fig)



# In[ ]:


theta0_vals = np.linspace(-40, 60, 100)
theta1_vals = np.linspace(-30, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = loss(X_train, Y, W_temp)

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

theta0_history = []
theta1_history = []

for W1 in params:
    theta0_history.append(W1[0, 0])
    theta1_history.append(W1[1, 0])

    clear_output(wait=True) 

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contourf(theta0_vals, theta1_vals, J_vals.T, levels=50, cmap='viridis')
    plt.colorbar(contour)

    ax.plot(theta0_history, theta1_history, 'r-', linewidth=1, label="Gradient Descent Path")

    ax.scatter(theta0_history[-1], theta1_history[-1], color='red', s=50)

    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_title(f'Contour Plot of Loss Function (Iteration {len(theta0_history)})')
    ax.legend()

    display(fig) 
    plt.pause(0.2)
    plt.close(fig)


# In[ ]:


import csv,pandas as pd,numpy as np
import matplotlib.pyplot as plt

X = pd.read_csv('linearX.csv',header=None)
Y = pd.read_csv('linearY.csv',header=None)
X = np.array(X)  
X_train  = np.hstack((np.ones((X.shape[0],1)),X))
Y_train = np.array(Y)
n,m = X_train.shape

def grad(X_batch,Y_batch,Y_pred):
	m_ = Y_batch.shape[0]
	return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(Y_batch,Y_pred):
	error = Y_batch-Y_pred
	return float(np.sum(error ** 2))
num_epoch = 100

lr, e = 0.1,0.0001 
params = []
W = np.zeros((X_train.shape[1],1))
epoch = 0
prev_cost,curr_cost=0,float('inf')
while(abs(prev_cost-curr_cost)&gt;e):
	train_loss = 0
	Y_pred = np.dot(X_train,W)
	dW = grad(X_train,Y_train,Y_pred)
	train_loss += compute_loss(Y_train,Y_pred)
	W -= lr*dW
	train_loss /= len(X_train)
	prev_cost = curr_cost
	curr_cost = train_loss
	epoch+=1	
	print(f"Epoch {epoch + 1}/{num_epoch}: Train Loss = {(train_loss):.4f}, {prev_cost-curr_cost}.{W}")
	params.append(W.copy())
	
theta0_vals = np.linspace(-40, 60, 100)
theta1_vals = np.linspace(-30, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = loss(X_train, Y, W_temp)

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

theta0_history = []
theta1_history = []

for W1 in params:
    theta0_history.append(W1[0, 0])
    theta1_history.append(W1[1, 0])

    clear_output(wait=True)  

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contourf(theta0_vals, theta1_vals, J_vals.T, levels=50, cmap='viridis')
    plt.colorbar(contour)

    ax.plot(theta0_history, theta1_history, 'r-', linewidth=1, label="Gradient Descent Path")

    ax.scatter(theta0_history[-1], theta1_history[-1], color='red', s=50)

    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_title(f'Contour Plot of Loss Function (Iteration {len(theta0_history)})')
    ax.legend()

    display(fig) 
    plt.pause(0.2) 
    plt.close(fig)


# In[ ]:


import csv,pandas as pd,numpy as np
import matplotlib.pyplot as plt

X = pd.read_csv('linearX.csv',header=None)
Y = pd.read_csv('linearY.csv',header=None)
X = np.array(X)  
X_train  = np.hstack((np.ones((X.shape[0],1)),X))
Y_train = np.array(Y)
n,m = X_train.shape

def grad(X_batch,Y_batch,Y_pred):
	m_ = Y_batch.shape[0]
	return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(Y_batch,Y_pred):
	error = Y_batch-Y_pred
	return float(np.sum(error ** 2))
num_epoch = 100

lr, e = 0.025,0.0001 
params = []
W = np.zeros((X_train.shape[1],1))
epoch = 0
prev_cost,curr_cost=0,float('inf')
while(abs(prev_cost-curr_cost)&gt;e):
	train_loss = 0
	Y_pred = np.dot(X_train,W)
	dW = grad(X_train,Y_train,Y_pred)
	train_loss += compute_loss(Y_train,Y_pred)
	W -= lr*dW
	train_loss /= len(X_train)
	prev_cost = curr_cost
	curr_cost = train_loss
	epoch+=1	
	print(f"Epoch {epoch + 1}/{num_epoch}: Train Loss = {(train_loss):.4f}, {prev_cost-curr_cost}.{W}")
	params.append(W.copy())
	
theta0_vals = np.linspace(-40, 60, 100)
theta1_vals = np.linspace(-30, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = loss(X_train, Y, W_temp)

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

theta0_history = []
theta1_history = []

for W1 in params:
    theta0_history.append(W1[0, 0])
    theta1_history.append(W1[1, 0])

    clear_output(wait=True)

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contourf(theta0_vals, theta1_vals, J_vals.T, levels=50, cmap='viridis')
    plt.colorbar(contour)

    ax.plot(theta0_history, theta1_history, 'r-', linewidth=1, label="Gradient Descent Path")

    ax.scatter(theta0_history[-1], theta1_history[-1], color='red', s=50)

    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_title(f'Contour Plot of Loss Function (Iteration {len(theta0_history)})')
    ax.legend()

    display(fig) 
    plt.pause(0.2)
    plt.close(fig) 


# In[ ]:


import csv,pandas as pd,numpy as np
import matplotlib.pyplot as plt

X = pd.read_csv('linearX.csv',header=None)
Y = pd.read_csv('linearY.csv',header=None)
X = np.array(X)  
X_train  = np.hstack((np.ones((X.shape[0],1)),X))
Y_train = np.array(Y)
n,m = X_train.shape

def grad(X_batch,Y_batch,Y_pred):
	m_ = Y_batch.shape[0]
	return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(Y_batch,Y_pred):
	error = Y_batch-Y_pred
	return float(np.sum(error ** 2))
num_epoch = 100

lr, e = 0.001,0.0001 
params = []
W = np.zeros((X_train.shape[1],1))
epoch = 0
prev_cost,curr_cost=0,float('inf')
while(abs(prev_cost-curr_cost)&gt;e):
	train_loss = 0
	Y_pred = np.dot(X_train,W)
	dW = grad(X_train,Y_train,Y_pred)
	train_loss += compute_loss(Y_train,Y_pred)
	W -= lr*dW
	train_loss /= len(X_train)
	prev_cost = curr_cost
	curr_cost = train_loss
	epoch+=1	
	print(f"Epoch {epoch + 1}/{num_epoch}: Train Loss = {(train_loss):.4f}, {prev_cost-curr_cost}.{W}")
	params.append(W.copy())
	
theta0_vals = np.linspace(-40, 60, 100)
theta1_vals = np.linspace(-30, 30, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        W_temp = np.array([[theta0], [theta1]])
        J_vals[i, j] = loss(X_train, Y, W_temp)

theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

theta0_history = []
theta1_history = []

for W1 in params:
    theta0_history.append(W1[0, 0])
    theta1_history.append(W1[1, 0])

    clear_output(wait=True)

    fig, ax = plt.subplots(figsize=(8, 6))

    contour = ax.contourf(theta0_vals, theta1_vals, J_vals.T, levels=50, cmap='viridis')
    plt.colorbar(contour)

    ax.plot(theta0_history, theta1_history, 'r-', linewidth=1, label="Gradient Descent Path")

    ax.scatter(theta0_history[-1], theta1_history[-1], color='red', s=50)

    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_title(f'Contour Plot of Loss Function (Iteration {len(theta0_history)})')
    ax.legend()

    display(fig) 
    plt.pause(0.2) 
    plt.close(fig)





#!/usr/bin/env python
# coding: utf-8

# In[5]:


import csv,pandas as pd,numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[6]:


X = pd.read_csv('linearX.csv',header=None)
Y = pd.read_csv('linearY.csv',header=None)
X = np.array(X)

X_gen  = np.array(X)
Y_gen = np.array(Y)


# In[7]:


n_samples = 1000000
X1 =  np.random.normal(3, 2, n_samples).T.reshape(-1,1)
X2 =  np.random.normal(-1, 2, n_samples).T.reshape(-1,1)
W = np.array([[3], [1], [2]])
X_gen = np.hstack((np.ones((n_samples,1)),X1,X2))
Y_gen = X_gen@W
epsilon =  np.random.normal(0, float(np.sqrt(2)), n_samples).T.reshape(-1,1)
Y_gen = Y_gen+epsilon 


# In[8]:


def split(X,Y,ratio=0.8):
	split_idx = int(X.shape[0] * (ratio)) 
	X_train, X_test = X[:split_idx], X[split_idx:]
	Y_train, Y_test = Y[:split_idx].reshape(-1, 1), Y[split_idx:].reshape(-1, 1)
	return X_train,X_test,Y_train,Y_test


# In[9]:


X_train,X_test,Y_train,Y_test = split(X_gen,Y_gen,ratio=0.8)


# In[10]:


def prepare_batches(X,Y,batch_size):
	n = X.shape[0]
	batches = []
	for start_idx in range(0,n,batch_size):
		end_idx = min(start_idx+batch_size,n)
		X_batch  = X[start_idx:end_idx,:]
		Y_batch  = Y[start_idx:end_idx,:]
		batches.append((X_batch,Y_batch))
	return batches

def grad(X_batch,Y_batch,Y_pred):
	m_ = Y_batch.shape[0]
	return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(Y_batch,Y_pred):
	error = (Y_batch-Y_pred)
	sq_err = error**2
	return float(np.sum(sq_err))


# In[12]:


learning_rate = 0.001
W1 = [np.zeros((X_train.shape[1],1)).reshape(-1,1)]*4

list = []
batch_sizes = [1,80,8000,800000]
e = [0.001,0.00001,0.000001,0.0000001]
for i in range(len(batch_sizes)):

	train_batches = prepare_batches(X_train,Y_train,batch_sizes[i])
	epoch,max_epoch = 0, 30000
	prev_cost,diff = 0, float('inf') 
	params = []
	W1[i] = np.zeros((X_train.shape[1],1)).reshape(-1,1)
	print(i,batch_sizes[i],W1[i],epoch,e[i])
	while((diff &gt; e[i]) and (epoch&lt;max_epoch)):
		curr_cost = 0
		for X_batch,Y_batch in train_batches:
			Y_pred = X_batch@(W1[i])
			dW = grad(X_batch,Y_batch,Y_pred)
			train_loss = compute_loss(Y_batch,Y_pred)
			curr_cost += train_loss
			W1[i] -= learning_rate*dW
			params.append((W1[i]).flatten())
		diff = abs(curr_cost-prev_cost)
		prev_cost = curr_cost
		epoch += 1
	params = np.array(params)
	print(i,batch_sizes[i],W1[i],epoch,e[i])
	list.append(params)
	print(epoch)


# In[18]:


W1


# 3.3. (6 points) \\\
# (a) Do different algorithms in the part above (for varying values of r) converge to the same
# parameter values? Comment on the relative speed of convergence and also on number
# of iterations in each case.\\\
# (b) We know that a closed form solution exists for Linear Regression, given by:
# θ = (XT X)
# −1XT Y
# Relearn θ =
# "
# θ0
# θ1
# θ2
# #
# using sampled data points of part a) from this closed form. \\\
# (c) Compare the true parameters (from which we sampled), the parameters learned in closed
# form, and the parameters learnt by SGD. Write what you observe.

# In[109]:


def theta(X,Y):
	return (np.linalg.inv((X.T@X))@(X.T@Y))

print(theta(X_train,Y_train))


# 4. (4 points) Find the mean squared error on the test set (the 20% portion kept in part a)) called
# the test error. Compare with the error on the training set called training error.
# 

# In[110]:


ans = []
for W2 in W1:
	y_pred = X_test@W2
	ans.append(y_pred.flatten())
ans


# In[111]:


ans1 = []
for W2 in W1:
	y_pred = X_train@W2
	ans1.append(y_pred.flatten())
ans1


# In[112]:


for i in range(len(W1)):
	print(compute_loss(ans[i].reshape(-1,1),Y_test)/X_test.shape[0])
for i in range(len(W1)):
	print(compute_loss(ans1[i].reshape(-1,1),Y_train)/X_train.shape[0])


# In[113]:


print(Y_test.shape,ans[0].shape)


# In[ ]:


list[3].shape


# In[120]:


fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

x, y, z = list[0][:, 0], list[0][:, 1], list[0][:, 2]

ax.plot(x, y, z, marker='o', linestyle='-', markersize=2, alpha=0.7, label='Trajectory')

ax.scatter(x[0], y[0], z[0], color='green', s=100, label="Start (θ₀)")
ax.scatter(x[-1], y[-1], z[-1], color='red', s=100, label="End (θ*)")

ax.set_xlabel(r'$\theta_1$')
ax.set_ylabel(r'$\theta_2$')
ax.set_zlabel(r'$\theta_3$')
ax.set_title("Plot")
ax.legend()
plt.show()


# In[121]:


fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

x, y, z = list[1][:, 0], list[1][:, 1], list[1][:, 2]

ax.plot(x, y, z, marker='o', linestyle='-', markersize=2, alpha=0.7, label='Trajectory')

ax.scatter(x[0], y[0], z[0], color='green', s=100, label="Start (θ₀)")
ax.scatter(x[-1], y[-1], z[-1], color='red', s=100, label="End (θ*)")

ax.set_xlabel(r'$\theta_1$')
ax.set_ylabel(r'$\theta_2$')
ax.set_zlabel(r'$\theta_3$')
ax.set_title("Plot")
ax.legend()
plt.show()


# In[122]:


fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

x, y, z = list[2][:, 0], list[2][:, 1], list[2][:, 2]

ax.plot(x, y, z, marker='o', linestyle='-', markersize=2, alpha=0.7, label='Trajectory')

ax.scatter(x[0], y[0], z[0], color='green', s=100, label="Start (θ₀)")
ax.scatter(x[-1], y[-1], z[-1], color='red', s=100, label="End (θ*)")

ax.set_xlabel(r'$\theta_1$')
ax.set_ylabel(r'$\theta_2$')
ax.set_zlabel(r'$\theta_3$')
ax.set_title("Plot")
ax.legend()
plt.show()


# In[123]:


fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

x, y, z = list[3][:, 0], list[3][:, 1], list[3][:, 2]

ax.plot(x, y, z, marker='o', linestyle='-', markersize=2, alpha=0.7, label='Trajectory')

ax.scatter(x[0], y[0], z[0], color='green', s=100, label="Start (θ₀)")
ax.scatter(x[-1], y[-1], z[-1], color='red', s=100, label="End (θ*)")

ax.set_xlabel(r'$\theta_1$')
ax.set_ylabel(r'$\theta_2$')
ax.set_zlabel(r'$\theta_3$')
ax.set_title("Plot")
ax.legend()
plt.show()





#!/usr/bin/env python
# coding: utf-8

# In[2]:


import csv,pandas as pd,numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[4]:


X = pd.read_csv('logisticX.csv',header=None)
Y = pd.read_csv('logisticY.csv',header=None)
X = np.array(X)
Y = np.array(Y)
X = (X - X.mean(axis=0)) / X.std(axis=0)


# In[5]:


X = np.hstack((np.ones((X.shape[0],1)),X))


# In[6]:


def split(X,Y,ratio=0.8):
	split_idx = int(X.shape[0] * (ratio)) 
	X_train, X_test = X[:split_idx], X[split_idx:]
	Y_train, Y_test = Y[:split_idx].reshape(-1, 1), Y[split_idx:].reshape(-1, 1)
	return X_train,X_test,Y_train,Y_test


# In[7]:


X_train,X_test,Y_train,Y_test = split(X,Y,ratio=0.8)


# In[8]:


def sigmoid(z):
	return 1/(1+np.exp(-z))

def grad(X_batch,Y_batch,Y_pred):
	return (X_batch.T @ (Y_pred-Y_batch))

def compute_loss(y, h):
    return -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))



# In[9]:


W = np.zeros((X.shape[1],1))
epoch,num_epochs = 0,1000
prev_cost,curr_cost=0,float('inf')
for i in range(num_epochs):
	train_loss = 0
	Y_pred = sigmoid(X@W)
	R = np.diagflat(Y_pred * (1 - Y_pred))
	H = X.T @ R @ X
	gr = grad(X,Y,Y_pred)
	dW = np.linalg.inv(H)@gr
	W -= dW
	loss = compute_loss(Y,Y_pred)
	print(loss,W)
	


# In[10]:


W


# In[94]:


plt.figure(figsize=(8,6))
plt.scatter(X[Y.ravel() == 0, 1], X[Y.ravel() == 0, 2], marker='o', label='Label 0')
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match236-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.scatter(X[Y.ravel() == 1, 1], X[Y.ravel() == 1, 2], marker='x', label='Label 1')

# Plot decision boundary
x1_vals = np.linspace(X[:,1].min(), X[:,1].max(), 100)
x2_vals = -(W[0] + W[1] * x1_vals) / W[2]
</FONT><A NAME="1"></A><FONT color = #00FF00><A HREF="match236-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.plot(x1_vals, x2_vals, 'r-', label='Decision Boundary')

plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
</FONT>plt.show()


# In[95]:


y_test_pred = sigmoid(X_test @ W) &gt;= 0.5
accuracy = np.mean(y_test_pred == Y_test)
print(f"Test Accuracy: {accuracy:.4f}")


# In[85]:


Y_pred.shape





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        pass
    def grad(self,X_train,Y_train,Y_pred):
        m_ = Y_train.shape[0]
        return (1 / m_) * (X_train.T @ (Y_pred-Y_train))

    def compute_loss(self,Y_train,Y_pred):
        error = Y_train-Y_pred
        return float(np.sum(error ** 2))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        X_train =  np.hstack((np.ones((X.shape[0],1)),X))
        Y = y.reshape(-1,1)
        self.W = np.zeros((X_train.shape[1],1))
        prev_cost,curr_cost=0,float('inf')
        epoch,e = 0,0.0001 
        params = []
        while(abs(prev_cost-curr_cost)&gt;e):
            Y_pred = np.dot(X_train,self.W)
            dW = self.grad(X_train,Y,Y_pred)
            train_loss = self.compute_loss(Y,Y_pred)
            self.W -= learning_rate*dW
            train_loss /= len(X_train)
            prev_cost = curr_cost
            curr_cost = train_loss
            epoch+=1	
            params.append((self.W.copy()).flatten()) 
        params = np.array(params)
        print(f"Converged in {epoch} iterations. Params shape: {params.shape}")
        return params
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_test = np.hstack((np.ones((X.shape[0],1)),X))
        Y_pred = X_test@self.W
        return np.array(Y_pred).flatten()
        







# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X1 = np.random.normal(input_mean[0],input_sigma[0],N).T.reshape(-1,1)
    X2 = np.random.normal(input_mean[1],input_sigma[1],N).T.reshape(-1,1)
    epsilon = np.random.normal(0,noise_sigma,N).T.reshape(-1,1)
    X = np.hstack((X1,X2))
    X_gen  =np.hstack((np.ones((N,1)),X))
    y = (X_gen@theta).reshape(-1,1)
    y = y + epsilon
    y = y.flatten()
    return X,y


class StochasticLinearRegressor:
    def __init__(self):
        pass
    def prepare_batches(self,X,Y,batch_size):
        n = X.shape[0]
        batches = []
        for start_idx in range(0,n,batch_size):
            end_idx = min(start_idx+batch_size,n)
            X_batch  = X[start_idx:end_idx,:]
            Y_batch  = Y[start_idx:end_idx,:]
            batches.append((X_batch,Y_batch))
        return batches
    
    def grad(self,X_batch,Y_batch,Y_pred):
        m_ = Y_batch.shape[0]
        return (1 / m_) * (X_batch.T @ (Y_pred-Y_batch))
    
    def compute_loss(self,Y_batch,Y_pred):
        error = (Y_batch-Y_pred)
        sq_err = error**2
        return float(np.sum(sq_err))


    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples = X.shape[0]
        X_train = np.hstack((np.ones((n_samples,1)),X))
        Y_train = y.reshape(-1,1)
        self.W = [np.zeros((X_train.shape[1],1)).reshape(-1,1)]*4
        
        list = []
        batch_sizes = [1,80,8000,800000]
        e = [0.001,0.00001,0.000001,0.0000001]
        for i in range(len(batch_sizes)):
            train_batches = self.prepare_batches(X_train,Y_train,batch_sizes[i])
            epoch,max_epoch = 0, 30000
            prev_cost,diff = 0, float('inf') 
            params = []
            self.W[i] = np.zeros((X_train.shape[1],1)).reshape(-1,1)
            # print(i,batch_sizes[i],self.W[i],epoch,e[i])
            while((diff &gt; e[i]) and (epoch&lt;max_epoch)):
                curr_cost = 0
                for X_batch,Y_batch in train_batches:
                    Y_pred = X_batch@(self.W[i])
                    dW = self.grad(X_batch,Y_batch,Y_pred)
                    train_loss = self.compute_loss(Y_batch,Y_pred)
                    curr_cost += train_loss
                    self.W[i] -= learning_rate*dW
                    params.append((self.W[i].copy()).flatten())
                diff = abs(curr_cost-prev_cost)
                prev_cost = curr_cost
                epoch += 1
            params = np.array(params)
            # print(i,batch_sizes[i],self.W[i],epoch,e[i])
            list.append(params)
        return list
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        X1 = np.hstack((np.ones((n_samples,1)),X))
        ans = []
        for W in self.W:
            y_pred = X1@W
            ans.append(y_pred.flatten())
        return ans





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
	# Assume Binary Classification
	def __init__(self):
		self.W = None
		self.mean = None
		self.std = None
	
	def sigmoid(self,z):
		return 1/(1+np.exp(-z))
	
	def grad(self,X_batch,Y_batch,Y_pred):
		return (X_batch.T @ (Y_pred-Y_batch))

	def compute_loss(self,y, h):
		return -np.mean(y * np.log(h + 1e-9) + (1 - y) * np.log(1 - h + 1e-9))

	def fit(self, X, y, learning_rate=0.01):
		"""
		Fit the linear regression model to the data using Newton's Method.
		Remember to normalize the input data X before fitting the model.
		
		Parameters
		----------
		X : numpy array of shape (n_samples, n_features)
			The input data.
			
		y : numpy array of shape (n_samples,)
			The target labels - 0 or 1.
		
		learning_rate : float
			The learning rate to use in the update rule.
		
		Returns
		-------
		List of Parameters: numpy array of shape (n_iter, n_features+1,)
			The list of parameters obtained after each iteration of Newton's Method.
		"""
		Y = y.reshape(-1,1)
		self.mean = X.mean(axis=0)
		self.std = X.std(axis=0)
		X_train = (X - X.mean(axis=0)) / X.std(axis=0)
		X_train = np.hstack((np.ones((X_train.shape[0],1)),X_train))
		self.W = np.zeros((X_train.shape[1],1)).reshape(-1,1)
		num_epoch = 100
		params = []
		for _ in range(num_epoch):
			Y_pred = self.sigmoid(X_train@self.W)
			R = np.diagflat(Y_pred * (1 - Y_pred))
			H = X_train.T @ R @ X_train
			gr = self.grad(X_train,Y,Y_pred)
			dW = np.linalg.inv(H)@gr
			self.W -= dW
			# loss = self.compute_loss(Y,Y_pred)
			# print(loss,self.W)
			params.append((self.W.copy()).flatten())
		return np.array(params)
	
	def predict(self, X):
		"""
		Predict the target values for the input data.
		
		Parameters
		----------
		X : numpy array of shape (n_samples, n_features)
			The input data.
			
		Returns
		-------
		y_pred : numpy array of shape (n_samples,)
			The predicted target label.
		"""
		X_test = (X-self.mean)/(self.std+1e-9)
		X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test))
		Y_pred = self.sigmoid(X_test@self.W)
		Y_pred = (Y_pred.flatten() &gt;= 0.5).astype(int)
		return Y_pred




# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mean = None
        self.std = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.mean = np.mean(X,axis=0)
        self.std = np.std(X,axis=0)
        X_norm = (X-self.mean)/(self.std+1e-9)
        Y = y.reshape(-1,1)
        self.phi = np.mean(Y)
        self.mu_0 = np.mean(X_norm[Y==0],axis=0).reshape(-1,1)
        self.mu_1 = np.mean(X_norm[Y==1],axis=0).reshape(-1,1)
        m = len(X)
        self.same_cov = assume_same_covariance
        if(assume_same_covariance):
            self.sigma = np.zeros((2,2))
            for i in range(m):
                x_i = X_norm[i].reshape(-1,1)
                mu_i = self.mu_0 if(Y[i] == 0) else self.mu_1
                self.sigma += (x_i-mu_i)@((x_i-mu_i).T)
            self.sigma /= m
            self.sigma_inv = np.linalg.inv(self.sigma)
            self.w = self.sigma_inv @ (self.mu_1 - self.mu_0)
            self.b = -0.5 * (self.mu_1.T @ self.sigma_inv @ self.mu_1 - self.mu_0.T @ self.sigma_inv @ self.mu_0) + np.log(self.phi/(1-self.phi))
            return self.mu_0,self.mu_1,self.sigma
            # print(self.w,self.b)
        else:
            self.sigma_0 = np.zeros((2,2))
            self.sigma_1 = np.zeros((2,2))
            for i in range(m):
                x_i = X_norm[i].reshape(-1,1)
                mu_i = self.mu_0 if(Y[i] == 0) else self.mu_1
                if(Y[i] == 0):
                    self.sigma_0 += (x_i-mu_i)@((x_i-mu_i).T)
                else:
                    self.sigma_1 += (x_i-mu_i)@((x_i-mu_i).T)
            m_0 = np.sum(Y == 0)
            m_1 = np.sum(Y == 1)
            self.sigma_0 /= m_0
            self.sigma_1 /= m_1
            self.inv_sigma_0 = np.linalg.inv(self.sigma_0)
            self.inv_sigma_1 = np.linalg.inv(self.sigma_1)
            return self.mu_0,self.mu_1,self.sigma_0,self.sigma_1
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = (X-self.mean)/(self.std+1e-9)
        if(self.same_cov):
            score = X_norm@self.w + self.b
            # print(self.w,self.b)
            return np.where(score&gt;=0,1,0).flatten()
        else:
            # print(self.mu_0,self.mu_1,self.sigma_0,self.sigma_1)
            scores = []
            for x in X_norm:
                x = x.reshape(-1,1)
                term_0 = (x - self.mu_0).T @ self.inv_sigma_0 @ (x - self.mu_0)
                term_1 = (x - self.mu_1).T @ self.inv_sigma_1 @ (x - self.mu_1)
                log_det_ratio = np.log(np.linalg.det(self.sigma_0) / np.linalg.det(self.sigma_1))
                score = term_0 - term_1 + log_det_ratio+  2 * np.log((1 - self.phi) / self.phi)
                scores.append(score.item())
            return np.where(np.array(scores) &gt;= 0, 1, 0)

</PRE>
</PRE>
</BODY>
</HTML>
