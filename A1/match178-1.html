<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1L6WT.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_HJFII.py<p><PRE>


from linear_regression import LinearRegressor
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import time
X = np.loadtxt("..//data//Q1//linearX.csv")
# print(X.shape) #returns (1000,)
if len(X.shape) == 1:
    X = X.reshape(-1, 1)
y = np.loadtxt("..//data//Q1//linearY.csv")

model  = LinearRegressor()
theta_history = model.fit(X,y)

# theta_history_001 = model.fit(X,y,learning_rate=0.001)

# theta_history_025 = model.fit(X,y,learning_rate=0.025)

# theta_history_1 = model.fit(X,y,learning_rate=0.1)

# print(np.linalg.norm(theta_history_1-theta_history_001))

def plot_contour(X, y, theta_history):
    import numpy as np
    import matplotlib.pyplot as plt



   
    theta0_min, theta0_max = np.min(theta_history[:, 0]), np.max(theta_history[:, 0])
    theta1_min, theta1_max = np.min(theta_history[:, 1]), np.max(theta_history[:, 1])


    theta0_vals = np.linspace(theta0_min - 1, theta0_max + 1, 100)
    theta1_vals = np.linspace(theta1_min - 1, theta1_max + 1, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    error_mesh = np.zeros_like(theta0_mesh)

    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    m = X.shape[0]  


    for i in range(theta0_vals.shape[0]):
        for j in range(theta1_vals.shape[0]):
            theta = np.array([theta0_vals[i], theta1_vals[j]])
            err = X_b @ theta - y
            error_mesh[i, j] = (1 / (2 * m)) * np.sum(err**2)


    plt.figure(figsize=(10, 7))
    plt.contourf(theta0_mesh, theta1_mesh, error_mesh, levels=50, cmap='viridis')
    plt.colorbar(label='J(θ)')
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.title('Contour Plot of Error Function with Gradient Descent Path')

   
    line, = plt.plot([], [], color='red', marker='o', markersize=2, label='Gradient Descent Path')

    def update_plot(iteration):
        
        current_thetas = theta_history[:iteration + 1]
        line.set_data(current_thetas[:, 0], current_thetas[:, 1])
        plt.legend()
        plt.pause(0.2)

   
    for iteration in range(len(theta_history)):
        update_plot(iteration)
    
    plt.savefig("contour_plot_final.png", dpi=300)
    plt.show()

def plot_3d_mesh_with_trajectory_dynamic(X, y, theta_history):
    import numpy as np
    import matplotlib.pyplot as plt


    
    m = X.shape[0]

    X_b = np.c_[np.ones((m, 1)), X]

    theta0_min, theta0_max = theta_history[:, 0].min(), theta_history[:, 0].max()
    theta1_min, theta1_max = theta_history[:, 1].min(), theta_history[:, 1].max()
    
   
    theta0_vals = np.linspace(theta0_min - 1, theta0_max + 1, 100)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match178-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta1_vals = np.linspace(theta1_min - 1, theta1_max + 1, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    error_mesh = np.zeros_like(theta0_mesh)
    
  
    for i in range(theta0_mesh.shape[0]):
</FONT>        for j in range(theta0_mesh.shape[1]):
            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            err = X_b @ theta - y
            error_mesh[i, j] = (1 / (2 * m)) * np.sum(err ** 2)
  
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(theta0_mesh, theta1_mesh, error_mesh, cmap='viridis', alpha=0.8)
    fig.colorbar(surf, shrink=0.5, aspect=5, label='J(θ)')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('J(θ)')
    ax.set_title('3D Mesh Plot with Dynamic GD Trajectory')
    
    ax.view_init(elev=30, azim=30)
    ax.dist = 15  
    
   
    traj_line, = ax.plot([], [], [], 'ro-', markersize=2, label='GD Trajectory')
    
    
    def update_plot(iteration):
        current_thetas = theta_history[:iteration + 1]
        errors = [ (1 / (2 * m)) * np.sum((X_b @ current_thetas[k] - y)**2)
                   for k in range(current_thetas.shape[0]) ]
        traj_line.set_data(current_thetas[:, 0], current_thetas[:, 1])
        traj_line.set_3d_properties(errors)
        plt.draw()
        plt.pause(0.2)
    

    for i in range(len(theta_history)):
        update_plot(i)
    
    ax.legend()
    plt.show()
# print(theta_history_001)
# print(theta_history_1)

# plot_contour(X,y,theta_history_001)
# plot_contour(X,y,theta_history_1)
# plot_contour(X,y,theta_history_025)
plot_contour(X,y,theta_history)

plot_3d_mesh_with_trajectory_dynamic(X,y,theta_history)



# Imports - you can add any other permitted libraries
import numpy as np


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):

        self.theta = None

        self.error_history = []

        self.error  = None 

        self.prev_error = None 

        self.w_history = [] 
        
        
    
    def fit(self, X, y, learning_rate=0.01):

        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.

        """
        m= X.shape[0]

        n = X.shape[1]   

        X_b = np.c_[np.ones((m,1)),X]

        self.theta  = np.zeros(n+1)

        eps = 10**(-12) 

        self.error = float('inf')
        
        for i in range(10000):
        
            self.prev_error  = self.error
            err = X_b @ self.theta - y 
            self.error = (1/(2*m)) * np.sum(err**2)
            self.error_history.append(self.error)
            self.w_history.append(self.theta.copy())
            if abs(self.error- self.prev_error ) &lt;eps:
                # print(self.error,self.prev_error,i)
                break 
            self.theta -= learning_rate * (1/m * (X_b.T@(err)))

        # print(self.error)

        # print(np.array(self.w_history).shape)

        return np.array(self.w_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_b = np.c_[np.ones((X.shape[0],1)),X]
        y_pred  = X_b@self.theta
        
        
        

        return   y_pred
    
        
        













#!/usr/bin/env python
# coding: utf-8

# In[18]:


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import time


# In[2]:


import importlib


# In[33]:


import linear_regression
importlib.reload(linear_regression)



# In[34]:


from linear_regression import LinearRegressor


# In[35]:


X = np.loadtxt("../data/Q1/linearX.csv")
print(X.shape) #returns (1000,)
if len(X.shape) == 1:
    X = X.reshape(-1, 1)
y = np.loadtxt("../data/Q1/linearY.csv")


# In[36]:


model  = LinearRegressor()


# In[37]:


theta_history = model.fit(X,y)




# In[14]:


theta_history_001 = model.fit(X,y,learning_rate=0.001)


# In[89]:


theta_history_1 =model.fit(X,y,learning_rate=0.1)


# In[102]:


theta_history_025 = model.fit(X,y,learning_rate=0.025)


# In[31]:


y_p= model.predict(X)


# In[9]:


print(y_p.shape)


# In[38]:


print(model.theta)


# In[65]:


def plot_data( X, y):
    plt.scatter(X, y, label="Training Data")
    plt.xlabel("Feature (Acidity)")
    plt.ylabel("Target (Density)")
    plt.title("Wine Density vs Acidity")
    plt.legend()
    plt.show()




# In[66]:


def plot_regression( X, y, prediction):
    plt.scatter(X, y, label="Training Data")
    plt.plot(X, prediction, color='red', label="Regression Line")
    plt.xlabel("Feature (Acidity)")
    plt.ylabel("Target (Density)")
    plt.title("Linear Regression Fit")
    plt.legend()
    plt.savefig("Linear_regression_fit.png")
    plt.show()


# In[15]:


def plot_3d_error(X, y, theta_history):

    
    m = X.shape[0]
   
    X_b = np.c_[np.ones((m, 1)), X]
    
 
    theta0_min, theta0_max = theta_history[:, 0].min(), theta_history[:, 0].max()
    theta1_min, theta1_max = theta_history[:, 1].min(), theta_history[:, 1].max()

    theta0_vals = np.linspace(theta0_min - 1, theta0_max + 1, 100)
    theta1_vals = np.linspace(theta1_min - 1, theta1_max + 1, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    
    error_mesh = np.zeros_like(theta0_mesh)
 
    for i in range(theta0_mesh.shape[0]):
        for j in range(theta0_mesh.shape[1]):
            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            err = X_b @ theta - y
            error_mesh[i, j] = (1 / (2 * m)) * (err ** 2).sum()
    
   
    error_history = []
    for theta in theta_history:
        err = X_b @ theta - y
        error_history.append((1 / (2 * m)) * (err ** 2).sum())
    error_history = np.array(error_history)
    
 
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(theta0_mesh, theta1_mesh, error_mesh, cmap='viridis', alpha=0.8)
    fig.colorbar(surf, shrink=0.5, aspect=5)
    
    
    ax.plot(theta_history[:, 0], theta_history[:, 1], error_history,
            color='red', marker='o', markersize=2, label='Gradient Descent Path')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('J(θ)')
    ax.set_title('3D Mesh Plot of Error Function with Gradient Descent Path')
    ax.legend()
    
 
    ax.view_init(elev=30, azim=90)
    plt.savefig("3d_plot_view1.png", dpi=300)
    
 
    ax.view_init(elev=30, azim=135)
    plt.savefig("3d_plot_view2.png", dpi=300)
    
    plt.show()


# In[ ]:





# In[16]:


def plot_contour_static(X, y, theta_history):
    

 


    theta0_min, theta0_max = np.min(theta_history[:, 0]), np.max(theta_history[:, 0])
    theta1_min, theta1_max = np.min(theta_history[:, 1]), np.max(theta_history[:, 1])

 
    theta0_vals = np.linspace(theta0_min - 1, theta0_max + 1, 100)
    theta1_vals = np.linspace(theta1_min - 1, theta1_max + 1, 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)
    error_mesh = np.zeros_like(theta0_mesh)


    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    m = X.shape[0] 

    
    for i in range(theta0_mesh.shape[0]):
        for j in range(theta0_mesh.shape[1]):
            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            err = X_b @ theta - y
            error_mesh[i, j] = (1 / (2 * m)) * np.sum(err**2)


    plt.figure(figsize=(10, 7))
    cp = plt.contourf(theta0_mesh, theta1_mesh, error_mesh, levels=50, cmap='viridis')
    plt.colorbar(cp, label='J(θ)')
    plt.xlabel('Theta 0')
    plt.ylabel('Theta 1')
    plt.title('Contour Plot of Error Function with Gradient Descent Path')

    
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'ro-', markersize=2, label='Gradient Descent Path')
    plt.legend()

    
    plt.savefig("contour_plot_final.png", dpi=300)
    plt.show()


# In[69]:


X_b = np.c_[np.ones((X.shape[0],1)),X]
plot_regression(X,y,X_b@model.w)


# In[25]:


plot_3d_error(X,y,theta_history)


# In[24]:


plot_contour_static(X,y,theta_history)


# In[17]:


plot_contour_static(X,y,theta_history_001)




# In[90]:


plot_contour_static(X,y,theta_history_1)


# In[105]:


plot_contour_static(X,y,theta_history_025)


# In[ ]:








#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import time


# In[2]:


import importlib


# In[3]:


from sampling_sgd import closed_form_solution


# In[4]:


from sampling_sgd import generate


# In[221]:


import sampling_sgd
importlib.reload(sampling_sgd)

from sampling_sgd import StochasticLinearRegressor
from sampling_sgd import closed_form_solution


# In[222]:


theta_true = np.array([3, 1, 2])


# In[223]:


<A NAME="1"></A><FONT color = #00FF00><A HREF="match178-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)


# In[224]:


X, y = generate(1000000, theta_true, input_mean, input_sigma, noise_sigma)
</FONT>X_train, y_train = X[:800000], y[:800000]
X_test, y_test = X[800000:], y[800000:]


# In[225]:


theta_closed = closed_form_solution(X_train, y_train)
print(theta_closed)


# In[226]:


def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)


# In[227]:


model  =StochasticLinearRegressor()
theta_history = model.fit(X_train,y_train)


# In[228]:


batch_sizes = [1,80,8000,800000]


# In[240]:


test_ans = model.predict(X_test)
train_ans = model.predict(X_train)
test_error = {}
train_error ={}
epochs = {}
timings= {}
diff = {}
for b in range(4):
    test_error[batch_sizes[b]] = mean_squared_error(y_test,test_ans[b])
    train_error[batch_sizes[b]] = mean_squared_error(y_train,train_ans[b])
    epochs[batch_sizes[b]] = theta_history[b].shape[0]
    timings[batch_sizes[b]] = model.time[b]
    diff[batch_sizes[b]] = np.linalg.norm(model.thetas[b]-theta_true)

    


# In[245]:


X_train_b = np.c_[np.ones((X_train.shape[0],1)),X_train]
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match178-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X_test_b = np.c_[np.ones((X_test.shape[0],1)),X_test]
closed_form_ans_train = X_train_b @ theta_closed
closed_form_ans_test = X_test_b @ theta_closed
closed_form_ans_train.flatten()
</FONT>closed_form_ans_test.flatten
print(mean_squared_error(y_train,closed_form_ans_train))
print(mean_squared_error(y_test,closed_form_ans_test))


# In[241]:


print(diff)


# In[242]:


print(np.linalg.norm(theta_closed-theta_true))


# In[233]:


print(train_error)


# In[234]:


print(test_error)


# In[232]:


print(epochs)


# In[236]:


print(timings)


# In[247]:


import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def plot_theta_trajectory(theta_histories, batch_sizes):
    """
    Plots the trajectory of theta in 3D space for different batch sizes.

    Parameters
    ----------
    theta_histories : list of numpy arrays
        Each entry in the list is an array of shape (n_iter, 3), where each row represents θ at a given iteration.
        
    batch_sizes : list of int
        The batch sizes corresponding to each trajectory.
    """
<A NAME="0"></A><FONT color = #FF0000><A HREF="match178-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    
    colors = ['r', 'g', 'b', 'm']  # Different colors for each batch size
    for i, theta_history in enumerate(theta_histories):
        theta_history = np.array(theta_history)  # Convert list to NumPy array
        ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], 
                color=colors[i], label=f'Batch Size: {batch_sizes[i]}')

        # Mark start and end points
        ax.scatter(theta_history[0, 0], theta_history[0, 1], theta_history[0, 2], 
</FONT>                   color=colors[i], marker='o', label=f'Start {batch_sizes[i]}')
        ax.scatter(theta_history[-1, 0], theta_history[-1, 1], theta_history[-1, 2], 
                   color=colors[i], marker='x', label=f'End {batch_sizes[i]}')

    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title('Trajectory of $\Theta$ Updates in 3D Space')
    ax.legend()
    plt.savefig("trajectory.png")
    plt.show()


# In[248]:


plot_theta_trajectory(theta_history, batch_sizes)


# In[239]:


print (model.time)





# Imports - you can add any other permitted libraries
import numpy as np
from collections import deque
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.

    """
    # np.random.seed(40)
    
    x1 = np.random.normal(input_mean[0],input_sigma[0],N)
    x2 = np.random.normal(input_mean[1],input_sigma[1],N)
    X = np.random.normal(input_mean,input_sigma,(N,2))

    X =np.column_stack((x1,x2))
    X_b= np.column_stack((np.ones(N),X))

<A NAME="5"></A><FONT color = #FF0000><A HREF="match178-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    eps  =np.random.normal(0,noise_sigma,N)
    y = X_b @ theta + eps 

    return (X,y) 



    

class StochasticLinearRegressor:
    def __init__(self):
</FONT>        self.thetas = []
        self.time= []
        pass
        
        
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m= X.shape[0]
        n= X.shape[1]
        X_b= np.column_stack((np.ones(m),X))
        batch_sizes = [1,80,8000,800000]
        ans  = []
       
        for r in batch_sizes:
           

            theta = np.zeros(n+1)
            theta_history = [theta.copy()]
            n_batches = m // r
            mse= deque()
            ver =False
            lr1 =learning_rate
            start_time =time.time()

            if r==1:

                for i in range(20) :
                    indices= np.random.permutation(m)
                    X_shuffled =X_b[indices]
                    y_shuffled =y[indices]

                    for j in range(0,m,r):
                        X_batch  = X_shuffled[j:j+r]
                        y_batch = y_shuffled[j:j+r]
                        err = X_batch @ theta - y_batch
                        error = (1/(2*r)) * np.sum(err**2)
                        
                        mse.append(error)
                        if len(mse)&gt;10000:
                     
                            if abs(mse[-1]-mse[-1-10000])/10000&lt;1e-12:
                                ver =True 
                                break 
                            mse.popleft()

                        theta -= lr1 * ((1/r) * X_batch.T @ (-y_batch + X_batch @ theta))
                    # lr1-= 0.1*lr1
                    
                        
                
                    theta_history.append(np.copy(theta))
                    if ver : 
                        break 

            elif r==80:

                for i in range(2000) :
                    indices= np.random.permutation(m)
                    X_shuffled =X_b[indices]
                    y_shuffled =y[indices]

                    for j in range(0,m,r):
                        X_batch  = X_shuffled[j:j+r]
                        y_batch = y_shuffled[j:j+r]
                        err = X_batch @ theta - y_batch
                        error = (1/(2*r)) * np.sum(err**2)
                        
                        mse.append(error)
                        if len(mse)&gt;n_batches:
                     
                            if abs(mse[-1]-mse[-1-n_batches])/n_batches&lt;1e-11:
                                ver =True 
                                break 
                            mse.popleft()

                        theta -= lr1 * ((1/r) * X_batch.T @ (-y_batch + X_batch @ theta))
                   
                    
                        
     
                
                    theta_history.append(np.copy(theta))
                    if ver : 
                        break 


            elif r==8000:
                
                for i in range(2000) :
                    indices= np.random.permutation(m)
                    X_shuffled =X_b[indices]
                    y_shuffled =y[indices]

                    for j in range(0,m,r):
                        X_batch  = X_shuffled[j:j+r]
                        y_batch = y_shuffled[j:j+r]
                        err = X_batch @ theta - y_batch
                        error = (1/(2*r)) * np.sum(err**2)
                        
                        mse.append(error)
                        if len(mse)&gt;n_batches:
                           
                            if abs(mse[-1]-mse[-1-n_batches])/n_batches&lt;1e-9:
                                ver =True 
                                break 
                            mse.popleft()

                        theta -= learning_rate * ((1/r) * X_batch.T @ (-y_batch + X_batch @ theta))

                    
                        
                    
                
                    theta_history.append(np.copy(theta))
                    if ver : 
                        break 
                    # if np.linalg.norm(theta_history[-1] - theta_history[-2]) &lt; 1e-6:

                    #     break
            else:
                
                for i in range(2500) :
                    indices= np.random.permutation(m)
                    X_shuffled =X_b[indices]
                    y_shuffled =y[indices]

                    for j in range(0,m,r):
                        X_batch  = X_shuffled[j:j+r]
                        y_batch = y_shuffled[j:j+r]
                        err = X_batch @ theta - y_batch
                        error = (1/(2*r)) * np.sum(err**2)
                        
                        mse.append(error)
                        if len(mse)&gt;n_batches:
                            
                            if abs(mse[-1]-mse[-1-n_batches])/n_batches&lt;1e-9:
                                ver =True 
                                break 
                            mse.popleft()

                        theta -= learning_rate * ((1/r) * X_batch.T @ (-y_batch + X_batch @ theta))

                    
                        
       
                
                    theta_history.append(np.copy(theta))
                    if ver : 
                        break 
                    
            
            end_time =time.time()
            elapsed_time = end_time - start_time 
            self.time.append(elapsed_time)
            self.thetas.append(theta)
            
            
            ans.append(np.array(theta_history))
            
        return ans


                



        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        ans = []

        for theta in self.thetas:

            X_b = np.c_[np.ones((X.shape[0],1)),X]

            pred  = X_b @ theta

            pred.flatten()

            ans.append(pred)

        

        return   ans
        
def closed_form_solution(X, y):
    m = X.shape[0]
    X_b = np.column_stack((np.ones(m), X))
    return np.linalg.inv(X_b.T @ X_b) @ (X_b.T @ y)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification

    def __init__(self):
        self.theta = None
        pass

    def sigmoid(self,z):
        return 1/(1+ np.exp(-z))
    
#     θ =&gt; θ − H−1∇θℓ(θ).
#     Hij =(∂2ℓ(θ))/(∂θi∂θj)

    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        m= X.shape[0]
        n= X.shape[1]

        X_norm = (X - np.mean(X,axis = 0))/ np.std(X,axis=0)

        X_b= np.column_stack((np.ones(m), X_norm))
        
        theta = np.zeros(n+1)
        theta_history = [theta.copy()]

        for i in range(10000):

            h_theta = self.sigmoid(X_b@theta)

            delta_l_theta  = X_b.T @ (y-h_theta)

            D = np.diag(h_theta*(1-h_theta))

            H = -X_b.T @ D @ X_b 
            theta = theta -  (np.linalg.inv(H) @ delta_l_theta)
            theta_history.append(theta.copy())

            if np.linalg.norm(theta- theta_history[-2])&lt; 1e-15:
               
                self.theta =  theta
                break 
        return np.array(theta_history)



    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        X_c = np.column_stack((np.ones(X_norm.shape[0]), X))
        
        
        pred = self.sigmoid(X_c @ self.theta)
        
        return (pred &gt;= 0.5).astype(int)
        



#!/usr/bin/env python
# coding: utf-8

# In[2]:


import importlib


# In[3]:


import matplotlib.pyplot as plt


# In[4]:


import numpy as np


# In[5]:


import logistic_regression
importlib.reload(logistic_regression)

from logistic_regression import LogisticRegressor


# In[6]:


<A NAME="2"></A><FONT color = #0000FF><A HREF="match178-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = np.loadtxt("..//data//Q3//logisticX.csv",delimiter=",")
y = np.loadtxt("..//data//Q3//logisticY.csv",delimiter=",")


# In[7]:


model = LogisticRegressor()
theta_history = model.fit(X, y)
</FONT>print(theta_history)
print(model.theta)


# In[ ]:





# In[8]:


prediction  = model.predict(X)
print(prediction)
print(prediction.shape)


# In[9]:


def plot_decision_boundary(X, y, theta):
  

    plt.scatter(X[y == 0, 1], X[y == 0, 2], marker='o', label='Class 0', edgecolors='k')
    plt.scatter(X[y == 1, 1], X[y == 1, 2], marker='x', label='Class 1', edgecolors='k')

   
    x1_vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
    x2_vals = -(theta[0] + theta[1] * x1_vals) / theta[2]  # Solve for x2

    plt.plot(x1_vals, x2_vals, 'r-', label='Decision Boundary')

  
    plt.xlabel("Feature 1 (x1)")
    plt.ylabel("Feature 2 (x2)")
    plt.legend()
    plt.title("Logistic Regression Decision Boundary")
    plt.savefig("logistic.png")
    plt.show()


# In[10]:


X_norm=(X - np.mean(X,axis = 0))/ np.std(X,axis=0)
if X_norm.shape[1] == 2:  
     X_norm = np.column_stack((np.ones(X.shape[0]), X))


plot_decision_boundary(X_norm,y,model.theta)





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu0 =None
        self.mu1 = None 
        self.sigma =None 
        self.sigma0 = None 
        self.sigma1 = None 
        self.phi = None
        self.ver = False 
   

        
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
      
        X = (X - np.mean(X,axis=0))/  np.std(X,axis= 0)
        m = X.shape[0]
        n= X.shape[1]

        self.phi = np.mean(y)
        # print(self.phi) 

        self.mu0 = np.mean(X[y==0],axis= 0 )
        self.mu1 = np.mean(X[y==1],axis=0 )
        X0 = X[y==0]
        X1=X[y==1]
        if assume_same_covariance:
            self.ver =True
            sigma =np.zeros((n,n))
            

            sigma += (X0 -self.mu0).T @ (X0-self.mu0)
            sigma+= (X1-self.mu1).T @ (X1-self.mu1)
            sigma /=m 
            self.sigma = sigma 
            return (self.mu0,self.mu1,self.sigma) 
        else:
            self.var =False
            m0 = X0.shape[0]
            m1= X1.shape[0]
            sigma0 = ((X0-self.mu0).T @ (X0-self.mu0)) / m0 
            sigma1 = ((X1-self.mu1).T @ (X1-self.mu1)) / m1
            self.sigma0 =sigma0
            self.sigma1 =sigma1 
            return (self.mu0,self.mu1,self.sigma0,self.sigma1)



  
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_norm = (X - np.mean(X,axis=0))/  np.std(X,axis= 0)
        m= X_norm.shape[0]
        n= X_norm.shape[1]
        if self.ver : 
            sigma_inv = np.linalg.inv(self.sigma)

            theta = sigma_inv@(self.mu1-self.mu0)

            bias = (-0.5 * (self.mu_1.T @ sigma_inv @ self.mu_1) +
                      0.5 * (self.mu_0.T @ sigma_inv @ self.mu_0) +
                      np.log(self.phi / (1 - self.phi)))
            
            y_pred=  X@theta + bias 
            pred = (y_pred &gt;0).astype(int)

        else: 
            pred= np.zeros(m,dtype=int)
         
            sigma0_inv = np.linalg.inv(self.sigma0)
            sigma1_inv =np.linalg.inv(self.sigma1)
            sigma0_det = np.linalg.det(self.sigma0)
            sigma1_det = np.linalg.det(self.sigma1)
            for i in range(m):
                x= X[i]
                diff0 = (x-self.mu0)
                diff1 =(x-self.mu1) 
                p0  = (-0.5 * diff0.T @ sigma0_inv @ diff0 -0.5 * np.log(sigma0_det) +np.log(1 - self.phi))
                p1 = (-0.5*diff1.T@sigma1_inv@diff1 - 0.5*np.log(sigma1_det)+ np.log(self.phi))
                if p1&gt;p0:
                    pred[i]=1 
                else:
                    pred[i] =0
        return pred
                



        



#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np 
import matplotlib.pyplot as plt 


# In[6]:


import importlib


# In[7]:


import gda 


# In[8]:


importlib.reload(gda)
from gda import GaussianDiscriminantAnalysis


# In[9]:


X = np.loadtxt("..//data//Q4//q4x.dat")
y_original = np.loadtxt("..//data//Q4//q4y.dat",dtype =str)


# In[10]:


y = np.array([0 if label.strip().lower() == 'alaska' else 1 for label in y_original])


# In[11]:


model_linear  =GaussianDiscriminantAnalysis()
mu0_l, mu1_l, sigma = model_linear.fit(X,y,assume_same_covariance=True)
print(mu0_l,mu1_l,sigma)


# In[12]:


X_norm = (X - np.mean(X,axis = 0))/ np.std(X,axis=0)


plt.figure(figsize=(10, 8))
plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', color='magenta', label='Canada')
plt.xlabel('Normalized x1 (fresh water growth ring diameter)')
plt.ylabel('Normalized x2 (marine water growth ring diameter)')
plt.legend()
# plt.title('Training Data: Alaska vs Canada')
plt.savefig("linear_gda.png")


# In[13]:


X_norm = (X - np.mean(X,axis = 0))/ np.std(X,axis=0)

plt.figure(figsize=(10, 8))
plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', color='magenta', label='Canada')
plt.xlabel('Normalized x1 (fresh water growth ring diameter)')
plt.ylabel('Normalized x2 (marine water growth ring diameter)')
plt.title('Training Data: Alaska vs Canada')
sigma_inv = np.linalg.inv(sigma)
theta = sigma_inv @ (mu1_l - mu0_l)
phi_val = model_linear.phi
theta0 = (-0.5 * (mu1_l.T @ sigma_inv @ mu1_l) +
          0.5 * (mu0_l.T @ sigma_inv @ mu0_l) +
          np.log(phi_val/(1-phi_val)))


x1_vals = np.linspace(X_norm[:, 0].min()-1, X_norm[:, 0].max()+1, 200)

if theta[1] != 0:
    x2_vals = -(theta[0] * x1_vals + theta0) / theta[1]
    plt.plot(x1_vals, x2_vals, 'g-', linewidth=2, label='Linear Decision Boundary')
else:
   
    x_boundary = -theta0/theta[0]
    plt.axvline(x=x_boundary, color='g', linewidth=2, label='Linear Decision Boundary')
plt.legend()
plt.savefig("gda_linear_boundary.png")


# In[14]:


model_q = GaussianDiscriminantAnalysis()
mu0_q, mu1_q, sigma0,sigma1 = model_q.fit(X,y)
# print(model_q.phi)
print(mu0_q,mu1_q,sigma0,sigma1)


# In[15]:


X_norm = (X - np.mean(X,axis = 0))/ np.std(X,axis=0)

plt.figure(figsize=(10, 8))
plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], marker='o', color='blue', label='Alaska')
plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], marker='x', color='magenta', label='Canada')
plt.xlabel('Normalized x1 (fresh water growth ring diameter)')
plt.ylabel('Normalized x2 (marine water growth ring diameter)')
plt.title('Training Data: Alaska vs Canada')
sigma_inv = np.linalg.inv(sigma)
theta = sigma_inv @ (mu1_l - mu0_l)
phi_val = model_linear.phi
theta0 = (-0.5 * (mu1_l.T @ sigma_inv @ mu1_l) +
          0.5 * (mu0_l.T @ sigma_inv @ mu0_l) +
          np.log(phi_val/(1-phi_val)))


x1_vals = np.linspace(X_norm[:, 0].min()-1, X_norm[:, 0].max()+1, 200)

if theta[1] != 0:
    x2_vals = -(theta[0] * x1_vals + theta0) / theta[1]
    plt.plot(x1_vals, x2_vals, 'g-', linewidth=2, label='Linear Decision Boundary')
else:
   
    x_boundary = -theta0/theta[0]
    plt.axvline(x=x_boundary, color='g', linewidth=2, label='Linear Decision Boundary')
x1_range = np.linspace(X_norm[:, 0].min()-1, X_norm[:, 0].max()+1, 300)
x2_range = np.linspace(X_norm[:, 1].min()-1, X_norm[:, 1].max()+1, 300)
X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)
grid_points = np.c_[X1_grid.ravel(), X2_grid.ravel()]


sigma0_inv = np.linalg.inv(sigma0)
sigma1_inv = np.linalg.inv(sigma1)
det_sigma0 = np.linalg.det(sigma0)
det_sigma1 = np.linalg.det(sigma1)


g_diff = np.zeros(grid_points.shape[0])
for i, x in enumerate(grid_points):
    diff0 = x - mu0_q
    diff1 = x - mu1_q
    g0 = (-0.5 * diff0.T @ sigma0_inv @ diff0 -
          0.5 * np.log(det_sigma0) +
          np.log(1 - model_q.phi))
    g1 = (-0.5 * diff1.T @ sigma1_inv @ diff1 -
          0.5 * np.log(det_sigma1) +
          np.log(model_q.phi))
    g_diff[i] = g1 - g0  # boundary when g1 - g0 = 0

g_diff = g_diff.reshape(X1_grid.shape)
g_diff2 = g_diff.reshape(X2_grid.shape)

quad_contour = plt.contour(X1_grid, X2_grid, g_diff, levels=[0],
                           colors='r', linestyles='--', linewidths=1)
quad_contour2 = plt.contour(X1_grid, X2_grid, g_diff2, levels=[0],
                           colors='r', linestyles='--', linewidths=1)

quad_contour.collections[0].set_label('Quadratic Decision Boundary')

plt.legend()
plt.savefig("quad_gda.png")
plt.show()



</PRE>
</PRE>
</BODY>
</HTML>
