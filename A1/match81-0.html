<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_46U3K.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_46U3K.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.
 
class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match81-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m, n = X.shape
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X]
        # Initialize the parameters (including the bias term) to zeros
        self.theta = np.zeros(n+1)
</FONT>        # Train model using batch gradient descent
        parameter_array, cost_history = self.train(X_new,y,learning_rate,10000,m,n)
        return parameter_array

    # Training function
    def train(self,X,y,alpha,n_iter,num_samples,num_features):
        m = num_samples
        n = num_features
        cost_history = []
        parameter_array = np.zeros((n_iter, n+1))
        for i in range(n_iter):
            # predicted h_theta(x)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match81-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            h_theta = X.dot(self.theta)
            # error
            error = h_theta - y
            # gradient
            gradient = (1/m) * X.T.dot(error)
            # update rule
            self.theta = self.theta - alpha * gradient
</FONT>            # Save parameters at the current iteration
            parameter_array[i] = self.theta
            # calculate cost
            cost = (1 / (2 * m)) * np.sum(error ** 2)
            cost_history.append(cost)
            # stopping criteria                        -----&gt; later add based on theta change
            if i &gt; 0 and abs(cost_history[i-1] - cost_history[i]) &lt; 1e-6:
                parameter_array = parameter_array[:i+1] # Remove the remaining zeros
                # print(f'Converged at iteration {i} for learning rate {alpha}')
                break
        return parameter_array , cost_history
                
 
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
            
        m, n = X.shape
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X]
        # Predict target values
        y_pred = X_new.dot(self.theta)
        return y_pred





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from linear_regression import LinearRegressor

def extract_Xy_from_csv(X_csv, y_csv):
    """
    Load the input and target values from the given CSV files.
    
    Returns
    -------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target values.
    """
    
    X = pd.read_csv(X_csv, header=None).values  # Read feature column
    y = pd.read_csv(y_csv, header=None).values.flatten()  # flatten to 1D
    
    return X, y

# Initialise the model
model = LinearRegressor()
learning_rate = .1
# Load the data
X, y = extract_Xy_from_csv('../data/Q1/linearX.csv', '../data/Q1/linearY.csv')

X_train , y_train = X[:int(.8*len(X)),:], y[:int(.8*len(y))]
X_test , y_test = X[int(0.8*len(X)):,:], y[int(0.8*len(y)):]
# Train the model
parameters = model.fit(X, y, learning_rate)
# print("Parameters: ", parameters[-1])

# Predict target values
y_pred = model.predict(X_test)

# Save the parameters and predictions
np.savetxt(f'{learning_rate}_parameters.csv', parameters, delimiter=',')

# section 1.2 
def plot_data_and_hypothesis(X, y, y_pred):
    plt.figure(figsize=(8, 6))
    plt.scatter(X, y, color='blue', label='Data Points') 
<A NAME="6"></A><FONT color = #00FF00><A HREF="match81-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.plot(X, y_pred, color='red', label='Hypothesis Function')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Data Points and Hypothesis Function')
    plt.legend()
    plt.grid(True)
    plt.show()
</FONT>
# section 1.3 (a) & (b)
def plot_mesh_error_function(X, y, parameters):
    X_with_bias = np.c_[np.ones(len(X)), X]  # Add bias column to X

    # Compute range of θ_0 and θ_1
    theta_min = np.min(parameters, axis=0)
    theta_max = np.max(parameters, axis=0)
    theta_0_range = (theta_min[0] - 2, theta_max[0] + 2)
    theta_1_range = (theta_min[1] - 2, theta_max[1] + 2)

    # Generate grid of θ_0 and θ_1 values
    theta_0_values = np.linspace(*theta_0_range, 100)
    theta_1_values = np.linspace(*theta_1_range, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_values, theta_1_values)

    # Compute cost values for grid
    cost_values = np.zeros_like(theta_0)
    m = len(y)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            predictions = X_with_bias.dot(theta)
            errors = predictions - y
            cost_values[i, j] = (1 / (2 * m)) * np.sum(errors ** 2)

    # Create a 3D surface plot
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(theta_0, theta_1, cost_values, cmap='viridis', edgecolor='none', alpha=0.8)
    ax.set_xlabel('θ_0 (Bias)')
    ax.set_ylabel('θ_1 (Weight)')
    ax.set_zlabel('Cost J(θ)')
    ax.set_title('Error Function J(θ)')
    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)

    # Plot parameter progression during gradient descent
    for i, theta in enumerate(parameters):
        theta_0_val, theta_1_val = theta
        prediction = X_with_bias.dot(theta)
        error = prediction - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        
        ax.scatter(theta_0_val, theta_1_val, cost, color='red', s=50)
        
        # Add time delay to observe the change in parameters
        plt.pause(0.2)
    
    plt.show()

# Section 1.4
def plot_contour_error_function(X, y, parameters):
    X_with_bias = np.c_[np.ones(len(X)), X]  # Add bias column to X

    # Compute range of θ_0 and θ_1
    theta_min = np.min(parameters, axis=0)
    theta_max = np.max(parameters, axis=0)
    theta_0_range = (theta_min[0] - 2, theta_max[0] + 2)
    theta_1_range = (theta_min[1] - 2, theta_max[1] + 2)

    # Generate grid of θ_0 and θ_1 values
    theta_0_values = np.linspace(*theta_0_range, 100)
    theta_1_values = np.linspace(*theta_1_range, 100)
    theta_0, theta_1 = np.meshgrid(theta_0_values, theta_1_values)

    # Compute cost values for every value of θ_0 and θ_1
    cost_values = np.zeros_like(theta_0)
    m = len(y)
    for i in range(theta_0.shape[0]):
        for j in range(theta_0.shape[1]):
            theta = np.array([theta_0[i, j], theta_1[i, j]])
            predictions = X_with_bias.dot(theta)
            errors = predictions - y
            cost_values[i, j] = (1 / (2 * m)) * np.sum(errors ** 2)

    # Create contour plot
    plt.figure(figsize=(10, 8))
    plt.contour(theta_0, theta_1, cost_values, levels=50, cmap='viridis')
    plt.xlabel('θ_0 (Bias)')
    plt.ylabel('θ_1 (Weight)')
    plt.title('Contours of Error Function J(θ)')

    # Plot parameter progression during gradient descent
    for i, theta in enumerate(parameters):
        theta_0_val, theta_1_val = theta
        prediction = X_with_bias.dot(theta)
        error = prediction - y
        cost = (1 / (2 * m)) * np.sum(error ** 2)
        
        plt.scatter(theta_0_val, theta_1_val, color='red', s=50)
        
        # Add a delay to visualize each iteration
        plt.pause(0.2)
    
    plt.legend()
    plt.show()


def mse(y, y_pred):
    return np.mean((y - y_pred) ** 2)

# *** print and plot the results ***
# plot_data_and_hypothesis(X, y, y_pred)
# plot_mesh_error_function(X, y, parameters)
# plot_contour_error_function(X, y, parameters)
# print(mse(y_test, y_pred))

    



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
from sampling_sgd import StochasticLinearRegressor

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    np.random.seed(0)
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*X[:,0] + theta[2]*X[:,1] + noise
    return X, y

# Closed form solution
def closed_form_solution(X, y):
    """
    Compute the closed form solution for linear regression.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features+1)
        The input data.
        
<A NAME="2"></A><FONT color = #0000FF><A HREF="match81-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y : numpy array of shape (n_samples,)
        The target values.
        
    Returns
    -------
    theta : numpy array of shape (n_features+1,)
        The parameters of the linear regression model.
</FONT>    """
    X_new = np.c_[np.ones(len(X)), X]
    theta = np.linalg.inv(X_new.T.dot(X_new)).dot(X_new.T).dot(y)
    return theta


# Training and testing mean squared error
def mean_squared_error(y_true, y_pred):
    """
    Compute the mean squared error between the true and predicted target values.
    
    Parameters
<A NAME="0"></A><FONT color = #FF0000><A HREF="match81-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ----------
    y_true : numpy array of shape (n_samples,)
        The true target values.
        
    y_pred : numpy array of shape (n_samples,)
        The predicted target values.
        
    Returns
    -------
    mse : float
</FONT>        The mean squared error between the true and predicted target values.
    """
    mse = np.mean((y_true - y_pred)**2)
    return mse

# Generate data
model = StochasticLinearRegressor()
N = 1000000
theta = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([4, 4])
noise_sigma = np.sqrt(2)  # variance σ² = 2
X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

# split data into training and testing sets
X_train, y_train = X[:int(0.8*N)], y[:int(0.8*N)]
X_test, y_test = X[int(0.8*N):], y[int(0.8*N):]

# Fit the model
parameter_list_of_4_batches = model.fit(X_train, y_train, learning_rate=0.001)

parameter_batch1 = parameter_list_of_4_batches[0][-1]
np.savetxt(f'1_parameters.csv', parameter_list_of_4_batches[0], delimiter=',')
parameter_batch2 = parameter_list_of_4_batches[1][-1]
np.savetxt(f'80_parameters.csv', parameter_list_of_4_batches[1], delimiter=',')
parameter_batch3 = parameter_list_of_4_batches[2][-1]
np.savetxt(f'8000_parameters.csv', parameter_list_of_4_batches[2], delimiter=',')
parameter_batch4 = parameter_list_of_4_batches[3][-1]
np.savetxt(f'800000_parameters.csv', parameter_list_of_4_batches[3], delimiter=',')


print([("Batch 1: ", parameter_batch1), ("Batch 80: ", parameter_batch2), ("Batch 8000: ", parameter_batch3), ("Batch 800000: ", parameter_batch4)])
y_pred_list = model.predict(X_test)
# y_pred_batch1 = y_pred_list[0]
# y_pred_batch2 = y_pred_list[1]
# y_pred_batch3 = y_pred_list[2]
# y_pred_batch4 = y_pred_list[3]

# Section 2.3 (b)  & (c)  
theta_closed_form = closed_form_solution(X_train, y_train)
print([("True theta: ", theta ), ("Closed form theta: ", theta_closed_form) ])

# Section 2.4
batch_list = [1, 80, 8000, 800000]
for i in range(4):
    testing_error = mean_squared_error(y_test, y_pred_list[i])
    training_error = mean_squared_error(y_train, model.predict(X_train)[i])
    print(f'Training Error: {training_error} Testing Error: {testing_error} for batch size {batch_list[i]}')

# testing_error = mean_squared_error(y_test, y_pred_batch1)
# training_error = mean_squared_error(y_train, model.predict(X_train)[0])
# print(f'Training Error: {training_error} Testing Error: {testing_error} for batch size 1')


# Plot movement of theta
def plot_movement_of_theta(file_name):

    batch_size = file_name.split('_')[0]
    theta_history = np.loadtxt(file_name, delimiter=',')

    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')

    # Plot movement of theta
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o', label=f'Movement of Theta for batch size {batch_size}')

    # Labels
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title('Theta Updates in Parameter Space')

    plt.legend()
    plt.show()

# plot_movement_of_theta('1_parameters.csv')




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    np.random.seed(0)
    X = np.random.normal(input_mean, input_sigma, (N, 2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*X[:,0] + theta[2]*X[:,1] + noise
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        """
        Initialize the model parameters.
        """
        self.theta = None

    # Split the data into training and testing sets
    def split_X_data(self, X):
        """
        Split the input data into training and testing sets.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        X_train : numpy array of shape (n_train, n_features)
            The training set.
            
        X_test : numpy array of shape (n_test, n_features)
            The testing set.
        """
        m = len(X)
        X_train = X[:int(0.8*m)]
        X_test = X[int(0.8*m):]
        return X_train, X_test
    
    def split_y_data(self, y):
        """
        Split the target values into training and testing sets.
        
        Parameters
        ----------
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        y_train : numpy array of shape (n_train,)
            The training set.
            
        y_test : numpy array of shape (n_test,)
            The testing set.
        """
        m = len(y)
        y_train = y[:int(0.8*m)]
        y_test = y[int(0.8*m):]
        return y_train, y_test
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m, n = X.shape
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X]
        # Train model using batch gradient descent
        final_list = []
        # Initialize the parameters (including the bias term) to zeros
        self.theta = [np.zeros(n+1) for i in range(4)]
        parameter_array_1, cost_history = self.train(X_new,y,learning_rate,10,m,n,batch_size=1,index=0,window_size=5)
        parameter_array_2, cost_history = self.train(X_new,y,learning_rate,10000,m,n,batch_size=80,index=1,window_size=10)
        parameter_array_3, cost_history = self.train(X_new,y,learning_rate,10000,m,n,batch_size=8000,index=2,window_size=5)
        parameter_array_4, cost_history = self.train(X_new,y,learning_rate,1500,m,n,batch_size=800000,index=3,window_size=5)
        final_list.append(parameter_array_1)
        final_list.append(parameter_array_2)
        final_list.append(parameter_array_3)
        final_list.append(parameter_array_4)
        return final_list
    
    def train(self, X, y, learning_rate, n_iter, m, n, batch_size, index, window_size):
        """
        Train the model using Stochastic Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features+1)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.
            
        learning_rate : float
            The learning rate to use in the update rule.
            
        n_iter : int
            The number of iterations to train the model for.
            
        m : int
            The number of samples in the data.
            
        n : int
            The number of features in the data.
            
        Returns
        -------
        parameter_array : numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        start_time = time.time()
        parameter_array = np.zeros((n_iter, n+1))
        cost_history = []
        for i in range(n_iter):
            # Shuffle the data
            permuted_indices = np.random.permutation(m)
            X = X[permuted_indices]
            y = y[permuted_indices]
            for j in range(0, m, batch_size):
<A NAME="7"></A><FONT color = #0000FF><A HREF="match81-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                X_batch = X[j:j+batch_size]
                y_batch = y[j:j+batch_size]
                # h_theta
                h_theta = np.dot(X_batch, self.theta[index])
</FONT>                # Error
                error = h_theta - y_batch
                # Gradient
                gradient = np.dot(X_batch.T, error) / batch_size
                # Update the parameters
                self.theta[index] = self.theta[index] - learning_rate * gradient
            # Save parameters at the current iteration
            parameter_array[i] = self.theta[index]
            # Compute the cost
            cost = np.sum((np.dot(X, self.theta[index]) - y)**2) / (2*m)
            cost_history.append(cost)
            # Stop if the DMA is less than 1e-6
            if i &gt; window_size:
                # Difference in moving average
                dma = abs(np.mean(cost_history[i-window_size:i+1]) - np.mean(cost_history[i-window_size-1:i])) 
                if dma &lt; 1e-6:
                    parameter_array = parameter_array[:i+1]
                    # print(f'Converged after {i+1} iterations for batch size {batch_size}')
                    break
        # print(f'Time taken to train: {time.time() - start_time} seconds for batch size {batch_size}')
        return parameter_array, cost_history
    

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X]
        y_pred = []
        for i in range(len(self.theta)):
            y_pred.append(np.dot(X_new, self.theta[i]))
        return y_pred                                                         





# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.X_mean = None
        self.X_std = None

    # Split the data into training and testing sets
    def split_X_data(self, X):
        """
        Split the input data into training and testing sets.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        X_train : numpy array of shape (n_train, n_features)
            The training set.
            
        X_test : numpy array of shape (n_test, n_features)
            The testing set.
        """
        m = len(X)
        X_train = X[:int(0.8*m)]
        X_test = X[int(0.8*m):]
        return X_train, X_test
    
    def split_y_data(self, y):
        """
        Split the target values into training and testing sets.
        
        Parameters
        ----------
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        y_train : numpy array of shape (n_train,)
            The training set.
            
        y_test : numpy array of shape (n_test,)
            The testing set.
        """
        m = len(y)
        y_train = y[:int(0.8*m)]
        y_test = y[int(0.8*m):]
        return y_train, y_test
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        m, n = X.shape
        # Normalize input using training mean and std
        self.X_mean = np.mean(X, axis=0)
        self.X_std = np.std(X, axis=0)
        X_normal = (X - self.X_mean) / self.X_std
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X_normal]
        # Initialize theta
        self.theta = np.zeros(n+1)
        parameter_array = self.train(X_new, y, learning_rate,1000,m,n)
        return parameter_array

    # Activation function  
    def sigmoid(self,z):
        return 1/(1+np.exp(-z))
        
    # Function for training
    def train(self,X,y,alpha,n_iter,num_samples,num_features):
        m = num_samples
        n = num_features
        likelihood_history = []
        parameter_array = np.zeros((n_iter,n+1))
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match81-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for i in range(n_iter):
            # Define the sigmoid function
            h_theta = self.sigmoid(X @ self.theta)
            # Calculate the gradient
            gradient = X.T @ (y - h_theta) 
            # Calculate the Hessian
            R = np.diag(h_theta * (1 - h_theta))  # Hessian matrix diagonal elements
</FONT>            # Hessian matrix            
            hessian = -X.T @ R @ X
            # check if Hessian is invertible
            if np.linalg.det(hessian) == 0:
                lambda_ = 1e-10
                hessian = hessian + lambda_*np.eye(hessian.shape[0])
            # Delta
            delta = np.linalg.inv(hessian).dot(gradient)
            # Update theta and save it in the parameter array
            self.theta = self.theta - delta
            parameter_array[i] = self.theta
            # Calculate the likelihood
            likelihood = np.sum(y * np.log(h_theta) + (1 - y) * np.log(1 - h_theta))
            likelihood_history.append(likelihood)
            # Check for convergence
            if i &gt; 0 and abs(likelihood_history[i-1] - likelihood_history[i]) &lt; 1e-6:
                parameter_array = parameter_array[:i+1]
                break
        return parameter_array


    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m, n = X.shape
        # Normalize input using training mean and std
        X_normal = (X - self.X_mean) / self.X_std
        # Add column of 1s to X
        X_new = np.c_[np.ones(m), X_normal]
        # Predict the target values
        y_prob = self.sigmoid(X_new @ self.theta)
        y_pred = np.where(y_prob &gt; 0.5, 1, 0)
        return y_pred







import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor

# Initialise the model
model = LogisticRegressor()

def extract_Xy_from_csv(X_csv, y_csv):
    """
    Load the input and target values from the given CSV files.
    
    Returns
    -------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target values.
    """
    
    X = pd.read_csv(X_csv, header=None).values  # Read feature column
    y = pd.read_csv(y_csv, header=None).values.flatten()  # Read target column and flatten to 1D
    
    return X, y

# Load the data
X, y = extract_Xy_from_csv('../data/Q3/logisticX.csv', '../data/Q3/logisticY.csv')
 
# Split the data into training and testing sets
X_train , y_train = X[:int(.8*len(X)),:], y[:int(.8*len(y))]
X_test , y_test = X[int(0.8*len(X)):,:], y[int(0.8*len(y)):]
X_mean = np.mean(X, axis=0)
X_std = np.std(X, axis=0)
X_normal = (X - X_mean) / X_std  
# Train the model and predict
parameters = model.fit(X_train,y_train,learning_rate = 0.01)
# print(parameters[-1])
y_pred = model.predict(X)
theta_final = model.theta

# Plot training data with x1 and x2 axes
plt.figure(figsize=(8,6))
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], marker='x', color='red', label="label 0")
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], marker='o', color='blue', label="label 1")

# Decision Boundary: x2 in θ0 + θ1*x1 + θ2*x2 = 0
x1_vals = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)
x1_norm = (x1_vals - X_mean[0]) / X_std[0]
x2_vals = -(theta_final[0] + theta_final[1] * x1_norm) / theta_final[2]
# Convert x2 back to original scale
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match81-1.html#8" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

x2_vals = x2_vals * X_std[1] + X_mean[1]


# Plot the decision boundary
plt.plot(x1_vals, x2_vals, color='black', linewidth=2, label='Decision Boundary')

# Labels & Legends
plt.xlabel('Feature x1')
plt.ylabel('Feature x2')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
plt.grid(True)
</FONT>plt.show()



import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None 
        self.X_mean = None
        self.X_std = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        """
        m, n = X.shape
        self.X_mean = np.mean(X, axis=0)
        self.X_std = np.std(X, axis=0)
        X_normal = (X - self.X_mean) / self.X_std

        # Calculate class means
        self.mu_0 = np.mean(X_normal[y=='Alaska'], axis=0)
        self.mu_1 = np.mean(X_normal[y=='Canada'], axis=0)
        self.phi = np.sum(y=='Canada') / m
        
        # If both classes have the same covariance matrix
        if assume_same_covariance:
            mean_subtracted = X_normal - np.array([self.mu_0 if y[i] == 'Alaska' else self.mu_1 for i in range(m)])
            self.sigma = (mean_subtracted.T @ mean_subtracted) / m
            self.sigma_0 = self.sigma
            self.sigma_1 = self.sigma
            return self.mu_0, self.mu_1, self.sigma

        # If each class has its own covariance matrix
        else:
            mean_subtracted_0 = X_normal - self.mu_0
            mean_subtracted_1 = X_normal - self.mu_1
            
            # One-hot encoding
            indicator_0 = (y == 'Alaska').astype(int)
            indicator_1 = (y == 'Canada').astype(int)
            
           # Calculate sigma_0
            self.sigma_0 = (indicator_0[:, np.newaxis] * mean_subtracted_0).T @ mean_subtracted_0 / np.sum(indicator_0)
            
            # Calculate sigma_1
            self.sigma_1 = (indicator_1[:, np.newaxis] * mean_subtracted_1).T @ mean_subtracted_1 / np.sum(indicator_1)

            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m, n = X.shape

        # Normalize input using training mean and std
        X_normal = (X - self.X_mean) / self.X_std  

        numerator_0 = np.exp(-0.5 * np.sum((X_normal - self.mu_0) @ np.linalg.inv(self.sigma_0) * (X_normal - self.mu_0), axis=1))  
        numerator_1 = np.exp(-0.5 * np.sum((X_normal - self.mu_1) @ np.linalg.inv(self.sigma_1) * (X_normal - self.mu_1), axis=1))  

        denominator_0 = np.sqrt((2 * np.pi) ** n * np.linalg.det(self.sigma_0))
        denominator_1 = np.sqrt((2 * np.pi) ** n * np.linalg.det(self.sigma_1))

        prob_0 = (numerator_0 / denominator_0) * (1 - self.phi) # P(x|y=0)P(y=0)
        prob_1 = (numerator_1 / denominator_1) * self.phi # P(x|y=1)P(y=1)

        # Predict the target values
        y_pred = np.where(prob_1 &gt; prob_0, 'Canada', 'Alaska')

        return y_pred






import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis

# Load data
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)

# Split the data into training and testing sets
X_train , y_train = X[:int(0.8*len(X)),:], y[:int(0.8*len(y))]
X_test , y_test = X[int(0.8*len(X)):,:], y[int(0.8*len(y)):]
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match81-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X_normal = (X - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)


# Linear boundary ****************************************************
model_1 = GaussianDiscriminantAnalysis()
mu_0,mu_1,sigma = model_1.fit(X_train, y_train, assume_same_covariance=True)
</FONT># print(mu_0, mu_1, sigma)
# Calculate w and b
sigma_inv = np.linalg.inv(sigma)
w = sigma_inv @ (mu_1 - mu_0)
b = 0.5 * (mu_0.T @ sigma_inv @ mu_0 - mu_1.T @ sigma_inv @ mu_1) + np.log((1 - model_1.phi) / model_1.phi)
# Generate decision boundary line
x_vals = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)
# Convert normalized boundary equation back to original scale
x_vals_norm = (x_vals - model_1.X_mean[0]) / model_1.X_std[0]
y_vals_norm = - (w[0] / w[1]) * x_vals_norm - (b / w[1])
# Convert back to original scale
y_vals = y_vals_norm * model_1.X_std[1] + model_1.X_mean[1]


# Quadratic boundary ****************************************************
model_2 = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = model_2.fit(X_train, y_train, assume_same_covariance=False)
# print(mu_0, mu_1, sigma_0, sigma_1)
sigma_0_inv = np.linalg.inv(sigma_0)
sigma_1_inv = np.linalg.inv(sigma_1)

# Compute the quadratic coefficients A, B, and C in normalized space
A = 0.5 * (sigma_0_inv - sigma_1_inv)
B = (sigma_1_inv @ mu_1) - (sigma_0_inv @ mu_0)
C = (0.5 * (mu_0.T @ sigma_0_inv @ mu_0 - mu_1.T @ sigma_1_inv @ mu_1)
     + np.log(np.linalg.det(sigma_0) / np.linalg.det(sigma_1))
     + np.log((1 - model_2.phi) / model_2.phi))

# Generate mesh grid in original scale
x1_vals = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)
x2_vals = np.linspace(np.min(X_train[:, 1]), np.max(X_train[:, 1]), 100)
X1, X2 = np.meshgrid(x1_vals, x2_vals)

# Normalize mesh grid
X1_norm = (X1 - model_2.X_mean[0]) / model_2.X_std[0]
X2_norm = (X2 - model_2.X_mean[1]) / model_2.X_std[1]

# Compute quadratic decision boundary in normalized space
Z = (A[0, 0] * X1_norm**2 + 2 * A[0, 1] * X1_norm * X2_norm + A[1, 1] * X2_norm**2
     + B[0] * X1_norm + B[1] * X2_norm + C)


# Extract data points for Alaska and Canada
X_train_alaska = X_train[y_train == 'Alaska']
X_train_canada = X_train[y_train == 'Canada']

# Plot the training data
plt.figure(figsize=(8, 6))

# Plot Alaska data (use circles) and Canada data (use crosses)
plt.scatter(X_train_alaska[:, 0], X_train_alaska[:, 1], marker='o', color='red', label='Alaska')
plt.scatter(X_train_canada[:, 0], X_train_canada[:, 1], marker='x', color='violet', label='Canada')

# Plot linear descision boundary
plt.plot(x_vals, y_vals, color='green', linestyle='--')
linear_label = plt.Line2D([0], [0], color='green', linestyle='--', label="Linear Decision Boundary")
## Plot Quadratic decision boundary
quad_contour = plt.contour(X1, X2, Z, levels=[0], colors='blue', linestyles='solid')
# Add labels to the contour lines
quadratic_label = plt.Line2D([0], [0], color='blue', linestyle='solid', label="Quadratic Decision Boundary")

# # Add labels and legend
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Gaussian Discriminant Analysis Showing Linear and Quadratic Decision Boundaries")
# Combine all legend handles into a single legend
plt.legend(handles=[quadratic_label, linear_label, 
                    plt.Line2D([0], [0], marker='o', linestyle='None', color='red', markersize=8, label="Alaska"),
                    plt.Line2D([0], [0], marker='x', linestyle='None', color='violet', markersize=8, label="Canada")], 
           loc='upper left')

plt.grid(True)

plt.show()



</PRE>
</PRE>
</BODY>
</HTML>
