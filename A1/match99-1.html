<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_VBNQO.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

class LinearRegressor:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.theta = None
        self.cost_history = []
    
    def calculate_cost(self, X, y):
        num_samples = len(y)
        y_pred = np.dot(X, self.theta)
        cost = np.sum((y_pred - y) ** 2) / (2 * num_samples)
        return cost
    
    def update_params(self, X, y):
        num_samples = len(y)
        y_pred = np.dot(X, self.theta)
        gradient = np.dot(X.T, (y_pred - y)) / num_samples
        self.theta -= self.learning_rate * gradient
    
    def gradient_descent(self, X, y):
        for epoch in range(self.epochs):
            cost = self.calculate_cost(X, y)
            self.update_params(X, y)
            self.cost_history.append(cost)
            if epoch % 100 == 0:
                print(f"Cost after {epoch+1} iterations: {cost}")
    
    def fit(self, X, y):
        num_samples, num_features = X.shape
        X_b = np.c_[np.ones((num_samples, 1)), X]  # Add bias term as first column
        self.theta = np.zeros((num_features + 1, 1))
        y = y.reshape(-1, 1)
        
        self.gradient_descent(X_b, y)
        return self.theta, self.cost_history
    
    def predict(self, X):
        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term
        return np.dot(X_b, self.theta)

# Load dataset
x = pd.read_csv('../data/Q1/linearX.csv')
y = pd.read_csv('../data/Q1/linearX.csv')

X_train = np.array(x)
y_train = np.array(y)

indices = np.random.permutation(len(X_train))
split_idx = int(0.8 * len(X_train))

train_indices = indices[:split_idx]
test_indices = indices[split_idx:]

X_train, X_test = X_train[train_indices], X_train[test_indices]
y_train, y_test = y_train[train_indices], y_train[test_indices]

model = LinearRegressor(learning_rate=0.01, epochs=1000)
model.fit(X_train, y_train)
predictions = model.predict(X_test) # Do prediction
mse = np.mean((predictions - y_test) ** 2)
print(f"Mean Squared Error: {mse}")

plt.plot(model.cost_history)
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.title('Cost History')
plt.show()

#2d graph scatter plot
plt.scatter(x,y)
plt.show()

#Hypo
plt.plot(X_train, model.predict(X_train))
plt.show()
# After model training and before visualization, add:
# Prepare data for visualization
num_samples = len(X_train)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match99-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X_b = np.c_[np.ones((num_samples, 1)), X_train]  # Add bias term

# Create theta values for visualization
theta0_vals = np.linspace(-10, 10, 100)
theta1_vals = np.linspace(-10, 10, 100)
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
</FONT>
# Setup the figure
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Calculate cost for each theta combination
for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        theta_temp = np.array([[theta0_vals[i]], [theta1_vals[j]]])
        J_vals[i, j] = np.sum((np.dot(X_b, theta_temp) - y_train.reshape(-1,1)) ** 2) / (2 * len(y_train))

# Create meshgrid for 3D surface
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

# Create the 3D surface plot with color gradient
surf = ax.plot_surface(T0, T1, J_vals, 
                      cmap='viridis',     # Color map
                      alpha=0.8,          # Transparency
                      linewidth=0,        # No grid lines
                      antialiased=True)   # Smooth surface

# Add color bar and labels
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match99-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='Cost J(θ)')
ax.set_xlabel('Theta 0')
ax.set_ylabel('Theta 1')
ax.set_zlabel('Cost J(θ)')
ax.set_title('Cost Function Surface')

# Plot optimal point found by gradient descent
optimal_theta = model.theta.flatten()
</FONT>optimal_cost = model.calculate_cost(X_b, y_train.reshape(-1,1))
ax.scatter(optimal_theta[0], optimal_theta[1], optimal_cost,
          color='red', marker='*', s=200, label='Optimal θ')

# Print final parameters after training
print("Final Parameters (θ):")
print(f"Intercept (θ0): {model.theta[0][0]}")
print(f"Slope (θ1): {model.theta[1][0]}")


# Adjust view and show plot
ax.view_init(elev=30, azim=45)
ax.legend()
plt.tight_layout()
plt.show()



import numpy as np
import matplotlib.pyplot as plt

<A NAME="0"></A><FONT color = #FF0000><A HREF="match99-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
    y = np.dot(X, theta[1:]) + theta[0] + noise
</FONT>    return X, y
def compute_cost(X,y,theta):
    m = len(y)
    predictions = np.dot(X, theta[1:]) + theta[0]
    errors = y - predictions
    cost = (1 / (2 * m)) * np.sum(errors ** 2)
    return cost
def closed(X,y):
    X_bias = np.c_[np.ones(X.shape[0]), X]
    return np.linalg.pinv(X_bias) @ y
    #not using inverse because it mighgt fail if singular matrix to avoid we dont use np,lianlg,inv fn

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01, epochs=100, batch_size=10):
        nsam = X.shape[0]
        nfeat = X.shape[1]
        self.theta = np.ones(nfeat + 1)
        theta_history = []
        cost_history = []

        for epoch in range(epochs):
            indices = np.random.permutation(nsam)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            err_prev=float('inf')
            mse=[]

            for i in range(0, nsam, batch_size):
                xx = X_shuffled[i:i + batch_size]
                yy = y_shuffled[i:i + batch_size]
                predictions = np.sum(xx * self.theta[1:], axis=1) + self.theta[0]
                errors = yy - predictions
                mse.append(errors**2)

                gradient = np.zeros(nfeat + 1)
                gradient[0] = -2 * np.sum(errors)
                gradient[1:] = -2 * np.dot(errors, xx)

                self.theta -= learning_rate * gradient
                theta_history.append(self.theta.copy())

            cost = np.mean(errors ** 2)
            cost_history.append(cost)
            if (err_prev-cost)&lt;10**(-3):
                print("stop")
                break
            #calculating mean sq  error
            meansq=np.sum(mse)/len(mse)
        return np.array(theta_history), np.array(cost_history)

    def predict(self, X):
        theta1 = self.theta[1:]
        theta0 = self.theta[0]
        return np.dot(X, theta1) + theta0

theta_true = np.array([3, 1.5, -2])  # True coefficients
input_mean = np.array([0, 0])
input_sigma = np.array([1, 1])
noise_sigma = 0.5

# Generate synthetic data
N = 1000
X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train the model
model = StochasticLinearRegressor()
theta_history, cost_history = model.fit(X, y, learning_rate=0.01, epochs=100, batch_size=10)

# Predict on new data
X_test = np.array([[0.5, -1], [-0.3, 2], [1, 1.5]])
y_pred = model.predict(X_test)
print("Learned Parameters:", model.theta)
print("Predictions:", y_pred)
theta_closed = closed(X, y)
print("Closed-form solution:", theta_closed)

# Compute MSE for train and test set
X_test = np.random.normal(input_mean, input_sigma, (200, 2))
y_test = np.dot(X_test, theta_true[1:]) + theta_true[0] + np.random.normal(0, noise_sigma, 200)

train_mse = compute_cost(X, y, model.theta)
test_mse = compute_cost(X_test, y_test, model.theta)
print("Train MSE:", train_mse)
print("Test MSE:", test_mse)

# Plot parameter movement in 3D
theta_history = np.array(theta_history)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
<A NAME="2"></A><FONT color = #0000FF><A HREF="match99-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o')
ax.set_xlabel("Theta 0")
ax.set_ylabel("Theta 1")
ax.set_zlabel("Theta 2")
ax.set_title("SGD Parameter Updates for last batch size")
</FONT><A NAME="6"></A><FONT color = #00FF00><A HREF="match99-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class LogisticRegressor:
    def __init__(self):
        self.theta = None
</FONT>    
    def normalize(self, X):
        """Normalize features using (x - mean) / std"""
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return (X - self.mean) / self.std
    
    def fit(self, X, y, max_iter=100):
        """
        Fit logistic regression using Newton's Method
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
        y : numpy array of shape (n_samples,)
        max_iter : int, maximum iterations for Newton's Method
        
        Returns
        -------
        theta_history : numpy array of shape (n_iter, n_features+1)
        """
        # Normalize features
        X_normalized = self.normalize(X)
        
        # Add intercept term
        X = np.c_[np.ones(X_normalized.shape[0]), X_normalized]
        
        # Initialize parameters
        self.theta = np.zeros(X.shape[1])
        theta_history = []
        
        for iter in range(max_iter):
            # Compute hypothesis h(x)
            z = X @ self.theta
            h = 1 / (1 + np.exp(-z))
            
            # Compute log-likelihood gradient
            grad = X.T @ (y - h)
            
            # Compute Hessian
            diag_h = np.diag(h * (1 - h))
            H = -X.T @ diag_h @ X
            
            # Update parameters using Newton's method
            try:
                delta = np.linalg.solve(-H, grad)
            except np.linalg.LinAlgError:
                delta = np.linalg.pinv(-H) @ grad
            
            self.theta += delta
            theta_history.append(self.theta.copy())
            
            # Check convergence
            if np.linalg.norm(delta) &lt; 1e-5:
                print(f"Converged after {iter+1} iterations")
                break
                
        return np.array(theta_history)
    
    def predict(self, X):
        """Predict class labels for samples in X"""
        X_normalized = (X - self.mean) / self.std
        X = np.c_[np.ones(X_normalized.shape[0]), X_normalized]
        probs = 1 / (1 + np.exp(-X @ self.theta))
        return (probs &gt;= 0.5).astype(int)
    
    def decision_boundary(self, X):
        """Return coordinates for decision boundary"""
        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        
        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),
                              np.linspace(x2_min, x2_max, 100))
        grid = np.c_[xx1.ravel(), xx2.ravel()]
        
        # Normalize grid points
        grid_normalized = (grid - self.mean) / self.std
        
        # Get predictions
        probs = 1 / (1 + np.exp(-(np.c_[np.ones(len(grid_normalized)), grid_normalized] @ self.theta)))
        Z = probs.reshape(xx1.shape)
        
        return xx1, xx2, Z

if __name__ == "__main__":
    # Load data
    X = pd.read_csv('../data/Q3/logisticX.csv', header=None).values
    y = pd.read_csv('../data/Q3/logisticY.csv', header=None).values.ravel()
    
    # Train model
    model = LogisticRegressor()
    theta_history = model.fit(X, y)
    
    # Print final coefficients
    print("\nFinal coefficients (theta):")
    print(f"Intercept: {model.theta[0]:.4f}")
    print(f"Coefficient 1: {model.theta[1]:.4f}")
    print(f"Coefficient 2: {model.theta[2]:.4f}")
    
    # Plot data and decision boundary
    plt.figure(figsize=(10, 8))
    
    # Plot training data
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='red', marker='o', label='Class 0')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='blue', marker='+', label='Class 1')
    
    # Plot decision boundary
    xx1, xx2, Z = model.decision_boundary(X)
    plt.contour(xx1, xx2, Z, levels=[0.5], colors='k')
    
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Logistic Regression Decision Boundary')
    plt.legend()
    plt.grid(True)
    plt.show()



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.phi = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
    
    def fit(self, X, y, assume_same_covariance=True):
        self.phi = np.mean(y)
<A NAME="5"></A><FONT color = #FF0000><A HREF="match99-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.mu_0 = np.mean(X[y==0], axis=0)
        self.mu_1 = np.mean(X[y==1], axis=0)
        
        if assume_same_covariance:
            self.sigma = np.cov(X.T, bias=True)
</FONT><A NAME="1"></A><FONT color = #00FF00><A HREF="match99-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_1.gif" ALT="other" BORDER="0" ALIGN=left></A>

        else:
            self.sigma_0 = np.cov(X[y==0].T, bias=True)
            self.sigma_1 = np.cov(X[y==1].T, bias=True)
</FONT>        
        print("phi:", self.phi)
        print("mu_0 (Alaska):", self.mu_0)
        print("mu_1 (Canada):", self.mu_1)
        print("sigma:", self.sigma if assume_same_covariance else None)
        
    def calc_coeff(self, x, u, sigma):
        d = len(x)
        return 1/((2*np.pi)**(d/2)*np.linalg.det(sigma)**0.5)
    
    def exponential(self, x, u, sigma):
        diff = x - u
        sigma_inverse = np.linalg.inv(sigma)
        return np.exp(-0.5*np.dot(np.dot(diff.T, sigma_inverse), diff))
    
    def calc_probability(self, X, u0, sigma0, u1, sigma1):
        prob0s = []
        prob1s = []
        for x in X:
            prob0 = self.calc_coeff(x, u0, sigma0)*self.exponential(x, u0, sigma0)
            prob1 = self.calc_coeff(x, u1, sigma1)*self.exponential(x, u1, sigma1)
            prob0s.append(prob0)
            prob1s.append(prob1)
        return prob0s, prob1s
    
    def predict(self, X, u0, sigma0, u1, sigma1):
        prob0s, prob1s = self.calc_probability(X, u0, sigma0, u1, sigma1)
        return np.array([0 if prob0&gt;prob1 else 1 for prob0, prob1 in zip(prob0s, prob1s)])

def read_data(filename):
    """Read data from .dat file with support for categorical data"""
    try:
        data = pd.read_csv(filename, header=None, delim_whitespace=True)
        if data.dtypes.iloc[0] == 'object':
            return data.iloc[:, 0].values
        return data.values
    except:
        return np.loadtxt(filename)

def plot_decision_boundary(model, X, y):
    # Create mesh grid
<A NAME="7"></A><FONT color = #0000FF><A HREF="match99-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
</FONT>    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))
    
    # Make predictions for each point in the mesh
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()], 
                     model.mu_0, model.sigma, model.mu_1, model.sigma)
    Z = Z.reshape(xx.shape)
    
    # Plot decision boundary and data points
    plt.contour(xx, yy, Z, levels=[0.5], colors='k')
    plt.scatter(X[y==0][:, 0], X[y==0][:, 1], c='red', marker='o', label='Alaska')
    plt.scatter(X[y==1][:, 0], X[y==1][:, 1], c='blue', marker='+', label='Canada')

if __name__ == "__main__":
    # Load and preprocess data
    X = read_data('../data/Q4/q4x.dat')
    y_raw = read_data('../data/Q4/q4y.dat')
    y = np.array([1 if label=='Canada' else 0 for label in y_raw])
    
    # Train model with shared covariance
    print("\nTraining GDA with shared covariance:")
    model = GaussianDiscriminantAnalysis()
    model.fit(X, y, assume_same_covariance=True)
    
    # Make predictions
    predictions = model.predict(X, model.mu_0, model.sigma, model.mu_1, model.sigma)
    accuracy = np.mean(predictions == y)
    print(f"\nAccuracy: {accuracy:.4f}")
    
    # Plot results
    plt.figure(figsize=(10, 8))
    plot_decision_boundary(model, X, y)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('GDA Classification Results (Shared Covariance)')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    # Train model with separate covariances
    print("\nTraining GDA with separate covariances:")
    model_separate = GaussianDiscriminantAnalysis()
    model_separate.fit(X, y, assume_same_covariance=False)
    
    # Make predictions with separate covariances
    predictions_separate = model_separate.predict(X, model_separate.mu_0, 
                                                model_separate.sigma_0, 
                                                model_separate.mu_1, 
                                                model_separate.sigma_1)
    accuracy_separate = np.mean(predictions_separate == y)
    print(f"\nAccuracy (separate covariances): {accuracy_separate:.4f}")

</PRE>
</PRE>
</BODY>
</HTML>
