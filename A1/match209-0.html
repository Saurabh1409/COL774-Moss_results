<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_DALFY.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_DALFY.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import time
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None

    def gradient(self,X,y,theta):
        hypothesis = np.dot(X, theta)
        m = X.shape[0]
        difference = hypothesis - y
        X_T = X.T
        final_gradient = (np.dot(X_T, difference))/m
        return final_gradient

    def compute_J_theta(self,X,y,theta):
        m = X.shape[0]
        error =  np.dot(X, theta) - y
        squarred_error = np.square(error)
        total_error = np.sum(squarred_error)
        J_theta = total_error / (2 * m)
        return J_theta

    def cost_corresponding_theta(self, X, y, theta_array):
        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X))
        store_J_theta = []
        for i in theta_array:
            i = i.reshape(-1, 1)
            new_cost = self.compute_J_theta(X_features, y, i)
            store_J_theta.append(new_cost)
        return store_J_theta

    """
    def plot3Graph(self, X, y):

        thetas = self.fit(X, y, 0.1)
        start_time = time.time()
        store_j_theta = self.cost_corresponding_theta(X, y, thetas)
        end_time = time.time()
        total_time = end_time - start_time
        #print("time taken for learning rate 0.1", total_time)
        self.plot2DContoursWithoutPause(thetas, store_j_theta, X, y)

        thetas = self.fit(X, y, 0.025)
        start_time = time.time()
        store_j_theta = self.cost_corresponding_theta(X, y, thetas)
        end_time = time.time()
        total_time = end_time - start_time
        #print("time taken for learning rate 0.025", total_time)
        self.plot2DContoursWithoutPause(thetas, store_j_theta, X, y)

        thetas = self.fit(X, y, 0.001)
        start_time = time.time()
        store_j_theta = self.cost_corresponding_theta(X, y, thetas)
        end_time = time.time()
        total_time = end_time - start_time
        #print("time taken for learning rate 0.001", total_time)
        self.plot2DContoursWithoutPause(thetas, store_j_theta, X, y)

    def plot(self,X,y):
        plt.scatter(X, y, color='blue', label='Actual data')
        hypothesis = self.predict(X)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match209-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.plot(X, hypothesis, color='red', label='Learned hypothesis')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Linear Regression Data vs. Learned Hypothesis')
        plt.legend()
        plt.show()

    def plot3D(self, thetas, store_j_theta, X, y):
</FONT>        stored_thetas = np.array(thetas)
        stored_corresponding_j = np.array(store_j_theta)
        theta_0_values = stored_thetas[:, 0]
        theta_1_values = stored_thetas[:, 1]

        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X))

<A NAME="0"></A><FONT color = #FF0000><A HREF="match209-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_0_vals = np.linspace(min(theta_0_values) - 5, max(theta_0_values) + 10, 100)
        theta_1_vals = np.linspace(min(theta_1_values) - 5, max(theta_1_values) + 30, 100)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)
        Z = np.zeros_like(T0)
</FONT>        for i in range(len(theta_0_vals)):
            for j in range(len(theta_1_vals)):
                temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
                Z[i, j] = self.compute_J_theta(X_features, y, temp_theta)


        #print(Z)
        #print("----------")
        #print(stored_corresponding_j)

        fig = plt.figure(figsize=(10, 7))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(T0, T1, Z, cmap='viridis', alpha=0.5)
        ax.set_xlabel("Theta 0")
        ax.set_ylabel("Theta 1")
        ax.set_zlabel("Cost J(Theta)")
        ax.set_title("3D Visualization")
        for i in range(1, len(stored_thetas)):
            ax.scatter(stored_thetas[i, 0], stored_thetas[i, 1], stored_corresponding_j[i],
                       color='r', marker='o')
            plt.draw()
            plt.pause(0.2)

        plt.show()
        

    def plot2DContoursWithoutPause(self, thetas, store_j_theta, X, y):
        stored_thetas = np.array(thetas)
        theta_0_values = stored_thetas[:, 0]
        theta_1_values = stored_thetas[:, 1]
<A NAME="1"></A><FONT color = #00FF00><A HREF="match209-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta_0_vals = np.linspace(min(theta_0_values) - 30, max(theta_0_values) + 30, 50)
        theta_1_vals = np.linspace(min(theta_1_values) - 30, max(theta_1_values) + 30, 50)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

        m = X.shape[0]
</FONT>        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X))

        Z = np.zeros_like(T0)
        for i in range(len(theta_0_vals)):
            for j in range(len(theta_1_vals)):
                temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
                Z[i, j] = self.compute_J_theta(X_features, y, temp_theta)

<A NAME="2"></A><FONT color = #0000FF><A HREF="match209-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        fig, ax = plt.subplots(figsize=(8, 6))
        contour = ax.contourf(T0, T1, Z, levels=50, cmap="viridis", alpha=0.8)
        plt.colorbar(contour)
</FONT>
        for i in range(1, len(stored_thetas)):
            ax.scatter(stored_thetas[i, 0], stored_thetas[i, 1], color='r', marker='o')
            ax.set_xlabel("Theta 0")
            ax.set_ylabel("Theta 1")
            ax.set_title("Contour Plot")
            plt.draw()
        plt.show()
    

    def plot2DContours(self, thetas, store_j_theta, X, y):
        stored_thetas = np.array(thetas)
        theta_0_values = stored_thetas[:, 0]
        theta_1_values = stored_thetas[:, 1]
        theta_0_vals = np.linspace(min(theta_0_values) - 30, max(theta_0_values) + 30, 50)
        theta_1_vals = np.linspace(min(theta_1_values) - 30, max(theta_1_values) + 30, 50)
        T0, T1 = np.meshgrid(theta_0_vals, theta_1_vals)

        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X))

        Z = np.zeros_like(T0)
        for i in range(len(theta_0_vals)):
            for j in range(len(theta_1_vals)):
                temp_theta = np.array([[T0[i, j]], [T1[i, j]]])
                Z[i, j] = self.compute_J_theta(X_features, y, temp_theta)

        fig, ax = plt.subplots(figsize=(8, 6))
        contour = ax.contourf(T0, T1, Z, levels=50, cmap="viridis", alpha=0.8)
        plt.colorbar(contour)

        for i in range(1, len(stored_thetas)):
            ax.scatter(stored_thetas[i, 0], stored_thetas[i, 1], color='r', marker='o')
            ax.set_xlabel("Theta 0")
            ax.set_ylabel("Theta 1")
            ax.set_title("Contour Plot")
            plt.draw()
            plt.pause(0.2)
        plt.show()
        
    """
    
    def fit(self, X, y, learning_rate=0.08):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1,1)

        X_features = np.hstack((ones_intercept_column, X))

        #print(X_features.shape)
        #print("---------")
        #print(y.shape)
        num_features_rows = X_features.shape[1]
        num_features_cols = y.shape[1]

        store_theta = []
        self.theta = np.zeros((num_features_rows,num_features_cols))
        store_theta.append(np.array(self.theta.copy()))
        #print(self.theta.shape)

        while (True):
            old_cost = self.compute_J_theta(X_features, y, self.theta)
            grad = self.gradient(X_features, y, self.theta)
            self.theta = self.theta - (learning_rate * grad)
            new_cost = self.compute_J_theta(X_features, y, self.theta)
            store_theta.append(np.array(self.theta.copy()))
            diff = abs(old_cost - new_cost)
            if (diff &lt; pow(10, -5)):
                break

        store_theta = np.array(store_theta).squeeze()
        return store_theta

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X))
        y_pred = np.dot(X_features, self.theta).flatten()
        #print(y_pred.shape)
        return y_pred

def main():
    features_path = "../data/Q1/linearX.csv"
    actual_output_path = "../data/Q1/linearY.csv"

    X = np.loadtxt(features_path)
    y = np.loadtxt(actual_output_path)

    model = LinearRegressor()
    thetas = model.fit(X, y)
    #print(thetas[-1])
    #ans = model.predict(X)
    #print(ans)
    store_j_theta = model.cost_corresponding_theta(X, y, thetas)
    #model.plot(X, y)
    #model.plot3D(thetas, store_j_theta, X, y)
    #model.plot2DContours(thetas, store_j_theta, X, y)

    #model.plot3Graph(X,y)
    #print("final cost is : ",store_j_theta[-1])

if __name__ == '__main__':
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.

    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.

    input_mean : numpy array of shape (2,)
        The mean of the input data.

    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.

    noise_sigma : float
        The standard deviation of the Gaussian noise.

    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.

    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, size=(N, 2))
    ones_intercept_column = np.ones((X.shape[0], 1))
    X_Bias = np.hstack((ones_intercept_column, X))

    noise = np.random.normal(0, noise_sigma, size=(N,))
    y = np.dot(X_Bias, theta) + noise

    #print(X_Bias.shape)
    #print(y.shape)
    return X, y

"""
def plot_Xandy(X, y):
    plt.scatter(X[:, 0], y, label="Feature 1 vs Target", alpha=0.5)
    plt.scatter(X[:, 1], y, label="Feature 2 vs Target", alpha=0.5, color='green')
    plt.xlabel("Feature Values")
    plt.ylabel("Target Value")
    plt.title("Scatter Plot of Features vs Target")
    plt.show()


def plot_theta_trajectory(theta_updates):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    theta_updates = np.array(theta_updates)
    ax.plot(theta_updates[:, 0], theta_updates[:, 1], theta_updates[:, 2])
    ax.set_xlabel(r'theta_0')
    ax.set_ylabel(r'theta_1')
    ax.set_zlabel(r'theta_2')
    ax.set_title(f'Trajectory of Theta Updates')
    plt.pause(0.2)
    plt.show()


def plot_theta_trajectories(theta_batches, batch_sizes):
    fig = plt.figure(figsize=(10, 10))

    for i, (theta_updates, batch_size) in enumerate(zip(theta_batches, batch_sizes)):
        ax = fig.add_subplot(2, 2, i + 1, projection='3d')
        theta_updates = np.array(theta_updates)
        ax.plot(theta_updates[:, 0], theta_updates[:, 1], theta_updates[:, 2])
        ax.set_xlabel(r'theta')
        ax.set_ylabel(r'theta_1')
        ax.set_zlabel(r'theta_2')
        ax.set_title(f'Batch Size: {batch_size}')

    plt.tight_layout()
    plt.show()
"""

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.batch = 8000

    def gradient(self, X, y, theta):
        hypothesis = np.dot(X, theta)
        m = X.shape[0]
        difference = hypothesis - y.reshape(-1,1)
        X_T = X.T
        final_gradient = (np.dot(X_T, difference)) / m
        return final_gradient

    def compute_J_theta(self, X, y, theta):
        m = X.shape[0]
        #print(np.dot(X, theta))
        error = np.dot(X, theta) - y.reshape(-1, 1)
        total_error = np.dot(error.T,error)
        J_theta = total_error / (2 * m)
        return J_theta

    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        ones_intercept_column_train = np.ones((X.shape[0], 1))
        X_train_features = np.hstack((ones_intercept_column_train, X))
        column_X = X_train_features.shape[1]
        self.theta = np.zeros((column_X,1))

        store_theta = []
        m = X_train_features.shape[0]
        batch_size = self.batch
        threshold = 1e-8

        permuted_indices = np.random.permutation(m)
        X_shuffled = X_train_features[permuted_indices]
        y_shuffled = y[permuted_indices]

        while True:

            old_cost = self.compute_J_theta(X_shuffled,y_shuffled,self.theta)
            for i in range(0, m, batch_size):
                end = min(i + batch_size, m)
                X_batch = X_shuffled[i:end]
                y_batch = y_shuffled[i:end].reshape(-1, 1)

                grad = self.gradient(X_batch, y_batch, self.theta)
                grad = np.clip(grad, -1000000, 1000000)
                self.theta = self.theta - (learning_rate * grad)


            new_cost = self.compute_J_theta(X_shuffled,y_shuffled,self.theta)
            store_theta.append(self.theta.copy())

            # Check for convergence
            cost_diff = abs(new_cost - old_cost)
            #print( "Cost Diff:", cost_diff)

            if cost_diff &lt; threshold:
                break


        return np.array(store_theta)

    def fit_different_batch(self,X,y,learning_rate = 0.001):
        ones_intercept_column_train = np.ones((X.shape[0], 1))
        X_train_features = np.hstack((ones_intercept_column_train, X))
        column_X = X_train_features.shape[1]
        self.theta = np.zeros((column_X, 1))

        m = X_train_features.shape[0]
        batches = [1,80,8000,800000]
        result = []
        threshold = 1e-8
        permuted_indices = np.random.permutation(m)
        X_shuffled = X_train_features[permuted_indices]
        y_shuffled = y[permuted_indices]
        self.theta = np.zeros((column_X,1))

        for batch in batches:
            store_theta = []
            start_time = time.time()
            itr = 0
            while True:
                old_cost = self.compute_J_theta(X_shuffled, y_shuffled, self.theta)
                itr = itr +1
                for i in range(0, m, batch):
                    end = min(i + batch, m)
                    X_batch = X_shuffled[i:end]
                    y_batch = y_shuffled[i:end].reshape(-1, 1)

                    grad = self.gradient(X_batch, y_batch, self.theta)
                    grad = np.clip(grad, -1000000, 1000000)
                    self.theta = self.theta - (learning_rate * grad)
                    if(i%1000 == 0):
                        store_theta.append(self.theta.copy())

                new_cost = self.compute_J_theta(X_shuffled, y_shuffled, self.theta)
                store_theta.append(self.theta.copy())

                cost_diff = abs(new_cost - old_cost)

                if cost_diff &lt; threshold:
                    break
            end_time = time.time()
            total_time = end_time - start_time
            result.append(np.array(store_theta))
            #print("Batch Size", batch, ", theta is : ", store_theta[-1])
            #print("Time Taken : ", total_time)
            #print("Number of iteration  ", itr)
            self.theta = np.zeros((column_X,1))

        return np.array(result, dtype=object)


    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features+1)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        # print("X shape is :", X.shape)
        #print("theta shape is :", self.theta.shape)
        ones_intercept_column_test = np.ones((X.shape[0], 1))
        X = np.hstack((ones_intercept_column_test, X))
        y_pred = np.dot(X, self.theta)
        y_pred = y_pred.flatten()
        # print(y_pred.shape)
        return y_pred

    def LinearRegressionNormalization(self, X, y):
        ones_intercept_column_train = np.ones((X.shape[0], 1))
        X_train_features = np.hstack((ones_intercept_column_train, X))
        X_T = X_train_features.T
        y = y.reshape(-1, 1)
        theta = np.dot(np.linalg.inv(np.dot(X_T, X_train_features)), np.dot(X_T, y))
        return theta

    def meanSquareError(self, X, y):
        m = X.shape[0]
        hypothesis = self.predict(X)
        # print(hypothesis.shape)
        error = hypothesis - y
        error = np.sum(error ** 2) / m
        return error

    def meanSquareErrorNormal(self, X, y, theta):
        ones_intercept_column_test = np.ones((X.shape[0], 1))
        X = np.hstack((ones_intercept_column_test, X))
        m = X.shape[0]
        hypothesis = np.dot(X, theta)
        # print(hypothesis.shape)
        error = hypothesis - y.reshape(-1, 1)
        mse = np.sum(error ** 2) / m
        return mse


def main():
    theta = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2])
    number = 1000000
    noise_variance = 2
    noise_standard = np.sqrt(noise_variance)

    X, y = generate(number, theta, input_mean, input_sigma, noise_standard)

    test_index = int(0.8 * number)
    train_x, train_y = X[:test_index], y[:test_index]
    test_x, test_y = X[test_index:], y[test_index:]
    #plot_Xandy(train_x, train_y)

    # print(ones_intercept_column)
    # print(X_test_features.shape[0])

    model = StochasticLinearRegressor()
    thetas = model.fit(train_x, train_y)
    linear_theta = model.LinearRegressionNormalization(train_x, train_y)
    #print("The result from the SGD is ", thetas[-1])
    #print("The paramter form the closed form equation is ", linear_theta)

    mean_test_error = model.meanSquareError(test_x, test_y)
    normal_mean_test_error = model.meanSquareErrorNormal(test_x, test_y, linear_theta)
    mean_train_error = model.meanSquareError(train_x,train_y)
    normal_mean_train_error = model.meanSquareErrorNormal(train_x, train_y, linear_theta)

    #print("The Mean test error of SGD is ", mean_test_error)
    #print(normal_mean_test_error)
    #print("The Mean train error of SGD is ", mean_train_error)

    #print("The Mean test error of closed form solution is ", normal_mean_test_error)
    # print(normal_mean_test_error)
    #print("The Mean train error of closed form solution  is ", normal_mean_train_error)

    #thetas_batches = model.fit_different_batch(train_x, train_y)
    #ans = model.predict(test_x)
    #print(ans)
    #print(thetas_batches)

    #print(mean_test_error)
    #print(normal_mean_test_error)


    #plot_theta_trajectory(thetas)
    #plot_theta_trajectories(thetas_batches,[1,80,8000,800000])
    return 0

if __name__ == '__main__':
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_normalized = (X - X_min) / (X_max - X_min)

        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        if y.ndim == 1:
            y = y.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X_normalized))

        #print(X_features.shape)
        # print("---------")
        # print(y.shape)
        num_features_rows = X_features.shape[1]
        num_features_cols = y.shape[1]
        self.theta = np.zeros((num_features_rows, 1))
        store_parameter = []
        threshold = 1e-3

        while True:
            hypothesis = self.sigmoid(X_features @ self.theta)
            gradient = (X_features.T @ (hypothesis - y)) / m
            Hessian = (X_features.T @ np.diag((hypothesis * (1 - hypothesis)).flatten()) @ X_features) / m

            theta_new = self.theta - np.linalg.inv(Hessian) @ gradient
            store_parameter.append(np.array(theta_new.copy()))

            if np.linalg.norm(theta_new - self.theta, ord=2) &lt; threshold:
                break

            self.theta = theta_new

        store_parameter = np.array(store_parameter).squeeze()
        return store_parameter

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_normalized = (X - X_min) / (X_max - X_min)

        m = X.shape[0]
        ones_intercept_column = np.ones((m, 1))
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        X_features = np.hstack((ones_intercept_column, X_normalized))
        hypothesis = self.sigmoid(X_features @ self.theta)
        for i in range(len(hypothesis)):
            if hypothesis[i] &lt;= 0.5:
                hypothesis[i] = 0
            else:
                hypothesis[i] = 1
        hypothesis = hypothesis.squeeze()
        #print(hypothesis)
        return hypothesis
"""
<A NAME="5"></A><FONT color = #FF0000><A HREF="match209-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

def plot_decision_boundary(X, y, model):
    X_min = np.min(X, axis=0)
    X_max = np.max(X, axis=0)
    X_norm = (X - X_min) / (X_max - X_min)
</FONT>    X_class0 = X_norm[y == 0]
    X_class1 = X_norm[y == 1]
    x0 = X_class0[:, 0]
    y0 = X_class0[:, 1]
    x1 = X_class1[:, 0]
    y1 = X_class1[:, 1]
    plt.scatter(x0, y0, marker='o', color='red', label='Class 0')
    plt.scatter(x1, y1, marker='x', color='blue', label='Class 1')
    theta_final = model.theta.flatten()
    x_values = np.linspace(0, 1, 100)
    y_values = -(theta_final[0] + theta_final[1] * x_values) / theta_final[2]
    plt.plot(x_values, y_values, color='green', label='Decision Boundary')
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.title('Logistic Regression Decision Boundary')
    plt.grid(True)
    plt.show()
"""
def main():
    features_path = "../data/Q3/logisticX.csv"
    actual_output_path = "../data/Q3/logisticY.csv"

    X = np.loadtxt(features_path, delimiter=",")
    y = np.loadtxt(actual_output_path)

    model = LogisticRegressor()
    parameter = model.fit(X, y)
    #print(parameter[-1])
    ans = model.predict(X)
    #print(ans)

    #print(X)
    #print(y)
    #plot_decision_boundary(X, y, model)

if __name__ == '__main__':
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mean_feature1 = None
        self.mean_feature2 = None
        self.sigma = None
        self.phi_feature1 = None
        self.phi_feature2 = None
        self.sigma_feature1 = None
        self.sigma_feature2= None

    def calculate_prior(self, y, label):
        size = y.shape[0]
        count = np.sum(y == label)
        return count / size

    def calculate_mean(self, X, y, label):
        count_label = np.sum(y == label)
        mean = np.sum(X[y == label], axis=0)
        return mean/count_label

    def calculate_covariance(self, X, y, mean_feature1, mean_feature2, assume_same_covariance):
        m = X.shape[0]

        sigma_feature1 = np.cov(X[y == 0] - mean_feature1, rowvar=False, bias=True)
        sigma_feature2= np.cov(X[y == 1] - mean_feature2, rowvar=False, bias=True)

        if assume_same_covariance:
            sigma = (sigma_feature1 * np.sum(y == 0) + sigma_feature2* np.sum(y == 1)) / m
            return sigma
        else:
            return sigma_feature1, sigma_feature2

    def calculte_bias_discriminant(self):
        sigma_inv = np.linalg.inv(self.sigma)
        term1 = self.mean_feature1.T @ sigma_inv @ self.mean_feature1
        term2 = self.mean_feature2.T @ sigma_inv @ self.mean_feature2
        bias = 0.5 * (term1- term2) + np.log(self.phi_feature2 / self.phi_feature1)
        return bias

    def quadratic_discriminant(self, x):
        term1 = -0.5 * np.log(np.linalg.det(self.sigma_feature1)) - 0.5 * (x - self.mean_feature1).T @ np.linalg.inv(
            self.sigma_feature1) @ (
                        x - self.mean_feature1)
        term2 = -0.5 * np.log(np.linalg.det(self.sigma_feature2)) - 0.5 * (x - self.mean_feature2).T @ np.linalg.inv(
            self.sigma_feature2) @ (
                        x - self.mean_feature2)
        return term1 - term2

    """
    def plot(self, X, y):
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_norm = (X - X_min) / (X_max - X_min)

        X_class0 = X_norm[y == 0]
        X_class1 = X_norm[y == 1]
        x0 = X_class0[:, 0]
        y0 = X_class0[:, 1]
        x1 = X_class1[:, 0]
        y1 = X_class1[:, 1]

        sigma_inv = np.linalg.inv(self.sigma)
        w = sigma_inv @ (self.mean_feature2 - self.mean_feature1)
        b = 0.5 * (
                self.mean_feature1.T @ sigma_inv @ self.mean_feature1 - self.mean_feature2.T @ sigma_inv @ self.mean_feature2) + np.log(
            self.phi_feature2 / self.phi_feature1)
        x_vals = np.linspace(0, 1, 10)
        y_vals = (-w[0] * x_vals - b) / w[1]
        plt.figure(figsize=(6, 6))
        plt.scatter(x0, y0, marker='o', color='red', label='Alaska')
        plt.scatter(x1, y1, marker='x', color='blue', label='Canada')
        plt.plot(x_vals, y_vals, 'r-', label="Decision Boundary")
        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.grid(True)
        plt.show()

    def plot2(self, X, y):
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_norm = (X - X_min) / (X_max - X_min)
        X = X_norm

        plt.figure(figsize=(10, 8))
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0 (Canada)', color='Red', alpha=0.6)
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1 (Alaska)', color='black', alpha=0.6)

        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max()

        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),np.linspace(y_min, y_max, 100))

        sigma_inv = np.linalg.inv(self.sigma)
        weights = np.dot(sigma_inv, self.mean_feature2 - self.mean_feature1)
        b_linear = -0.5 * (np.dot(self.mean_feature2.T, np.dot(sigma_inv, self.mean_feature2)) - np.dot(self.mean_feature1.T,np.dot(sigma_inv,self.mean_feature1)))
        Z_linear = weights[0] * xx + weights[1] * yy + b_linear
        sigma_feature1_inv = np.linalg.inv(self.sigma_feature1)
        sigma_feature2_inv = np.linalg.inv(self.sigma_feature2)
        Z_quadratic = np.array([self.quadratic_discriminant(np.array([x_, y_])) for x_, y_ in zip(np.ravel(xx), np.ravel(yy))])
        Z_quadratic = Z_quadratic.reshape(xx.shape)
        plt.contour(xx, yy, Z_linear, levels=[0], colors='blue', linestyles='solid', linewidths=2,
                    label='Linear Decision Boundary (LDA)')
        plt.contour(xx, yy, Z_quadratic, levels=[0], colors='green', linestyles='dashed', linewidths=2,
                    label='Quadratic Decision Boundary (QDA)')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('GDA: Linear and Quadratic Decision Boundaries')
        plt.legend()
        plt.grid(True)
        plt.show()
    """

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mean_feature1, mean_feature2, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mean_feature1, mean_feature2, sigma_feature1, sigma_feature2
            The parameters learned by the model.
        """
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_normalized = (X - X_min) / (X_max - X_min)

        self.phi_feature1 = self.calculate_prior(y, 0)
        self.phi_feature2 = 1 - self.phi_feature1

        m = X.shape[0]

        prior_alaska = self.calculate_prior(y, 0)
        prior_canada = 1 - prior_alaska

        self.mean_feature1 = self.calculate_mean(X_normalized,y,0)
        self.mean_feature2 = self.calculate_mean(X_normalized,y,1)

        if(assume_same_covariance):
            self.sigma = self.calculate_covariance(X_normalized,y,self.mean_feature1,self.mean_feature2,True)
            #print(self.sigma)
            return self.mean_feature1,self.mean_feature2,self.sigma
        else:
            self.sigma_feature1, self.sigma_feature2= self.calculate_covariance(X_normalized,y,self.mean_feature1,self.mean_feature2,False)
            #print(self.sigma_feature1, self.sigma_feature2)
            return self.mean_feature1,self.mean_feature2,self.sigma_feature1,self.sigma_feature2

        #print(mean_feature1)
        #print(mean_feature2)
        return -1


    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_min = np.min(X, axis=0)
        X_max = np.max(X, axis=0)
        X_normalized = (X - X_min) / (X_max - X_min)

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match209-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        sigma_inv = np.linalg.inv(self.sigma)
        weights = sigma_inv @ (self.mean_feature2 - self.mean_feature1)
        b = self.calculte_bias_discriminant()
        g_x = X_normalized @ weights + b
</FONT>
        y_pred = np.where(g_x &gt;= 0, "Canada", "Alaska")
        return y_pred



def main():
    features_path = "../data/Q4/q4x.dat"
    actual_output_path = "../data/Q4/q4y.dat"

    X = np.loadtxt(features_path)
    y = np.loadtxt(actual_output_path, dtype=str)
    y = np.where(y == "Alaska", 0, 1)

    #print(X.shape)
    #print(y.shape)

    model = GaussianDiscriminantAnalysis()
    mean1, mean2, sigma = model.fit(X,y,True)
    #print(f'mean1 is {mean1} , mean2 is {mean2} , sigma is {sigma}')

    #model.plot(X,y)
    mean1, mean2 , sigma1 , sigma2 = model.fit(X,y,False)
    ans = model.predict(X)
    #print(ans)
    #print(f'mean1 is {mean1} , mean2 is {mean2} , sigma1 is {sigma1}, sigma2 is {sigma2}')
    #model.plot2(X,y)

if __name__ == '__main__':
    main()

</PRE>
</PRE>
</BODY>
</HTML>
