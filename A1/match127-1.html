<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_1HEUC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_7EOU1.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class LinearRegressor:
    def __init__(self):
        self.theta = 0
        self.thresh = 1e-5  # tinker with it

    def grad(self, X, Y, learning_rate):
        m = X.shape[0]
        diff = X @ self.theta - Y
        
        return learning_rate * np.resize(
                (np.sum((X * diff).T, axis=1) * (1 / m)), (X.shape[1], 1)
            )
    
    def cost(self, X, Y, theta0, theta1):
        m = X.shape[0]
        diff = X @ np.array([[theta0], [theta1]]) - Y
        return (diff.T @ diff)[0][0] * (1 / (2 * m))
    
    # For plotting Hypothesis function with the scatter plot
    def plot(self, X, Y):
        plt.scatter(X[:, 1], Y)
        plt.plot(X[:, 1], self.predict(X), color="orange" , label="Hypothesis Function")
        
        plt.legend()
        plt.title("Linear Regression Hypothesis function against data samples")
        plt.show()
    
    # For plotting Gradient movement
    def plotGradDescent(self, X, Y, theta0s, theta1s, costs, iters, contour=False):
        theta0 = np.arange(-80, 80, 0.25)
        theta1 = np.arange(-80, 80, 0.25)
        theta0, theta1 = np.meshgrid(theta0, theta1)
        
        cost_vals = np.array([[self.cost(X, Y, t0, t1) for t0, t1 in zip(row0, row1)] for row0, row1 in zip(theta0, theta1)])
        
        if not contour:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111, projection='3d')
            ax.plot_surface(theta0, theta1, cost_vals, cmap='viridis', alpha=0.7)
        
            descent_path, = ax.plot([], [], [], 'ro-', markersize=5, label="Gradient Descent Path")
            ax.legend()

            ax.set_xlabel('Theta0')
            ax.set_ylabel('Theta1')
            ax.set_zlabel('Cost')
            ax.set_title('3D Cost Function')
        else:
            fig, ax = plt.subplots(figsize=(10, 7))
            
            contour = ax.contourf(theta0, theta1, cost_vals, levels=50, cmap='viridis')
            fig.colorbar(contour, label="Cost")
            
            descent_path, = ax.plot([], [], 'ro-', markersize=5, label="Gradient Descent Path")
            ax.legend()
            
            ax.set_xlabel("Theta0")
            ax.set_ylabel("Theta1")
            ax.set_title("Gradient Descent in Contour Plot")
            
        def update(frame):
            n = 5
            descent_path.set_data(theta0s[:frame * n +1], theta1s[:frame* n +1])  # Update X, Y
            if not contour:
                descent_path.set_3d_properties(costs[:frame * n +1])  # Update Z
                
            return descent_path,
        
        ani = FuncAnimation(fig, update, frames=iters, interval=200)
        
        plt.show()
    
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        # Assuming X has intercept term
        n = X.shape[1]
        self.theta = np.zeros((X.shape[1], 1))
        Y = np.resize(y, (y.shape[0], 1))

        grad_norm = np.Inf
        
        iters = 0
        theta0s = [self.theta[0][0]]
        theta1s = [self.theta[1][0]]
        costs = [self.cost(X, Y, theta0s[-1], theta1s[-1])]
        ans = [np.resize(self.theta, n)]
        
        while grad_norm &gt; self.thresh:
            iters+=1
            grad = self.grad(X, Y, learning_rate)
            
            grad_norm = np.sqrt((grad.T @ grad))[0][0]
            self.theta -= grad # Update rule

            ans.append(np.resize(self.theta, n))
            theta0s.append(self.theta[0][0])
            theta1s.append(self.theta[1][0])
            cost = self.cost(X, Y, theta0s[-1], theta1s[-1])
            costs.append(cost)
        
        # self.plot(X, y)
        # self.plotGradDescent(X, Y, theta0s, theta1s, costs, iters, True)
        return ans

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        y_pred = X @ self.theta
        y_pred.resize(y_pred.shape[0])
        return y_pred


if __name__ == "__main__":
    X = np.loadtxt("data/Q1/linearX.csv", delimiter=",")
    X = np.vstack((np.ones(X.shape[0]), X)).T
    Y = np.loadtxt("data/Q1/linearY.csv", delimiter=",")

    lol = LinearRegressor()
    lol.fit(X, Y, 0.1)[-1]
    
    print(lol.predict(X))




# Imports - you can add any other permitted libraries
<A NAME="6"></A><FONT color = #00FF00><A HREF="match127-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
</FONT>    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
<A NAME="1"></A><FONT color = #00FF00><A HREF="match127-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    x0 = np.ones(N)
    # Generation of data
    x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)
</FONT>    x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)
    
    X = np.vstack((x0, x1, x2)).T
    Y = X@theta + np.random.normal(loc=0, scale=noise_sigma, size=N)
    
    return X[:, 1:], Y

def closedFormSolution(X, Y):
    return np.linalg.inv(X.T @ X) @ X.T @ Y

def plot_theta_movement(theta_iters):
    theta_iters = np.array(theta_iters)
    
    fig = plt.figure()   
    ax = fig.add_subplot(111, projection='3d')
    
<A NAME="0"></A><FONT color = #FF0000><A HREF="match127-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax.plot(theta_iters[:, 0], theta_iters[:, 1], theta_iters[:, 2], marker='o', linestyle='-', label='Theta Movement')
    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')
    ax.set_title('Theta Movement Over Iteration')
    
    ax.legend()
    plt.show() 
</FONT>
class StochasticLinearRegressor:
    def __init__(self):
        self.theta = 0
        self.thetas = [0] * 4
        self.thresh = 1e-6
        self.hist = 20
        
    def grad(self, X, Y, learning_rate):
        m = X.shape[0]
        diff = X @ self.theta - Y
        return learning_rate * np.resize(
                (np.sum((X * diff).T, axis=1) * (1 / m)), (X.shape[1], 1)
            )
    
    def cost(self, X, Y, theta0, theta1, theta2):
        m = X.shape[0]
        diff = X @ np.array([[theta0], [theta1], [theta2]]) - Y
        return (diff.T @ diff)[0][0] * (1 / (2 * m))
    
    def fit_batch(self, X, y, batchsize, learning_rate=0.01):
        m = X.shape[0]
        self.theta = np.zeros((X.shape[1], 1))
        Y = np.resize(y, (y.shape[0], 1))
        
        iters = 0
        mov_avg = 0
        batches = m // batchsize + m % batchsize
        self.hist = batches * 2 # sliding window size
        cost_history = [np.Inf] * self.hist # maintenance of sliding window
        
        # Splitting into batches
        X_batches = []
        Y_batches = []
        for i in range(batches):
            X_batches.append(X[i * batchsize : (i+1) * batchsize])
            Y_batches.append(Y[i * batchsize : (i+1) * batchsize])
            
        ans = [np.resize(self.theta, (self.theta.shape[0]))]
        last_mov_avg = 0
        breakFlag = False
        while iters &lt; self.hist or np.abs(mov_avg - last_mov_avg) &gt; self.thresh:
            avg_cost = 0
            for i in range(0, batches):
                grad = self.grad(X_batches[i], Y_batches[i], learning_rate)
                
                self.theta -= grad
                
                cost = self.cost(X, Y, self.theta[0][0], self.theta[1][0], self.theta[2][0])
                avg_cost += cost
                
                # Maintaing moving average of the sliding window
                if(iters &lt; self.hist):
                    last_mov_avg = mov_avg
                    mov_avg = (mov_avg * iters + cost)/(iters + 1)
                    cost_history[iters] = cost
                else:
                    last_mov_avg = mov_avg
                    mov_avg = (mov_avg * self.hist - cost_history[iters%self.hist] + cost) / self.hist
                    cost_history[iters%self.hist] = cost
                
                if(np.abs(mov_avg - last_mov_avg) &lt;= self.thresh): # Convergence criterion
                    breakFlag = True
                    break
                iters += 1
            if breakFlag:
                ans.append(np.resize(self.theta, (self.theta.shape[0]))) # Before breaking, store the params of this epoch
                break
            
            ans.append(np.resize(self.theta, (self.theta.shape[0])))
        
        return np.array(ans)
        
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        # Had to return ans for batchsizes given in assn
        # Assuming X has intercept term
        ans = [self.fit_batch(X, y, batchsize, learning_rate) for batchsize in [1, 80, 8000, 800000]]
        for i in range(4):
            self.thetas[i] = (ans[i])[-1]
        
        print(self.thetas)
        return ans
                
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        # Had to predict for every batch size as given in assn
        return [X@theta for theta in self.thetas]
    
if __name__ == "__main__":
    np.random.seed(0)
    
    X, Y = generate(1000000, np.array([3, 1, 2]), np.array([3, -1]), np.array([4, 4]), 3)
    X = np.vstack((np.ones(X.shape[0]), X.T)).T
    
    index_split = int(X.shape[0] * 0.8)
    Y.resize((Y.shape[0], 1))
    X_train, Y_train = X[:index_split], Y[:index_split]
    X_test, Y_test = X[index_split:], Y[index_split:]
    
    lol = StochasticLinearRegressor()
    lol.fit(X_train, Y_train, 0.001)
    



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = 0  
        self.thresh = 1e-6
        
        self.mean = None
        self.std = None
        
    def plot_decision_boundary(self, X, Y):
        x1 = X[:, 1]
        x2 = X[:, 2]
        
        pos = (Y.flatten() == 1)
        neg = (Y.flatten() == 0)
        
        plt.scatter(x1[pos], x2[pos], marker='x', color='red', label='Positive')
        plt.scatter(x1[neg], x2[neg], marker='o', color='blue', label='Negative')
        
        x_vals = np.linspace(np.min(x1), np.max(x1), 100)
        theta0, theta1, theta2 = self.theta[0][0], self.theta[1][0], self.theta[2][0]
        y_vals = -(theta0 + theta1 * x_vals) / theta2 # Decision boudary Equation
        
        plt.plot(x_vals, y_vals, color='black', label='Decision Boundary')
        
        plt.xlabel('x1')
        plt.ylabel('x2')
        plt.legend()
        plt.grid()
        plt.title("Decision Boundary of Logisitic Regression")
        
        plt.show()
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Assuming X has intercept term
        m = X.shape[0]
        n = X.shape[1]
        self.theta = np.zeros((n, 1))
        Y = np.resize(y, (m, 1))
        
        # Normalization
        mean = X[:, 1:].mean(axis=0)
        std = X[:, 1:].std(axis=0)
        self.mean = mean
        self.std = std
        X[:, 1:] = (X[:, 1:] - mean) / std
        
        diff_norm = np.Inf
        ans = [np.resize(self.theta, n)]
        iters = 0
        while(diff_norm &gt; self.thresh):
            X_theta = X@self.theta + 1e-10
            hthetaX = 1/(1+np.exp(-X_theta))
    
            h_one_minus_h = hthetaX * (1 - hthetaX)
            h_one_minus_h.resize(m)
            Hessian = -X.T @ np.diag(h_one_minus_h) @ X
            
            Hessian += np.eye(n) * 1e-10 # For the case when Hessian is non-invertible
            
            diff = Y - hthetaX
            
            delLtheta = np.resize(
                (np.sum((X * diff).T, axis=1)), (n, 1)
            )
            
            last_theta = self.theta
            self.theta = self.theta - learning_rate * np.linalg.inv(Hessian) @ delLtheta # Update rule
            diff_norm = np.sqrt((self.theta - last_theta).T@(self.theta - last_theta))[0][0] # Convergence rule
            
            ans.append(np.resize(self.theta, n))
            iters+=1
        
        print(ans[-1])
        return ans
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize again for prediction purposes
        X[:, 1:] = (X[:, 1:] - self.mean) / self.std
        # Calculation of probability
        hthetaX = 1/(1+np.exp(-X@self.theta))
        
        # If lies above decision boundary then positive sample
        ans = np.int32(hthetaX&gt;0.5)
        ans.resize(ans.shape[0])
        return ans
    
if __name__ == "__main__":
    X = np.loadtxt("data/Q3/logisticX.csv", delimiter=",")
    X = np.vstack((np.ones(X.shape[0]), X.T)).T
    Y = np.loadtxt("data/Q3/logisticY.csv", delimiter=",")
    
    index_split = int(X.shape[0] * 0.8)
    X_train, Y_train = X[:index_split], Y[:index_split]
    X_test, Y_test = X[index_split:], Y[index_split:]
    
    lol = LogisticRegressor()
    lol.fit(X_train, Y_train)
    
    
    
    Y_predict = lol.predict(X_test)
    print(np.sum(Y_test==Y_predict)/Y_test.shape[0])
    
    lol.plot_decision_boundary(X, Y)



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.assume_same_covariance = True
        self.phi = None
        self.mu0 = None
        self.mu1 = None
        self.covar = None
        self.covar0 = None
        self.covar1 = None
        
        self.mean = None
        self.std = None
    
<A NAME="2"></A><FONT color = #0000FF><A HREF="match127-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def plot(self, X, y, mu0, mu1, covar, covar0, covar1):
        m = X.shape[0]
        # Setting up params
        covar_inv = np.linalg.inv(covar)
        covar0_inv = np.linalg.inv(covar0)
</FONT>        covar1_inv = np.linalg.inv(covar1)
        pos = (y==1)
        neg = (y==0)
        
        # For scatter plot
        x0 = X[:, 0]
        x1 = X[:, 1]
        
        sc0 = plt.scatter(x0[pos], x1[pos], marker='x', color='red', label='Canada')
        sc1 = plt.scatter(x0[neg], x1[neg], marker='o', color='blue', label='Alaska')
        
        # For linear decision boundary
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match127-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta0 = -0.5*(mu1.T@covar_inv@mu1 - mu0.T@covar_inv@mu0) + np.log(np.sum(pos)/(m-np.sum(pos)))
</FONT>        theta1 = covar_inv@(mu1 - mu0)
        
        x_vals = np.linspace(np.min(x0), np.max(x0), 200)
        y_vals = (-theta0 - theta1[0] * x_vals) / theta1[1]
        
        pl = plt.plot(x_vals, y_vals, color='black', label='Linear Decision Boundary for GDA')[0]
        
        # for quadratic decision boundary
        A = -0.5 * (covar1_inv - covar0_inv)
        B = covar1_inv @ mu1 - covar0_inv @ mu0
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match127-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        C = -0.5 * (mu1.T @ covar1_inv @ mu1 - mu0.T @ covar0_inv @ mu0) + np.log(np.sum(pos)/(m-np.sum(pos))) + 0.5  * np.log(np.linalg.det(covar0)/np.linalg.det(covar1))
</FONT>        
        x_min, x_max = np.min(x0), np.max(x0)
        y_min, y_max = np.min(x1), np.max(x1)
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))
        
        Z = (A[0, 0] * xx**2) + ((A[0, 1] + A[1, 0]) * xx * yy) + (A[1, 1] * yy**2) + (B[0] * xx + B[1] * yy + C)
        
        plt.contour(xx, yy, Z, levels=[0], colors='green', linewidths=2, label='Quadratic Decision Boundary for GDA')
        contour_legend = Line2D([0], [0], color='green', lw=2, label="Quadratic Decision Boundary for GDA")
        
        # Labelling
        plt.xlabel('x0')
        plt.ylabel('x1')
        plt.legend(handles=[sc0, sc1, pl, contour_legend], loc='best')
        plt.grid()
        plt.title('GDA Decision Boundary')
        plt.show()
        
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="5"></A><FONT color = #FF0000><A HREF="match127-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # Normalizing X
        self.assume_same_covariance = assume_same_covariance
        m = X.shape[0]
</FONT>        mean = X.mean(axis=0)
        std = X.std(axis=0)
        self.mean = mean
        self.std = std
        X = (X - mean) / std
        
        Y = np.resize(y, (m, 1))
        pos_samples = np.sum(Y)
        
        phi = pos_samples / m
        
        # Calculating GDA params
        mu0 = np.sum((X*(1-Y)), axis=0) / (m-pos_samples)
        mu1 = np.sum((X*Y), axis=0) / pos_samples
        
        mu = np.vstack((mu0, mu1))
        mu_y = mu[y]
        
        covar = ((X - mu_y).T @ (X - mu_y)) / m
        
        interm0 = (X-mu_y) * (1-Y)
        covar0 = (interm0.T @ interm0) / (m-pos_samples)
        interm1 = (X-mu_y) * Y
        covar1 = (interm1.T @ interm1) / pos_samples

        self.phi = phi
        self.mu0 = mu0
        self.mu1 = mu1
        self.covar = covar
        self.covar0 = covar0
        self.covar1 = covar1
        
        # for showing the plot
        # self.plot(X, y, mu0, mu1, covar, covar0, covar1)
        
        if assume_same_covariance:
            return (mu0, mu1, covar)
        else:
            return (mu0, mu1, covar0, covar1)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        m = X.shape[0]
        
        # Normalize again for prediction purposes
        X = (X - self.mean) / self.std
        
        # in case of same covariance
        if self.assume_same_covariance:
            self.covar0 = self.covar
            self.covar1 = self.covar
        
<A NAME="7"></A><FONT color = #0000FF><A HREF="match127-0.html#7" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        covar0_inv = np.linalg.inv(self.covar0)
        covar1_inv = np.linalg.inv(self.covar1)
        mu0 = np.reshape(self.mu0.T, (-1, 1))
</FONT>        mu1 = np.reshape(self.mu1.T, (-1, 1))
        
        y_pred = np.zeros(m)
        for i in range(m):
            x = np.resize(X[i], (X.shape[1], 1))
            val = 0.5 * np.log(np.linalg.det(self.covar0)/np.linalg.det(self.covar1)) + np.log(self.phi/(1-self.phi)) -0.5 * ((x-mu1).T @ covar1_inv @ (x-mu1) - (x-mu0).T @ covar0_inv @ (x-mu0))

            # When lies above decision boundary, hence positive sample
            y_pred[i] = (val[0][0] &gt; 0)
        
        return y_pred
            


if __name__ == "__main__":
    X = np.loadtxt("data/Q4/q4x.dat")
    Y = np.loadtxt("data/Q4/q4y.dat", dtype=str)
    Y = np.int8(Y == 'Canada')
    
    lol = GaussianDiscriminantAnalysis();
    print(lol.fit(X, Y, False))
    
    # y_pred = lol.predict(X)
    # print(np.sum(y_pred==Y)/Y.shape[0])

</PRE>
</PRE>
</BODY>
</HTML>
