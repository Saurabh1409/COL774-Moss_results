<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_D82V3.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_D82V3.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import os
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        self.n_iter = 20000
        self.error_values = []
    
    def fit(self, X, y, learning_rate=0.01, convergence_threshold=1e-6):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
        
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        self.learning_rate = learning_rate
        # add bias term to X
        n_samples, n_features = X.shape
        X_b = np.c_[np.ones((n_samples, 1)), X]

        # initialize parameters
        self.theta = np.zeros(n_features + 1)
        theta_history = np.zeros((self.n_iter, n_features + 1))

        # gradient descent
        for i in range(0, self.n_iter):
            y_pred = X_b.dot(self.theta)

            error = y_pred - y
            error_value = self.mse(X, y)
            self.error_values.append(error_value)

            gradients = (1/n_samples) * X_b.T.dot(error)
            self.theta = self.theta - learning_rate * gradients
            theta_history[i] = self.theta.copy()
            # convergence check
            if i &gt;0 and np.linalg.norm(theta_history[i] - theta_history[i-1]) &lt; convergence_threshold:
                print(f"Model converged after {i} iterations for {learning_rate * 100}% learning rate")
                return theta_history[:i+1]
        
        print("Model did not converge")
        
        return theta_history
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """ 
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        return np.dot(X_b, self.theta)


    def mse(self, X, y):
        n_samples = len(y)
        y_pred = self.predict(X)
        error = y_pred - y

        cost = (1/ n_samples) * np.sum(error ** 2)

        return cost
    
    
    def plot_hypothesis(self, X, y):
        # Set the figure size to be larger
        plt.figure(figsize=(12, 8))
        
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        y_pred = X_b.dot(self.theta)
        
        plt.scatter(X, y, color='blue', label='Data points')
        plt.plot(X, y_pred, color='red', label='Hypothesis')
        
        # Add labels and title
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Linear Regression Fit')
        
        # Add grid for better readability
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.legend()
        plt.show()
        plt.close()

    
    # 3D mesh showing the error function
    def plot_error_function(self, X, y, theta_history):
        theta0_vals = np.linspace(theta_history[:, 0].min(), theta_history[:, 0].max(), 100)
        theta1_vals = np.linspace(theta_history[:, 1].min(), theta_history[:, 1].max(), 100)

        # Create meshgrid for proper 3D plotting
        theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

        error_mesh = np.zeros_like(theta0_mesh)
        
        # Calculate error values
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                self.theta = np.array([theta0, theta1])
                error_mesh[i, j] = self.mse(X, y)

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        # Plot surface with color gradient
        surf = ax.plot_surface(theta0_mesh, theta1_mesh, error_mesh, 
                              cmap='viridis',
                              alpha=0.8,
                              linewidth=0,
                              antialiased=True)

        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_zlabel('Error')
        ax.set_title('Error Surface')

        # Add a color bar
        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)

        # Plot the error value using the current set of parameters at each iteration
        for i, theta in enumerate(theta_history):
            try:
                if not plt.fignum_exists(fig.number):
                    break
                error_value = self.error_values[i]
                ax.scatter(theta[0], theta[1], error_value, color='r', s=10, alpha= 0.5)
                plt.pause(0.2)
            except:
                break
        
        if plt.fignum_exists(fig.number):
            plt.show()
            plt.close()


        # print(theta_history.shape)
        error_values = np.array(self.error_values)
        # print(error_values.shape)
        plt.figure(figsize=(10, 8))
        ax = plt.axes(projection='3d')
        surf = ax.plot_surface(theta0_mesh, theta1_mesh, error_mesh, cmap='viridis', alpha=0.8, linewidth=0, antialiased=True)

        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_zlabel('Error')

        ax.scatter(theta_history[:, 0], theta_history[:, 1], self.error_values, color='r', s=10)

        plt.title('Error Surface')

        plt.show()
        plt.close()

    def plot_contour(self, X, y, theta_history):
        theta0_vals = np.linspace(theta_history[:, 0].min(), theta_history[:, 0].max(), 100)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match189-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        theta1_vals = np.linspace(theta_history[:, 1].min(), theta_history[:, 1].max(), 100)

        
        theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

        error_mesh = np.zeros_like(theta0_mesh)
</FONT>        
        # Calculate error values
        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                self.theta = np.array([theta0, theta1])
                error_mesh[i, j] = self.mse(X, y)

        # Plot contour
        fig = plt.figure(figsize=(10, 8))
        plt.clf()
        plt.contour(theta0_mesh, theta1_mesh, error_mesh, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.xlabel('theta0')
        plt.ylabel('theta1')
        plt.title(f'Contour Plot of Error Function with learning rate = {self.learning_rate * 100}%')

        # Animate theta points with 0.2 second delay
        for i in range(len(theta_history)):
            try: 
                if not plt.fignum_exists(fig.number):
                    break
                plt.scatter(theta_history[i, 0], theta_history[i, 1], color='r', s=50, alpha=0.5)
                plt.pause(0.2)  # 0.2 second delay
            except:
                break

        if plt.fignum_exists(fig.number):
            plt.show()

            plt.close(fig)

        fig = plt.figure(figsize=(10, 8))
        plt.clf()
        plt.contour(theta0_mesh, theta1_mesh, error_mesh, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.xlabel('theta0')
        plt.ylabel('theta1')
        plt.title(f'Contour Plot of Error Function with learning rate = {self.learning_rate * 100}%')

        plt.scatter(theta_history[:, 0], theta_history[:, 1], color='r', s=50, alpha=0.5)
        plt.show()
        plt.close()

        
        

if __name__ == "__main__":

    # Get the directory where the script is located
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Construct paths relative to the script directory
    x_path = os.path.join(script_dir, "..", "data", "Q1", "linearX.csv")
    y_path = os.path.join(script_dir, "..", "data", "Q1", "linearY.csv")
    
    # load data
    X = np.loadtxt(x_path, delimiter=",")
    y = np.loadtxt(y_path, delimiter=",")

    # Reshape X to be 2-dimensional (1000, 1)
    X = X.reshape(-1, 1)

    # print(X.shape)
    # print(y.shape)

    # 1. initialize model
    model = LinearRegressor()
    theta_history = model.fit(X, y)
    print(f"Model Theta: {model.theta}")
    # print(theta_history.shape)
    # print(type(theta_history))

    # 2. plot hypothesis
    model.plot_hypothesis(X, y)

    # 3. plot error function
    model.plot_error_function(X, y, theta_history)

    # 4. plot contour
    model.plot_contour(X, y, theta_history)

    #  5.
    learning_rates = [0.001, 0.025, 0.1]

    for lr in learning_rates:
        theta_history = model.fit(X, y, learning_rate=lr)
        # print(theta_history.shape)
        model.plot_contour(X, y, theta_history)




# Imports - you can add any other permitted libraries
import time
import numpy as np
import math
import matplotlib.pyplot as plt
import matplotlib.animation as animation

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """

    X = np.random.normal(input_mean, input_sigma, (N, len(input_mean)))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise

    return X, y


class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.01, max_epoch=100, batch_size=1, convergence_threshold=1e-6):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        n_samples, n_features = X.shape

        X_b = np.c_[np.ones((n_samples, 1)), X]

        self.theta = np.zeros(n_features + 1)

        theta_history = []
        J_history = []
        gradient_history = []

        num_batches = math.ceil(n_samples / batch_size) 

        min_iteration = 0.7 * max_epoch

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match189-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        for epoch in range(max_epoch):
            indices = np.random.permutation(n_samples)
            X_shuffled = X_b[indices]
            y_shuffled = y[indices]

            for j in range(num_batches):
</FONT>                start = j * batch_size
                end = start + batch_size
                X_batch = X_shuffled[start:end]
                y_batch = y_shuffled[start:end]

                y_pred = X_batch.dot(self.theta)

                error = y_pred - y_batch

                # J = (1/(2 * batch_size)) * np.sum(error ** 2)
                # J_history.append(J)

                gradient = (1/batch_size) * X_batch.T.dot(error)
                self.theta = self.theta - learning_rate * gradient
                theta_history.append(self.theta.copy())

                # if len(gradient_history) &gt; 10:
                #     gradient_history.pop(0)
                # gradient_history.append(gradient)

            # Convergence condition
            # if len(gradient_history) &gt; 1:
            #     gradient_matrix = np.array(gradient_history)
            #     element_wise_var = np.var(gradient_matrix, axis=0)
            #     sigma_squared = np.max(element_wise_var)
            # else:
            #     sigma_squared = 1

            # sigma_squared = 1

            # theta_mean = np.mean(theta_history, axis=0)
            # min_J_index = np.argmin(J_history)
            # theta_star = theta_history[min_J_index]

            # J_theta_mean = (1/(2 * batch_size)) * np.sum((X_batch.dot(theta_mean) - y_batch) ** 2)
            # J_theta_star = (1/(2 * batch_size)) * np.sum((X_batch.dot(theta_star) - y_batch) ** 2)

            # theta_0 = theta_history[0] if len(theta_history) &gt; 0 else np.zeros(n_features + 1)

            # k = epoch * num_batches + j + 1

            # bound = (np.linalg.norm(theta_star - theta_0) ** 2) / (2 * learning_rate * k) + learning_rate * sigma_squared / 2

            # print(f"Epoch {epoch}: J_theta_mean = {J_theta_mean}, J_theta_star = {J_theta_star}, bound = {bound}")

            # if epoch &gt; min_iteration and J_theta_mean - J_theta_star &lt;= bound:
            #     print(f"Model converged after {epoch} iterations")
            #     return np.array(theta_history)
            

            # print(f"Epoch {epoch}")
            # diff = np.linalg.norm(theta_history[-1] - theta_history[-2]) if len(theta_history) &gt; 1 else 1
            # print(f"Diff: {diff}")
                if len(theta_history) &gt; 1 and np.linalg.norm(theta_history[-1] - theta_history[-2]) &lt; convergence_threshold:
                    print(f"Model converged after {epoch} epochs")
                    return np.array(theta_history)

        print("Model did not converge")
        return np.array(theta_history)

    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples = X.shape[0]
        X_b = np.c_[np.ones((n_samples, 1)), X]

        y_pred = X_b.dot(self.theta)

        return y_pred
    
    def mse(self, X, y):
        y_pred = self.predict(X)
        n_samples = len(y)

        error = y_pred - y
        cost = (1/ n_samples) * np.sum(error ** 2)

        return cost
    
    def relative_error(self, X, y):
        y_pred = self.predict(X)
        n_samples = len(y)

        error = y_pred - y
        cost = (1/ (2 * n_samples)) * np.sum(error ** 2)

        return cost / np.mean(y)
    
    def accuracy(self, X, y):
        y_pred = self.predict(X)
        n_samples = len(y)

        error = y_pred - y
        cost = (1/ (2 * n_samples)) * np.sum(error ** 2)

        return 1 - cost / np.var(y)
    
    def plot_theta_movement(self, theta_histories):
        # Calculate number of rows and columns for subplots
        n_plots = len(theta_histories)
        n_cols = 2
        n_rows = (n_plots + n_cols - 1) // n_cols  # Ceiling division

        # Create figure and 3D axes
        fig = plt.figure(figsize=(15, 5 * n_rows))

        # Find global min and max values for consistent axis limits
        all_thetas = np.concatenate(list(theta_histories.values()))
        x_min, x_max = all_thetas[:, 0].min(), all_thetas[:, 0].max()
        y_min, y_max = all_thetas[:, 1].min(), all_thetas[:, 1].max()
        z_min, z_max = all_thetas[:, 2].min(), all_thetas[:, 2].max()
        
        # Add padding to limits
        padding = 0.1
        x_range = [x_min - padding, x_max + padding]
        y_range = [y_min - padding, y_max + padding]
        z_range = [z_min - padding, z_max + padding]

        # Create subplots and plot trajectories
        for i, (batch_size, history) in enumerate(theta_histories.items()):
            ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')
            
            # Set labels and title
            ax.set_xlabel('Theta 0')
            ax.set_ylabel('Theta 1')
            ax.set_zlabel('Theta 2')
            ax.set_title(f'Batch Size: {batch_size}')
            
            # Set consistent axis limits
            ax.set_xlim(x_range)
            ax.set_ylim(y_range)
            ax.set_zlim(z_range)

            # Plot trajectory
            ax.plot(history[:, 0], history[:, 1], history[:, 2], label='Trajectory')
            
            # Plot final point
            ax.scatter([history[-1, 0]], [history[-1, 1]], [history[-1, 2]], 
                      color='red', s=100, label='Final Position')
            
            ax.legend()
            ax.view_init(elev=30, azim=45)

        plt.tight_layout()
        plt.show()
        plt.savefig('theta_movement.png')
        plt.close()

def closed_form_sol (X, y):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match189-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
</FONT>    return theta

def compare_with_closed_form (theta_closed_form, X_train, y_train):
    X_b = np.c_[np.ones((X.shape[0], 1)), X]
    y_closed_form = X_b.dot(theta_closed_form)

    mse_closed_form = np.mean((y_closed_form - y) ** 2)

    print(f"MSE for closed form solution: {mse_closed_form}")

if __name__ == "__main__":
    
    # 1. Generate 1 million data and divide it into 80% training and 20% testing
    print("Generating data")
    N = 1_000_000
    theta = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([2, 2])
    noise_sigma = math.sqrt(2)

    X, y = generate(N, theta, input_mean, input_sigma, noise_sigma)

    print("Done generating data")

    train_size = int(0.8 * N)
    X_train, y_train = X[:train_size], y[:train_size]
    X_test, y_test = X[train_size:], y[train_size:]

    # 2. Fit the model using Stochastic Gradient Descent
    model = StochasticLinearRegressor()

    batch_sizes = [1, 80, 8_000, 800_000]
    convergence_threshold = [1e-8, 1e-5, 1e-5, 1e-4]
    max_epochs = [50, 100, 1000, 10000]
    # batch_sizes = [1]

    print("\n\n")

    theta_closed_form = closed_form_sol(X_train, y_train)
    print(f"Theta for closed form solution: {theta_closed_form}")
    compare_with_closed_form(theta_closed_form, X_train, y_train)

    print("\n\n")

    theta_histories = {}
    for i, batch_size in enumerate(batch_sizes):
        print(f"Fitting data using SGD with batch size {batch_size}")

        num_samples, num_features = X_train.shape
        num_batches = num_samples // batch_size
        max_epoch = max(100_000 // num_batches, 50)

        print("Max Epoch:", max_epochs[i])

        start_time = time.time()
        theta_history = model.fit(X_train, y_train, learning_rate=0.001, batch_size=batch_size, convergence_threshold=convergence_threshold[i], max_epoch=max_epochs[i])
        end_time = time.time()

        theta_histories[batch_size] = theta_history

        print(f"Time taken to fit the model with batch_size = {batch_size}: {end_time - start_time} seconds")
        print(f"Theta for batch_size = {batch_size}: ", model.theta)

        print(f"MSE for training data with batch_size = {batch_size}: ", model.mse(X_train, y_train))
        print(f"Relative Error for training data with batch_size = {batch_size}: ", model.relative_error(X_train, y_train))

        print(f"MSE for testing data with batch_size = {batch_size}: ", model.mse(X_test, y_test))
        print(f"Relative Error for testing data with batch_size = {batch_size}: ", model.relative_error(X_test, y_test))

        print("\n\n")
    
    model.plot_theta_movement(theta_histories)




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
import os

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
<A NAME="2"></A><FONT color = #0000FF><A HREF="match189-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def __init__(self):
        self.theta = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01, max_iter=1000, convergence_threshold=1e-6):
</FONT>        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        n_samples, n_features = X.shape

        # normalize X
        X = self.normalize(X)

        # add bias term to X
        X_b = np.c_[np.ones((X.shape[0], 1)), X]

        # initialize parameters
        self.theta = np.zeros(n_features + 1)
        theta_history = []

        # Newton's Method
        for i in range(max_iter):
            z = X_b.dot(self.theta)
            y_pred = self.sigmoid(z)
            error = y_pred - y

            gradient = X_b.T.dot(error) / n_samples
            # Hessian matrix
            H = X_b.T.dot(np.diag(y_pred * (1 - y_pred))).dot(X_b) / n_samples
            self.theta = self.theta - np.linalg.inv(H).dot(gradient)

            theta_history.append(self.theta.copy())

            # convergence check
            if len(theta_history) &gt; 1 and np.linalg.norm(theta_history[-1] - theta_history[-2]) &lt; convergence_threshold:
                print(f"Model converged after {i} iterations")
                return np.array(theta_history)
        
        print("Model did not converge")
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        
        prob = self.sigmoid(X_b.dot(self.theta))

        return np.where(prob &gt;= 0.5, 1, 0)
    
    def normalize(self, X):
        return (X - X.mean(axis=0)) / X.std(axis=0)
    
    def plot_decision_boundary(self, X, y):
        X = self.normalize(X)
        X_b = np.c_[np.ones((X.shape[0], 1)), X]

        # Plotting decision boundary
        x1 = np.linspace(X.min(), X.max(), 100)
        x2 = -(self.theta[0] + self.theta[1] * x1) / self.theta[2]
        plt.plot(x1, x2, label='Decision Boundary')
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0')
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1')
        plt.legend()
        plt.show()


if __name__ == "__main__":
    
    script_dir = os.path.dirname(__file__)

    x_path = os.path.join(script_dir, "..", "data", "Q3", "logisticX.csv")
    y_path = os.path.join(script_dir, "..", "data", "Q3", "logisticY.csv")

    X = np.loadtxt(x_path, delimiter=',')
    y = np.loadtxt(y_path, delimiter=',')

    X = X.reshape(-1, 2)

    model = LogisticRegressor()
    theta_history = model.fit(X, y)

    print(f"Model parameters: {model.theta}")

    model.plot_decision_boundary(X, y)



# Imports - you can add any other permitted libraries
import numpy as np
import os
import matplotlib.pyplot as plt

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu = [None, None]
        self.sigma = [None, None]
        self.phi = None
        

    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            I - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        n_samples, n_features = X.shape

        # normalize X
        X = self.normalize(X)

        self.phi = np.mean(y)

        self.mu[0] = np.mean(X[y == 0], axis=0)
        self.mu[1] = np.mean(X[y == 1], axis=0)

        if assume_same_covariance:
            self.sigma[0] = np.cov(X.T, bias=True)
            self.sigma[1] = self.sigma[0]
            return self.mu[0], self.mu[1], self.sigma[0]
        else:
            self.sigma[0] = np.cov(X[y == 0].T, bias=True)
            self.sigma[1] = np.cov(X[y == 1].T, bias=True)
            return self.mu[0], self.mu[1], self.sigma[0], self.sigma[1]

    
    def normalize(self, X):
        return (X - X.mean(axis=0)) / X.std(axis=0)
    

    def gaussian_pdf(self, X, mu, sigma):
        n_features = X.shape[1]
        det = np.linalg.det(sigma)
        inv = np.linalg.inv(sigma)
        diff = X - mu
        return (1 / (np.sqrt((2 * np.pi) ** n_features * det))) * np.exp(-0.5 * np.sum(diff.dot(inv) * diff, axis=1))
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)

        prob_x_given_y0 = self.gaussian_pdf(X, self.mu[0], self.sigma[0])
        prob_x_given_y1 = self.gaussian_pdf(X, self.mu[1], self.sigma[1])

        prob_y0 = prob_x_given_y0 * (1 - self.phi)
        prob_y1 = prob_x_given_y1 * self.phi

        return (prob_y0 &lt; prob_y1).astype(int)
    

    def plot_training_data(self, X, y):
        """
        Plot the training data with different symbols for each class.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        """
        plt.figure(figsize=(12, 8))

        # Scatter plot for the data points with different markers
        plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue', label='Alaska', marker='o')  # Circle for Alaska
        plt.scatter(X[y==1][:,0], X[y==1][:,1], color='red', label='Canada', marker='x')   # X for Canada

        plt.xlabel('X1')
        plt.ylabel('X2')
        plt.title('Training Data: Alaska vs Canada')
        plt.legend()
        plt.show()
    
    def plot_decision_boundary(self, X, y, assume_same_covariance=False):
        X = self.normalize(X)

        plt.figure(figsize=(12, 8))


        plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue', label='Alaska', marker='o')
        plt.scatter(X[y==1][:,0], X[y==1][:,1], color='red', label='Canada', marker='x')


        if assume_same_covariance:

            sig = self.sigma[0]
            inv_sig = np.linalg.inv(sig)
            w = inv_sig.dot(self.mu[0] - self.mu[1])

<A NAME="6"></A><FONT color = #00FF00><A HREF="match189-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            b = -0.5 * (self.mu[1].dot(inv_sig).dot(self.mu[1]) - self.mu[0].dot(inv_sig).dot(self.mu[0])) + np.log((1 - self.phi) / self.phi)

            x1 = np.linspace(X[:,0].min(), X[:,0].max(), 100)
            x2 = -(w[0] * x1 + b) / w[1]
</FONT>
            plt.plot(x1, x2, label='Decision Boundary')

        else:
            inv_sig0 = np.linalg.inv(self.sigma[0])
            inv_sig1 = np.linalg.inv(self.sigma[1])

            A = 0.5 * (inv_sig0 - inv_sig1)
            B = self.mu[1].dot(inv_sig1) - self.mu[0].dot(inv_sig0)
            C = 0.5 * (self.mu[0].dot(inv_sig0).dot(self.mu[0]) - self.mu[1].dot(inv_sig1).dot(self.mu[1])) + np.log((1 - self.phi) / self.phi)

            x1 = np.linspace(X[:,0].min(), X[:,0].max(), 100)
<A NAME="1"></A><FONT color = #00FF00><A HREF="match189-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            x2 = np.linspace(X[:,1].min(), X[:,1].max(), 100)

            X1, X2 = np.meshgrid(x1, x2)

            Z = np.zeros_like(X1)
            for i in range(X1.shape[0]):
</FONT>                for j in range(X1.shape[1]):
                    x = np.array([X1[i,j], X2[i,j]])
<A NAME="0"></A><FONT color = #FF0000><A HREF="match189-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                    Z[i,j] = x.dot(A).dot(x) + B.dot(x) + C

            plt.contour(X1, X2, Z, levels=[0], colors='green', label='Decision Boundary')

        plt.xlabel('X1')
        plt.ylabel('X2')
        plt.legend()
        plt.title('Gaussian Discriminant Analysis')
</FONT>        plt.show()
    


if __name__ == "__main__":
    
    script_dir = os.path.dirname(__file__)

    x_path = os.path.join(script_dir, "..", "data", "Q4", "q4x.dat")
    y_path = os.path.join(script_dir, "..", "data", "Q4", "q4y.dat")

    X = np.genfromtxt(x_path)
    y = np.genfromtxt(y_path,delimiter=",", dtype=str)

    # make y = 0 where y = Alaska and y = 1 where y = Canada
    y = np.where(y == "Alaska", 0, 1)

    X = X.reshape(-1, 2)

    gda = GaussianDiscriminantAnalysis()

    out = gda.fit(X, y, assume_same_covariance=True)
    y_pred = gda.predict(X)
    # y_pred = gda.predict(np.array([np.array([80,400])]))
    # print(y_pred)
    print(f"For same covariance matrix:\nMu_0 - {out[0]}\nMu_1 - {out[1]}\nSigma - {out[2]}")

    gda.plot_training_data(X, y)
    # Plotting decision boundary
    gda.plot_decision_boundary(X, y, assume_same_covariance=True)

    out = gda.fit(X, y)
    print(f"For different covariance matrix:\nMu_0 - {out[0]}\nMu_1 - {out[1]}\nSigma_0 - {out[2]}\nSigma_1 - {out[3]}")
    # print(out)
    y_pred = gda.predict(X)
    # y_pred = gda.predict(np.array([np.array([80,400])]))
    # print(y_pred)

    # Plotting decision boundary
    gda.plot_decision_boundary(X, y)


    


</PRE>
</PRE>
</BODY>
</HTML>
