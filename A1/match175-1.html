<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_ARW4B.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_JDGB0.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.num_features : int = None
        self.num_data_points : int = None
        self.X : np.array = None
        self.y : np.array = None 
        self.theta : np.array = None
        self.num_iter : int = int(1e5)
        self.eps : float = 1e-7
        self.loss_data : list = []
        self.theta_history : list = []
    

    def fit(self, X, y, learning_rate=0.018, random_start = False):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """


        self.X = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
        self.y = y
        self.num_data_points, self.num_features = self.X.shape
        self.theta = np.zeros(self.num_features)
        if random_start:
            self.theta = np.random.uniform(-100, 100, self.num_features)
        n_iter = self.num_iter
        # Gradient Descent
        prev_loss : float = float('inf')
        for iter in range(n_iter):
            predictions = np.dot(self.X, self.theta)  # Calculate predictions
            errors = predictions - self.y              # Calculate errors
            loss = (1 / (2 * self.num_data_points)) * np.sum(errors ** 2)
            # print(f"Iteration {iter+1} - Loss: {loss}")
            gradient = (1 / self.num_data_points) * np.dot(self.X.T, errors)  # Compute gradient

            if ( prev_loss - loss &lt; self.eps): # Check for convergence
                break
            prev_loss = loss
            self.loss_data.append([self.theta.copy(),loss])
            # Ensure gradient is 1D and matches the shape of theta
            self.theta -= learning_rate * gradient.flatten()  # Update theta
            self.theta_history.append(self.theta.copy())
        
        return np.array(self.theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X_pred = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
        y_pred = np.dot(X_pred, self.theta)  # Calculate predictions
        # print("X_pred: ", X_pred.shape)
        # print("y_pred: ", y_pred.shape)
        return y_pred



from linear_regression import LinearRegressor
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


def q1_1():
    print("Q1.1")
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None).values)
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None).values).flatten()  # Flatten y to 1D array
    for learning_rate in np.arange(0.001, 0.05, 0.001):
        model = LinearRegressor()
        theta_history = model.fit(X, y,learning_rate=learning_rate)
        theta = theta_history[-1]
        y_predict = model.predict(X=X)
        errors = y_predict - y
        loss = (1 / (2 * X.shape[0])) * np.sum(errors ** 2)
        print(f"eta: {learning_rate:.3f}, theta: {theta}, loss: {loss:3f}")
    print("--"*20)

def q1_2(learning_rate=0.018):
    print("Q1.2")
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None).values)
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None).values).flatten()  # Flatten y to 1D array
    model = LinearRegressor()
    theta_history = model.fit(X, y,learning_rate=learning_rate)
    theta = theta_history[-1]
    print(theta)
    y_pred = model.predict(X)
    errors = y_pred - y
    loss = (1 / (2 * X.shape[0])) * np.sum(errors ** 2)
    # print(f"eta: {learning_rate} , l: {loss}")
    y_line = theta[0] + theta[1] * X.flatten()  # Ensure X is flattened for calculation
    # Plotting
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, color='blue', label='Data Points')  # Plot original data points
    plt.plot(X, y_line, color='red', label='Hypothesis (Linear Equation)')  # Plot linear equation line
    plt.title('Linear Regression Fit')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'q1_2_{learning_rate}.png')
    plt.close()
    print(f"Image saved to q1_2_{learning_rate}")
    print("--"*20)



def q1_3(learning_rate=0.018):
    print("Q1.3")
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None).values)
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None).values).flatten()  # Flatten y to 1D array
    model = LinearRegressor()
    theta_history = model.fit(X, y,learning_rate=learning_rate)
    theta = theta_history[-1]
    loss_data = model.loss_data
    theta0 = np.array([item[0][0] for item in loss_data])
    theta1 = np.array([item[0][1] for item in loss_data])
    loss = np.array([item[1] for item in loss_data])
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    # Plotting the data
    ax.scatter(theta0, theta1, loss, c=loss, cmap='viridis')

    # Adding labels
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Loss')

    # Show the plot
    plt.savefig(f'q1_3_{learning_rate}.png')
    plt.close()
    print("--"*20)

    
def q1_4(learning_rate=0.018):
    print("Q1.4")
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None).values)
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None).values).flatten()  # Flatten y to 1D array
    model = LinearRegressor()
    theta_history = model.fit(X, y,learning_rate=learning_rate)
    theta = theta_history[-1]
    loss_data = model.loss_data
    theta0 = np.array([item[0][0] for item in loss_data])
    theta1 = np.array([item[0][1] for item in loss_data])
    loss = np.array([item[1] for item in loss_data])
    fig, ax = plt.subplots()
    scatter = ax.scatter(theta0, theta1, c=loss, cmap='viridis', edgecolors='k')

    # Add colorbar
    cbar = plt.colorbar(scatter)
    cbar.set_label('Loss')

    # Labels
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_title('Loss Heatmap')
    ax.grid(True)
    # Save and show
    plt.savefig(f'q1_4_{learning_rate}.png')
    plt.close()
    print("--"*20)

def q1_5():
    print("Q1.5")
    X = np.array(pd.read_csv('../data/Q1/linearX.csv', header=None).values)
    y = np.array(pd.read_csv('../data/Q1/linearY.csv', header=None).values).flatten()  # Flatten y to 1D array
    # fig, ax = plt.subplots()
    for learning_rate in [0.001,0.025,0.1]:
        fig, ax = plt.subplots()
        model = LinearRegressor()
        theta_history = model.fit(X, y,learning_rate=learning_rate)
        theta = theta_history[-1]
        loss_data = model.loss_data
        theta0 = np.array([item[0][0] for item in loss_data])
        theta1 = np.array([item[0][1] for item in loss_data])
        loss = np.array([item[1] for item in loss_data])
        
        scatter = ax.scatter(theta0, theta1, c=loss, cmap='viridis', edgecolors='k')

        # Add colorbar
        cbar = plt.colorbar(scatter)
        cbar.set_label('Loss')

        # Labels
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_title('Loss Heatmap')
        ax.grid(True)
        # Save and show
        plt.savefig(f'q1_5_{learning_rate}.png')
        plt.close()
        print("--"*5)
    print("--"*20)


q1_1()
q1_2()
q1_3()
q1_4()
q1_5()
    
    

    




from sampling_sgd import *
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time

def get_data(file_name, split_ratio=0.8):
    df = pd.read_csv(file_name)
    X = df[['Feature_1', 'Feature_2']].to_numpy()
    y = df['Target'].to_numpy()

    data = np.column_stack((X, y))

    # Shuffle the data
    np.random.shuffle(data)

    # Calculate the split index for an 80-20 split
    split_index = int(split_ratio * len(data))

    # Split the data into training and testing sets
    train_data = data[:split_index]
    test_data = data[split_index:]

    # Separate the features and target for training and testing sets
    X_train = train_data[:, :-1]
    y_train = train_data[:, -1]
    X_test = test_data[:, :-1]
    y_test = test_data[:, -1]
    return X_train, y_train, X_test, y_test

def plot_theta(loss_data, batch_size):
    theta0 = np.array([item[0][0] for item in loss_data])
    theta1 = np.array([item[0][1] for item in loss_data])
    theta2 = np.array([item[0][2] for item in loss_data])
    loss = np.array([item[1] for item in loss_data])

    # Create a figure and 3D axis
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')  # Ensure 3D projection

    # 3D scatter plot
    scatter = ax.scatter(theta0, theta1, theta2, c=loss, cmap='viridis', edgecolors='k')

    # Add colorbar
    cbar = fig.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)
    cbar.set_label('Loss')
    ax.plot(theta0, theta1, theta2, color='red', linestyle='-', marker='o', markersize=3, alpha=0.7)
    # Axis labels
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')

    ax.set_title('Loss Heatmap in 3D')

    # Save and show plot
    plt.savefig(f'q2_5_{batch_size}.png')
    plt.close()
    print(f"File saved as q2_5_{batch_size}.png")

def q2_5(X_train, y_train, X_test, y_test):
    print("Q2.5")
    # batch_size_list = [ 1, 80, 80_000, 800_000]
    model = StochasticLinearRegressor()
    model.fit(X_train, y_train,learning_rate=0.001)
    theta_sgd_history = model.theta_history
    test_error = model.loss(X=X_test,y=y_test)
    loss_data = model.loss_data
    theta_sgd = model.theta
    i = 0
    print("--"*5)
    for batch_size in model.batch_size_list:
        print(f"Batch Size: {batch_size}")
        print(f"Iterations: {len(theta_sgd_history[batch_size])}")
        print(f"Parameters: {theta_sgd[batch_size]}")
        print(f"Train MSE: {loss_data[batch_size][-1][-1]}")
        print(f"Test Loss: {test_error[i]}")
        i += 1
        plot_theta(loss_data[batch_size],batch_size)
        print("--"*5)
    print("--"*20)

def q2_3b(X_train, y_train, X_test, y_test):
    print("Q2.3b")
    start_time = time.time()
    model = StochasticLinearRegressor()
    theta_closed = model.closed_form_solution(X_train, y_train)
    end_time = time.time()
    print(f"Closed Form Solution: {theta_closed}")
    print(f"Time Taken: {end_time-start_time}")
    print("--"*20)




X,y = generate(N = 1000000, theta = np.array([3, 1, 2]), input_mean = np.array([3, -1]), input_sigma = np.array([4, 4]), noise_sigma = 2)
df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])
df['Target'] = y
df.to_csv('generated_data.csv', index=False)

X_train, y_train, X_test, y_test = get_data('generated_data.csv')


q2_5(X_train,y_train,X_test,y_test)
# q2_3b(X_train,y_train,X_test,y_test)





# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def fit(X, y, learning_rate, batch_size, num_iter, eps):
    """
    Fit the linear regression model to the data using Gradient Descent.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target values.
    learning_rate : float
        The learning rate to use in the update rule.
        
    Returns
    -------
    List of Parameters: numpy array of shape (n_iter, n_features,)
        The list of parameters obtained after each iteration of Gradient Descent.
    """
    X = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
    y = y
    num_data_points, num_features = X.shape
    theta = np.zeros(num_features)
    n_iter = num_iter
    batch_size = batch_size

    theta_history = []
    loss_data = []
    
    prev_loss : float = float('inf')
    for iter in range(n_iter):
        indices = np.arange(num_data_points)
        np.random.shuffle(indices)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        total_loss = 0
        for i in range(0, num_data_points, batch_size):
            # Create mini-batch
            X_batch = X_shuffled[i:i + batch_size]
            y_batch = y_shuffled[i:i + batch_size]
            predictions = np.dot(X_batch, theta)
            errors = predictions - y_batch
            loss = np.sum(errors ** 2)
            total_loss += loss
            gradient = (1 / len(y_batch)) * np.dot(X_batch.T, errors)
            theta -= learning_rate * gradient.flatten()  # Ensure gradient is 1D
        total_loss = total_loss / (2 * num_data_points)
        if abs(prev_loss - total_loss) &lt; eps:
            # print(f"Iterations finished at {iter+1}")
            break
        prev_loss = total_loss
        loss_data.append([theta.copy(), total_loss])
        # print(f"Epoch {iter + 1} - Loss: {total_loss}")
        theta_history.append(theta.copy())
    return theta, np.array(theta_history), loss_data


def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values.
    
    Note that we have 2 input features.
    
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match175-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    # Generate input data X from a normal distribution
    X = np.random.normal(loc=input_mean, scale=input_sigma, size=(N, 2))
    
    # Calculate the target values y using the linear model y = theta0 + theta1*x1 + theta2*x2 + noise
    noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
</FONT><A NAME="5"></A><FONT color = #FF0000><A HREF="match175-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    y = theta[0] + theta[1] * X[:, 0] + theta[2] * X[:, 1] + noise
</FONT>    
    return X, y

class StochasticLinearRegressor:
    def __init__(self):
        self.num_features : int = None
        self.num_data_points : int = None
        self.X : np.array = None
        self.y : np.array = None 
        self.theta : dict = {}
        self.num_iter : int = int(1e3)
        self.eps : float = 1e-8
        self.loss_data : dict = {}
        self.theta_history : dict = {}
        self.batch_size_list = [ 1, 80, 8_000, 800_000]


    def closed_form_solution(self, X, y):
        """
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match175-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        Compute the closed form solution for linear regression.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
</FONT>            
        y : numpy array of shape (n_samples,)
            The target values.
            
        Returns
        -------
        theta : numpy array of shape (n_features,)
            The parameters of the linear regression model.
        """
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match175-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        self.X = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
        self.y = y
        theta_closed_form = np.linalg.inv(self.X.T.dot(self.X)).dot(self.X.T).dot(y)
</FONT>        return theta_closed_form

    
    def fit(self, X, y, learning_rate=0.01):
        return_list = []
        for size in self.batch_size_list:
            theta, theta_history, loss_data = fit(X, y, learning_rate, size, self.num_iter, self.eps)
            self.theta[size] = theta
            self.theta_history[size] = theta_history
            self.loss_data[size] = loss_data
            return_list.append(theta_history)
        
        return return_list

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        return_list = []
        X_pred = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
        for size in self.batch_size_list:
            y_pred = np.dot(X_pred, self.theta[size])
            return_list.append(y_pred)
        return return_list
    
    def loss(self,X,y):
        """
<A NAME="1"></A><FONT color = #00FF00><A HREF="match175-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        Return the loss value after the last iteration.
        
        Returns
        -------
        loss : float
            The loss value after the last iteration.
        """
        y_pred_list = self.predict(X)
</FONT>        return_list = []
        for y_pred in y_pred_list:
            errors = y_pred - y
            loss = (1 / (2 * X.shape[0])) * np.sum(errors ** 2)
            return_list.append(loss)

        return return_list



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.num_features : int = None
        self.num_data_points : int = None
        self.X : np.array = None
        self.y : np.array = None 
        self.theta : np.array = None
        self.num_iter : int = int(1e3)
        self.eps : float = 0
        self.loss_data : list = []
        self.mean : float = None
        self.std : float = None
        self.theta_history : list = []
        pass
    
    def sigmoid(self, z):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match175-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The list of parameters obtained after each iteration of Newton's Method.
        """
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X = (X - self.mean) / (self.std + 1e-8)
</FONT>        self.X = np.hstack((np.ones((X.shape[0], 1)), X)) # Add a column of ones to X for the intercept term
        self.y = y
        self.num_data_points, self.num_features = self.X.shape
        self.theta = np.zeros(self.num_features)
        n_iter = self.num_iter
        prev_loss : float = float('inf')

        for i in range(n_iter):
            z = np.dot(self.X, self.theta)
            h = self.sigmoid(z)
            gradient = np.dot(self.X.T, (h - self.y))
            R = np.diag(h * (1 - h))
            H = np.dot(self.X.T, np.dot(R, self.X))
            try:
                H_inv = np.linalg.inv(H)
            except np.linalg.LinAlgError:
                print("Hessian is singular, stopping updates.")
                break
            
            delta_theta = np.dot(H_inv, gradient)
            self.theta -= delta_theta
            # Compute loss (negative log-likelihood)
            loss = -np.sum(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8))
            self.loss_data.append((self.theta.copy(), loss))
            self.theta_history.append(self.theta.copy())

            
            # Check for convergence
            if np.linalg.norm(delta_theta) &lt; self.eps:
                break

        return np.array(self.theta_history)

        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        def predict_proba(X):
            X = (X - self.mean) / (self.std + 1e-8)
            X = np.hstack((np.ones((X.shape[0], 1)), X))
            return self.sigmoid(np.dot(X, self.theta))
        
        return (predict_proba(X) &gt;= 0.5).astype(int)




from logistic_regression import *
import pandas as pd
import matplotlib.pyplot as plt



def q3_1(X,y):
    print("Q3.1")
    model = LogisticRegressor()
    theta_lgst_history = model.fit(X, y,learning_rate=0.01)
    theta_lgst = theta_lgst_history[-1]
    print(f"Parameters: {theta_lgst}")
    print("Normalization Constants: ")
    print(f"Mean: {model.mean}")
    print(f"Std: {model.std}")
    print("--"*20)

def q3_2(X,y):
    print("Q3.2")
    model = LogisticRegressor()
    theta_lgst_history = model.fit(X, y,learning_rate=0.01)
    theta_lgst = theta_lgst_history[-1]   
    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Class 0')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1')

    # Create a mesh grid for decision boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    # Predict on the grid
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match175-0.html#6" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='dashed', linewidths=2)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
</FONT>    plt.title('Logistic Regression Decision Boundary')
    plt.savefig('q3_2.png') 
    print("Image saved as q3.png")
    plt.close()
    print("--"*20)


X = np.array(pd.read_csv('../data/Q3/logisticX.csv', header=None).values)
y = np.array(pd.read_csv('../data/Q3/logisticY.csv', header=None).values).flatten()

q3_1(X,y)
q3_2(X,y)



# Imports - you can add any other permitted libraries
import numpy as np
from scipy.stats import multivariate_normal
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.num_features : int = None
        self.num_data_points : int = None
        self.X : np.array = None
        self.y : np.array = None 
        self.mean : float = None
        self.std : float = None
        
        self.phi : float = None
        self.mu_0 : np.array = None
        self.mu_1 : np.array = None
        self.sigma : np.array = None
        self.sigma_0 : np.array = None
        self.sigma_1 : np.array = None
        self.assume_same_covariance : bool = False
        pass
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        X = (X - self.mean) / (self.std + 1e-8)
        self.X = X

        self.y = y
        self.num_data_points, self.num_features = self.X.shape
        self.assume_same_covariance = assume_same_covariance
        
        self.phi = np.mean(self.y)
        self.mu_0 = np.mean(self.X[y == 0], axis=0)
        self.mu_1 = np.mean(self.X[y == 1], axis=0)
        n = self.num_data_points

        if self.assume_same_covariance:
            self.sigma = np.zeros((self.num_features, self.num_features))
            for i in range(n):
                x_i = self.X[i] - (self.mu_1 if self.y[i] == 1 else self.mu_0)
                self.sigma += np.outer(x_i, x_i)
            self.sigma /= n
            return self.mu_0, self.mu_1, self.sigma
        
        else:
            self.sigma_0 = np.zeros((self.num_features, self.num_features))
            self.sigma_1 = np.zeros((self.num_features, self.num_features))
            
            count_0 = np.sum(y == 0)
            count_1 = np.sum(y == 1)
            for i in range(n):
                if self.y[i] == 0:
                    x_i = self.X[i] - self.mu_0
                    self.sigma_0 += np.outer(x_i, x_i)
                else:
                    x_i = self.X[i] - self.mu_1
                    self.sigma_1 += np.outer(x_i, x_i)

            self.sigma_0 /= count_0
            self.sigma_1 /= count_1

            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict_proba(self, X):
        """
        Compute the probability of belonging to class 1 using Bayes' rule.
        """
        X = (X - self.mean) / (self.std + 1e-8)
        # X = np.hstack((np.ones((X.shape[0], 1)), X))
        
        if self.assume_same_covariance:
            p_x_given_y0 = multivariate_normal.pdf(X, mean=self.mu_0, cov=self.sigma, allow_singular=True)
            p_x_given_y1 = multivariate_normal.pdf(X, mean=self.mu_1, cov=self.sigma, allow_singular=True)
        else:
            p_x_given_y0 = multivariate_normal.pdf(X, mean=self.mu_0, cov=self.sigma_0, allow_singular=True)
            p_x_given_y1 = multivariate_normal.pdf(X, mean=self.mu_1, cov=self.sigma_1, allow_singular=True)
        
        p_y1_x = (self.phi * p_x_given_y1) / ((self.phi * p_x_given_y1) + ((1 - self.phi) * p_x_given_y0))
        return p_y1_x

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        return (self.predict_proba(X) &gt;= 0.5).astype(int)



from gda import *
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import multivariate_normal

def plot_theta(theta):
    theta0 = np.array([item[0][0] for item in theta])
    theta1 = np.array([item[0][1] for item in theta])
    theta2 = np.array([item[0][2] for item in theta])
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    # Plotting the data
    ax.scatter(theta0, theta1, c=theta2, cmap='viridis')

    # Adding labels
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Theta 2')

    # Show the plot
    plt.savefig(f'q2_4.png')
    
def q4_1(X,y):
    print("Q4.1")
    model = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)
    print(f"mu_0: {mu_0}")
    print(f"mu_1: {mu_1}")
    print(f"sigma: {sigma}")
    print("Normalization Constant")
    print(f"Mean: {model.mean}")
    print(f"Std: {model.std}")
    print("--"*20)
    
def q4_2_3(X,y):
    print("Q4.2")
    model = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma = model.fit(X, y, assume_same_covariance=True)
    # Plot data points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Class 0')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1')

    # Create a mesh grid for decision boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    sigma_inv = np.linalg.inv(sigma)
    w = sigma_inv @ (mu_1 - mu_0)
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0) + np.log(np.sum(y) / (len(y) - np.sum(y)))
    # print(w)
    # print(b)
    # Predict on the grid
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='dashed', linewidths=2)
    
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid(True)
    plt.title('Logistic Regression Decision Boundary')
    plt.savefig('q4_2.png')
    print(f"Image saved as q4_2.png")
    plt.close()
    print("--"*20)

def q4_4(X,y):
    print("Q4.4")
    model = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)
    print(f"mu_0: {mu_0}")
    print(f"mu_1: {mu_1}")
    print(f"sigma_0: {sigma_0}")
    print(f"sigma_1: {sigma_1}")
    print("--"*20)

def q4_5(X,y):
    print("Q4.5")
    model = GaussianDiscriminantAnalysis()
    mu_0, mu_1, sigma_0, sigma_1 = model.fit(X, y, assume_same_covariance=False)
    phi = model.phi
    inv_sigma_0 = np.linalg.inv(sigma_0)
    inv_sigma_1 = np.linalg.inv(sigma_1)
    A = inv_sigma_1 - inv_sigma_0
    B = -2 * (mu_1.T @ inv_sigma_1 - mu_0.T @ inv_sigma_0)
    C = mu_1.T @ inv_sigma_1 @ mu_1 - mu_0.T @ inv_sigma_0 @ mu_0
    C -= 2 * np.log(phi / (1 - phi)) + 2 * np.log(np.linalg.det(sigma_0) / np.linalg.det(sigma_1))
    print(f"A: {A}")
    print(f"B: {B}")
    print(f"C: {C}")

    # Plot data points
    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Class 0')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1')

    # Create a mesh grid for decision boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    # Predict on the grid
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='dashed', linewidths=2)

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid(True)
    plt.title('Logistic Regression Decision Boundary')
    plt.savefig('q4_5.png')
    plt.close()
    print(f"Image saved as q4_5.png")
    print("--"*20)


X = np.loadtxt('../data/Q4/q4x.dat', delimiter=None)
y = np.genfromtxt('../data/Q4/q4y.dat', dtype=str)

mp = {'Alaska': 0, 'Canada': 1}
y = np.array([mp[i] for i in y])

q4_1(X,y)
# q4_2_3(X,y)
# q4_4(X,y)
q4_5(X,y)


</PRE>
</PRE>
</BODY>
</HTML>
