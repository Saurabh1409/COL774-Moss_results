<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_62Q7T.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_62Q7T.py<p><PRE>


import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from linear_regression import LinearRegressor  

def plot(model, X, y):
    
    if model.theta is None:
        raise ValueError("Model is not trained yet. Please call fit() first.")

   
    plt.scatter(X, y, color='blue', label='Data Points')

    
    line_X = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)  
    line_y = model.predict(line_X)
    plt.plot(line_X, line_y, color='red', label='Hypothesis Function')

    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Data and Learned Hypothesis')
    plt.legend()
    plt.show()
    
def plot_error_surface(model, X, y):
    
    if model.theta is None:
        raise ValueError("Model is not trained yet. Please call fit() first.")

   
    m, n = X.shape
    X_intercept = np.zeros((m, n + 1))
    X_intercept[:, 0] = 1  
    X_intercept[:, 1:] = X  

    
    theta_0_vals = np.linspace(-10, 20, 100)
    theta_1_vals = np.linspace(-10, 50, 100)
    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

<A NAME="4"></A><FONT color = #FF00FF><A HREF="match181-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for i, theta_0 in enumerate(theta_0_vals):
        for j, theta_1 in enumerate(theta_1_vals):
            theta_temp = np.array([theta_0, theta_1])

            
            predictions = np.zeros(m)
</FONT>            for p in range(m):
                prediction = 0
                for q in range(n + 1):
                    prediction += X_intercept[p, q] * theta_temp[q]
                predictions[p] = prediction

            error = predictions - y
            squared_errors = error ** 2
            sum_squared_errors = np.sum(squared_errors)
            J_vals[i, j] = (1 / (2 * m)) * sum_squared_errors

    
    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    surface = ax.plot_surface(theta_0_vals, theta_1_vals, J_vals, cmap='viridis')
    fig.colorbar(surface, shrink=0.6, aspect=12)

    
    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Value of Cost Function J')
    ax.set_title('Error Surface')
    plt.show()
    
def animate_gradient_descent(model, theta_history, X, y):
    
    m, n = X.shape

    
    X_intercept = np.zeros((m, n + 1))
    X_intercept[:, 0] = 1  
    X_intercept[:, 1:] = X  

   
    theta_0_vals = np.linspace(-10, 20, 100)
    theta_1_vals = np.linspace(-10, 50, 100)
    J_vals = np.zeros((len(theta_0_vals), len(theta_1_vals)))

<A NAME="5"></A><FONT color = #FF0000><A HREF="match181-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    for i, theta_0 in enumerate(theta_0_vals):
        for j, theta_1 in enumerate(theta_1_vals):
            theta_temp = np.array([theta_0, theta_1])

            
            predictions = np.zeros(m)
</FONT>            for p in range(m):
                prediction = 0
                for q in range(n + 1):
                    prediction += X_intercept[p, q] * theta_temp[q]
                predictions[p] = prediction

            error = predictions - y
            squared_errors = error ** 2
            sum_squared_errors = np.sum(squared_errors)
            J_vals[i, j] = (1 / (2 * m)) * sum_squared_errors

    
    theta_0_vals, theta_1_vals = np.meshgrid(theta_0_vals, theta_1_vals)
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    surface = ax.plot_surface(theta_0_vals, theta_1_vals, J_vals, cmap='viridis', alpha=0.8)

    
    for theta in theta_history:
        current_theta_0 = theta[0]
        current_theta_1 = theta[1]

        
        predictions = np.zeros(m)
        for p in range(m):
            prediction = 0
            for q in range(n + 1):
                prediction += X_intercept[p, q] * theta[q]
            predictions[p] = prediction

        error = predictions - y
        squared_errors = error ** 2
        sum_squared_errors = np.sum(squared_errors)
        current_cost = (1 / (2 * m)) * sum_squared_errors

        
        ax.scatter(current_theta_0, current_theta_1, current_cost, color='red', s=50)

        plt.pause(0.2)

    plt.show()

def compute_cost(X, y, theta):
    
    m = len(y)
    predictions = X @ theta
    error = predictions - y
    return (1 / (2 * m)) * np.sum(error ** 2)

def animate_contour_descent2(model, theta_history, X, y):
    
    m, n = X.shape
    X_intercept = np.column_stack((np.ones(m), X))

    
    theta0_vals = np.linspace(min(theta_history[:, 0]) - 5, max(theta_history[:, 0]) + 5, 100)
    theta1_vals = np.linspace(min(theta_history[:, 1]) - 5, max(theta_history[:, 1]) + 5, 100)
    J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

    for i, t0 in enumerate(theta0_vals):
        for j, t1 in enumerate(theta1_vals):
            J_vals[i, j] = compute_cost(X_intercept, y, np.array([t0, t1]))

    
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    plt.figure(figsize=(8, 6))
    plt.contour(theta0_grid, theta1_grid, J_vals.T, levels=np.logspace(-5,10,30), cmap="viridis")
<A NAME="1"></A><FONT color = #00FF00><A HREF="match181-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.xlabel("Theta 0")
    plt.ylabel("Theta 1")
    plt.title("Gradient Descent Path")

    
    for i in range(len(theta_history)):
        plt.plot(theta_history[:i+1, 0], theta_history[:i+1, 1], "r-", alpha=0.7)  
</FONT>        plt.scatter(theta_history[i, 0], theta_history[i, 1], color="red", s=40) 
        plt.pause(0.2)  

    plt.show()

if __name__ == "__main__":
    import pandas as pd
    
    X = pd.read_csv("../data/Q1/linearX.csv", header=None).values
    y = pd.read_csv("../data/Q1/linearY.csv", header=None).values.flatten()

    
    model = LinearRegressor()
    theta_history = model.fit(X, y)
    
    print(model.theta)
    
    animate_contour_descent2(model,theta_history, X, y)
    # plot_error_surface(model, X, y)
    




# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
        pass
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        stopping_threshold = 1e-6
        max_iter = 1000000
        m, n = X.shape
        
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1 
        X_intercept[:, 1:] = X  

        self.theta = np.zeros(n + 1)
        theta_history = []
        for iteration in range(max_iter):
            
            predictions = np.zeros(m)
            for i in range(m):
                prediction = 0
                for j in range(n + 1):
                    prediction += X_intercept[i, j] * self.theta[j]
                predictions[i] = prediction

            
            error = predictions - y
            gradient = np.zeros(n + 1)
            for j in range(n + 1):
                gradient[j] = (1 / m) * np.sum(error * X_intercept[:, j])
            
            squared_errors = error ** 2  
            sum_squared_errors = np.sum(squared_errors)  
            cost = (1 / (2 * m)) * sum_squared_errors  
            
       #     print(cost,previous_cost)
            
            self.theta -= learning_rate * gradient
            theta_history.append(self.theta.copy())

            if iteration == 0:
                previous_cost = 0
            
            
            if abs(cost - previous_cost) &lt; stopping_threshold:
                print(f"Converged after {iteration} iterations.")
                break

            previous_cost = cost
            

        return np.array(theta_history)
        
        
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        if self.theta is None:
            raise ValueError("Model is not trained yet. Please call fit() first.")

        
        m, n = X.shape
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1  
        X_intercept[:, 1:] = X  
        
        predictions = np.zeros(m)
        for i in range(m):
            prediction = 0
            for j in range(n + 1):
                prediction += X_intercept[i, j] * self.theta[j]
            predictions[i] = prediction

        return predictions
        
    



import numpy as np
import matplotlib.pyplot as plt
from sampling_sgd import StochasticLinearRegressor 

def plot_theta_movement(theta_history, batch_size):
    
    theta_history = np.array(theta_history)  
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    
    theta0 = theta_history[:, 0]
    theta1 = theta_history[:, 1]
    theta2 = theta_history[:, 2]

   
    ax.plot(theta0, theta1, theta2, label=f'Batch Size: {batch_size}', marker='o', markersize=2)

    
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_zlabel(r'$\theta_2$')
    ax.set_title(f'Trajectory of θ Updates (Batch Size: {batch_size})')
    ax.legend()
    plt.show()



if __name__ == "__main__":
    
    model = StochasticLinearRegressor()
    N = 1000000
    X,y = model.generate(N, np.array([3, 1, 2]), np.array([3, -1]), np.array([2, 2]), (2 ** 0.5))
    X_train, y_train, X_test, y_test = model.train_test_split(X,y,0.8)
    theta_history = model.fit(X_train, y_train)
    print("final parameters1", model.theta1)
    print("final parameters2", model.theta2)
    print("final parameters3", model.theta3)
    print("final parameters3", model.theta4)
    lst = model.predict(X_test)
    
    # plot_theta_movement(theta_history, 1)
    # t = model.closed_form_solution(X_train, y_train)
    # print(t)
    # y_pred = model.predict(X_test)
    # err = model.mean_squared_error(y_test,y_pred)
    # print(err)
    # print("final parameters", model.theta)





# Imports - you can add any other permitted libraries
import numpy as np
from math import sqrt
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def theta_norm(theta1, theta2):
    squared_sum = 0
    for i in range(len(theta1)):
        squared_sum += (theta1[i] - theta2[i]) ** 2
    return squared_sum**0.5    


class StochasticLinearRegressor:
    def __init__(self):
        self.theta1 = None
        self.theta2 = None
        self.theta3 = None
        self.theta4 = None
    
    def generate(self, N, theta, input_mean, input_sigma, noise_sigma):
        """
        Generate normally distributed input data and target values
        Note that we have 2 input features
        Parameters
        ----------
        N : int
            The number of samples to generate.
            
        theta : numpy array of shape (3,)
            The true parameters of the linear regression model.
            
        input_mean : numpy array of shape (2,)
            The mean of the input data.
            
        input_sigma : numpy array of shape (2,)
            The standard deviation of the input data.
            
        noise_sigma : float
            The standard deviation of the Gaussian noise.
            
        Returns
        -------
        X : numpy array of shape (N, 2)
            The input data.
            
        y : numpy array of shape (N,)
            The target values.
        """
        
        x1 = np.random.normal(loc=input_mean[0], scale=input_sigma[0], size=N)  
        x2 = np.random.normal(loc=input_mean[1], scale=input_sigma[1], size=N)  
        
        x0 = np.ones((N, 1))
       
        x1 = x1.reshape(-1, 1)
        x2 = x2.reshape(-1, 1)
        X_intercept = np.hstack((x0, x1, x2))
        X = np.hstack((x1, x2))
        noise = np.random.normal(loc=0, scale=noise_sigma, size=N)
        y = X_intercept @ theta + noise
        
        return X, y
        
    def train_test_split(self,X, y, train_ratio=0.8):
        
        train_size = int(len(X) * train_ratio)
        
        
        indices = np.random.permutation(len(X))
        
       
        train_indices = indices[:train_size]
<A NAME="2"></A><FONT color = #0000FF><A HREF="match181-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        test_indices = indices[train_size:]
        
        
        X_train, y_train = X[train_indices], y[train_indices]
        X_test, y_test = X[test_indices], y[test_indices]
        
        return X_train, y_train, X_test, y_test

    

    def closed_form_solution(self,X, y):
</FONT>        
        m, n = X.shape
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1 
        X_intercept[:, 1:] = X  
        
        # Compute θ = (X^T X)^(-1) X^T y
        X_transpose = X_intercept.T
        theta = np.linalg.inv(X_transpose @ X_intercept) @ X_transpose @ y
        
        return theta
        
    def fit2(self, X, y, batch_size,learning_rate=0.001):
        
        
        m, n = X.shape
        
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1  
        X_intercept[:, 1:] = X  
        
        thetaf = np.zeros(n + 1)
        # batch_sizes = [1,80,8000,800000]
        # lst_theta_history = []
        # batch_size = 800000
        
            
        # print("batch size", batch_size)
        theta_history = []
        num_epoch = 100000
        if batch_size == 1:
            num_epoch = 20
        total_cost = 0
        prev_cost = 0
        prev_theta = np.zeros(n + 1)
        num_batches = m // batch_size
        for iteration in range(num_epoch):
            # print("iteration", iteration)
            
            indices = np.random.permutation(m)
            X_intercept = X_intercept[indices]
            y = y[indices]
            for b in range(0,m,batch_size):
                X_batch = X_intercept[b:b+batch_size]
                y_batch = y[b:b+batch_size]
                predictions = np.zeros(batch_size)
                for i in range(batch_size):
                    prediction = 0
                    for j in range(n + 1):
                        prediction += X_batch[i, j] * thetaf[j]
                    predictions[i] = prediction

                
                error = predictions - y_batch
                gradient = np.zeros(n + 1)
                for j in range(n + 1):
                    gradient[j] = (1 / batch_size) * np.sum(error * X_batch[:, j])
                
                gradient_norm = 0
                for g in gradient:
                    gradient_norm += g ** 2
                gradient_norm = gradient_norm ** 0.5

                # if(b == 0) :
                #     print("gradient norm", gradient_norm)
                # if gradient_norm &lt; 1e-8:
                #     print(f"Converged at iteration {iteration}, batch {b // batch_size}")
                #     return theta_history
                
                
                squared_errors = error ** 2  
                sum_squared_errors = np.sum(squared_errors)  
                cost = (1 / (2 * batch_size)) * sum_squared_errors  
                thetaf -= learning_rate * gradient
                # print(cost)
                total_cost += cost
            
            
            theta_history.append(thetaf.copy())
            theta_difference_norm = theta_norm(thetaf ,prev_theta)
            # print("theta difference", theta_difference_norm)
            # if theta_difference_norm &lt; 1e-4:
            #     break
            # print("theta", thetaf)
            # print(abs(total_cost - prev_cost)/num_batches)
            if(abs(total_cost - prev_cost)/num_batches &lt; 1e-6):
                print("converged at iteration",iteration)
                break
            prev_cost = total_cost
            total_cost = 0
            prev_theta = thetaf.copy()
        
        if batch_size == 1:
            self.theta1 = thetaf
        elif batch_size == 80:
            self.theta2 = thetaf
        elif batch_size == 8000:
            self.theta3 = thetaf
        elif batch_size == 800000:
            self.theta4 = thetaf
            
        theta_history = np.array(theta_history)
        return theta_history
        # return lst_theta_history
            
    def fit(self,X,y,learning_rate=0.001):
        
        lst_theta_history = []
        theta_history1 = self.fit2(X,y,1,learning_rate)
        theta_history2 = self.fit2(X,y,80,learning_rate)
        theta_history3 = self.fit2(X,y,8000,learning_rate)
        theta_history4 = self.fit2(X,y,800000,learning_rate)
        lst_theta_history.append(theta_history1)
        lst_theta_history.append(theta_history2)
        lst_theta_history.append(theta_history3)
        lst_theta_history.append(theta_history4)
        return lst_theta_history
        
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        m, n = X.shape
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1  
        X_intercept[:, 1:] = X  
        
        # Compute predictions: y_pred = X * theta
        lst_y_pred = []
        y_pred1 = X_intercept @ self.theta1
        y_pred2 = X_intercept @ self.theta2
        y_pred3 = X_intercept @ self.theta3
        y_pred4 = X_intercept @ self.theta4
        lst_y_pred.append(y_pred1)
        lst_y_pred.append(y_pred2)
        lst_y_pred.append(y_pred3)
        lst_y_pred.append(y_pred4)
        
        return lst_y_pred
    def mean_squared_error(self,y_true, y_pred):
        
        squared_errors = (y_true - y_pred) ** 2
        sum_squared_errors = np.sum(squared_errors)
        cost = (1 / (2 * len(y_true))) * sum_squared_errors
        
        return cost
    

    




import numpy as np
import matplotlib.pyplot as plt
from logistic_regression import LogisticRegressor  


def plot_decision_boundary(model, X, y):
    
    X = model.normalize(X)

    
    fig, ax = plt.subplots(figsize=(8, 6))

   
    class_0_indices = (y == 0)
    class_1_indices = (y == 1)

    ax.scatter(X[class_0_indices, 0], X[class_0_indices, 1], color='red', label="Class 0", marker="o")
    ax.scatter(X[class_1_indices, 0], X[class_1_indices, 1], color='blue', label="Class 1", marker="x")

    
    x_min, x_max = np.min(X[:, 0]) - 0.5, np.max(X[:, 0]) + 0.5  
    x_vals = np.linspace(x_min, x_max, 100) 

    
    if abs(model.theta[2]) &lt; 1e-6:
        raise ValueError("Theta[2] is too close to zero, decision boundary may be undefined.")

    y_vals = - (model.theta[0] + model.theta[1] * x_vals) / model.theta[2]  # Solve for x2 using θ0 + θ1*x1 + θ2*x2 = 0

    
    ax.plot(x_vals, y_vals, color='green', linestyle='-', linewidth=2, label="Decision Boundary")

    
    ax.set_xlabel("x1") 
    ax.set_ylabel("x2")
    ax.set_title("Logistic Regression Decision Boundary")
    ax.legend()

   
    plt.show()


if __name__ == "__main__":
  
    X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",") 
    y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")  
    
   
    model = LogisticRegressor()
    theta_history = model.fit(X, y)
    print("Fitted parameters (theta):", model.theta)
    predictions = model.predict(X)
    accuracy = model.compute_accuracy(y, predictions)
    print("Accuracy:", accuracy)
    plot_decision_boundary(model,X, y)

    # print("Predictions (probabilities):", predictions)
    # print("Predictions (binary):",y, (predictions &gt;= 0.5).astype(int))



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    
    def __init__(self):
        self.theta = None
    
    def sigmoid(self, z):
        """Compute the sigmoid function."""
        return 1 / (1 + np.exp(-z))

    def normalize(self, X):
        """Normalize the feature values."""
        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    def fit(self, X, y, learning_rate=0.1):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        num_iterations = 10000
        stopping_threshold = 1e-6
        X = self.normalize(X)
        m, n = X.shape
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1  
        X_intercept[:, 1:] = X  
        self.theta = np.zeros(n + 1)
        theta_history = []
        for iteration in range(num_iterations):
            
            h_theta = self.sigmoid(X_intercept @ self.theta)
            error = h_theta - y
            gradient = np.zeros(X_intercept.shape[1])
            for j in range(X_intercept.shape[1]):
                gradient[j] = np.sum(error * X_intercept[:, j]) / m
                
            
            R = np.zeros((m, m)) 
            for i in range(m):
                R[i, i] = h_theta[i] * (1 - h_theta[i])  
            H = X_intercept.T @ R @ X_intercept / m
            
            
            H_inv = np.linalg.inv(H)
            delta_theta = H_inv @ gradient
            self.theta -= learning_rate * delta_theta
            
           
            theta_norm = 0
            for j in range(len(delta_theta)):
                theta_norm += delta_theta[j] ** 2
            theta_norm = theta_norm ** 0.5

            if theta_norm &lt; stopping_threshold:
                print(f"Converged after {iteration + 1} iterations.")
                break
            theta_history.append(self.theta.copy())
            
        theta_history = np.array(theta_history)
        return theta_history
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = self.normalize(X)
        m, n = X.shape
        X_intercept = np.zeros((m, n + 1))
        X_intercept[:, 0] = 1  
        X_intercept[:, 1:] = X  
        probablities =  self.sigmoid(X_intercept @ self.theta)
        y_pred = (probablities &gt;= 0.5).astype(int)
        return y_pred
    
    def compute_accuracy(self,y_true, y_pred):
        
        y_pred_binary = (y_pred &gt;= 0.5).astype(int)
        return np.mean(y_true == y_pred_binary)
    
    
        




import numpy as np
import matplotlib.pyplot as plt
from gda import GaussianDiscriminantAnalysis  

def plot_decision_boundary(model,X, y):
    
    X_mean = np.mean(X, axis=0)
    X_std = np.std(X, axis=0)
    X = (X - X_mean) / X_std  
    
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska", marker="o")
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada", marker="x")

  
    sigma_inv = np.linalg.inv(model.sigma)
    w = sigma_inv @ (model.mu_1 - model.mu_0)  
    b = -0.5 * (model.mu_1.T @ sigma_inv @ model.mu_1 - model.mu_0.T @ sigma_inv @ model.mu_0)  

    
    x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    y_vals = - (w[0] * x_vals + b) / w[1]  

    
    plt.plot(x_vals, y_vals, 'g-', label="Decision Boundary")

<A NAME="3"></A><FONT color = #00FFFF><A HREF="match181-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    plt.xlabel("Feature 1 (x1)")
    plt.ylabel("Feature 2 (x2)")
    plt.title("Gaussian Discriminant Analysis Decision Boundary")
    plt.legend()
    plt.show()
    
def plot_decision_boundary_quadratic(model,X,y):
    X_mean = np.mean(X, axis=0)
</FONT>    X_std = np.std(X, axis=0)
    X = (X - X_mean) / X_std
    
    plt.figure(figsize=(8, 6))

   
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label="Alaska", marker="o")
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label="Canada", marker="x")

   
    sigma_inv = np.linalg.inv(model.sigma)
    w = sigma_inv @ (model.mu_1 - model.mu_0)
    b = -0.5 * (model.mu_1.T @ sigma_inv @ model.mu_1 - model.mu_0.T @ sigma_inv @ model.mu_0)

    x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    y_vals = (-b - w[0] * x_vals) / w[1]  # Solve for x2
    plt.plot(x_vals, y_vals, 'g-', label="Linear Boundary", linewidth=2)

   
<A NAME="0"></A><FONT color = #FF0000><A HREF="match181-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x1_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
    x2_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)
    X1, X2 = np.meshgrid(x1_vals, x2_vals)
    Z = np.zeros(X1.shape)
</FONT>
    
    for i in range(X1.shape[0]):
        for j in range(X1.shape[1]):
            x = np.array([X1[i, j], X2[i, j]])
            term_0 = (x - model.mu_0).T @ np.linalg.inv(model.sigma_0) @ (x - model.mu_0)
            term_1 = (x - model.mu_1).T @ np.linalg.inv(model.sigma_1) @ (x - model.mu_1)
            Z[i, j] = term_1 - term_0  

    
    plt.contour(X1, X2, Z, levels=[0], colors='purple', linewidths=2, linestyles='dashed', label="Quadratic Boundary")

    plt.xlabel("Feature 1 (x1)")
    plt.ylabel("Feature 2 (x2)")
    plt.title("GDA: Linear and Quadratic Decision Boundaries")
    plt.legend()
    plt.show()
        

    
    

if __name__ == "__main__":
    
    X = np.loadtxt("../data/Q4/q4x.dat")
    y_raw = np.loadtxt("../data/Q4/q4y.dat",dtype = str)
    y = np.where(y_raw == "Alaska", 0, 1)
    model = GaussianDiscriminantAnalysis()
    model.fit(X, y, assume_same_covariance=True)
    model.fit(X, y, assume_same_covariance=False)
    print(model.mu_0)
    print(model.mu_1)
    print(model.sigma)
    print(model.sigma_0)
    print(model.sigma_1)
    predictions = model.predict(X)
    print(predictions)
    plot_decision_boundary_quadratic(model,X, y)



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    
    def __init__(self):
        self.mu_0 = None  
        self.mu_1 = None  
        self.sigma_0 = None  
        self.sigma_1 = None  
        self.sigma = None  
        self.phi = None  
    
    def gaussian_likelihood(self,X, mu, sigma):
        """Compute the Gaussian probability density for each sample."""
        d = X.shape[1] 
        sigma_inv = np.linalg.inv(sigma)
        det_sigma = np.linalg.det(sigma)
        coef = 1 / (np.sqrt((2 * np.pi) ** d * det_sigma))

        diff = X - mu 
        exp_term = np.exp(-0.5 * np.sum(diff @ sigma_inv * diff, axis=1))
        return coef * exp_term
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std  
        
        m,n = X.shape
        
        X_0 = X[y == 0]  
        X_1 = X[y == 1]  
        
        self.mu_0 = np.mean(X_0, axis=0)  
        self.mu_1 = np.mean(X_1, axis=0)  
        self.phi = np.mean(y)  
        
        centered_0 = X_0 - self.mu_0  
        centered_1 = X_1 - self.mu_1  
        
        self.sigma_0 = (centered_0.T @ centered_0) / len(X_0)  
        self.sigma_1 = (centered_1.T @ centered_1) / len(X_1)  
        
        if assume_same_covariance:
            
            sigma = np.zeros((n, n))

            for i in range(m):
                x_i = X[i].reshape(-1, 1)  
                mu_class = self.mu_0 if y[i] == 0 else self.mu_1
                mu_class = mu_class.reshape(-1, 1)  
                sigma += (x_i - mu_class) @ (x_i - mu_class).T

            self.sigma = sigma / m  
            return self.mu_0, self.mu_1, self.sigma
        
        else:
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
        

    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X_mean = np.mean(X, axis=0)
        X_std = np.std(X, axis=0)
        X = (X - X_mean) / X_std 
        
        
        sigma0 = self.sigma_0 if self.sigma_0 is not None else self.sigma
        sigma1 = self.sigma_1 if self.sigma_1 is not None else self.sigma
        likelihood_0 = self.gaussian_likelihood(X, self.mu_0, sigma0)
        likelihood_1 = self.gaussian_likelihood(X, self.mu_1, sigma1)

       
        posterior_0 = likelihood_0 * (1 - self.phi)
        posterior_1 = likelihood_1 * self.phi

        
        y_pred = (posterior_1 &gt; posterior_0).astype(int)

        return y_pred
        
        
    

</PRE>
</PRE>
</BODY>
</HTML>
