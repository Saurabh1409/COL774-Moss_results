<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_6DF52.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_ZWC3W.py<p><PRE>


# -*- coding: utf-8 -*-
"""ml1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z_RNnFHoLHMDQcsMYKjseG3LjfXBYmmL
"""

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from numpy import genfromtxt
import matplotlib.animation as animation
from IPython.display import HTML
from matplotlib.animation import FFMpegWriter

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def calculate_loss(X,y,theta):
  """
  X : numpy array with intercept of shape (n_samples, n_features)
            The input data.

  y : numpy array of shape (n_samples,)
            The target values.
  """
  n_samples, n_features = X.shape
  return (((X @ theta) - y) @ ((X @ theta) - y))/(2*n_samples)

class LinearRegressor:
    def __init__(self):
      self.tolerance=1e-6
      self.max_iterations=10000
      self.theta = None
      self.cost_history = []
      self.theta_history = []

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape

        #ADDING INTERCEPT TERM TO X
        X = np.hstack([np.ones((n_samples, 1)), X])

        n_samples, n_features = X.shape

        self.theta = np.zeros(n_features)
        self.theta_history.append(self.theta.copy())

        for iteration in range(self.max_iterations):
            predictions = X @ self.theta
            error_array = predictions - y
            cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)

            self.cost_history.append(cost)

            # Check convergence
            if len(self.cost_history) &gt; 1 and abs(self.cost_history[-2] - cost) &lt; self.tolerance:
                print(f"Converged at iteration {iteration}")
                break

            # Update theta
            gradient = (1 / n_samples) * (X.T @ error_array)
            self.theta -= learning_rate * gradient
            self.theta_history.append(self.theta.copy())

        return np.array(self.theta_history)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples, n_features = X.shape
        X = np.hstack([np.ones((n_samples, 1)), X])
        return X @ self.theta

    def plot_2d_graph(self, X, y):
        X= X.flatten()
        n_samples= X.shape[0]
        plt.scatter(X,y, label='Data')

        x_line = np.array([X.min(), X.max()])
        x_array=x_line.reshape(-1,1)
        x_array= np.hstack([np.ones((2, 1)), x_array])
        y_line=x_array @ self.theta

        plt.plot(x_line, y_line, color='red', label='Regression Line')

        plt.xlabel('Acidity')
        plt.ylabel('Density')
        plt.legend()
        plt.title('Linear Regression Fit')
        # plt.savefig('Q1_plot.png')
        plt.show()



    def plot_error_mesh(self, X, y):
        """
        Draw a 3D mesh of the error function J(theta) and overlay the gradient descent path.
        A pause of 0.2 seconds is included between iterations for visualization.
        """
        n_samples = X.shape[0]
        # Augment X with intercept.
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        # Define grid range for theta_0 and theta_1.
        theta0_vals = np.linspace(-6, 20, 100)
        theta1_vals = np.linspace(-6, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost for each grid point.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_zlabel('Cost J')
        ax.set_title('Error Surface with Gradient Descent Path')

        # Animate the gradient descent path.
        theta_hist = np.array(self.theta_history)
        for th in theta_hist:
            cost_val = calculate_loss(X_aug, y, th)
            ax.scatter(th[0], th[1], cost_val, color='red', s=50)
            plt.pause(0.02)
        plt.show()

    def plot_error_contours(self, X, y):
        """
        Draw 2D contour plots of the error function and animate the gradient descent trajectory.
        A 0.2 second pause is included between iterations.
        """
        n_samples = X.shape[0]
        # Augment X with intercept.
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost over the grid.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        plt.figure()
        cp = plt.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.clabel(cp, inline=True, fontsize=8)
        plt.xlabel('theta_0')
        plt.ylabel('theta_1')
        plt.title('Contour Plot of Error Function with Gradient Descent Path')

        # Animate gradient descent path on contour plot.
        theta_hist = np.array(self.theta_history)
        for th in theta_hist:
            plt.plot(th[0], th[1], 'rx')
            plt.pause(0.2)
        plt.show()

    def plot_error_contours_multiple(self, X, y, learning_rates):
        """
        For each learning rate in learning_rates, run gradient descent and plot the contour path.
        This function overlays the gradient descent trajectories for different learning rates on one contour plot.
        """
        plt.figure()
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute the error surface.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        cp = plt.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.clabel(cp, inline=True, fontsize=8)
        plt.xlabel('theta_0')
        plt.ylabel('theta_1')
        plt.title('Gradient Descent Paths for Different Learning Rates')

        colors = ['r', 'g', 'b']
        for idx, lr in enumerate(learning_rates):
            # Reinitialize theta and histories for each learning rate.
            theta = np.zeros(X_aug.shape[1])
            theta_hist = [theta.copy()]
            cost_history = []
            for iteration in range(self.max_iterations):
                predictions = X_aug @ theta
                error_array = predictions - y
                cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)
                cost_history.append(cost)

                if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; self.tolerance:
                    break

                gradient = (1 / n_samples) * (X_aug.T @ error_array)
                theta -= lr * gradient
                theta_hist.append(theta.copy())

            theta_hist = np.array(theta_hist)
            plt.plot(theta_hist[:, 0], theta_hist[:, 1], marker='o', color=colors[idx], label=f'eta = {lr}')

        plt.legend()
        plt.show()


    def animate_error_contours(self, X, y):
      """
      Animate the gradient descent path on a 2D contour plot of the error function using
      matplotlib.animation.FuncAnimation.

      Returns
      -------
      ani : matplotlib.animation.FuncAnimation object
          The animation object that can be displayed in a notebook.
      """
      n_samples = X.shape[0]
      # Augment X with intercept.
      X_aug = np.hstack([np.ones((n_samples, 1)), X])

      theta0_vals = np.linspace(-10, 24, 100)
      theta1_vals = np.linspace(-10, 80, 100)
      Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
      J_vals = np.zeros(Theta0.shape)

      # Compute cost values over the grid.
      for i in range(Theta0.shape[0]):
          for j in range(Theta0.shape[1]):
              theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
              J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

      fig, ax = plt.subplots()
      cp = ax.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
      ax.set_xlabel('theta_0')
      ax.set_ylabel('theta_1')
      ax.set_title('Animated Contour Plot of Error Function')

      # Initialize scatter plot for the gradient descent point.
      scatter, = ax.plot([], [], 'rx', markersize=8)

      def init():
          scatter.set_data([], [])
          return scatter,

      def update(frame):
          # frame is an index in theta_history.
          th = self.theta_history[frame]
          # Wrap the scalar values in a list.
          scatter.set_data([th[0]], [th[1]])
          return scatter,

      ani = animation.FuncAnimation(fig, update, frames=len(self.theta_history),
                                    init_func=init, blit=True, interval=200)

      plt.close(fig)  # Prevent duplicate static plot display.
      return ani

    def animate_error_mesh(self, X, y):
        """
        Animate the gradient descent path on a 3D error surface (mesh) using
        matplotlib.animation.FuncAnimation.

        Returns
        -------
        ani : matplotlib.animation.FuncAnimation object
            The animation object that can be displayed in a notebook.
        """
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost values over the grid.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_zlabel('Cost J')
        ax.set_title('Animated Error Mesh with Gradient Descent Path')

        # Initialize a scatter point for the current theta.
        init_theta = self.theta_history[0]
        init_cost = calculate_loss(X_aug, y, init_theta)
        sc = ax.scatter([init_theta[0]], [init_theta[1]], [init_cost], color='red', s=50)

        def update(frame):
            th = self.theta_history[frame]
            cost_val = calculate_loss(X_aug, y, th)
            # For 3D scatter, update _offsets3d.
            sc._offsets3d = ([th[0]], [th[1]], [cost_val])
            return sc,

        ani = animation.FuncAnimation(fig, update, frames=len(self.theta_history),
                                      interval=200, blit=False)
        plt.close(fig)
        return ani

    def animate_error_contours_multiple(self, X, y, learning_rates):
        """
        Animate the gradient descent trajectories for multiple learning rates on a single
        2D contour plot of the error function using matplotlib.animation.FuncAnimation.

        Parameters
        ----------
        learning_rates : list of floats
            The learning rates to compare.

        Returns
        -------
        ani : matplotlib.animation.FuncAnimation object
            The animation object that can be displayed in a notebook.
        """
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute error surface.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig, ax = plt.subplots()
        cp = ax.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        ax.clabel(cp, inline=True, fontsize=8)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_title('Animated GD Paths for Different Learning Rates')

        trajectories = []  # List to store trajectory for each learning rate.
        colors = ['r', 'g', 'b']
        for lr in learning_rates:
            theta = np.zeros(X_aug.shape[1])
            traj = [theta.copy()]
            cost_history = []
            for iteration in range(self.max_iterations):
                predictions = X_aug @ theta
                error_array = predictions - y
                cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)
                cost_history.append(cost)
                if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; self.tolerance:
                    break
                gradient = (1 / n_samples) * (X_aug.T @ error_array)
                theta -= lr * gradient
                traj.append(theta.copy())
            trajectories.append(np.array(traj))

        max_len = max(traj.shape[0] for traj in trajectories)

        # Create a marker for each learning rate.
        markers = []
        for idx, traj in enumerate(trajectories):
            marker, = ax.plot([], [], marker='o', color=colors[idx], label=f'eta = {learning_rates[idx]}')
            markers.append(marker)
        ax.legend()

        def init():
            for marker in markers:
                marker.set_data([], [])
            return markers

        def update(frame):
            for i, traj in enumerate(trajectories):
                index = frame if frame &lt; traj.shape[0] else traj.shape[0] - 1
                th = traj[index]
                markers[i].set_data([th[0]], [th[1]])
            return markers

        ani = animation.FuncAnimation(fig, update, frames=max_len,
                                      init_func=init, interval=200, blit=True)
        plt.close(fig)
        return ani





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from numpy import genfromtxt
import matplotlib.animation as animation
from IPython.display import HTML
from matplotlib.animation import FFMpegWriter

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


def calculate_loss(X,y,theta):
  """
  X : numpy array with intercept of shape (n_samples, n_features)
            The input data.

  y : numpy array of shape (n_samples,)
            The target values.
  """
  n_samples, n_features = X.shape
  return (((X @ theta) - y) @ ((X @ theta) - y))/(2*n_samples)

class LinearRegressor:
    def __init__(self):
      self.tolerance=1e-6
      self.max_iterations=10000
      self.theta = None
      self.cost_history = []
      self.theta_history = []

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        n_samples, n_features = X.shape

        #ADDING INTERCEPT TERM TO X
        X = np.hstack([np.ones((n_samples, 1)), X])

        n_samples, n_features = X.shape

        self.theta = np.zeros(n_features)
        self.theta_history.append(self.theta.copy())

        for iteration in range(self.max_iterations):
            predictions = X @ self.theta
            error_array = predictions - y
            cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)

            self.cost_history.append(cost)

            # Check convergence
            if len(self.cost_history) &gt; 1 and abs(self.cost_history[-2] - cost) &lt; self.tolerance:
                print(f"Converged at iteration {iteration}")
                break

            # Update theta
            gradient = (1 / n_samples) * (X.T @ error_array)
            self.theta -= learning_rate * gradient
            self.theta_history.append(self.theta.copy())

        return np.array(self.theta_history)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        n_samples, n_features = X.shape
        X = np.hstack([np.ones((n_samples, 1)), X])
        return X @ self.theta

    def plot_2d_graph(self, X, y):
        X= X.flatten()
        n_samples= X.shape[0]
        plt.scatter(X,y, label='Data')

        x_line = np.array([X.min(), X.max()])
        x_array=x_line.reshape(-1,1)
        x_array= np.hstack([np.ones((2, 1)), x_array])
        y_line=x_array @ self.theta

        plt.plot(x_line, y_line, color='red', label='Regression Line')

        plt.xlabel('Acidity')
        plt.ylabel('Density')
        plt.legend()
        plt.title('Linear Regression Fit')
        # plt.savefig('Q1_plot.png')
        plt.show()



    def plot_error_mesh(self, X, y):
        """
        Draw a 3D mesh of the error function J(theta) and overlay the gradient descent path.
        A pause of 0.2 seconds is included between iterations for visualization.
        """
        n_samples = X.shape[0]
        # Augment X with intercept.
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        # Define grid range for theta_0 and theta_1.
        theta0_vals = np.linspace(-6, 20, 100)
        theta1_vals = np.linspace(-6, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost for each grid point.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_zlabel('Cost J')
        ax.set_title('Error Surface with Gradient Descent Path')

        # Animate the gradient descent path.
        theta_hist = np.array(self.theta_history)
        for th in theta_hist:
            cost_val = calculate_loss(X_aug, y, th)
            ax.scatter(th[0], th[1], cost_val, color='red', s=50)
            plt.pause(0.02)
        plt.show()

    def plot_error_contours(self, X, y):
        """
        Draw 2D contour plots of the error function and animate the gradient descent trajectory.
        A 0.2 second pause is included between iterations.
        """
        n_samples = X.shape[0]
        # Augment X with intercept.
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost over the grid.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        plt.figure()
        cp = plt.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.clabel(cp, inline=True, fontsize=8)
        plt.xlabel('theta_0')
        plt.ylabel('theta_1')
        plt.title('Contour Plot of Error Function with Gradient Descent Path')

        # Animate gradient descent path on contour plot.
        theta_hist = np.array(self.theta_history)
        for th in theta_hist:
            plt.plot(th[0], th[1], 'rx')
            plt.pause(0.2)
        plt.show()

    def plot_error_contours_multiple(self, X, y, learning_rates):
        """
        For each learning rate in learning_rates, run gradient descent and plot the contour path.
        This function overlays the gradient descent trajectories for different learning rates on one contour plot.
        """
        plt.figure()
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute the error surface.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        cp = plt.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        plt.clabel(cp, inline=True, fontsize=8)
        plt.xlabel('theta_0')
        plt.ylabel('theta_1')
        plt.title('Gradient Descent Paths for Different Learning Rates')

        colors = ['r', 'g', 'b']
        for idx, lr in enumerate(learning_rates):
            # Reinitialize theta and histories for each learning rate.
            theta = np.zeros(X_aug.shape[1])
            theta_hist = [theta.copy()]
            cost_history = []
            for iteration in range(self.max_iterations):
                predictions = X_aug @ theta
                error_array = predictions - y
                cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)
                cost_history.append(cost)

                if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; self.tolerance:
                    break

                gradient = (1 / n_samples) * (X_aug.T @ error_array)
                theta -= lr * gradient
                theta_hist.append(theta.copy())

            theta_hist = np.array(theta_hist)
            plt.plot(theta_hist[:, 0], theta_hist[:, 1], marker='o', color=colors[idx], label=f'eta = {lr}')

        plt.legend()
        plt.show()


    def animate_error_contours(self, X, y):
      """
      Animate the gradient descent path on a 2D contour plot of the error function using
      matplotlib.animation.FuncAnimation.

      Returns
      -------
      ani : matplotlib.animation.FuncAnimation object
          The animation object that can be displayed in a notebook.
      """
      n_samples = X.shape[0]
      # Augment X with intercept.
      X_aug = np.hstack([np.ones((n_samples, 1)), X])

      theta0_vals = np.linspace(-10, 24, 100)
      theta1_vals = np.linspace(-10, 80, 100)
      Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
      J_vals = np.zeros(Theta0.shape)

      # Compute cost values over the grid.
      for i in range(Theta0.shape[0]):
          for j in range(Theta0.shape[1]):
              theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
              J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

      fig, ax = plt.subplots()
      cp = ax.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
      ax.set_xlabel('theta_0')
      ax.set_ylabel('theta_1')
      ax.set_title('Animated Contour Plot of Error Function')

      # Initialize scatter plot for the gradient descent point.
      scatter, = ax.plot([], [], 'rx', markersize=8)

      def init():
          scatter.set_data([], [])
          return scatter,

      def update(frame):
          # frame is an index in theta_history.
          th = self.theta_history[frame]
          # Wrap the scalar values in a list.
          scatter.set_data([th[0]], [th[1]])
          return scatter,

      ani = animation.FuncAnimation(fig, update, frames=len(self.theta_history),
                                    init_func=init, blit=True, interval=200)

      plt.close(fig)  # Prevent duplicate static plot display.
      return ani

    def animate_error_mesh(self, X, y):
        """
        Animate the gradient descent path on a 3D error surface (mesh) using
        matplotlib.animation.FuncAnimation.

        Returns
        -------
        ani : matplotlib.animation.FuncAnimation object
            The animation object that can be displayed in a notebook.
        """
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute cost values over the grid.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        surf = ax.plot_surface(Theta0, Theta1, J_vals, cmap='viridis', alpha=0.6)
        fig.colorbar(surf, shrink=0.5, aspect=5)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_zlabel('Cost J')
        ax.set_title('Animated Error Mesh with Gradient Descent Path')

        # Initialize a scatter point for the current theta.
        init_theta = self.theta_history[0]
        init_cost = calculate_loss(X_aug, y, init_theta)
        sc = ax.scatter([init_theta[0]], [init_theta[1]], [init_cost], color='red', s=50)

        def update(frame):
            th = self.theta_history[frame]
            cost_val = calculate_loss(X_aug, y, th)
            # For 3D scatter, update _offsets3d.
            sc._offsets3d = ([th[0]], [th[1]], [cost_val])
            return sc,

        ani = animation.FuncAnimation(fig, update, frames=len(self.theta_history),
                                      interval=200, blit=False)
        plt.close(fig)
        return ani

    def animate_error_contours_multiple(self, X, y, learning_rates):
        """
        Animate the gradient descent trajectories for multiple learning rates on a single
        2D contour plot of the error function using matplotlib.animation.FuncAnimation.

        Parameters
        ----------
        learning_rates : list of floats
            The learning rates to compare.

        Returns
        -------
        ani : matplotlib.animation.FuncAnimation object
            The animation object that can be displayed in a notebook.
        """
        n_samples = X.shape[0]
        X_aug = np.hstack([np.ones((n_samples, 1)), X])

        theta0_vals = np.linspace(-10, 24, 100)
        theta1_vals = np.linspace(-10, 80, 100)
        Theta0, Theta1 = np.meshgrid(theta0_vals, theta1_vals)
        J_vals = np.zeros(Theta0.shape)

        # Compute error surface.
        for i in range(Theta0.shape[0]):
            for j in range(Theta0.shape[1]):
                theta_temp = np.array([Theta0[i, j], Theta1[i, j]])
                J_vals[i, j] = calculate_loss(X_aug, y, theta_temp)

        fig, ax = plt.subplots()
        cp = ax.contour(Theta0, Theta1, J_vals, levels=np.logspace(-2, 3, 20), cmap='viridis')
        ax.clabel(cp, inline=True, fontsize=8)
        ax.set_xlabel('theta_0')
        ax.set_ylabel('theta_1')
        ax.set_title('Animated GD Paths for Different Learning Rates')

        trajectories = []  # List to store trajectory for each learning rate.
        colors = ['r', 'g', 'b']
        for lr in learning_rates:
            theta = np.zeros(X_aug.shape[1])
            traj = [theta.copy()]
            cost_history = []
            for iteration in range(self.max_iterations):
                predictions = X_aug @ theta
                error_array = predictions - y
                cost = (1 / (2 * n_samples)) * np.sum(error_array ** 2)
                cost_history.append(cost)
                if len(cost_history) &gt; 1 and abs(cost_history[-2] - cost) &lt; self.tolerance:
                    break
                gradient = (1 / n_samples) * (X_aug.T @ error_array)
                theta -= lr * gradient
                traj.append(theta.copy())
            trajectories.append(np.array(traj))

        max_len = max(traj.shape[0] for traj in trajectories)

        # Create a marker for each learning rate.
        markers = []
        for idx, traj in enumerate(trajectories):
            marker, = ax.plot([], [], marker='o', color=colors[idx], label=f'eta = {learning_rates[idx]}')
            markers.append(marker)
        ax.legend()

        def init():
            for marker in markers:
                marker.set_data([], [])
            return markers

        def update(frame):
            for i, traj in enumerate(trajectories):
                index = frame if frame &lt; traj.shape[0] else traj.shape[0] - 1
                th = traj[index]
                markers[i].set_data([th[0]], [th[1]])
            return markers

        ani = animation.FuncAnimation(fig, update, frames=max_len,
                                      init_func=init, interval=200, blit=True)
        plt.close(fig)
        return ani








# In[2]:


X = genfromtxt('linearX.csv', delimiter=',').reshape(-1, 1)
y = genfromtxt('linearY.csv', delimiter=',')

LR= LinearRegressor()
LR.fit(X,y)
LR.plot_2d_graph(X,y)
print(LR.theta)


# In[3]:


writer = FFMpegWriter(fps=5, bitrate=500)
ani_contour = LR.animate_error_contours(X, y)
ani_contour.save('b.mp4', writer=writer)



# In[ ]:


writer = FFMpegWriter(fps=5, bitrate=500)
ani_mesh = LR.animate_error_mesh(X, y)
ani_mesh.save('A.mp4', writer=writer)


# In[6]:


ani_multi_contour = LR.animate_error_contours_multiple(X, y,[0.001, 0.025, 0.1])
<A NAME="1"></A><FONT color = #00FF00><A HREF="match232-0.html#1" TARGET="0"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

ani_multi_contour.save('c.mp4', writer=writer)





#!/usr/bin/env python
# coding: utf-8

# In[25]:


# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
</FONT>    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    X = np.column_stack((x1, x2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise
    return X, y

def calculate_loss(X,y,theta):
    """
    X : numpy array with intercept of shape (n_samples, n_features)
              The input data.

    y : numpy array of shape (n_samples,)
              The target values.
    """
    n_samples, n_features = X.shape
    return (((X @ theta) - y) @ ((X @ theta) - y))/(2*n_samples)

class StochasticLinearRegressor:
    def __init__(self):
        #self.batch_size = 1
        self.theta = None
        self.theta_history = []
        self.loss_history = []
        self.train_errors = []

    def has_converged(self):
      window=1
      if batch_size&lt;10000:
        window = 1000
      elif batch_size&lt;100000:
        window=100
      else:
        window=1
      if len(self.loss_history) &gt; 2 * window:
          recent_avg = np.mean(self.loss_history[-window:])
          previous_avg = np.mean(self.loss_history[-2*window:-window])
          if np.abs(recent_avg - previous_avg) &lt; 1e-5:
              return True
      return False

<A NAME="0"></A><FONT color = #FF0000><A HREF="match232-0.html#0" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def fit(self, X, y, learning_rate=0.001):

        # Add intercept term by augmenting X with a column of ones.
        n_samples = X.shape[0]
        X_augmented = np.hstack((np.ones((n_samples, 1)), X))
        n_features = X_augmented.shape[1]  # This should be 3.

        # Initialize theta to zeros.
        self.theta = np.zeros(n_features)
</FONT>        self.theta_history = [self.theta.copy()]
        self.train_errors = []
        self.loss_history = []



        # Define convergence criteria.
        max_epochs = 1000
        epoch = 0
        converged = False

        while epoch &lt; max_epochs and not converged:
            # Shuffle the data at the beginning of each epoch.
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_augmented_shuffled = X_augmented[indices]
            y_shuffled = y[indices]


            # Process the data in mini-batches.
            for start in range(0, n_samples, batch_size):
                end = min(start + batch_size, n_samples)
                X_batch = X_augmented_shuffled[start:end]
                y_batch = y_shuffled[start:end]
                r = X_batch.shape[0]  # Current mini-batch size.

                # Compute predictions: y_pred = X_batch dot theta.
                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                # Gradient of the loss (1/(2r) * sum(error^2)) is (1/r) * X_batchᵀ*(error).
                grad = (1.0 / r) * X_batch.T.dot(error)
                # Update the parameters.
                self.theta = self.theta - learning_rate * grad

                # Record the parameters after each mini-batch update.
                self.theta_history.append(self.theta.copy())

                loss = np.mean((X_augmented.dot(self.theta) - y)**2) / 2.0
                self.loss_history.append(loss)

                # Check the stopping criterion after each update.
                if self.has_converged():
                    print(f"Stopping criteria met at epoch {epoch}")
                    converged = True
                    break
            if not converged:
              total_error = np.mean((X_augmented.dot(self.theta) - y)**2) / 2.0
              self.train_errors.append(total_error)
            epoch += 1

        return np.array(self.theta_history)


    def predict(self, X):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        return X.dot(self.theta)

    def closed_form_solution(self,X, y):
      n_samples = X.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X))
      theta = np.linalg.inv(X_augmented.T.dot(X_augmented)).dot(X_augmented.T).dot(y)
      return theta

    def plot_training_error(self):
        """
        Plot the training error (MSE/2) versus epochs.
        """
        plt.figure()
        plt.plot(range(len(self.train_errors)), self.train_errors, marker='o')
        plt.xlabel('Epoch')
        plt.ylabel('Training Error (MSE/2)')
        plt.title('Training Error vs. Epoch')
        plt.show()

    def plot_parameter_trajectory(self):
        """
        Plot the trajectory of the parameters (theta0, theta1, theta2) in 3D space.
        This helps visualize the updates to theta over iterations.
        """
        params = np.array(self.theta_history)
<A NAME="2"></A><FONT color = #0000FF><A HREF="match232-0.html#2" TARGET="0"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.plot(params[:, 0], params[:, 1], params[:, 2], marker='o')
</FONT>        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_zlabel('theta2')
        ax.set_title(f'Parameter Trajectory during SGD for batch size {batch_size}')
        plt.show()


# In[26]:


import time
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])  # Standard deviations (sqrt of variance 4)
noise_sigma = np.sqrt(2)
X, y = generate(10**6, theta_true, input_mean, input_sigma, noise_sigma)

# Split into train and test
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

#Initialize different models with different batch size
model_1=StochasticLinearRegressor()
model_80=StochasticLinearRegressor()
model_8000=StochasticLinearRegressor()
model_800000=StochasticLinearRegressor()

batch_size=1
start_time = time.time()
theta_1=model_1.fit(X_train, y_train)
end_time = time.time()
runtime_1 = end_time - start_time

batch_size=80
start_time = time.time()
theta_80=model_80.fit(X_train, y_train)
end_time = time.time()
runtime_80 = end_time - start_time

batch_size=8000
start_time = time.time()
theta_8000=model_8000.fit(X_train, y_train)
end_time = time.time()
runtime_8000 = end_time - start_time

batch_size=800000
start_time = time.time()
theta_800000=model_800000.fit(X_train, y_train)
end_time = time.time()
runtime_80000 = end_time - start_time




#md.plot_parameter_trajectory()


# In[28]:


batch_size=1
model_1.plot_parameter_trajectory()
batch_size=80
model_80.plot_parameter_trajectory()
batch_size=8000
model_8000.plot_parameter_trajectory()
batch_size=800000
model_800000.plot_parameter_trajectory()


# In[23]:


theta_closed = model_1.closed_form_solution(X_train, y_train)

print("Learned Params comparison :")
print("True theta",theta_true)
print("Batch size 1 theta :",model_1.theta)
print("Batch size 80 theta :",model_80.theta)
print("Batch size 8000 theta :",model_8000.theta)
print("Batch size 800000 theta :",model_800000.theta)

print("Runtime comparison")
print("Runtime for batch size 1 :",runtime_1)
print("Runtime for batch size 80 :",runtime_80)
print("Runtime for batch size 8000 :",runtime_8000)
print("Runtime for batch size 800000 :",runtime_80000)

print("Iterations comparison")
print("No of iterations in batch size 1 :",len(model_1.theta_history))
print("No of iterations in batch size 80 :",len(model_80.theta_history))
print("No of iterations in batch size 8000 :",len(model_8000.theta_history))
print("No of iterations in batch size 800000 :",len(model_800000.theta_history))

print("Epochs comparison")
print("No of epochs in batch size 1 :",len(model_1.train_errors))
print("No of epochs in batch size 80 :",len(model_80.train_errors))
print("No of epochs in batch size 8000 :",len(model_8000.train_errors))
print("No of epochs in batch size 800000 :",len(model_800000.train_errors))

print("Closed form theta :",theta_closed)

n_test_samples = X_test.shape[0]
X_test_augmented = np.hstack((np.ones((n_test_samples, 1)), X_test))

n_train_samples = X_train.shape[0]
X_train_augmented = np.hstack((np.ones((n_train_samples, 1)), X_train))

mse_1=calculate_loss(X_test_augmented,y_test,model_1.theta)
mse_80=calculate_loss(X_test_augmented,y_test,model_80.theta)
mse_8000=calculate_loss(X_test_augmented,y_test,model_8000.theta)
mse_800000=calculate_loss(X_test_augmented,y_test,model_800000.theta)

training_error_1=calculate_loss(X_train_augmented,y_train,model_1.theta)
training_error_80=calculate_loss(X_train_augmented,y_train,model_80.theta)
training_error_8000=calculate_loss(X_train_augmented,y_train,model_8000.theta)
training_error_800000=calculate_loss(X_train_augmented,y_train,model_800000.theta)

print("Test and training error comparison :")
print("Batch size 1 :- Test: ",mse_1," Train: ",training_error_1)
print("Batch size 80 :- Test: ",mse_80," Train: ",training_error_80)
print("Batch size 8000 :- Test: ",mse_8000," Train: ",training_error_8000)
print("Batch size 800000 :- Test: ",mse_800000," Train: ",training_error_800000)







# -*- coding: utf-8 -*-
"""ml2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m91BQ6HSyyzA57ZPbRymtvCOesi79EHf
"""

# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    X = np.column_stack((x1, x2))
    noise = np.random.normal(0, noise_sigma, N)
    y = theta[0] + theta[1]*x1 + theta[2]*x2 + noise
    return X, y

def calculate_loss(X,y,theta):
    """
    X : numpy array with intercept of shape (n_samples, n_features)
              The input data.

    y : numpy array of shape (n_samples,)
              The target values.
    """
    n_samples, n_features = X.shape
    return (((X @ theta) - y) @ ((X @ theta) - y))/(2*n_samples)

class StochasticLinearRegressor:
    def __init__(self):
        #self.batch_size = 1
        self.theta = None
        self.theta_history = []
        self.loss_history = []
        self.train_errors = []

    def has_converged(self):
      window=1
      batch_size=1
      if batch_size&lt;10000:
        window = 1000
      elif batch_size&lt;100000:
        window=100
      else:
        window=1
      if len(self.loss_history) &gt; 2 * window:
          recent_avg = np.mean(self.loss_history[-window:])
          previous_avg = np.mean(self.loss_history[-2*window:-window])
          if np.abs(recent_avg - previous_avg) &lt; 1e-5:
              return True
      return False

    def fit(self, X, y, learning_rate=0.001):

        # Add intercept term by augmenting X with a column of ones.
        n_samples = X.shape[0]
        X_augmented = np.hstack((np.ones((n_samples, 1)), X))
        n_features = X_augmented.shape[1]  # This should be 3.

        # Initialize theta to zeros.
        self.theta = np.zeros(n_features)
        self.theta_history = [self.theta.copy()]
        self.train_errors = []
        self.loss_history = []



        # Define convergence criteria.
        max_epochs = 1000
        epoch = 0
        converged = False
        batch_size=1

        while epoch &lt; max_epochs and not converged:
            # Shuffle the data at the beginning of each epoch.
            indices = np.arange(n_samples)
            np.random.shuffle(indices)
            X_augmented_shuffled = X_augmented[indices]
            y_shuffled = y[indices]


            # Process the data in mini-batches.
            for start in range(0, n_samples, batch_size):
                end = min(start + batch_size, n_samples)
                X_batch = X_augmented_shuffled[start:end]
                y_batch = y_shuffled[start:end]
                r = X_batch.shape[0]  # Current mini-batch size.

                # Compute predictions: y_pred = X_batch dot theta.
                y_pred = X_batch.dot(self.theta)
                error = y_pred - y_batch
                # Gradient of the loss (1/(2r) * sum(error^2)) is (1/r) * X_batchᵀ*(error).
                grad = (1.0 / r) * X_batch.T.dot(error)
                # Update the parameters.
                self.theta = self.theta - learning_rate * grad

                # Record the parameters after each mini-batch update.
                self.theta_history.append(self.theta.copy())

                loss = np.mean((X_augmented.dot(self.theta) - y)**2) / 2.0
                self.loss_history.append(loss)

                # Check the stopping criterion after each update.
                if self.has_converged():
                    print(f"Stopping criteria met at epoch {epoch}")
                    converged = True
                    break
            if not converged:
              total_error = np.mean((X_augmented.dot(self.theta) - y)**2) / 2.0
              self.train_errors.append(total_error)
            epoch += 1

        return np.array(self.theta_history)


    def predict(self, X):
        X = np.hstack([np.ones((X.shape[0], 1)), X])
        return X.dot(self.theta)

    def closed_form_solution(self,X, y):
      n_samples = X.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X))
      theta = np.linalg.inv(X_augmented.T.dot(X_augmented)).dot(X_augmented.T).dot(y)
      return theta

    def plot_training_error(self):
        """
        Plot the training error (MSE/2) versus epochs.
        """
        plt.figure()
        plt.plot(range(len(self.train_errors)), self.train_errors, marker='o')
        plt.xlabel('Epoch')
        plt.ylabel('Training Error (MSE/2)')
        plt.title('Training Error vs. Epoch')
        plt.show()

    def plot_parameter_trajectory(self):
        """
        Plot the trajectory of the parameters (theta0, theta1, theta2) in 3D space.
        This helps visualize the updates to theta over iterations.
        """
        params = np.array(self.theta_history)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match232-0.html#3" TARGET="0"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.plot(params[:, 0], params[:, 1], params[:, 2], marker='o')
</FONT>        ax.set_xlabel('theta0')
        ax.set_ylabel('theta1')
        ax.set_zlabel('theta2')
        ax.set_title(f'Parameter Trajectory during SGD for batch size {batch_size}')
        plt.show()






# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BTZeBw1uTuMHi9SrGHQdvY7zVhGxp5to
"""

import numpy as np
import matplotlib.pyplot as plt


class LogisticRegressor:
  def __init__(self):
      self.theta = None
      self.mean = None
      self.std = None
      self.theta_history = []

  def _sigmoid(self, z):
      """Compute the sigmoid function."""
      return 1.0 / (1.0 + np.exp(-z))

  def fit(self, X, y, learning_rate=0.01):
      """
      Fit a logistic regression model using Newton's Method.
      Input features are normalized and an intercept term is added.

      Parameters
      ----------
      X : numpy array of shape (n_samples, n_features)
          The input data.

      y : numpy array of shape (n_samples,)
          The target labels (0 or 1).

      learning_rate : float
          The learning rate (damping factor) used in the Newton update.

      Returns
      -------
      params_history : numpy array of shape (n_iter, n_features+1)
          A record of the parameters at each iteration.
      """
      # Normalize X (column-wise)
      self.mean = np.mean(X, axis=0)
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match232-0.html#4" TARGET="0"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

      self.std = np.std(X, axis=0)

      X_normalized = (X - self.mean) / self.std

      # Add intercept term (column of ones)
      n_samples = X_normalized.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X_normalized))
</FONT>
      # Initialize parameters as zeros
      n_features_augmented = X_augmented.shape[1]
      theta = np.zeros(n_features_augmented)
      self.theta_history = [theta.copy()]

      tol = 1e-6
      max_iter = 100
      for iteration in range(max_iter):
          # Compute the prediction p = sigmoid(X_aug dot theta)
          z = X_augmented.dot(theta)
          p = self._sigmoid(z)

          # Compute gradient: grad = X_augᵀ (y - p)
          grad = X_augmented.T.dot(y - p)

          # Compute Hessian:
          R = np.diag(p * (1 - p))
          H = - X_augmented.T.dot(R).dot(X_augmented)

          # Since H is negative definite, we use -H
          try:
              H_inv = np.linalg.inv(-H)
          except np.linalg.LinAlgError:
              print("Hessian is singular. Stopping early.")
              break

          # Newton update with damping via learning_rate
          delta = learning_rate * H_inv.dot(grad)
          theta_new = theta + delta
          self.theta_history.append(theta_new.copy())

          # Check for convergence using the norm of the update step
          if np.linalg.norm(delta) &lt; tol:
              theta = theta_new
              break

          theta = theta_new

      self.theta = theta
      return np.array(self.theta_history)

  def predict(self, X):
      """
      Predict target labels for given input data.

      Parameters
      ----------
      X : numpy array of shape (n_samples, n_features)
          The input data.

      Returns
      -------
      y_pred : numpy array of shape (n_samples,)
          Predicted labels (0 or 1).
      """
      # Normalize X using stored mean and std
      X_normalized = (X - self.mean) / self.std
      n_samples = X_normalized.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X_normalized))
      z = X_augmented.dot(self.theta)
      p = self._sigmoid(z)
      return (p &gt;= 0.5).astype(int)

  def plot_decision_boundary(self, X, y):
      """
      Plot the training data along with the decision boundary.
      Training data is plotted with different markers based on the label.
      The decision boundary is computed by solving for x₂ in the equation:

          θ₀ + θ₁ * ((x₁ - μ₁)/σ₁) + θ₂ * ((x₂ - μ₂)/σ₂) = 0

      where μ and σ are the means and standard deviations computed during training.
      """
      plt.figure()
      # Assume X has two features
      pos = (y == 1)
      neg = (y == 0)
      plt.scatter(X[pos, 0], X[pos, 1], marker='x', color='r', label='Label 1')
      plt.scatter(X[neg, 0], X[neg, 1], marker='o', color='b', label='Label 0')

      # Plot decision boundary if possible θ₂ must be nonzero
      if self.theta[2] == 0:
          print("Theta[2] is zero; cannot plot decision boundary.")
      else:
          # Create a range of x₁ values (in original scale)
          x1_vals = np.linspace(np.min(X[:,0]) - 1, np.max(X[:,0]) + 1, 100)
          # In normalized space, x₁_norm = (x₁ - μ₁) / σ₁.
          x1_norm = (x1_vals - self.mean[0]) / self.std[0]
          # The decision boundary in normalized space is given by:
          # θ₀ + θ₁*x₁_norm + θ₂*x₂_norm = 0  =&gt;  x₂_norm = - (θ₀ + θ₁*x₁_norm) / θ₂.
          # To get back to original space: x₂ = x₂_norm * σ₂ + μ₂.
          x2_vals = ((- (self.theta[0] + self.theta[1] * x1_norm) / self.theta[2]) * self.std[1]) + self.mean[1]
          plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

      plt.xlabel('x1')
      plt.ylabel('x2')
      plt.legend()
      plt.title('Logistic Regression Decision Boundary')
      plt.show()





#!/usr/bin/env python
# coding: utf-8

# In[3]:


import numpy as np
import matplotlib.pyplot as plt


class LogisticRegressor:
  def __init__(self):
      self.theta = None
      self.mean = None
      self.std = None
      self.theta_history = []

  def _sigmoid(self, z):
      """Compute the sigmoid function."""
      return 1.0 / (1.0 + np.exp(-z))

  def fit(self, X, y, learning_rate=0.01):
      """
      Fit a logistic regression model using Newton's Method.
      Input features are normalized and an intercept term is added.

      Parameters
      ----------
      X : numpy array of shape (n_samples, n_features)
          The input data.

      y : numpy array of shape (n_samples,)
          The target labels (0 or 1).

      learning_rate : float
          The learning rate (damping factor) used in the Newton update.

      Returns
      -------
      params_history : numpy array of shape (n_iter, n_features+1)
          A record of the parameters at each iteration.
      """
      # Normalize X (column-wise)
      self.mean = np.mean(X, axis=0)
      self.std = np.std(X, axis=0)

      X_normalized = (X - self.mean) / self.std

      # Add intercept term (column of ones)
      n_samples = X_normalized.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X_normalized))

      # Initialize parameters as zeros
      n_features_augmented = X_augmented.shape[1]
      theta = np.zeros(n_features_augmented)
      self.theta_history = [theta.copy()]

      tol = 1e-6
      max_iter = 100
      for iteration in range(max_iter):
          # Compute the prediction p = sigmoid(X_aug dot theta)
          z = X_augmented.dot(theta)
          p = self._sigmoid(z)

          # Compute gradient: grad = X_augᵀ (y - p)
          grad = X_augmented.T.dot(y - p)

          # Compute Hessian:
          R = np.diag(p * (1 - p))
          H = - X_augmented.T.dot(R).dot(X_augmented)

          # Since H is negative definite, we use -H
          try:
              H_inv = np.linalg.inv(-H)
          except np.linalg.LinAlgError:
              print("Hessian is singular. Stopping early.")
              break

          # Newton update with damping via learning_rate
          delta = learning_rate * H_inv.dot(grad)
          theta_new = theta + delta
          self.theta_history.append(theta_new.copy())

          # Check for convergence using the norm of the update step
          if np.linalg.norm(delta) &lt; tol:
              theta = theta_new
              break

          theta = theta_new

      self.theta = theta
      return np.array(self.theta_history)

  def predict(self, X):
      """
      Predict target labels for given input data.

      Parameters
      ----------
      X : numpy array of shape (n_samples, n_features)
          The input data.

      Returns
      -------
      y_pred : numpy array of shape (n_samples,)
          Predicted labels (0 or 1).
      """
      # Normalize X using stored mean and std
      X_normalized = (X - self.mean) / self.std
      n_samples = X_normalized.shape[0]
      X_augmented = np.hstack((np.ones((n_samples, 1)), X_normalized))
      z = X_augmented.dot(self.theta)
      p = self._sigmoid(z)
      return (p &gt;= 0.5).astype(int)

  def plot_decision_boundary(self, X, y):
      """
      Plot the training data along with the decision boundary.
      Training data is plotted with different markers based on the label.
      The decision boundary is computed by solving for x₂ in the equation:

          θ₀ + θ₁ * ((x₁ - μ₁)/σ₁) + θ₂ * ((x₂ - μ₂)/σ₂) = 0

      where μ and σ are the means and standard deviations computed during training.
      """
      plt.figure()
      # Assume X has two features
      pos = (y == 1)
      neg = (y == 0)
      plt.scatter(X[pos, 0], X[pos, 1], marker='x', color='r', label='Label 1')
      plt.scatter(X[neg, 0], X[neg, 1], marker='o', color='b', label='Label 0')

      # Plot decision boundary if possible θ₂ must be nonzero
      if self.theta[2] == 0:
          print("Theta[2] is zero; cannot plot decision boundary.")
      else:
          # Create a range of x₁ values (in original scale)
          x1_vals = np.linspace(np.min(X[:,0]) - 1, np.max(X[:,0]) + 1, 100)
          # In normalized space, x₁_norm = (x₁ - μ₁) / σ₁.
          x1_norm = (x1_vals - self.mean[0]) / self.std[0]
          # The decision boundary in normalized space is given by:
          # θ₀ + θ₁*x₁_norm + θ₂*x₂_norm = 0  =&gt;  x₂_norm = - (θ₀ + θ₁*x₁_norm) / θ₂.
          # To get back to original space: x₂ = x₂_norm * σ₂ + μ₂.
          x2_vals = ((- (self.theta[0] + self.theta[1] * x1_norm) / self.theta[2]) * self.std[1]) + self.mean[1]
          plt.plot(x1_vals, x2_vals, 'g-', label='Decision Boundary')

      plt.xlabel('x1')
      plt.ylabel('x2')
      plt.legend()
      plt.title('Logistic Regression Decision Boundary')
      plt.show()


# In[7]:


X = np.loadtxt('logisticX.csv', delimiter=',')
y = np.loadtxt('logisticY.csv', delimiter=',')



# # Train model
model = LogisticRegressor()

thetas=model.fit(X, y)
model.predict(X)

print("Learned theta :",model.theta)
model.plot_decision_boundary(X,y)





# -*- coding: utf-8 -*-
"""asiignment1_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYQMT_50OvLRLlqEPivYHBaNcVTegjvC
"""

import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.X_mean = None
        self.X_std = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = False

    def convert_labels(self, y):
        """Convert text labels ('Alaska', 'Canada') to binary (0, 1)."""
        return np.where(y == 'Canada', 1, 0)

    def fit(self, X, y, assume_same_covariance=False):
        # Normalize the data
        y = self.convert_labels(y)
        self.X_mean = np.mean(X, axis=0)
        self.X_std = np.std(X, axis=0)
        X_normalized = (X - self.X_mean) / self.X_std

        # Separate data by class
        mask = (y == 0)
        X0 = X_normalized[mask]
        X1 = X_normalized[~mask]

        # Compute means
        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)
        self.phi = len(X1)/len(X_normalized) # Prior probability of class 1

        # Compute covariance matrices
        self.assume_same_covariance = assume_same_covariance
        if assume_same_covariance:
            m = X.shape[0]
            self.sigma = ( (X0 - self.mu_0).T @ (X0 - self.mu_0) + (X1 - self.mu_1).T @ (X1 - self.mu_1) ) / m
            return (self.mu_0, self.mu_1, self.sigma)
        else:
            m0 = X0.shape[0]
            m1 = X1.shape[0]
            self.sigma_0 = (X0 - self.mu_0).T @ (X0 - self.mu_0) / m0
            self.sigma_1 = (X1 - self.mu_1).T @ (X1 - self.mu_1) / m1
            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)

    def predict(self, X):
        X_normalized = (X - self.X_mean) / self.X_std
        y_pred = np.zeros(X_normalized.shape[0], dtype=object)

        for i in range(X_normalized.shape[0]):
            x = X_normalized[i]
            if self.assume_same_covariance:
                inv_sigma = np.linalg.inv(self.sigma)
                delta_0 = (x - self.mu_0) @ inv_sigma @ (x - self.mu_0).T
                delta_1 = (x - self.mu_1) @ inv_sigma @ (x - self.mu_1).T
                log_p0 = np.log(1 - self.phi) - 0.5 * delta_0
<A NAME="5"></A><FONT color = #FF0000><A HREF="match232-0.html#5" TARGET="0"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                log_p1 = np.log(self.phi) - 0.5 * delta_1
            else:
                inv_sigma0 = np.linalg.inv(self.sigma_0)
                inv_sigma1 = np.linalg.inv(self.sigma_1)
                delta_0 = (x - self.mu_0) @ inv_sigma0 @ (x - self.mu_0).T
</FONT>                delta_1 = (x - self.mu_1) @ inv_sigma1 @ (x - self.mu_1).T
                log_p0 = np.log(1 - self.phi) - 0.5 * (delta_0 + np.log(np.linalg.det(self.sigma_0)))
                log_p1 = np.log(self.phi) - 0.5 * (delta_1 + np.log(np.linalg.det(self.sigma_1)))

            y_pred[i] = 'Canada' if log_p1 &gt; log_p0 else 'Alaska'
        return y_pred

    def plot_data_and_boundary(self, X, y, boundary_type='linear'):
        X_normalized = (X - self.X_mean) / self.X_std
        plt.figure()
        plt.scatter(X_normalized[y == 'Alaska'][:, 0], X_normalized[y == 'Alaska'][:, 1],
                    c='blue', marker='o', edgecolor='k', label='Alaska')
        # Plot Canada (original labels)
        plt.scatter(X_normalized[y == 'Canada'][:, 0], X_normalized[y == 'Canada'][:, 1],
                    c='red', marker='x', label='Canada')
        # Create grid
        x_min, x_max = X_normalized[:,0].min()-1, X_normalized[:,0].max()+1
        y_min, y_max = X_normalized[:,1].min()-1, X_normalized[:,1].max()+1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))
        grid = np.c_[xx.ravel(), yy.ravel()]

        if boundary_type == 'linear' and self.assume_same_covariance:
            inv_sigma = np.linalg.inv(self.sigma)
            w = inv_sigma @ (self.mu_1 - self.mu_0)
            b = np.log(self.phi / (1 - self.phi)) - 0.5 * (self.mu_1 @ inv_sigma @ self.mu_1 - self.mu_0 @ inv_sigma @ self.mu_0)
            zz = (grid @ w + b).reshape(xx.shape)
            plt.contour(xx, yy, zz, levels=[0], colors='k', linestyles='solid', label='Linear Boundary')
        elif boundary_type == 'quadratic' and not self.assume_same_covariance:
            # Compute discriminant difference for quadratic boundary
            z = np.zeros(grid.shape[0])
            for i in range(grid.shape[0]):
                x = grid[i]
                delta0 = (x - self.mu_0) @ np.linalg.inv(self.sigma_0) @ (x - self.mu_0).T
                delta1 = (x - self.mu_1) @ np.linalg.inv(self.sigma_1) @ (x - self.mu_1).T
                log_term = np.log((1 - self.phi)**2 / self.phi**2 * np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))
                z[i] = delta0 - delta1 + log_term
            z = z.reshape(xx.shape)
            plt.contour(xx, yy, z, levels=[0], colors='r', linestyles='dashed', label='Quadratic Boundary')

        plt.xlabel('Normalized x1')
        plt.ylabel('Normalized x2')
        plt.legend()
        plt.title('Decision Boundaries')
        plt.show()





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.X_mean = None
        self.X_std = None
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = False

    def convert_labels(self, y):
        """Convert text labels ('Alaska', 'Canada') to binary (0, 1)."""
        return np.where(y == 'Canada', 1, 0)

    def fit(self, X, y, assume_same_covariance=False):
        # Normalize the data
        y = self.convert_labels(y)
        self.X_mean = np.mean(X, axis=0)
        self.X_std = np.std(X, axis=0)
        X_normalized = (X - self.X_mean) / self.X_std

        # Separate data by class
        mask = (y == 0)
        X0 = X_normalized[mask]
        X1 = X_normalized[~mask]

        # Compute means
        self.mu_0 = np.mean(X0, axis=0)
        self.mu_1 = np.mean(X1, axis=0)
        self.phi = len(X1)/len(X_normalized) # Prior probability of class 1

        # Compute covariance matrices
        self.assume_same_covariance = assume_same_covariance
        if assume_same_covariance:
            m = X.shape[0]
            self.sigma = ( (X0 - self.mu_0).T @ (X0 - self.mu_0) + (X1 - self.mu_1).T @ (X1 - self.mu_1) ) / m
            return (self.mu_0, self.mu_1, self.sigma)
        else:
            m0 = X0.shape[0]
            m1 = X1.shape[0]
            self.sigma_0 = (X0 - self.mu_0).T @ (X0 - self.mu_0) / m0
            self.sigma_1 = (X1 - self.mu_1).T @ (X1 - self.mu_1) / m1
            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)

    def predict(self, X):
        X_normalized = (X - self.X_mean) / self.X_std
        y_pred = np.zeros(X_normalized.shape[0], dtype=object)

        for i in range(X_normalized.shape[0]):
            x = X_normalized[i]
            if self.assume_same_covariance:
                inv_sigma = np.linalg.inv(self.sigma)
                delta_0 = (x - self.mu_0) @ inv_sigma @ (x - self.mu_0).T
                delta_1 = (x - self.mu_1) @ inv_sigma @ (x - self.mu_1).T
                log_p0 = np.log(1 - self.phi) - 0.5 * delta_0
                log_p1 = np.log(self.phi) - 0.5 * delta_1
            else:
                inv_sigma0 = np.linalg.inv(self.sigma_0)
                inv_sigma1 = np.linalg.inv(self.sigma_1)
                delta_0 = (x - self.mu_0) @ inv_sigma0 @ (x - self.mu_0).T
                delta_1 = (x - self.mu_1) @ inv_sigma1 @ (x - self.mu_1).T
                log_p0 = np.log(1 - self.phi) - 0.5 * (delta_0 + np.log(np.linalg.det(self.sigma_0)))
                log_p1 = np.log(self.phi) - 0.5 * (delta_1 + np.log(np.linalg.det(self.sigma_1)))

            y_pred[i] = 'Canada' if log_p1 &gt; log_p0 else 'Alaska'
        return y_pred

    def plot_data_and_boundary(self, X, y, boundary_type='linear'):
        X_normalized = (X - self.X_mean) / self.X_std
        plt.figure()
        plt.scatter(X_normalized[y == 'Alaska'][:, 0], X_normalized[y == 'Alaska'][:, 1],
                    c='blue', marker='o', edgecolor='k', label='Alaska')
        # Plot Canada (original labels)
        plt.scatter(X_normalized[y == 'Canada'][:, 0], X_normalized[y == 'Canada'][:, 1],
                    c='red', marker='x', label='Canada')
        # Create grid
        x_min, x_max = X_normalized[:,0].min()-1, X_normalized[:,0].max()+1
        y_min, y_max = X_normalized[:,1].min()-1, X_normalized[:,1].max()+1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))
        grid = np.c_[xx.ravel(), yy.ravel()]

        if boundary_type == 'linear' and self.assume_same_covariance:
            inv_sigma = np.linalg.inv(self.sigma)
            w = inv_sigma @ (self.mu_1 - self.mu_0)
            b = np.log(self.phi / (1 - self.phi)) - 0.5 * (self.mu_1 @ inv_sigma @ self.mu_1 - self.mu_0 @ inv_sigma @ self.mu_0)
            zz = (grid @ w + b).reshape(xx.shape)
            plt.contour(xx, yy, zz, levels=[0], colors='k', linestyles='solid', label='Linear Boundary')
        elif boundary_type == 'quadratic' and not self.assume_same_covariance:
            # Compute discriminant difference for quadratic boundary
            z = np.zeros(grid.shape[0])
            for i in range(grid.shape[0]):
                x = grid[i]
                delta0 = (x - self.mu_0) @ np.linalg.inv(self.sigma_0) @ (x - self.mu_0).T
                delta1 = (x - self.mu_1) @ np.linalg.inv(self.sigma_1) @ (x - self.mu_1).T
                log_term = np.log((1 - self.phi)**2 / self.phi**2 * np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))
                z[i] = delta0 - delta1 + log_term
            z = z.reshape(xx.shape)
            plt.contour(xx, yy, z, levels=[0], colors='r', linestyles='dashed', label='Quadratic Boundary')

        plt.xlabel('Normalized x1')
        plt.ylabel('Normalized x2')
        plt.legend()
        plt.title('Decision Boundaries')
        plt.show()


# In[5]:


X = np.loadtxt('q4x.dat')
y = np.loadtxt('q4y.dat', dtype=str)

# Initialize and fit model
gda_same_cov = GaussianDiscriminantAnalysis()
gda_different_cov = GaussianDiscriminantAnalysis()

same_cov_params = gda_same_cov.fit(X, y, assume_same_covariance=True)

print("mu zero: ",gda_same_cov.mu_0)
print("mu one: ",gda_same_cov.mu_1)
print("sigma :",gda_same_cov.sigma)

gda_same_cov.plot_data_and_boundary(X, y, boundary_type='linear')

# Predict and get text labels
#predictions = gda.predict(X)

different_cov_params = gda_different_cov.fit(X, y, assume_same_covariance=False)

print("mu zero: ",gda_different_cov.mu_0)
print("mu one: ",gda_different_cov.mu_1)
print("sigma zero :",gda_different_cov.sigma_0)
print("sigma one :",gda_different_cov.sigma_1)

gda_different_cov.plot_data_and_boundary(X, y, boundary_type='quadratic')



</PRE>
</PRE>
</BODY>
</HTML>
