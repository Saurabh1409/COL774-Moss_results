<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_0A0UU.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_0A0UU.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.05):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        X=X.reshape(-1 ,1)

        # Normalize X (already normalized as per instructions)
        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term


        max_iters = 1000
        tol = 1e-6
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features)  # Initialize theta to zeros
        theta_history = []
        prev_loss = float('inf')
        
        for _ in range(max_iters):
            y_pred = X @ self.theta
            gradient = -X.T @ (y - y_pred) / n_samples
            self.theta -= learning_rate * gradient  # Update rule
            
            loss = np.mean((y - y_pred) ** 2) / 2  # Compute loss
            theta_history.append(self.theta.copy())
            
            if abs(prev_loss - loss) &lt; tol:
                break  # Stop if convergence is reached
            prev_loss = loss
        
        return np.array(theta_history)
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X=X.reshape(-1 ,1)

        # Normalize X (already normalized as per instructions)
        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term
        return X @ self.theta  # Linear model prediction


# if __name__ == "__main__":
#     # Load dataset
#     X = np.loadtxt('../data/Q1/linearX.csv', delimiter=',')
#     y = np.loadtxt('../data/Q1/linearY.csv', delimiter=',')

#     # Train model
#     regressor = LinearRegressor()
#     theta_history = regressor.fit(X, y)
#     print(theta_history)
#     #Predict
#     y_pred = regressor.predict(X)
#     print(y_pred)




import numpy as np
import matplotlib.pyplot as plt
import time



class LinearRegressor:
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y, learning_rate=0.05):
        max_iters = 1000
        tol = 1e-6
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features)  # Initialize theta to zeros
        theta_history = []
        prev_loss = float('inf')
        
        for _ in range(max_iters):
            y_pred = X @ self.theta
            gradient = -X.T @ (y - y_pred) / n_samples
            self.theta -= learning_rate * gradient  # Update rule
            
            loss = np.mean((y - y_pred) ** 2) / 2  # Compute loss
            theta_history.append(self.theta.copy())
            
            if abs(prev_loss - loss) &lt; tol:
                break  # Stop if convergence is reached
            prev_loss = loss
        
        return np.array(theta_history)
    
    def predict(self, X):
        return X @ self.theta  # Linear model prediction

if __name__ == '__main__':

    # Load dataset
    X = np.loadtxt('../data/Q1/linearX.csv', delimiter=',').reshape(-1, 1)
    y = np.loadtxt('../data/Q1/linearY.csv', delimiter=',')

    # Normalize X (already normalized as per instructions)
    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term

    # Train model
    regressor = LinearRegressor()
    theta_history = regressor.fit(X, y)
    print(f'Learned Parameters: {regressor.predict(X)}')
    print(f'Learned Parameters: {regressor.theta}')

    def plot_training_data(X, y, regressor):
<A NAME="0"></A><FONT color = #FF0000><A HREF="match60-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        plt.scatter(X[:, 1], y, label='Training Data', marker='o')
        plt.plot(X[:, 1], regressor.predict(X), label='Learned Hypothesis', color='red')
        plt.xlabel('Feature')
        plt.ylabel('Target')
        plt.legend()
        plt.show()
</FONT>
    # Call the function to plot training data
    plot_training_data(X, y, regressor)

    def plot_error_surface(X, y, theta_history):
        theta0_vals = np.linspace(-10, 10, 100)
        theta1_vals = np.linspace(-1, 4, 100)
        J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))

        for i, theta0 in enumerate(theta0_vals):
            for j, theta1 in enumerate(theta1_vals):
                t = np.array([theta0, theta1])
                J_vals[i, j] = np.mean((X @ t - y) ** 2) / 2

        theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(theta0_vals, theta1_vals, J_vals.T, cmap='viridis', alpha=0.6)
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_zlabel('Error')

        # for theta in theta_history:
        #     error = np.mean((X @ theta - y) ** 2) / 2
        #     ax.scatter(theta[0], theta[1], error, color='r')
        #     plt.pause(0.2)

        plt.show()

    # Call the function to plot error surface
    plot_error_surface(X, y, theta_history)


    def plot_contour_error_function(X, y, parameters):
        X_with_bias = X  # X already has the bias column added

        # Compute range of θ_0 and θ_1
        theta_min = np.min(parameters, axis=0)
        theta_max = np.max(parameters, axis=0)
        theta_0_range = (theta_min[0] - 2, theta_max[0] + 2)
        theta_1_range = (theta_min[1] - 2, theta_max[1] + 2)

        # Generate grid of θ_0 and θ_1 values
        theta_0_values = np.linspace(*theta_0_range, 100)
        theta_1_values = np.linspace(*theta_1_range, 100)
        theta_0, theta_1 = np.meshgrid(theta_0_values, theta_1_values)

        # Compute cost values for grid
        cost_values = np.zeros_like(theta_0)
        m = len(y)
        for i in range(theta_0.shape[0]):
            for j in range(theta_0.shape[1]):
                theta = np.array([theta_0[i, j], theta_1[i, j]])
                predictions = X_with_bias.dot(theta)
                errors = predictions - y
                cost_values[i, j] = (1 / (2 * m)) * np.sum(errors ** 2)

        # Create contour plot
        plt.figure(figsize=(10, 8))
        plt.contour(theta_0, theta_1, cost_values, levels=50, cmap='viridis')
        plt.xlabel('θ_0 (Bias)')
        plt.ylabel('θ_1 (Weight)')
        plt.title('Contours of Error Function J(θ)')

        # Plot parameter progression during gradient descent
        for i, theta in enumerate(parameters):
            theta_0_val, theta_1_val = theta
            prediction = X_with_bias.dot(theta)
            error = prediction - y
            cost = (1 / (2 * m)) * np.sum(error ** 2)
            
            plt.scatter(theta_0_val, theta_1_val, color='red', s=50, label='Iteration {}'.format(i) if i == 0 else "")
            
            # Add a delay to visualize each iteration
            plt.pause(0.2)
        
        plt.legend()
        plt.show()

    # Call the function to plot contour error function
    plot_contour_error_function(X, y, theta_history)




# Imports - you can add any other permitted libraries
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import time

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, size=(N, 2))  # Generate N samples with 2 features
    X = np.hstack((np.ones((N, 1)), X))  # Add intercept term (bias)
    noise = np.random.normal(0, noise_sigma, N)  # Gaussian noise
    y = X @ theta + noise  # Compute target values
    return X, y
    

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.sub_results=[]
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        batch_sizes = [1, 80, 8000, 800000]
        max_epoch_dict = {1: 20, 80: 3000, 8000: 3000, 800000: 3000}
        n_samples, n_features = X.shape
        results = {}
        self.sub_results = []
        times_to_converge = []
        iterations_to_converge = []
        dma_window_dict={1:5,80:10,8000:10,800000:5}

        for batch_size in batch_sizes:
            print(f"Training with batch size {batch_size}")
            theta = np.zeros(n_features)  # Reinitialize for each batch size
            dma_threshold = 1e-6  # Stopping criteria
            dma_window = dma_window_dict[batch_size]  # Window size for moving average
            losses = []
            history_list = []
            start_time = time.time()
            
            for epoch in range(max_epoch_dict[batch_size]):
                history_list.append(theta.copy())
                print(f"Epoch {epoch}")
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match60-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                indices = np.random.permutation(n_samples)  # Shuffle data
                X_shuffled, y_shuffled = X[indices], y[indices]
                
                for i in range(0, n_samples, batch_size):
                    X_batch = X_shuffled[i:i+batch_size]
</FONT>                    y_batch = y_shuffled[i:i+batch_size]
                    
                    y_pred = X_batch @ theta
                    gradient = -X_batch.T @ (y_batch - y_pred) / batch_size
                    theta -= learning_rate * gradient  # Update rule
                
                loss = np.mean((X @ theta - y) ** 2)  # Compute loss
                losses.append(loss)
                
                if len(losses) &gt; dma_window:
                    dma = np.mean(np.abs(np.diff(losses[-dma_window:])))
                    if dma &lt; dma_threshold:
                        break  # Stop if convergence is reached
            
            end_time = time.time()
            time_taken = end_time - start_time
            times_to_converge.append(time_taken)
            iterations_to_converge.append(epoch + 1)
            
            results[batch_size] = (theta, losses)
            self.sub_results.append(np.array(history_list))

        print(f"Times to converge: {times_to_converge}")
        print(f"Iterations to converge: {iterations_to_converge}")
        
        return self.sub_results


        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        sub_predictions=[]
        for k in self.sub_results:
            theta = k[-1]
            sub_predictions.append(X @ theta)
        return sub_predictions


if __name__ == "__main__":
    # Sampling 1 million data points
    theta_true = np.array([3, 1, 2])
    input_mean = np.array([3, -1])
    input_sigma = np.array([4, 4])
    noise_sigma = np.sqrt(2)
    N = 1000000
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

    # Splitting data into train (80%) and test (20%)
    split_idx = int(0.8 * N)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Fitting the model
    regressor = StochasticLinearRegressor()
    history = regressor.fit(X_train, y_train)
    print(history)
    # Predicting the target values
    y_pred = regressor.predict(X_test)
    print(y_pred)
    
    # Compute closed-form solution
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match60-1.html#8" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta_closed_form = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train

    print(f"Closed-form solution: {theta_closed_form}")

    # Compute MSE on training and test sets
    y_train_pred = X_train @ theta_closed_form
    y_test_pred = X_test @ theta_closed_form
</FONT>    mse_train = np.mean((y_train - y_train_pred) ** 2)
    mse_test = np.mean((y_test - y_test_pred) ** 2)

    print(f"Training MSE: {mse_train}")
    print(f"Test MSE: {mse_test}")

    # Print the final theta for each batch size
    print(f"Theta for batch size 1: {history[0][-1]}")
    print(f"Theta for batch size 80: {history[1][-1]}")
    print(f"Theta for batch size 8000: {history[2][-1]}")
    print(f"Theta for batch size 800000: {history[3][-1]}")

    #MSE train errors fro all batch sizes
    mse_train_1 = np.mean((y_train - X_train @ history[0][-1]) ** 2)
    mse_train_80 = np.mean((y_train - X_train @ history[1][-1]) ** 2)
    mse_train_8000 = np.mean((y_train - X_train @ history[2][-1]) ** 2)
    mse_train_800000 = np.mean((y_train - X_train @ history[3][-1]) ** 2)

    print(f"MSE train error for batch size 1: {mse_train_1}")
    print(f"MSE train error for batch size 80: {mse_train_80}")
    print(f"MSE train error for batch size 8000: {mse_train_8000}")
    print(f"MSE train error for batch size 800000: {mse_train_800000}")

    #MSE test errors fro all batch sizes
    mse_test_1 = np.mean((y_test - X_test @ history[0][-1]) ** 2)
    mse_test_80 = np.mean((y_test - X_test @ history[1][-1]) ** 2)
    mse_test_8000 = np.mean((y_test - X_test @ history[2][-1]) ** 2)
    mse_test_800000 = np.mean((y_test - X_test @ history[3][-1]) ** 2)

    print(f"MSE test error for batch size 1: {mse_test_1}")
    print(f"MSE test error for batch size 80: {mse_test_80}")
    print(f"MSE test error for batch size 8000: {mse_test_8000}")
    print(f"MSE test error for batch size 800000: {mse_test_800000}")

    

    import matplotlib.pyplot as plt

    def plot(history):
        fig = plt.figure(figsize=(18, 12))
        batch_sizes = [1, 80, 8000, 800000]
        
        for idx, batch_size in enumerate(batch_sizes):
            ax = fig.add_subplot(2, 2, idx + 1, projection='3d')
            theta_history = history[idx]
            
            ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o')
            ax.set_title(f'Batch Size: {batch_size}')
            ax.set_xlabel('Theta 0')
            ax.set_ylabel('Theta 1')
            ax.set_zlabel('Theta 2')
        
        plt.tight_layout()
        plt.show()

    # Call the plot function
    plot(history)








# Imports - you can add any other permitted libraries
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    X = np.random.normal(input_mean, input_sigma, size=(N, 2))  # Generate N samples with 2 features
    X = np.hstack((np.ones((N, 1)), X))  # Add intercept term (bias)
    noise = np.random.normal(0, noise_sigma, N)  # Gaussian noise
    y = X @ theta + noise  # Compute target values
    return X, y
    

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.sub_results=[]
    
    def fit(self, X, y, learning_rate=0.001):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        batch_sizes=[1, 80, 8000, 800000]
        max_epoch_dict = {1: 20, 80: 3000, 8000: 3000, 800000: 3000}
        dma_window_dict={1:5,80:10,8000:10,800000:5}
        max_epochs=1000
        n_samples, n_features = X.shape
        results = {}
        self.sub_results=[]

        for batch_size in batch_sizes:
            print(f"Training with batch size {batch_size}")
            theta = np.zeros(n_features)  # Reinitialize for each batch size
            dma_threshold = 1e-6  # Stopping criteria
            dma_window = 10  # Window size for moving average
            losses = []
            history_list=[]
            
            for epoch in range(max_epochs):
                history_list.append(theta.copy())
                print(f"Epoch {epoch}")
<A NAME="5"></A><FONT color = #FF0000><A HREF="match60-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                indices = np.random.permutation(n_samples)  # Shuffle data
                X_shuffled, y_shuffled = X[indices], y[indices]
                
                for i in range(0, n_samples, batch_size):
                    X_batch = X_shuffled[i:i+batch_size]
</FONT>                    y_batch = y_shuffled[i:i+batch_size]
                    
                    y_pred = X_batch @ theta
                    gradient = -X_batch.T @ (y_batch - y_pred) / batch_size
                    theta -= learning_rate * gradient  # Update rule
                
                loss = np.mean((X @ theta - y) ** 2)  # Compute loss
                losses.append(loss)
                
                if len(losses) &gt; dma_window:
                    dma = np.mean(np.abs(np.diff(losses[-dma_window:])))
                    if dma &lt; dma_threshold:
                        break  # Stop if convergence is reached
            
            results[batch_size] = (theta, losses)
            self.sub_results.append(np.array(history_list))

        return self.sub_results


        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        sub_predictions=[]
        for k in self.sub_results:
            theta = k[-1]
            sub_predictions.append(X @ theta)
        return sub_predictions


# if __name__ == "__main__":
#     # Sampling 1 million data points
#     theta_true = np.array([3, 1, 2])
#     input_mean = np.array([3, -1])
#     input_sigma = np.array([4, 4])
#     noise_sigma = np.sqrt(2)
#     N = 1000000
#     X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

#     # Splitting data into train (80%) and test (20%)
#     split_idx = int(0.8 * N)
#     X_train, X_test = X[:split_idx], X[split_idx:]
#     y_train, y_test = y[:split_idx], y[split_idx:]

#     # Fitting the model
#     regressor = StochasticLinearRegressor()
#     history = regressor.fit(X_train, y_train)
#     print(history)
#     # Predicting the target values
#     y_pred = regressor.predict(X_test)
#     print(y_pred)
    
#     # Compute closed-form solution
#     theta_closed_form = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train

#     print(f"Closed-form solution: {theta_closed_form}")

#     # Compute MSE on training and test sets
#     y_train_pred = X_train @ theta_closed_form
#     y_test_pred = X_test @ theta_closed_form
#     mse_train = np.mean((y_train - y_train_pred) ** 2)
#     mse_test = np.mean((y_test - y_test_pred) ** 2)

#     print(f"Training MSE: {mse_train}")
#     print(f"Test MSE: {mse_test}")


#     import matplotlib.pyplot as plt

#     def plot(history):
#         fig = plt.figure(figsize=(18, 12))
#         batch_sizes = [1, 80, 8000, 800000]
        
#         for idx, batch_size in enumerate(batch_sizes):
#             ax = fig.add_subplot(2, 2, idx + 1, projection='3d')
#             theta_history = history[idx]
            
#             ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2], marker='o')
#             ax.set_title(f'Batch Size: {batch_size}')
#             ax.set_xlabel('Theta 0')
#             ax.set_ylabel('Theta 1')
#             ax.set_zlabel('Theta 2')
        
#         plt.tight_layout()
#         plt.show()

#     # Call the plot function
#     plot(history)








# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
<A NAME="2"></A><FONT color = #0000FF><A HREF="match60-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Normalize X (feature scaling)
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
</FONT>        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term

        tol=1e-6
        max_iters = 1000
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features)  # Initialize theta to zeros
        history_list = []
        
        for _ in range(max_iters):
            history_list.append(self.theta.copy())
            h = self.sigmoid(X @ self.theta)
            gradient = X.T @ (h - y)
            H = (X.T @ np.diag(h * (1 - h)) @ X)  # Hessian matrix
            delta_theta = np.linalg.inv(H) @ gradient  # Newton's update
            self.theta -= delta_theta
            
            
            # Check for convergence
            if np.linalg.norm(delta_theta, ord=1) &lt; tol:
                history_list.append(self.theta.copy())
                break
        
        return np.array(history_list)
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)
    

# if __name__ == "__main__":
# #     # Load dataset
#     X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
#     y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')

#     # Fit the model
#     model = LogisticRegressor()
#     iktheta=model.fit(X, y)
#     print(iktheta)
#     # Predict
#     y_pred = model.predict(X)
#     print(y_pred)





import numpy as np
import matplotlib.pyplot as plt


class LogisticRegressor:
    def __init__(self):
        self.theta = None
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        tol=1e-6
        max_iters = 1000
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features)  # Initialize theta to zeros
        
        for _ in range(max_iters):
            print(self.theta)
            h = self.sigmoid(X @ self.theta)
            gradient = X.T @ (h - y)
            H = (X.T @ np.diag(h * (1 - h)) @ X)  # Hessian matrix
            delta_theta = np.linalg.inv(H) @ gradient  # Newton's update
            self.theta -= delta_theta
            
            # Check for convergence
            if np.linalg.norm(delta_theta, ord=1) &lt; tol:
                break
        
        return self.theta
    
    def predict(self, X):
        return (self.sigmoid(X @ self.theta) &gt;= 0.5).astype(int)
    
if __name__ == '__main__':

    # Load dataset
<A NAME="1"></A><FONT color = #00FF00><A HREF="match60-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')
    y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')

    # Normalize X (feature scaling)
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term
</FONT>
    # Train logistic regression model
    regressor = LogisticRegressor()
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match60-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta_learned = regressor.fit(X, y)
    print(f"Learned theta: {theta_learned}")

    def plot_decision_boundary(X, y, theta):
        # Plot training data
        plt.scatter(X[y == 0, 1], X[y == 0, 2], label='Class 0', marker='o')
        plt.scatter(X[y == 1, 1], X[y == 1, 2], label='Class 1', marker='x')
</FONT>
        # Plot decision boundary
        x_vals = np.linspace(min(X[:, 1]), max(X[:, 1]), 100)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match60-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y_vals = -(theta[0] + theta[1] * x_vals) / theta[2]
        plt.plot(x_vals, y_vals, label='Decision Boundary', color='red')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
</FONT>        plt.legend()
        plt.show()

    # Call the function to plot
    plot_decision_boundary(X, y, theta_learned)



# Imports - you can add any other permitted libraries
import numpy as np

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.


class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.sigma = None  # Shared covariance case
        self.phi = None
    
    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        Parameters: 
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma 
<A NAME="7"></A><FONT color = #0000FF><A HREF="match60-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        y = np.where(y == 'Canada', 1, 0)  # Convert labels to binary (0: Alaska, 1: Canada)

        # Normalize X (feature scaling)
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
</FONT>

        X_0, X_1 = X[y == 0], X[y == 1]
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)
        self.phi = np.mean(y)  # P(y=1)
        
        self.sigma_0 = np.cov(X_0, rowvar=False)
        self.sigma_1 = np.cov(X_1, rowvar=False)
        
        if assume_same_covariance:
            self.sigma = ((X_0.shape[0] * self.sigma_0) + (X_1.shape[0] * self.sigma_1)) / X.shape[0]
            return (self.mu_0, self.mu_1, self.sigma)
        else:
            return (self.mu_0, self.mu_1, self.sigma_0, self.sigma_1)
        
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        use_separate_covariances = True

        # Normalize X (feature scaling)
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        if use_separate_covariances:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma_0 * (X - self.mu_0), axis=1)
            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma_1 * (X - self.mu_1), axis=1)
        else:
            inv_sigma = np.linalg.inv(self.sigma)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma * (X - self.mu_0), axis=1)
            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma * (X - self.mu_1), axis=1)
        return (term_1 &gt; term_0).astype(int)
    

# if __name__ == "__main__":
#     X = np.loadtxt('../data/Q4/q4x.dat')
#     y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)

#     gda = GaussianDiscriminantAnalysis()
#     th=gda.fit(X, y)
#     y_pred = gda.predict(X)
#     print(th)
#     print(y_pred)




import numpy as np
import matplotlib.pyplot as plt

# Load dataset
X = np.loadtxt('../data/Q4/q4x.dat')
y = np.loadtxt('../data/Q4/q4y.dat', dtype=str)
y = np.where(y == 'Canada', 1, 0)  # Convert labels to binary (0: Alaska, 1: Canada)

# Normalize X (feature scaling)
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.sigma = None  # Shared covariance case
        self.phi = None
    
    def fit(self, X, y, assume_same_covariance=False):
        X_0, X_1 = X[y == 0], X[y == 1]
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)
        self.phi = np.mean(y)  # P(y=1)
        
        self.sigma_0 = np.cov(X_0, rowvar=False)
        self.sigma_1 = np.cov(X_1, rowvar=False)
        
        if assume_same_covariance:
            self.sigma = ((X_0.shape[0] * self.sigma_0) + (X_1.shape[0] * self.sigma_1)) / X.shape[0]
            return self.mu_0, self.mu_1, self.sigma
        else:
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1
    
    def predict(self, X, use_separate_covariances=False):
        if use_separate_covariances:
            inv_sigma_0 = np.linalg.inv(self.sigma_0)
            inv_sigma_1 = np.linalg.inv(self.sigma_1)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma_0 * (X - self.mu_0), axis=1)
            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma_1 * (X - self.mu_1), axis=1)
        else:
            inv_sigma = np.linalg.inv(self.sigma)
            term_0 = -0.5 * np.sum((X - self.mu_0) @ inv_sigma * (X - self.mu_0), axis=1)
            term_1 = -0.5 * np.sum((X - self.mu_1) @ inv_sigma * (X - self.mu_1), axis=1)
        return (term_1 &gt; term_0).astype(int)

# Train GDA model
regressor = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = regressor.fit(X, y, assume_same_covariance=True)
mu_0_sep, mu_1_sep, sigma_0, sigma_1 = regressor.fit(X, y, assume_same_covariance=False)

# Plot training data
plt.scatter(X[y == 0, 0], X[y == 0, 1], label='Alaska', marker='o')
plt.scatter(X[y == 1, 0], X[y == 1, 1], label='Canada', marker='x')

# Compute linear decision boundary
x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)
w = np.linalg.inv(sigma) @ (mu_1 - mu_0)
y_vals = -(w[0] * x_vals + (mu_0 @ np.linalg.inv(sigma) @ mu_0 - mu_1 @ np.linalg.inv(sigma) @ mu_1) / 2) / w[1]
# plt.plot(x_vals, y_vals, label='Linear Decision Boundary', color='red')

# Compute quadratic decision boundary
quad_x, quad_y = [], []
for x1 in np.linspace(min(X[:, 0]), max(X[:, 0]), 100):
    for x2 in np.linspace(min(X[:, 1]), max(X[:, 1]), 100):
        x_vec = np.array([x1, x2])
        prob = regressor.predict(x_vec.reshape(1, -1), use_separate_covariances=True)
        if prob == 1:
            quad_x.append(x1)
            quad_y.append(x2)
plt.scatter(quad_x, quad_y, color='blue', alpha=0.1, label='Quadratic Boundary')

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Compare linear vs quadratic boundaries
print("Linear GDA: mu_0:", mu_0, "mu_1:", mu_1, "Shared Sigma:", sigma)
print("Quadratic GDA: mu_0:", mu_0_sep, "mu_1:", mu_1_sep, "Sigma_0:", sigma_0, "Sigma_1:", sigma_1)


</PRE>
</PRE>
</BODY>
</HTML>
