<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_DFEHC.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_DFEHC.py<p><PRE>


# -*- coding: utf-8 -*-
"""
PART 1
"""

import numpy as np

class LinearRegressor:
    def __init__(self):
        self.theta = None  # Parameters
        self.cost_history = []  # To store the cost at each iteration

    def _compute_cost(self, X, y, theta):
        """
        Compute the cost function for linear regression.

        Parameters
        ----------
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match46-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target values.

        theta : numpy array of shape (n_features,)
            The parameters of the linear regression model.
</FONT>
        Returns
        -------
        cost : float
            The cost value.
        """
        m = len(y)
        predictions = X.dot(theta)
        errors = predictions - y
        cost = np.sum(errors ** 2) / (2 * m)
        return cost

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

<A NAME="5"></A><FONT color = #FF0000><A HREF="match46-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.

        stopping_threshold : float
            The threshold for stopping criteria based on the change in cost.
</FONT>
        max_iterations : int
            Maximum number of iterations to run the gradient descent.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        # Add intercept term to X
        X = np.c_[np.ones(X.shape[0]), X]

        # Initialize parameters
        self.theta = np.zeros(X.shape[1])

        # Count the number of iterations it took for convergence
        iters = 0
        stopping_threshold = 1e-7
        max_iterations=10000


        # Compute initial cost
        Jn = self._compute_cost(X, y, self.theta)
        self.cost_history.append((self.theta.copy(), Jn))
        converged = False

        print("Learning Rate: ", learning_rate)
        print("Initial Error: ", Jn)

        # Gradient Descent
        while not converged and iters &lt; max_iterations:
            # Compute predictions
            predictions = X.dot(self.theta)

            # Compute error
            errors = predictions - y

            # Compute gradient
            gradient = X.T.dot(errors) / len(y)

            # Update parameters
            self.theta -= learning_rate * gradient

            # Compute new cost
            Jp = Jn
            Jn = self._compute_cost(X, y, self.theta)
            self.cost_history.append((self.theta.copy(), Jn))

            # Check for convergence
            if Jp - Jn &lt; stopping_threshold:
                converged = True

            iters += 1

        print("Final Error: ", Jn)
        print("Number of iterations: ", iters)

        return self.theta

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
<A NAME="6"></A><FONT color = #00FF00><A HREF="match46-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            The predicted target values.
        """
        # Add intercept term to X
        X = np.c_[np.ones(X.shape[0]), X]

        # Predict
        return X.dot(self.theta)
</FONT>
# Load the data
X = np.loadtxt('linearX.csv')
y = np.loadtxt('linearY.csv')

# Initialize and fit the model
model = LinearRegressor()
learning_rate = 0.001
final_theta = model.fit(X, y, learning_rate=learning_rate)

# Print the final parameters
print("Final Parameters (theta):", final_theta)

# Make predictions
y_pred = model.predict(X)

print()

"""
PART 2
"""

import matplotlib.pyplot as plt

def plot_least_squares_regression(X, y, model):
    # Plot the original data points
    plt.scatter(X, y, color='red', marker='x', label='Training Data')

    # Plot the hypothesis function (regression line)
    x_values = np.linspace(min(X), max(X), 100)  # Generate 100 points between min and max X
    x_values_reshaped = x_values.reshape(-1, 1)
    y_values = model.predict(x_values_reshaped)  # Predict corresponding y values

    plt.plot(x_values, y_values, color='black', label='Learned Hypothesis')

    # Labels and title
    plt.xlabel('Acidity (Feature X)')
    plt.ylabel('Density (Target Y)')
    plt.title('Least Squares Linear Regression')
    plt.legend()

    # Show the plot
    plt.show()

plot_least_squares_regression(X, y, model)

"""
PART 3
"""
import matplotlib.animation as animation
from mpl_toolkits.mplot3d import Axes3D

def visualize_error_function(model, X, y):
    theta_history = [item[0] for item in model.cost_history]
    cost_history = [item[1] for item in model.cost_history]

    max_abs_theta0 = max(abs(th[0]) for th in theta_history)
    max_abs_theta1 = max(abs(th[1]) for th in theta_history)
    theta0_range = np.linspace(0, max_abs_theta0)
    theta1_range = np.linspace(0, max_abs_theta1)

    X_with_intercept = np.c_[np.ones(X.shape[0]), X]
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    cost_mesh = np.zeros_like(theta0_mesh)

    for i in range(theta0_mesh.shape[0]):
        for j in range(theta0_mesh.shape[1]):
            theta = np.array([theta0_mesh[i,j], theta1_mesh[i,j]])
            cost_mesh[i,j] = np.sum((X_with_intercept.dot(theta) - y) ** 2) / (2 * len(y))

    fig, axes = plt.subplots(1, 3, figsize=(18, 6), subplot_kw={'projection': '3d'})
    
    # Plot error surface with animation
    ax = axes[0]
    ax.plot_surface(theta1_mesh, theta0_mesh, cost_mesh, cmap='turbo', alpha=0.7)
    ax.set_xlabel('θ1')
    ax.set_ylabel('θ0')
    ax.set_zlabel('Cost J(θ)')
    ax.set_title('Error Surface with Gradient Descent Animation')
    
    theta0_data = [th[0] for th in theta_history]
    theta1_data = [th[1] for th in theta_history]
    cost_data = cost_history
    
    path, = ax.plot([], [], [], 'r-', linewidth=2, label='Gradient descent path')
    points, = ax.plot([], [], [], 'ro', markersize=3)
    iteration_text = ax.text2D(0.05, 0.95, '', transform=ax.transAxes)
    
    def init():
        path.set_data([], [])
        path.set_3d_properties([])
        points.set_data([], [])
        points.set_3d_properties([])
        iteration_text.set_text('')
        return path, points, iteration_text
    
    def update(frame):
        path.set_data(theta1_data[:frame+1], theta0_data[:frame+1])
        path.set_3d_properties(cost_data[:frame+1])
        points.set_data([theta1_data[frame]], [theta0_data[frame]])
        points.set_3d_properties([cost_data[frame]])
        iteration_text.set_text(f'Iteration: {frame+1}\nθ0: {theta0_data[frame]:.4f}, θ1: {theta1_data[frame]:.4f}')
        return path, points, iteration_text
    
    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), init_func=init, interval=200, blit=False)
    ax.legend()
    
    # Plot only gradient descent path without error function animation
    ax2 = axes[1]
    ax2.plot(theta1_data, theta0_data, cost_data, 'r-', linewidth=2, label='Gradient descent path')
    ax2.scatter(theta1_data, theta0_data, cost_data, c='red', s=10)
    ax2.set_xlabel('θ1')
    ax2.set_ylabel('θ0')
    ax2.set_zlabel('Cost J(θ)')
    ax2.set_title('Gradient Descent Path')
    ax2.legend()
    
    # Plot only error surface
    ax3 = axes[2]
    ax3.plot_surface(theta1_mesh, theta0_mesh, cost_mesh, cmap='turbo', alpha=0.8)
    ax3.set_xlabel('θ1')
    ax3.set_ylabel('θ0')
    ax3.set_zlabel('Cost J(θ)')
    ax3.set_title('Error Surface')
    
    plt.show()

visualize_error_function(model, X, y)

"""
PART 4
"""
def visualize_error_contour(model, X, y, learning_rate):
    # Extract theta history and cost history
    theta_history = [item[0] for item in model.cost_history]
    cost_history = [item[1] for item in model.cost_history]

    # Ensure we start from (0,0)
    if theta_history[0][0] != 0 or theta_history[0][1] != 0:
        theta_history.insert(0, (0.0, 0.0))  # Prepend (0,0) if needed
        cost_history.insert(0, cost_history[0])  # Keep cost history aligned

    # Get final theta values (where gradient descent converged)
    theta0_final, theta1_final = theta_history[-1]

    # Define contour range covering (0,0) and final theta values
    buffer0 = 0.5 * abs(theta0_final)
    buffer1 = 0.5 * abs(theta1_final)

    theta0_range = np.linspace(-buffer0, theta0_final + buffer0, 50)
    theta1_range = np.linspace(-buffer1, theta1_final + buffer1, 50)

    X_with_intercept = np.c_[np.ones(X.shape[0]), X]
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_range, theta1_range)
    cost_mesh = np.zeros_like(theta0_mesh)

    # Compute cost values
    for i in range(theta0_mesh.shape[0]):
        for j in range(theta0_mesh.shape[1]):
            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
<A NAME="7"></A><FONT color = #0000FF><A HREF="match46-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            cost_mesh[i, j] = np.sum((X_with_intercept.dot(theta) - y) ** 2) / (2 * len(y))

    # Extract gradient descent path
    theta0_data = [th[0] for th in theta_history]
    theta1_data = [th[1] for th in theta_history]

    # --- Animation Setup ---
    fig, ax = plt.subplots(figsize=(8, 6))
</FONT>
    # Contour plot
    ax.contour(theta0_mesh, theta1_mesh, cost_mesh, levels=20, cmap="coolwarm", linewidths=0.7)

    # Initialize animation elements
    scatter, = ax.plot([], [], 'ro', markersize=6, label="Current Point")
    path, = ax.plot([], [], 'r-', linewidth=1.5, label="Descent Path")
    iteration_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')

    # Mark the final convergence point
    ax.scatter(theta0_final, theta1_final, c="green", s=100, marker="x", label="Converged Minimum")

    # Labels and title
    ax.set_xlabel(r"$\theta_0$")
    ax.set_ylabel(r"$\theta_1$")
    ax.set_title(f'Gradient Descent Convergence (η = {learning_rate})')
    ax.legend()

    def init():
        scatter.set_data([], [])
        path.set_data([], [])
        iteration_text.set_text("")
        return scatter, path, iteration_text

    def update(frame):
        scatter.set_data([theta0_data[frame]], [theta1_data[frame]])  # Move red dot
        path.set_data(theta0_data[:frame+1], theta1_data[:frame+1])  # Extend path
        iteration_text.set_text(f"Iteration: {frame+1}/{len(theta_history)}")  # Display iteration count
        return scatter, path, iteration_text

    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), init_func=init, interval=100, blit=False)

    plt.show()

    # --- Final Converged Graph ---
    plt.figure(figsize=(8, 6))
    plt.contour(theta0_mesh, theta1_mesh, cost_mesh, levels=20, cmap="coolwarm", linewidths=0.7)
    plt.plot(theta0_data, theta1_data, 'r-', label="Gradient Path")
    plt.scatter(theta0_data, theta1_data, c='red', s=5)  # Plot all descent points
    plt.scatter(theta0_final, theta1_final, c="green", s=100, marker="x", label="Final Convergence")
    plt.xlabel(r"$\theta_0$")
    plt.ylabel(r"$\theta_1$")
    plt.title(f'Final Gradient Descent Path (η = {learning_rate})')
    plt.legend()
    plt.show()

visualize_error_contour(model, X, y, learning_rate)





"""
# Part 1: Data Generation and Train-Test Split
"""

import numpy as np
import time

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    X = np.column_stack((np.ones(N), x1, x2))
    return X, y

# Generate data
N = 1000000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train-test split
train_size = int(0.8 * N)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]

"""# Part 2 : Stochastic Gradient Descent Implementation"""

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []

    def fit(self, X, y, learning_rate, batch_size=1):
        start_time = time.time()
        m, n = X.shape
        self.theta = np.zeros((n, 1))
        self.theta_history = [self.theta.copy()]

        y = y.reshape(-1, 1)

        prev_avg_loss = float('inf')
        loss = 0
        iteration_count = 0
        epochs = 0
        max_epochs = 5000
        epsilon = 1e-5
        num_batches = 1  # Avoid division by zero

        while epochs &lt; max_epochs:
            prev_avg_loss = loss / num_batches if epochs &gt; 0 else float('inf')
            loss = 0

            # Shuffle data
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            # Process batches
            num_complete_batches = m // batch_size
            remainder = m % batch_size
            num_batches = num_complete_batches + (1 if remainder &gt; 0 else 0)

            for b in range(num_complete_batches):
                start_idx = b * batch_size
                end_idx = start_idx + batch_size

                X_batch = X_shuffled[start_idx:end_idx]
<A NAME="0"></A><FONT color = #FF0000><A HREF="match46-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                y_batch = y_shuffled[start_idx:end_idx]

                predictions = X_batch @ self.theta
                errors = predictions - y_batch
                gradient = (X_batch.T @ errors) / batch_size
                self.theta -= learning_rate * gradient
                self.theta_history.append(self.theta.copy())
</FONT>
                batch_loss = np.mean(errors**2) / 2
                loss += batch_loss
                iteration_count += 1

            # Handle remaining examples
            if remainder &gt; 0:
                X_batch = X_shuffled[num_complete_batches * batch_size:]
                y_batch = y_shuffled[num_complete_batches * batch_size:]
<A NAME="2"></A><FONT color = #0000FF><A HREF="match46-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

                predictions = X_batch @ self.theta
                errors = predictions - y_batch
                gradient = (X_batch.T @ errors) / remainder
                self.theta -= learning_rate * gradient
                self.theta_history.append(self.theta.copy())
</FONT>
                batch_loss = np.mean(errors**2) / 2
                loss += batch_loss
                iteration_count += 1

            avg_loss = loss / num_batches
            epochs += 1

            # Convergence check
            if abs(avg_loss - prev_avg_loss) &lt;= epsilon:
                break

        end_time = time.time()
        return self.theta_history, iteration_count, end_time - start_time

    def predict(self, X):
        return X @ self.theta

    def compute_mse(self, X, y):
        predictions = self.predict(X)
        return np.mean((predictions - y.reshape(-1, 1))**2)

"""# Part 3 and Part 4 : Training Models with Different Batch Sizes and Closed Form Solution along with Test MSE"""

# Define batch sizes
batch_sizes = [1, 80, 8000, 800000]
results = []

# Train models with different batch sizes
for batch_size in batch_sizes:
    print(f"\nTraining with batch size {batch_size}...")
    model = StochasticLinearRegressor()
    theta_history, iterations, time_taken = model.fit(X_train, y_train, learning_rate=0.001, batch_size=batch_size)

    # Calculate errors
    train_mse = model.compute_mse(X_train, y_train)
    test_mse = model.compute_mse(X_test, y_test)

    # Store results
    results.append({
        'batch_size': batch_size,
        'final_theta': model.theta.flatten(),
        'train_mse': train_mse,
        'test_mse': test_mse,
        'theta_history': theta_history,
        'iterations': iterations,
        'time_taken': time_taken
    })

# Closed form solution
def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)

# Print comparison
print("\nResults Summary:")
print(f"True parameters: {theta_true}")
print(f"Closed-form solution: {theta_closed_form.flatten()}")
print("\nSGD Results:")
for result in results:
    print(f"\nBatch size: {result['batch_size']}")
    print(f"Final theta: {result['final_theta']}")
    print(f"Training MSE: {result['train_mse']:.6f}")
    print(f"Test MSE: {result['test_mse']:.6f}")
    print(f"Iterations: {result['iterations']}")
    print(f"Time taken: {result['time_taken']:.2f} seconds")

"""# Part 5: Visualization of Parameter Trajectories"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(20, 15))

for i, result in enumerate(results):
    ax = fig.add_subplot(2, 2, i+1, projection='3d')
    theta_history = np.array([th.flatten() for th in result['theta_history']])
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match46-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2])
    ax.set_xlabel('θ0')
    ax.set_ylabel('θ1')
    ax.set_zlabel('θ2')
    ax.set_title(f'Parameter Trajectory (batch size = {result["batch_size"]})')
</FONT>
plt.tight_layout()
plt.show()



#!/usr/bin/env python
# coding: utf-8

# # Part 1: Data Generation and Train-Test Split

# In[ ]:


import numpy as np
import time

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    noise = np.random.normal(0, noise_sigma, N)

    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    X = np.column_stack((np.ones(N), x1, x2))
    return X, y

# Generate data
N = 1000000
theta_true = np.array([3, 1, 2])
input_mean = np.array([3, -1])
input_sigma = np.array([2, 2])
noise_sigma = np.sqrt(2)

X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

# Train-test split
train_size = int(0.8 * N)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]


# # Part 2 : Stochastic Gradient Descent Implementation

# In[ ]:


import numpy as np
<A NAME="8"></A><FONT color = #00FFFF><A HREF="match46-1.html#8" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

import time

class StochasticLinearRegressor:
    def __init__(self):
        self.theta = None
        self.theta_history = []

    def fit(self, X, y, learning_rate, batch_size=1):
</FONT>        start_time = time.time()
        m, n = X.shape
        self.theta = np.zeros((n, 1))
        self.theta_history = [self.theta.copy()]

        y = y.reshape(-1, 1)

        prev_avg_loss = float('inf')
        loss = 0
        iteration_count = 0
        epochs = 0
        max_epochs = 5000
        epsilon = 1e-5
        num_batches = 1  # Avoid division by zero

        while epochs &lt; max_epochs:
            prev_avg_loss = loss / num_batches if epochs &gt; 0 else float('inf')
            loss = 0

            # Shuffle data
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            # Process batches
            num_complete_batches = m // batch_size
            remainder = m % batch_size
            num_batches = num_complete_batches + (1 if remainder &gt; 0 else 0)

            for b in range(num_complete_batches):
                start_idx = b * batch_size
                end_idx = start_idx + batch_size

                X_batch = X_shuffled[start_idx:end_idx]
                y_batch = y_shuffled[start_idx:end_idx]

                predictions = X_batch @ self.theta
                errors = predictions - y_batch
                gradient = (X_batch.T @ errors) / batch_size
                self.theta -= learning_rate * gradient
                self.theta_history.append(self.theta.copy())

                batch_loss = np.mean(errors**2) / 2
                loss += batch_loss
                iteration_count += 1

            # Handle remaining examples
            if remainder &gt; 0:
                X_batch = X_shuffled[num_complete_batches * batch_size:]
                y_batch = y_shuffled[num_complete_batches * batch_size:]
                predictions = X_batch @ self.theta
                errors = predictions - y_batch
                gradient = (X_batch.T @ errors) / remainder
                self.theta -= learning_rate * gradient
                self.theta_history.append(self.theta.copy())

                batch_loss = np.mean(errors**2) / 2
                loss += batch_loss
                iteration_count += 1

            avg_loss = loss / num_batches
            epochs += 1

            # Convergence check
            if abs(avg_loss - prev_avg_loss) &lt;= epsilon:
                break

        end_time = time.time()
        return self.theta_history, iteration_count, end_time - start_time

    def predict(self, X):
        return X @ self.theta

    def compute_mse(self, X, y):
        predictions = self.predict(X)
        return np.mean((predictions - y.reshape(-1, 1))**2)


# # Part 3 and Part 4 : Training Models with Different Batch Sizes and Closed Form Solution along with Test MSE

# In[ ]:


# Define batch sizes
batch_sizes = [1, 80, 8000, 800000]
results = []

# Train models with different batch sizes
for batch_size in batch_sizes:
    print(f"\nTraining with batch size {batch_size}...")
    model = StochasticLinearRegressor()
    theta_history, iterations, time_taken = model.fit(X_train, y_train, learning_rate=0.001, batch_size=batch_size)

    # Calculate errors
    train_mse = model.compute_mse(X_train, y_train)
    test_mse = model.compute_mse(X_test, y_test)

    # Store results
    results.append({
        'batch_size': batch_size,
        'final_theta': model.theta.flatten(),
        'train_mse': train_mse,
        'test_mse': test_mse,
        'theta_history': theta_history,
        'iterations': iterations,
        'time_taken': time_taken
    })

# Closed form solution
def closed_form_solution(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

theta_closed_form = closed_form_solution(X_train, y_train)

# Print comparison
print("\nResults Summary:")
print(f"True parameters: {theta_true}")
print(f"Closed-form solution: {theta_closed_form.flatten()}")
print("\nSGD Results:")
for result in results:
    print(f"\nBatch size: {result['batch_size']}")
    print(f"Final theta: {result['final_theta']}")
    print(f"Training MSE: {result['train_mse']:.6f}")
    print(f"Test MSE: {result['test_mse']:.6f}")
    print(f"Iterations: {result['iterations']}")
    print(f"Time taken: {result['time_taken']:.2f} seconds")


# # Part 5: Visualization of Parameter Trajectories

# In[ ]:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(20, 15))

for i, result in enumerate(results):
    ax = fig.add_subplot(2, 2, i+1, projection='3d')
    theta_history = np.array([th.flatten() for th in result['theta_history']])
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2])
    ax.set_xlabel('θ0')
    ax.set_ylabel('θ1')
    ax.set_zlabel('θ2')
    ax.set_title(f'Parameter Trajectory (batch size = {result["batch_size"]})')

plt.tight_layout()
plt.show()





"""
# PART A
"""

# Imports - you can add any other permitted libraries
import numpy as np
import pandas as pd

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def hessian(X, theta):
    h = sigmoid(np.dot(X, theta))
    diag = np.diag(h * (1 - h))
    return X.T @ diag @ X

def gradient(X, y, theta):
    h = sigmoid(np.dot(X, theta))
    return np.dot(X.T, (h - y))

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None
        self.parameters = []
        self.iterations = 0

    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        num_iterations : int
            Number of iterations for Newton's Method.

        tol : float
            Tolerance for convergence.

        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """
        # Normalize the input data
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Add intercept term
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        # Initialize theta
        self.theta = np.zeros(X.shape[1])

        converged = False

        while not converged:
            H = hessian(X, self.theta)
            grad = gradient(X, y, self.theta)
            delta = np.linalg.inv(H) @ grad
            self.theta -= delta
            self.parameters.append(self.theta.copy())
            self.iterations += 1
            converged = np.linalg.norm(delta) &lt; learning_rate

<A NAME="9"></A><FONT color = #FF00FF><A HREF="match46-1.html#9" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        return np.array(self.parameters)

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
</FONT>        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize the input data
        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

        # Add intercept term
        X = np.hstack((np.ones((X.shape[0], 1)), X))

        # Predict
        y_pred = sigmoid(np.dot(X, self.theta)) &gt;= 0.5
        return y_pred.astype(int)

# Load the data
<A NAME="10"></A><FONT color = #FF0000><A HREF="match46-1.html#10" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

X = pd.read_csv('logisticX.csv', header=None).values
y = pd.read_csv('logisticY.csv', header=None).values.flatten()

# Initialize and fit the logistic regression model
model = LogisticRegressor()
parameters = model.fit(X, y)
</FONT>
# Print the coefficients (including intercept) and number of iterations
print("Final Parameters:", model.theta)
print("Number of iterations:", model.iterations)

"""# PART B"""

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.lines as mlines

def plot_decision_boundary(X, y, model, sigmoid):
    # Create a larger figure (e.g., 12 inches by 8 inches)
    fig, ax = plt.subplots(figsize=(12, 8))

    # Plot the training data
    colors = ["r" if cls else "b" for cls in y]
<A NAME="1"></A><FONT color = #00FF00><A HREF="match46-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    ax.scatter(X[:, 0], X[:, 1], c=colors, edgecolor='k')

    # Plot the decision boundary
    x1_min, x1_max = X[:, 0].min(), X[:, 0].max()
    x2_min, x2_max = X[:, 1].min(), X[:, 1].max()
</FONT>    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 500),
                           np.linspace(x2_min, x2_max, 500))
    grid = np.c_[xx1.ravel(), xx2.ravel()]
    grid = (grid - np.mean(grid, axis=0)) / np.std(grid, axis=0)
    grid = np.hstack((np.ones((grid.shape[0], 1)), grid))
    probs = sigmoid(np.dot(grid, model.theta)).reshape(xx1.shape)

    # Plot the contour and capture the contour set (for later reference, if needed)
    contour_set = ax.contour(xx1, xx2, probs, levels=[0.5], cmap="YlGn", vmin=0, vmax=0.6)

    # Create proxy legend handles
    decision_boundary_proxy = mlines.Line2D([], [], color='green', lw=2, label='Decision Boundary')
    cls0 = mpatches.Patch(color='blue', label='Class 0')
    cls1 = mpatches.Patch(color='red', label='Class 1')

    ax.set_xlabel('Feature 0 (X0)')
    ax.set_ylabel('Feature 1 (X1)')
    ax.set_title('Training Data and Decision Boundary')

    # Set custom ticks for better granularity
    ax.set_xticks(np.linspace(x1_min, x1_max, 20))
    ax.set_yticks(np.linspace(x2_min, x2_max, 20))

    # Add all legend items including the decision boundary
    ax.legend(handles=[cls0, cls1, decision_boundary_proxy])

    plt.show()

plot_decision_boundary(X, y, model, sigmoid)



"""
# PART A
"""

import numpy as np

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.phi = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # Normalize X
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        # Separate the data by class
        X_0 = X[y == 0]
        X_1 = X[y == 1]

        # Compute the mean vectors
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        # Compute the covariance matrix
        n_0 = X_0.shape[0]
        n_1 = X_1.shape[0]

        sigma_0 = np.cov(X_0.T, bias=True)
        sigma_1 = np.cov(X_1.T, bias=True)

        if assume_same_covariance:
            self.sigma = (n_0 * sigma_0 + n_1 * sigma_1) / (n_0 + n_1)

        # Compute phi
        self.phi = np.mean(y)

        if assume_same_covariance:
            return self.mu_0, self.mu_1, self.sigma
        else:
            return self.mu_0, self.mu_1, sigma_0, sigma_1

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize X
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        # Calculate the discriminant function for each class
        delta_0 = -0.5 * np.sum(np.dot(X - self.mu_0, np.linalg.inv(self.sigma)) * (X - self.mu_0), axis=1)
        delta_1 = -0.5 * np.sum(np.dot(X - self.mu_1, np.linalg.inv(self.sigma)) * (X - self.mu_1), axis=1)

        # Assign the class with the higher discriminant value
        y_pred = np.where(delta_1 &gt; delta_0, 1, 0)

        return y_pred

# Load data from files
X = np.loadtxt('q4x.dat')

# Load y and convert it to binary (0 for Alaska, 1 for Canada)
with open('q4y.dat', 'r') as file:
    y = file.readlines()
y = np.array([0 if label.strip() == 'Alaska' else 1 for label in y])

# Normalize X
X = (X - X.mean(axis=0)) / X.std(axis=0)


gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma = gda.fit(X, y, assume_same_covariance=True)
phi = gda.phi
print("Phi:", phi)
print("Mean vectors:")
print("Mu_0:", mu_0)
print("Mu_1:", mu_1)
print("Covariance matrix (Sigma):", sigma)

"""# PART B"""

import matplotlib.pyplot as plt

def plot_salmon_data(X, y):
    """
    Plots the salmon data with two categories: Alaska and Canada.

    Parameters:
    X : numpy.ndarray
        Feature matrix with two columns (growth ring diameters).
    y : numpy.ndarray
        Labels (0 for Alaska, 1 for Canada).
    """
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='r', label='Alaska')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='b', label='Canada')
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Alaska vs Canada')
    plt.legend()
    plt.show()
    
plot_salmon_data(X, y)


"""# PART C"""

# The decision boundary equation is:
# (x - mu0)^T Sigma^-1 (x - mu0) = (x - mu1)^T Sigma^-1 (x - mu1)
# This simplifies to a linear equation: w^T x + b = 0
# where w = Sigma^-1 (mu1 - mu0)
# and b = -0.5 * (mu1^T Sigma^-1 mu1 - mu0^T Sigma^-1 mu0)

# Calculate decision boundary parameters

def plot_decision_boundary(X, y, mu_0, mu_1, sigma):
    """
    Plots the data points along with the decision boundary for GDA.

    Parameters:
    X : numpy.ndarray
        Feature matrix with two columns.
    y : numpy.ndarray
        Labels (0 for Alaska, 1 for Canada).
    mu_0 : numpy.ndarray
        Mean vector for class 0 (Alaska).
    mu_1 : numpy.ndarray
        Mean vector for class 1 (Canada).
    sigma : numpy.ndarray
        Covariance matrix.
    """
    # Compute decision boundary parameters
    sigma_inv = np.linalg.inv(sigma)
    w = sigma_inv @ (mu_1 - mu_0)
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)

    # Create the plot
    plt.figure(figsize=(8, 6))

    # Plot the original data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='r', label='Alaska')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='b', label='Canada')

    # Plot the decision boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x_db = np.array([x_min, x_max])
    y_db = -(w[0] * x_db + b) / w[1]
    plt.plot(x_db, y_db, 'g-', label='Decision Boundary')

    # Labels and formatting
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Alaska vs Canada with GDA Decision Boundary')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_decision_boundary(X, y, mu_0, mu_1, sigma)

"""# PART D"""

class GaussianDiscriminantAnalysis:
    # Assume Binary Classification
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        Remember to normalize the input data X before fitting the model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.

        Returns
        -------
        Parameters:
            If assume_same_covariance = True - 3-tuple of numpy arrays mu_0, mu_1, sigma
            If assume_same_covariance = False - 4-tuple of numpy arrays mu_0, mu_1, sigma_0, sigma_1
            The parameters learned by the model.
        """
        # Normalize X
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        # Separate the data by class
        X_0 = X[y == 0]
        X_1 = X[y == 1]

        # Compute the mean vectors
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)

        # Compute the covariance matrices
        self.sigma_0 = np.cov(X_0.T, bias=True)
        self.sigma_1 = np.cov(X_1.T, bias=True)

        # Compute phi
        self.phi = np.mean(y)

        if assume_same_covariance:
            print("Phi:", self.phi)
            print("Mean vectors:")
            print("Mu_0:", self.mu_0)
            print("Mu_1:", self.mu_1)
            sigma = (X_0.shape[0] * self.sigma_0 + X_1.shape[0] * self.sigma_1) / (X_0.shape[0] + X_1.shape[0])
            print("Σ(shared):", sigma)
            return self.mu_0, self.mu_1, sigma
        else:
            print("Phi:", self.phi)
            print("Mean vectors:")
            print("Mu_0:", self.mu_0)
            print("Mu_1:", self.mu_1)
            print("Σ0:", self.sigma_0)
            print("Σ1:", self.sigma_1)
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        """
        Predict the target values for the input data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.

        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """
        # Normalize X
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        # Calculate the discriminant function for each class
<A NAME="11"></A><FONT color = #00FF00><A HREF="match46-1.html#11" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        inv_sigma_0 = np.linalg.inv(self.sigma_0)
        inv_sigma_1 = np.linalg.inv(self.sigma_1)

        delta_0 = -0.5 * np.sum(np.dot(X - self.mu_0, inv_sigma_0) * (X - self.mu_0), axis=1)
</FONT>        delta_1 = -0.5 * np.sum(np.dot(X - self.mu_1, inv_sigma_1) * (X - self.mu_1), axis=1)

        # Assign the class with the higher discriminant value
        y_pred = np.where(delta_1 &gt; delta_0, 1, 0)

        return y_pred

# Load data from files
X = np.loadtxt('q4x.dat')
with open('q4y.dat', 'r') as file:
    y = file.readlines()
y = np.array([0 if label.strip() == 'Alaska' else 1 for label in y])

# Normalize X
X = (X - X.mean(axis=0)) / X.std(axis=0)

gda = GaussianDiscriminantAnalysis()
mu_0, mu_1, sigma_0, sigma_1 = gda.fit(X, y, assume_same_covariance=False)
phi = gda.phi

"""# PART E"""

def quadratic_discriminant(x, mu0, mu1, Sigma0, Sigma1):
    """
    Computes the quadratic discriminant function values for given points.

    Parameters:
    x : numpy.ndarray
        Grid points where the function is evaluated.
    mu0, mu1 : numpy.ndarray
        Mean vectors for class 0 (Alaska) and class 1 (Canada).
    Sigma0, Sigma1 : numpy.ndarray
        Covariance matrices for both classes.

    Returns:
    numpy.ndarray
        Discriminant function values.
    """
    Sigma0_inv = np.linalg.inv(Sigma0)
    Sigma1_inv = np.linalg.inv(Sigma1)

    term0 = -0.5 * np.log(np.linalg.det(Sigma0))
    term1 = -0.5 * np.log(np.linalg.det(Sigma1))

    diff0 = x - mu0
    diff1 = x - mu1

    quad0 = -0.5 * np.sum((diff0 @ Sigma0_inv) * diff0, axis=1)
    quad1 = -0.5 * np.sum((diff1 @ Sigma1_inv) * diff1, axis=1)

    return quad0 + term0 - (quad1 + term1)

def plot_decision_boundaries(X, y, mu_0, mu_1, sigma_0, sigma_1):
    """
    Plots the data points along with linear and quadratic decision boundaries.

    Parameters:
    X : numpy.ndarray
        Feature matrix with two columns.
    y : numpy.ndarray
        Labels (0 for Alaska, 1 for Canada).
    mu_0, mu_1 : numpy.ndarray
        Mean vectors for class 0 (Alaska) and class 1 (Canada).
    sigma_0, sigma_1 : numpy.ndarray
        Covariance matrices for both classes.
    """
    # Compute linear boundary parameters
    sigma_inv = np.linalg.inv((sigma_0 + sigma_1) / 2)
    w = sigma_inv @ (mu_1 - mu_0)
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)

    # Create mesh grid for quadratic boundary
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]

    # Compute quadratic decision boundary
    Z = quadratic_discriminant(grid_points, mu_0, mu_1, sigma_0, sigma_1)
    Z = Z.reshape(xx.shape)

    # Create the plot
    plt.figure(figsize=(10, 8))

    # Plot original data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='r', label='Alaska')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='b', label='Canada')

    # Plot linear decision boundary
    x_db = np.array([x_min, x_max])
    y_db = -(w[0] * x_db + b) / w[1]
    plt.plot(x_db, y_db, 'g-', label='Linear Boundary')

    # Plot quadratic decision boundary
    plt.contour(xx, yy, Z, levels=[0], colors='purple', linestyles='-')
    plt.plot([], [], color='purple', linestyle='-', label='Quadratic Boundary')  # Proxy for legend

    # Labels and formatting
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Linear vs Quadratic Decision Boundaries')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_decision_boundaries(X, y, mu_0, mu_1, sigma_0, sigma_1)


"""# PART F"""

def plot_decision_boundaries_expanded(X, y, mu_0, mu_1, sigma_0, sigma_1, padding=3):
    """
    Plots data points with linear and quadratic decision boundaries, expanding the range.

    Parameters:
    X : numpy.ndarray
        Feature matrix with two columns.
    y : numpy.ndarray
        Labels (0 for Alaska, 1 for Canada).
    mu_0, mu_1 : numpy.ndarray
        Mean vectors for class 0 (Alaska) and class 1 (Canada).
    sigma_0, sigma_1 : numpy.ndarray
        Covariance matrices for both classes.
    padding : float
        Factor by which the view range is expanded.
    """
    def quadratic_discriminant(x, mu0, mu1, Sigma0, Sigma1):
        Sigma0_inv = np.linalg.inv(Sigma0)
        Sigma1_inv = np.linalg.inv(Sigma1)
        term0 = -0.5 * np.log(np.linalg.det(Sigma0))
        term1 = -0.5 * np.log(np.linalg.det(Sigma1))
        diff0 = x - mu0
        diff1 = x - mu1
        quad0 = -0.5 * np.sum((diff0 @ Sigma0_inv) * diff0, axis=1)
        quad1 = -0.5 * np.sum((diff1 @ Sigma1_inv) * diff1, axis=1)
        return quad0 + term0 - (quad1 + term1)
    
    # Compute linear boundary parameters
    sigma_inv = np.linalg.inv((sigma_0 + sigma_1) / 2)
    w = sigma_inv @ (mu_1 - mu_0)
    b = -0.5 * (mu_1.T @ sigma_inv @ mu_1 - mu_0.T @ sigma_inv @ mu_0)
    
    # Expanded range calculation
    x_min, x_max = X[:, 0].min(), X[:, 0].max()
    y_min, y_max = X[:, 1].min(), X[:, 1].max()
    x_center = (x_min + x_max) / 2
    y_center = (y_min + y_max) / 2
    x_range = x_max - x_min
    y_range = y_max - y_min
    x_min = x_center - (x_range * padding) / 2
    x_max = x_center + (x_range * padding) / 2
    y_min = y_center - (y_range * padding) / 2
    y_max = y_center + (y_range * padding) / 2
    
    # Create mesh grid for quadratic boundary
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    # Calculate quadratic boundary values
    Z = quadratic_discriminant(grid_points, mu_0, mu_1, sigma_0, sigma_1)
    Z = Z.reshape(xx.shape)
    
    # Create the plot
    plt.figure(figsize=(10, 8))
    
    # Plot the original data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', color='r', label='Alaska')
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', color='b', label='Canada')
    
    # Plot linear decision boundary with expanded range
    x_db = np.array([x_min, x_max])
    y_db = -(w[0] * x_db + b) / w[1]
    plt.plot(x_db, y_db, 'g-', label='Linear Boundary')
    
    # Plot quadratic decision boundary
    plt.contour(xx, yy, Z, levels=[0], colors='purple', linestyles='-')
    plt.plot([], [], color='purple', linestyle='-', label='Quadratic Boundary')
    
    # Set the plot limits
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    
    plt.xlabel('Feature 0 (Growth ring diameters in fresh water)')
    plt.ylabel('Feature 1 (Growth ring diameters in marine water)')
    plt.title('Salmon Data: Linear vs Quadratic Decision Boundaries')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_decision_boundaries_expanded(X, y, mu_0, mu_1, sigma_0, sigma_1, padding=3)


</PRE>
</PRE>
</BODY>
</HTML>
