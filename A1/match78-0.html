<HTML>
<HEAD>
<TITLE>./A1_processed_new/combined_089GD.py</TITLE>
</HEAD>
<BODY BGCOLOR=white>
<HR>
./A1_processed_new/combined_089GD.py<p><PRE>


# Imports - you can add any other permitted libraries
import numpy as np

import pandas as pd
import matplotlib.pyplot as plt
import time
import plotly.graph_objects as go
import matplotlib.animation as animation
from IPython.display import HTML
from plotly.offline import init_notebook_mode, iplot  # For Jupyter Notebook display
from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting




# init_notebook_mode(connected=True)  # Initialize for notebook




# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

class LinearRegressor:
    def __init__(self):
        pass
    
    def fit(self, X, y, learning_rate):
        """
        Fit the linear regression model to the data using Gradient Descent.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """
        
        m, n = X.shape
        X = np.c_[np.ones(m), X]  # Add intercept term
        self.theta = np.zeros(n + 1)  # Initialize parameters
        self.learning_rate = learning_rate
        self.iterations = 0
        
        params_history = []
        cost_history = []
        prev_cost = float('inf')
        max_iter = 49
        
        while True:
            y_pred = X @ self.theta
            error = y_pred - y
            cost = (1 / (2 * m)) * np.sum(error ** 2)
            
            if abs(prev_cost - cost) &lt; 1e-3:
                break

            self.iterations = self.iterations+1
            prev_cost = cost
            gradient = (1 / m) * (X.T @ error)
            self.theta -= learning_rate * gradient
            
            params_history.append(self.theta.copy())
            cost_history.append(cost)
            # print(cost)
            # plot_3d_cost_function(X, y, params_history)
            # plt.pause(0.2)  # Delay of 0.2 seconds
        
        return np.array(params_history)
        
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        
        X = np.c_[np.ones(X.shape[0]), X]
        return X @ self.theta
        
        pass

def load_data():
    """ Load dataset from CSV files. """
    X = pd.read_csv('../data/Q1/linearX.csv', header=None).values.flatten()
    y = pd.read_csv('../data/Q1/linearY.csv', header=None).values.flatten()
    return X.reshape(-1, 1), y

def plot_data(X, y, model):
    """ Plot the original data and the learned hypothesis function using all data points. """
    plt.scatter(X, y, color='red', label="Training Data", s=13)
    
    # Use the full dataset for predictions
    y_pred = model.predict(X)
    
    # Sort X and corresponding predictions for a smooth plot
    sorted_indices = np.argsort(X.flatten())
    X_sorted = X[sorted_indices]
    y_pred_sorted = y_pred[sorted_indices]
    
    plt.plot(X_sorted, y_pred_sorted, color='blue', label="Hypothesis Function")
    plt.xlabel("Acidity")
    plt.ylabel("Density")
    plt.legend()
    plt.title("Linear Regression Fit")
    plt.show()

def plot_cost_function(J_history):
    """ Plot the cost function over iterations. """
    plt.plot(J_history, label="Cost Function")
    plt.xlabel("Iterations")
    plt.ylabel("Cost J(θ)")
    plt.title("Cost Function Convergence")
    plt.legend()
    plt.show()

def compute_cost(X, y, theta):
    """ Compute the cost function J(θ) for linear regression. """
    m = len(y)
    predictions = X @ theta  # Matrix multiplication (X * theta)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)  # Compute the cost
    return cost
    

def plot_3d_cost_function_interactive(X, y, theta_history, theta_range_factor=5):
    """
    Plot the 3D cost function with interactive features and overlay the gradient descent progress.
    """
    # Dynamically adjust the range of theta0_vals and theta1_vals based on theta_history
    theta0_min = min([theta[0] for theta in theta_history]) - theta_range_factor
    theta0_max = max([theta[0] for theta in theta_history]) + theta_range_factor
    theta1_min = min([theta[1] for theta in theta_history]) - theta_range_factor
    theta1_max = max([theta[1] for theta in theta_history]) + theta_range_factor

    # Create a larger mesh grid for theta0 and theta1 based on the dynamic range
    theta0_vals = np.linspace(theta0_min, theta0_max, 700)
    theta1_vals = np.linspace(theta1_min, theta1_max, 700)
    
    # Create a mesh grid for theta0 and theta1
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)
    
    # Initialize an array to store the cost values for each (theta0, theta1) pair
    J_vals = np.zeros(theta0_grid.shape)
    
    # Compute the cost for each pair of (theta0, theta1)
    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            theta = np.array([theta0_grid[i, j], theta1_grid[i, j]])  # Create theta
            J_vals[i, j] = compute_cost(X, y, theta)  # Calculate cost

    # Create the mesh surface plot
    surface = go.Surface(z=J_vals, x=theta0_grid, y=theta1_grid, colorscale='Viridis', showscale=True)
    
    # Create the scatter plot for the gradient descent path
    theta_history = np.array(theta_history)
    cost_history = np.array([compute_cost(X, y, theta) for theta in theta_history])
    
    scatter = go.Scatter3d(
        x=theta_history[:, 0], 
        y=theta_history[:, 1], 
        z=cost_history, 
        mode='markers', 
        marker=dict(size=5, color='red', opacity=0.7)
    )
    

    # Combine the surface and scatter plots
    fig = go.Figure(data=[surface, scatter])

    # Update layout for better visualization
    fig.update_layout(
        title='3D Cost Function Surface with Gradient Descent Path',
        scene=dict(
            xaxis_title='Theta 0',
            yaxis_title='Theta 1',
            zaxis_title='Cost Function J(θ)',
        ),
        margin=dict(t=0, b=0, r=0, l=0)
    )

    # Show the plot (interactive)
    fig.show()

def plot_3d_cost_function_animation_mpl(X, y, theta_history, theta_range_factor=5):
    """Animate the 3D cost function using Matplotlib's FuncAnimation."""

    theta0_min = min([theta[0] for theta in theta_history]) - theta_range_factor
    theta0_max = max([theta[0] for theta in theta_history]) + theta_range_factor
    theta1_min = min([theta[1] for theta in theta_history]) - theta_range_factor
    theta1_max = max([theta[1] for theta in theta_history]) + theta_range_factor

    theta0_vals = np.linspace(theta0_min, theta0_max, 700)
    theta1_vals = np.linspace(theta1_min, theta1_max, 700)
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    J_vals = np.zeros(theta0_grid.shape)
    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            theta = np.array([theta0_grid[i, j], theta1_grid[i, j]])
            J_vals[i, j] = compute_cost(X, y, theta)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    surface = ax.plot_surface(theta0_grid, theta1_grid, J_vals, cmap='viridis', alpha=0.8)

    theta_history = np.array(theta_history)
    cost_history = np.array([compute_cost(X, y, theta) for theta in theta_history])

    scatter = ax.scatter(theta_history[0, 0], theta_history[0, 1], cost_history[0], c='red', s=20)  # Initial point

    line, = ax.plot(theta_history[:1, 0], theta_history[:1, 1], cost_history[:1], c='red', linewidth=2)

    ax.set_xlabel('Theta 0')
    ax.set_ylabel('Theta 1')
    ax.set_zlabel('Cost Function J(θ)')
    ax.set_title('3D Cost Function Surface with Gradient Descent Path Animation')

    def animate(i):
        x = theta_history[:i+1, 0]
        y = theta_history[:i+1, 1]
        z = cost_history[:i+1]

        scatter._offsets3d = (x, y, z)  # Update scatter plot
        line.set_data(x, y)
        line.set_3d_properties(z)  # Important for 3D line updates

        return scatter, line  # Return the updated artists

    ani = animation.FuncAnimation(fig, animate, frames=len(theta_history), interval=300, blit=False)  # blit=False for 3D

    # For Jupyter Notebook display:
    display(HTML(ani.to_html5_video()))
    # plt.show()  # Important: Use plt.show() here

    # For saving as a video file (optional):
    # ani.save('3d_animation.mp4', fps=10)  # Requires ffmpeg


    plt.close()  # Close the static plot




def plot_contour_animation(X, y, theta_history, cost_history, learning_rate):
    """ Animate gradient descent on a contour plot, showing trajectory """
    fig, ax = plt.subplots(figsize=(8, 6))

    # Compute dynamic limits
    theta0_min = min(t[0] for t in theta_history)
    theta0_max = max(t[0] for t in theta_history)
    theta1_min = min(t[1] for t in theta_history)
    theta1_max = max(t[1] for t in theta_history)

    # Define range for theta values
    theta0_vals = np.linspace(-5, 16, 200)
    theta1_vals = np.linspace(-2.5, 50, 200)
    theta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)

    # Compute cost function values for contour plot
    J_vals = np.zeros_like(theta0_grid)
    for i in range(theta0_grid.shape[0]):
        for j in range(theta0_grid.shape[1]):
            theta = np.array([theta0_grid[i, j], theta1_grid[i, j]])
            J_vals[i, j] = compute_cost(X, y, theta)

    # Plot contours
    
    contour = ax.contour(theta0_grid, theta1_grid, J_vals, levels=17, cmap='viridis')
    ax.clabel(contour, inline=True, fontsize=8)
    ax.set_xlabel(r'$\theta_0$')
    ax.set_ylabel(r'$\theta_1$')
    ax.set_title(f'Gradient Descent Trajectory on Cost Function Contours (Learning Rate = {learning_rate})')

    # Initialize scatter plot for trajectory points
    
    scat = ax.scatter(theta_history[0][0], theta_history[0][1], c="r", s=7)
    line2 = ax.plot(theta_history[0][0], theta_history[0][1], c="r")[0]
    #trajectory, = ax.plot([], [], 'r.', markersize=4)  # Smaller markers
    #ax.scatter(*zip(*theta_history), color='red', marker='x', s=20)

    # Store trajectory points
    # theta0_trajectory = []
    # theta1_trajectory = []
    ax.text(
        0.05, 0.95,  # X and Y position (normalized figure coordinates)
        f"Total Iterations: {len(theta_history)}",  
        transform=ax.transAxes,  # Position relative to figure size
        fontsize=10, 
        color='white',
        bbox=dict(boxstyle="round,pad=0.3", edgecolor="white", facecolor="black")
    )

    # Animation function to update the trajectory
    def update(frame):
        # theta = theta_history[frame]
        # theta0_trajectory.append(theta[0])  # Append new theta_0
        # theta1_trajectory.append(theta[1])  # Append new theta_1
        
        # # Ensure all past points stay visible
        # trajectory.set_data(theta0_trajectory, theta1_trajectory)
        # return trajectory,
        # for each frame, update the data stored on each artist.
        x = [ele[0] for ele in theta_history[:frame]]
        y = [ele[1] for ele in theta_history[:frame]]
        # update the scatter plot:
        data = np.stack([x, y]).T
        scat.set_offsets(data)
        # update the line plot:
        line2.set_xdata(x)
        line2.set_ydata(y)
        return (scat, line2)


    # Create animation
    plt.rcParams["animation.html"] = "jshtml"
    ani = animation.FuncAnimation(fig, update, frames=len(theta_history), interval=260, blit=True)
    #ani.save('A.mp4',fps=2)
    #display(HTML(ani.to_jshtml()))
    display(HTML(ani.to_html5_video()))
    plt.close(fig)

    # plt.show()



def main():
    """ Main function to run linear regression and visualize results. """
    X, y = load_data()
    model = LinearRegressor()
    model1 = LinearRegressor()
    model2 = LinearRegressor()
    theta_history = model.fit(X, y, learning_rate=0.1)
    print(model.theta)
    theta_history1 = model1.fit(X, y, learning_rate=0.001)
    theta_history2 = model2.fit(X, y, learning_rate=0.025)
    
    # Plot data and hypothesis
    plot_data(X, y, model)

    # Plot cost function convergence
    J_history = [(1 / (2 * len(y))) * np.sum((np.c_[np.ones(X.shape[0]), X] @ theta - y) ** 2) for theta in theta_history]

    plot_cost_function(J_history)

    m, n = X.shape
    X = np.c_[np.ones(m), X]
    plot_3d_cost_function_interactive(X, y, theta_history)
    #plot_3d_cost_function_animation(X, y, theta_history)
    plot_3d_cost_function_animation_mpl(X, y, theta_history)

    cost_history = []
    cost_history1 = []
    cost_history2 = []
    for i in range(len(theta_history)):
        cost_history.append(compute_cost(X, y, theta_history[i]))
        cost_history1.append(compute_cost(X, y, theta_history1[i]))
        cost_history2.append(compute_cost(X, y, theta_history2[i]))

    plot_contour_animation(X, y, theta_history, cost_history, model.learning_rate)
    plot_contour_animation(X, y, theta_history1, cost_history1, model1.learning_rate)
    plot_contour_animation(X, y, theta_history2, cost_history2, model2.learning_rate)
    # plot_contours(X, y, theta_history, cost_history)

    # # Contour plot of cost function
    # plot_contour(theta_history, np.c_[np.ones(X.shape[0]), X], y)

    # # Compare learning rates
    # compare_learning_rates(X, y)

if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import deque
import time


np.random.seed(42)

# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def generate(N, theta, input_mean, input_sigma, noise_sigma):
    """
    Generate normally distributed input data and target values
    Note that we have 2 input features
    Parameters
    ----------
    N : int
        The number of samples to generate.
        
    theta : numpy array of shape (3,)
        The true parameters of the linear regression model.
        
    input_mean : numpy array of shape (2,)
        The mean of the input data.
        
    input_sigma : numpy array of shape (2,)
        The standard deviation of the input data.
        
    noise_sigma : float
        The standard deviation of the Gaussian noise.
        
    Returns
    -------
    X : numpy array of shape (N, 2)
        The input data.
        
    y : numpy array of shape (N,)
        The target values.
    """
    
    # Sample x1 and x2 from normal distributions
    x1 = np.random.normal(input_mean[0], input_sigma[0], N)
    x2 = np.random.normal(input_mean[1], input_sigma[1], N)
    
    # Stack into feature matrix (excluding intercept)
    X = np.column_stack((x1, x2))
    
    # Generate noise
    noise = np.random.normal(0, noise_sigma, N)
    
    # Compute y = θ0 + θ1*x1 + θ2*x2 + noise
    y = theta[0] + theta[1] * x1 + theta[2] * x2 + noise
    
    return X, y
    pass

def closed_form_solution(X, y):
    """
    Compute the closed-form solution for linear regression using the normal equation.
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The target values.
        
    Returns
    -------
    theta : numpy array of shape (n_features + 1,)
        The estimated parameters.
    """
    m, n = X.shape
    X = np.c_[np.ones(m), X]  # Add intercept term
    theta = np.linalg.inv(X.T @ X) @ X.T @ y  # Normal equation
    return theta

def compute_mse(X, y, theta):
    """
    Compute Mean Squared Error (MSE).
    
    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input data.
        
    y : numpy array of shape (n_samples,)
        The true target values.
        
    theta : numpy array of shape (n_features + 1,)
        The learned parameters.
        
    Returns
    -------
    mse : float
        The mean squared error.
    """
    m = X.shape[0]
    X = np.c_[np.ones(m), X]  # Add intercept term
    y_pred = X @ theta  # Compute predictions
    mse = np.mean((y - y_pred) ** 2)  # Compute MSE
    return mse

def plot_theta_updates(theta_history):
    """
    Plot the movement of θ parameters in 3D space as they update during SGD.
    
    Parameters
    ----------
    theta_histories : list of numpy arrays
        List of theta history arrays for different batch sizes.
        
    batch_sizes : list of ints
        Corresponding batch sizes for each theta history.
    """
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')

    # True parameter values (for reference)
    theta_true = np.array([3, 1, 2])

    # Plot each trajectory for different batch sizes
    # for i, theta_history in enumerate(theta_histories):
<A NAME="5"></A><FONT color = #FF0000><A HREF="match78-1.html#5" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    theta_history = np.array(theta_history)  # Convert to NumPy array
    ax.plot(theta_history[:, 0], theta_history[:, 1], theta_history[:, 2])

    # Mark the true theta as a reference point
    ax.scatter(theta_true[0], theta_true[1], theta_true[2], color='red', s=100, label="True θ")
</FONT><A NAME="6"></A><FONT color = #00FF00><A HREF="match78-1.html#6" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>


    # Labels and title
    ax.set_xlabel("θ0 (Intercept)")
    ax.set_ylabel("θ1 (Coefficient for x1)")
    ax.set_zlabel("θ2 (Coefficient for x2)")
    ax.set_title("SGD Parameter Trajectory for Different Batch Sizes")
    ax.legend()
    plt.show()
</FONT>
class StochasticLinearRegressor:
<A NAME="2"></A><FONT color = #0000FF><A HREF="match78-1.html#2" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    def __init__(self):
        self.theta = []
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Gradient Descent.
</FONT>        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target values.

        learning_rate : float
            The learning rate to use in the update rule.
            
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features,)
            The list of parameters obtained after each iteration of Gradient Descent.
        """

        m, n = X.shape
        X = np.c_[np.ones(m), X]  # Add intercept term
        # self.theta = np.zeros(n + 1)  # Initialize theta

        # for batch_size in [1, 80, 8000, 800000]:
        learning_rate=0.001
        theta_history = []
        best_loss = float('inf')
        prev_loss = float('inf')
        epochs_no_improve = 0  # Counter for early stopping
        patience = 13
        batch_size = 1
        max_iter = 26000
        epsilon = 1e-8  # Convergence threshold
        window_size = 26  # Number of past epochs to consider for moving average
        loss_history = []        
        indices = np.random.permutation(m)
        X_shuffled, y_shuffled = X[indices], y[indices]
        result = []

        for batch_size in [1, 80, 8000, 800000]:
            start_time = time.time()
            print("batch size: ", batch_size)
            self.theta.append(np.zeros(n + 1))  # Initialize theta
            theta_history = []
            loss_history = []
            for epoch in range(max_iter):
                # **Reshuffle Data at the Start of Every Epoch**
                # indices = np.random.permutation(m)
                # X_shuffled, y_shuffled = X[indices], y[indices]
    
                # **Compute number of batches in one epoch**
                num_batches = (m // batch_size) + (1 if m % batch_size != 0 else 0)
                running_loss = 0  # Reset loss accumulator for this epoch
    
                for b in range(num_batches):
                    if(batch_size==1):
                        curr_theta = self.theta[0]
                    elif(batch_size==80):
                        curr_theta = self.theta[1]
                    elif(batch_size==8000):
                        curr_theta = self.theta[2]
                    elif(batch_size==800000):
                        curr_theta = self.theta[3]
                        
                    start = b * batch_size
                    end = min(start + batch_size, m)  # Handle last batch safely
                    X_batch = X_shuffled[start:end]
                    y_batch = y_shuffled[start:end]
    
                    # Compute loss (batch loss)
                    error = X_batch @ curr_theta - y_batch
                    batch_loss = (1 / (2 * len(y_batch))) * np.sum(error ** 2)
    
                    # prev_loss = batch_loss
                    running_loss += batch_loss  # Accumulate loss
    
                    # Compute gradient
                    gradient = (1 / len(y_batch)) * (X_batch.T @ error)
    
                    # Update theta
                    if(batch_size==1):
                        self.theta[0] = curr_theta - learning_rate * gradient
                        theta_history.append(self.theta[0].copy())
                    elif(batch_size==80):
                        self.theta[1] = curr_theta - learning_rate * gradient
                        theta_history.append(self.theta[1].copy())
                    elif(batch_size==8000):
                        self.theta[2] = curr_theta - learning_rate * gradient
                        theta_history.append(self.theta[2].copy())
                    elif(batch_size==800000):
                        self.theta[3] = curr_theta - learning_rate * gradient
                        theta_history.append(self.theta[3].copy())
    
    
                # **Compute Average Loss Over All Batches in This Epoch**
                avg_epoch_loss = running_loss / num_batches
                loss_history.append(avg_epoch_loss)
                print(f"Epoch {epoch+1}, Avg Loss: {avg_epoch_loss:.9f}")
    
                # **Early Stopping Logic**
                if abs(avg_epoch_loss - prev_loss) &lt; 1e-8:
                        break
                prev_loss = avg_epoch_loss
    
                # **Early Stopping Logic**
                # if avg_epoch_loss &lt; best_loss:
                #     best_loss = avg_epoch_loss
                #     epochs_no_improve = 0  # Reset counter
                # else:
                #     epochs_no_improve += 1
                #     if epochs_no_improve &gt;= patience:
                #         print(f"Early stopping triggered at epoch {epoch+1}, best loss: {best_loss:.6f}")
                #         break
            
                # if len(loss_history) &gt;= window_size:
                #     # Compute moving averages using last `window_size` values
                #     moving_avg_loss_t = np.mean(loss_history[-window_size:])  # Current window
                #     moving_avg_loss_t_1 = np.mean(loss_history[-window_size - 1: -1])  # Previous window
            
                #     print(f"Epoch {epoch+1}, Avg Loss: {avg_epoch_loss:.6f}, Moving Avg Loss: {moving_avg_loss_t:.9f}")
            
                #     # Check stopping condition
                #     if abs(moving_avg_loss_t - moving_avg_loss_t_1) &lt; epsilon:
                #         break
                        
            result.append(np.array(theta_history))
            end_time = time.time()  # End timing
            exec_time = end_time - start_time  # Compute execution time
            print(f"Execution Time for Batch Size {batch_size}: {exec_time:.4f} seconds")
        
        return result
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target values.
        """
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term
        result = []
        for theta in self.theta:
            result.append(np.array(X @ theta))
        return result
        pass

def main():
    """
    Main function to generate data, train the model, and evaluate performance.
    """
    # Given parameters
    N = 1000000  # Number of data points
    theta_true = np.array([3, 1, 2])  # True parameters
    input_mean = np.array([3, -1])  # Means of x1, x2
    input_sigma = np.array([2, 2])  # Standard deviations of x1, x2
    noise_sigma = np.sqrt(2)  # Noise variance

    # Generate dataset
    X, y = generate(N, theta_true, input_mean, input_sigma, noise_sigma)

    # Train-test split (80% train, 20% test)
    split_idx = int(0.8 * N)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Train model with different batch sizes
    theta_history = []
    model = StochasticLinearRegressor()
    all_batch_size_theta = model.fit(X_train, y_train, learning_rate=0.001)
    print("all batch size theta hist len", len(all_batch_size_theta))
    
    # Evaluate on test data
    all_batch_y_pred_test = model.predict(X_test)
    all_batch_y_pred_train = model.predict(X_train)
    for i in range(4):
        if(i==0): batch_size=1
        elif(i==1): batch_size=80
        elif(i==2): batch_size=8000
        elif(i==3): batch_size=800000
        train_mse = np.mean((y_train - all_batch_y_pred_train[i]) ** 2)
        test_mse = np.mean((y_test - all_batch_y_pred_test[i]) ** 2)
        print(f"Train MSE with batch size {batch_size}: {train_mse:.6f}")
        print(f"Test MSE with batch size {batch_size}: {test_mse:.6f}")
        print(f"Learned parameters: {model.theta[i]}")

    # Compute closed-form solution
    theta_closed_form = closed_form_solution(X_train, y_train)
    print("Closed-form solution:", theta_closed_form)

    # # Compute MSE for training and test sets
    # train_mse_sgd = compute_mse(X_train, y_train, theta_history[-1])  # Last updated theta from SGD
    # test_mse_sgd = compute_mse(X_test, y_test, theta_history[-1])
    
    train_mse_closed_form = compute_mse(X_train, y_train, theta_closed_form)
    test_mse_closed_form = compute_mse(X_test, y_test, theta_closed_form)
    
    # Print results
    print(f"Closed-Form Training MSE: {train_mse_closed_form:.6f}")
    print(f"Closed-Form Test MSE: {test_mse_closed_form:.6f}")

    for i in range(4):
        c=0
        theta = all_batch_size_theta[i]
        new_theta = theta
        if(len(theta)&gt;40000):
            new_theta = []
            for i in range(len(theta)):
                c = c+1
                if c%100 == 0:
                    new_theta.append(theta[i])
                   
    
        # Plot the movement of θ
        plot_theta_updates(new_theta)

if __name__ == "__main__":
    main()



# Imports - you can add any other permitted libraries
import numpy as np
import matplotlib.pyplot as plt


# You may add any other functions to make your code more modular. However,
# do not change the function signatures (name and arguments) of the given functions,
# as these functions will be called by the autograder.

def sigmoid(z):
        """Compute the sigmoid function."""
        return 1 / (1 + np.exp(-z))
    
def normalize_features(X):
    """Normalize the feature matrix (zero mean, unit variance)."""
    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)

def plot_decision_boundary(model, X, y):
    """
    Plot the training data with decision boundary.

    Parameters
    ----------
    model : LogisticRegressor
        The trained logistic regression model.
    
    X : numpy array of shape (n_samples, n_features)
        The input data.
    
    y : numpy array of shape (n_samples,)
        The target labels (0 or 1).
    """
    # Normalize features (since model uses normalized data)
    X_norm = normalize_features(X)

    # Scatter plot for data points
    plt.figure(figsize=(8, 6))
    plt.scatter(X_norm[y == 0, 0], X_norm[y == 0, 1], label="Class 0", marker='o', edgecolors='k')
    plt.scatter(X_norm[y == 1, 0], X_norm[y == 1, 1], label="Class 1", marker='x', edgecolors='k')

    # Extract θ values
    theta = model.theta
    if theta is None:
<A NAME="1"></A><FONT color = #00FF00><A HREF="match78-1.html#1" TARGET="1"><IMG SRC="../../../bitmaps/tm_1_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

        raise ValueError("Model has not been trained. Run fit() before plotting.")

    # Decision boundary equation: θ0 + θ1*x1 + θ2*x2 = 0
    x1_vals = np.linspace(np.min(X_norm[:, 0]), np.max(X_norm[:, 0]), 100)
    x2_vals = - (theta[0] + theta[1] * x1_vals) / theta[2]  # Solve for x2

    # Plot decision boundary
    plt.plot(x1_vals, x2_vals, 'r-', label="Decision Boundary")
</FONT>
    # Labels and legend
    plt.xlabel("x1 (normalized)")
    plt.ylabel("x2 (normalized)")
    plt.title("Logistic Regression Decision Boundary")
    plt.legend()
    plt.show()

class LogisticRegressor:
    # Assume Binary Classification
    def __init__(self):
        self.theta = None  # Initialize theta as None
        pass
    
    def fit(self, X, y, learning_rate=0.01):
        """
        Fit the linear regression model to the data using Newton's Method.
        Remember to normalize the input data X before fitting the model.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        y : numpy array of shape (n_samples,)
            The target labels - 0 or 1.
        
        learning_rate : float
            The learning rate to use in the update rule.
        
        Returns
        -------
        List of Parameters: numpy array of shape (n_iter, n_features+1,)
            The list of parameters obtained after each iteration of Newton's Method.
        """

        m, n = X.shape
        X = normalize_features(X)  # Normalize features
        X = np.c_[np.ones(m), X]  # Add intercept term
        self.theta = np.zeros(n + 1)  # Initialize theta with zeros
        theta_history = []  # Store theta values for debugging
        tol=1e-7
        max_iter=100
        learning_rate=0.1
        
        for _ in range(490):
<A NAME="7"></A><FONT color = #0000FF><A HREF="match78-1.html#7" TARGET="1"><IMG SRC="../../../bitmaps/tm_2_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            h = sigmoid(X @ self.theta)  # Compute hypothesis
            gradient = X.T @ (y - h)  # Compute gradient
            R = np.diag(h * (1 - h))  # Compute Hessian diagonal elements
            H = -(X.T @ R @ X)  # Compute Hessian matrix
</FONT>            
            try:
                delta_theta = np.linalg.inv(H) @ gradient  # Newton's update
            except np.linalg.LinAlgError:
                print("Hessian is singular, stopping optimization.")
                break
            
            self.theta -= learning_rate*delta_theta  # Update parameters
            theta_history.append(self.theta.copy())  # Store current theta
            
            # Check convergence
            if np.linalg.norm(delta_theta) &lt; tol:
                break
        
        return np.array(theta_history)
        pass
    
    def predict(self, X):
        """
        Predict the target values for the input data.
        
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input data.
            
        Returns
        -------
        y_pred : numpy array of shape (n_samples,)
            The predicted target label.
        """

        X = normalize_features(X)  # Normalize input
        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term
        return (sigmoid(X @ self.theta) &gt;= 0.5).astype(int)  # Threshold at 0.5
        pass

def main():
    """
    Main function to load data, train logistic regression using Newton's Method,
    and display learned parameters.
    """
    # Load the dataset
<A NAME="4"></A><FONT color = #FF00FF><A HREF="match78-1.html#4" TARGET="1"><IMG SRC="../../../bitmaps/tm_4_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    X = np.loadtxt("../data/Q3/logisticX.csv", delimiter=",")  # Features (n_samples, n_features)
    y = np.loadtxt("../data/Q3/logisticY.csv", delimiter=",")  # Labels (n_samples,)

    # Initialize and train the model
    model = LogisticRegressor()
    theta_history = model.fit(X, y)  # Train using Newton's Method
</FONT>    result = model.predict(X)
    print(result)
    print(np.array(list(map(int, y))))

    # Print the final learned parameters
    print("Final learned parameters (θ):", model.theta)

    # Plot decision boundary
    plot_decision_boundary(model, X, y)

if __name__ == "__main__":
    main()



import numpy as np
import matplotlib.pyplot as plt

class GaussianDiscriminantAnalysis:
    def __init__(self):
        self.mu_0 = None
        self.mu_1 = None
        self.sigma = None
        self.sigma_0 = None
        self.sigma_1 = None
        self.phi = None
        self.assume_same_covariance = None

    def fit(self, X, y, assume_same_covariance=False):
        """
        Fit the Gaussian Discriminant Analysis model to the data.
        """
        self.assume_same_covariance = assume_same_covariance
        n = X.shape[0]

        # Compute priors
        self.phi = np.mean(y)  # P(y=1)

        # Compute class means
        m0 = np.sum(y == 0)
        m1 = np.sum(y == 1)

        self.mu_0 = np.sum(X[y == 0], axis=0) / m0
        self.mu_1 = np.sum(X[y == 1], axis=0) / m1

        if assume_same_covariance:
            # Compute the shared covariance matrix
            sigma = np.zeros((X.shape[1], X.shape[1]))

            for i in range(n):
                xi = X[i].reshape(-1, 1)
                mui = self.mu_0 if y[i] == 0 else self.mu_1
                mui = mui.reshape(-1, 1)
                sigma += (xi - mui) @ (xi - mui).T

            self.sigma = sigma / n
            return self.mu_0, self.mu_1, self.sigma

        else:
            # Compute separate covariance matrices
            sigma_0 = np.zeros((X.shape[1], X.shape[1]))
            sigma_1 = np.zeros((X.shape[1], X.shape[1]))

            for i in range(n):
                xi = X[i].reshape(-1, 1)
                if y[i] == 0:
                    mui = self.mu_0.reshape(-1, 1)
                    sigma_0 += (xi - mui) @ (xi - mui).T
                else:
                    mui = self.mu_1.reshape(-1, 1)
                    sigma_1 += (xi - mui) @ (xi - mui).T

            self.sigma_0 = sigma_0 / m0
            self.sigma_1 = sigma_1 / m1
            return self.mu_0, self.mu_1, self.sigma_0, self.sigma_1

    def predict(self, X):
        """
        Predict target values.
        """
        if self.assume_same_covariance:
            sigma_inv = np.linalg.inv(self.sigma)
<A NAME="0"></A><FONT color = #FF0000><A HREF="match78-1.html#0" TARGET="1"><IMG SRC="../../../bitmaps/tm_0_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

            w = np.dot(sigma_inv, (self.mu_1 - self.mu_0))
            c = -0.5 * (self.mu_1.T @ sigma_inv @ self.mu_1) + 0.5 * (self.mu_0.T @ sigma_inv @ self.mu_0) + np.log(self.phi / (1 - self.phi))
            linear_combination = np.dot(X, w) + c
</FONT>            return (linear_combination &gt;= 0).astype(int)
        else:
            sigma_0_inv = np.linalg.inv(self.sigma_0)
            sigma_1_inv = np.linalg.inv(self.sigma_1)

            def gda_quadratic_boundary(x):
                term_0 = np.einsum('ij,ij-&gt;i', np.dot(x - self.mu_0, sigma_0_inv), (x - self.mu_0))
                term_1 = np.einsum('ij,ij-&gt;i', np.dot(x - self.mu_1, sigma_1_inv), (x - self.mu_1))
                log_det_diff = np.log(np.linalg.det(self.sigma_0)) - np.log(np.linalg.det(self.sigma_1))  # Swap order
                return term_1 - term_0 + log_det_diff - 2 * np.log(self.phi / (1 - self.phi))  # Swap order & sign

            # def gda_quadratic_boundary(x):
            #     # term_0 = np.einsum('ij,ij-&gt;i', np.dot(x - self.mu_0, sigma_0_inv), (x - self.mu_0))
            #     # term_1 = np.einsum('ij,ij-&gt;i', np.dot(x - self.mu_1, sigma_1_inv), (x - self.mu_1))
            #     # log_det_diff = np.log(np.linalg.det(self.sigma_1)) - np.log(np.linalg.det(self.sigma_0))
            #     # return term_0 - term_1 + log_det_diff + 2 * np.log(self.phi / (1 - self.phi))
            #     W = 0.5 * (sigma_0_inv - sigma_1_inv)  # Quadratic coefficients
            #     w = np.dot(sigma_1_inv, self.mu_1) - np.dot(sigma_0_inv, self.mu_0)  # Linear coefficients
            #     w0 = (
            #         -0.5 * self.mu_1.T @ sigma_1_inv @ self.mu_1
            #         + 0.5 * self.mu_0.T @ sigma_0_inv @ self.mu_0
            #         + 0.5 * np.log(np.linalg.det(self.sigma_1) / np.linalg.det(self.sigma_0))
            #         + np.log(self.phi / (1 - self.phi))
            #     )
            
            #     a, b, c = W[0, 0], W[0, 1] + W[1, 0], W[1, 1]
            #     d, e = w[0], w[1]
            #     f = w0
            #     return (a * x[:, 0] ** 2 + b * x[:, 0] * x[:, 1] + c * x[:, 1] ** 2
            #             + d * x[:, 0] + e * x[:, 1] + f)

            return (gda_quadratic_boundary(X) &lt; 0).astype(int)


def plot_decision_boundary(model_linear, model_quadratic, X, y):
    """
    Plot the training data along with both decision boundaries (linear & quadratic) and display their equations.
    """
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', label="Canada", edgecolors='k')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker='x', label="Alaska", edgecolors='k')

    x1_vals = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 100)
<A NAME="3"></A><FONT color = #00FFFF><A HREF="match78-1.html#3" TARGET="1"><IMG SRC="../../../bitmaps/tm_3_0.gif" ALT="other" BORDER="0" ALIGN=left></A>

    x2_vals = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), 100)
    X1, X2 = np.meshgrid(x1_vals, x2_vals)
    grid_points = np.c_[X1.ravel(), X2.ravel()]

    # --- Compute & Plot Linear Decision Boundary ---
    sigma_inv = np.linalg.inv(model_linear.sigma)
</FONT>    w = np.dot(sigma_inv, (model_linear.mu_1 - model_linear.mu_0))
    c = -0.5 * (model_linear.mu_1.T @ sigma_inv @ model_linear.mu_1) + 0.5 * (model_linear.mu_0.T @ sigma_inv @ model_linear.mu_0) + np.log(model_linear.phi / (1 - model_linear.phi))
    x2_boundary = (-w[0] * x1_vals - c) / w[1]
    plt.plot(x1_vals, x2_boundary, 'r-', label="Linear Decision Boundary")

    # Equation for the linear decision boundary
    equation_linear = f"{w[0]:.2f} * x1 + {w[1]:.2f} * x2 + {c:.2f} = 0"
    print(equation_linear)
    plt.text(np.min(X[:, 0]), np.max(X[:, 1]), equation_linear, color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))

    # --- Compute & Plot Quadratic Decision Boundary ---
    sigma_0_inv = np.linalg.inv(model_quadratic.sigma_0)
    sigma_1_inv = np.linalg.inv(model_quadratic.sigma_1)

    W = 0.5 * (sigma_0_inv - sigma_1_inv)  # Quadratic coefficients
    w = np.dot(sigma_1_inv, model_quadratic.mu_1) - np.dot(sigma_0_inv, model_quadratic.mu_0)  # Linear coefficients
    w0 = (
        -0.5 * model_quadratic.mu_1.T @ sigma_1_inv @ model_quadratic.mu_1
        + 0.5 * model_quadratic.mu_0.T @ sigma_0_inv @ model_quadratic.mu_0
        + 0.5 * np.log(np.linalg.det(model_quadratic.sigma_1) / np.linalg.det(model_quadratic.sigma_0))
        + np.log(model_quadratic.phi / (1 - model_quadratic.phi))
    )

    a, b, c = W[0, 0], W[0, 1] + W[1, 0], W[1, 1]
    d, e = w[0], w[1]
    f = w0

    # def gda_quadratic_boundary(x):
    #     return (
    #         a * x[:, 0] ** 2 + b * x[:, 0] * x[:, 1] + c * x[:, 1] ** 2
    #         + d * x[:, 0] + e * x[:, 1] + f
    #     )
    def gda_quadratic_boundary(x):
                term_0 = np.einsum('ij,ij-&gt;i', np.dot(x - model_quadratic.mu_0, sigma_0_inv), (x - model_quadratic.mu_0))
                term_1 = np.einsum('ij,ij-&gt;i', np.dot(x - model_quadratic.mu_1, sigma_1_inv), (x - model_quadratic.mu_1))
                log_det_diff = np.log(np.linalg.det(model_quadratic.sigma_0)) - np.log(np.linalg.det(model_quadratic.sigma_1))  # Swap order
                return term_1 - term_0 + log_det_diff - 2 * np.log(model_quadratic.phi / (1 - model_quadratic.phi))  # Swap order & sign

    decision_values = gda_quadratic_boundary(grid_points).reshape(X1.shape)
    plt.contour(X1, X2, decision_values, levels=[0], colors='blue', linewidths=2, linestyles='dashed', label="Quadratic Decision Boundary")

    # Display the quadratic decision boundary equation
    equation_quadratic = f"{a:.5f} * x1^2 + {b:.5f} * x1 * x2 + {c:.5f} * x2^2 + {d:.5f} * x1 + {e:.5f} * x2 + {f:.5f} = 0"
    print(f"a = {a:.10f}, b = {b:.10f}, c = {c:.10f}, d = {d:.10f}, e = {e:.10f}, f = {f:.10f}")

    plt.text(np.min(X[:, 0]), np.min(X[:, 1]), equation_quadratic, color='blue', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))

    plt.legend()
    plt.title("Decision Boundaries (Linear & Quadratic)")
    plt.xlabel("Feature X1")
    plt.ylabel("Feature X2")
    plt.show()

def normalize_data(X):
    """Manually normalizes the data to have zero mean and unit variance."""
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    X_normalized = (X - mean) / std
    return X_normalized

def main():
    X = np.loadtxt("./../data/q4/q4x.dat")
    y = np.loadtxt("./../data/q4/q4y.dat", dtype=str)
    y = np.where(y == "Canada", 0, 1)

    X = normalize_data(X)

    model_linear = GaussianDiscriminantAnalysis()
    result_fit_linear = model_linear.fit(X, y, assume_same_covariance=True)
    #print(type(result_fit_linear[0]))
    print("mu_0: ", model_linear.mu_0, "\nmu_1: ", model_linear.mu_1, "\nsigma: ", model_linear.sigma)

    model_quadratic = GaussianDiscriminantAnalysis()
    result_fit_quadratic = model_quadratic.fit(X, y, assume_same_covariance=False)
    #print(type(result_fit_quadratic))
    print("mu_0: ", model_quadratic.mu_0, "\nmu_1: ", model_quadratic.mu_1, "\nsigma_0: ", model_quadratic.sigma_0, "\nsigma_1: ", model_quadratic.sigma_1)

    
    plot_decision_boundary(model_linear, model_quadratic, X, y)
    print(type(model_quadratic.predict(X)))
    print(type(model_linear.predict(X)))
    print(model_quadratic.predict(X))
    print(model_linear.predict(X))
    print(y)
    

if __name__ == "__main__":
    main()


</PRE>
</PRE>
</BODY>
</HTML>
